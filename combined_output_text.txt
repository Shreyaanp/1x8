


TABLE OF CONTENTS 
3.7.1 
Conversion of an NFA to a DFA . . . . . . . . . . . . . .  
152 
3.7.2 
Simulation of an NFA . . . . . . . . . . . . . . . . . . . .  
156 
3.7.3 Efficiency of NFA Simulation . . . . . . . . . . . . . . . .  
157 
3.7.4 
Construction of an NFA from a Regular Expression . . .  
159 
3.7.5 Efficiency of String-Processing Algorithms . . . . . . . . .  
163 
3.7.6 
Exercises for Section 3.7 . . . . . . . . . . . . . . . . . . .  
166 
3.8 Design of a Lexical-Analyzer Generator . . . . . . . . . . . . . .  
166 
3.8.1 
The Structure of the Generated Analyzer . . . . . . . . .  
167 
3.8.2 
Pattern Matching Based on NFA's . . . . . . . . . . . . .  
168 
3.8.3 
DFA's for Lexical Analyzers . . . . . . . . . . . . . . . . .  
170 
3.8.4 
Implementing the Lookahead Operator . . . . . . . . . . .  
171 
3.8.5 Exercises for Section 3.8 . . . . . . . . . . . . . . . . . . .  
172 
3.9 Optimization of DFA-Based Pattern Matchers . . . . . . . . . . .  
173 
3.9.1 Important States of an NFA . . . . . . . . . . . . . . . . .  
173 
3.9.2 
Functions Computed From the Syntax Tree . . . . . . . .  
175 
3.9.3 Computing nullable, firstpos, and lastpos . . . . . . . . . .  
176 
. . . . . . . . . . . . . . . . . . . . .  
3.9.4 
Computing followpos 
177 
. . .  
3.9.5 
Converting a Regular Expression Directly to a DFA 
179 
3.9.6 
Minimizing the Number of States of a DFA . . . . . . . .  
180 
. . . . . . . . . .  
3.9.7 
State Minimization in Lexical Analyzers 
184 
. . . . . . . .  
3.9.8 
Trading Time for Space in DFA Simulation 
185 
. . . . . . . . . . . . . . . . . . .  
3.9.9 
Exercises for Section 3.9 
186 
. . . . . . . . . . . . . . . . . . . . . . . .  
3.10 Summary of Chapter 3 
187 
. . . . . . . . . . . . . . . . . . . . . . .  
3.11 References for Chapter 3 
189 
4 Syntax Analysis 
191 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
4.1 Introduction 
192 
. . . . . . . . . . . . . . . . . . . .  
4.1.1 The Role of the Parser 
192 
. . . . . . . . . . . . . . . . . .  
4.1.2 
Representative Grammars 
193 
. . . . . . . . . . . . . . . . . . . .  
4.1.3 
Syntax Error Handling 
194 
. . . . . . . . . . . . . . . . . .  
4.1.4 
Error-Recovery Strategies 
195 
. . . . . . . . . . . . . . . . . . . . . . .  
4.2 Context-Free Grammars 
197 
4.2.1 
The Formal Definition of a Context-Free Grammar . . . .  
197 
. . . . . . . . . . . . . . . . . . .  
4.2.2 
Notational Conventions 
198 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
4.2.3 Derivations 
199 
. . . . . . . . . . . . . . . . .  
4.2.4 
Parse Trees and Derivations 
201 
. . . . . . . . . . . . . . . . . . . . . . . . . . .  
4.2.5 
Ambiguity 
203 
. . . .  
4.2.6 
Verifying the Language Generated by a Grammar 
204 
. . .  
4.2.7 
Context-Free Grammars Versus Regular Expressions 
205 
. . . . . . . . . . . . . . . . . . .  
4.2.8 
Exercises for Section 4.2 
206 
. . . . . . . . . . . . . . . . . . . . . . . . .  
4.3 Writing a Grammar 
209 
. . . . . . . . . . . . . .  
4.3.1 
Lexical Versus Syntactic Analysis 
209 
. . . . . . . . . . . . . . . . . . . .  
4.3.2 
Eliminating Ambiguity 
210 
. . . . . . . . . . . . . . . .  
4.3.3 Elimination of Left Recursion 
212 
. . . . . . . . . . . . . . . . . . . . . . . .  
4.3.4 
Left Factoring 
214 
... 
TABLE OF CONTENTS 
xlll 
4.3.5 
Non-Context-Free Language Constructs . . . . . . . . . .  
215 
. . . . . . . . . . . . . . . . . . .  
4.3.6 
Exercises for Section 4.3 
216 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
4.4 Top-Down Parsing 
217 
. . . . . . . . . . . . . . . . . .  
4.4.1 
Recursive-Descent Parsing 
219 
. . . . . . . . . . . . . . . . . . . .  
4.4.2 
FIRST and FOLLOW 
220 
. . . . . . . . . . . . . . . . . . . . . . .  
4.4.3 LL(1) Grammars 
222 
. . . . . . . . . . . . . . .  
4.4.4 
Nonrecursive Predictive Parsing 
226 
. . . . . . . . . . . .  
4.4.5 Error Recovery in Predictive Parsing 
228 
. . . . . . . . . . . . . . . . . . .  
4.4.6 
Exercises for Section 4.4 
231 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
4.5 Bottom-Up Parsing 
233 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
4.5.1 
Reductions 
234 
. . . . . . . . . . . . . . . . . . . . . . . .  
4.5.2 
Handle Pruning 
235 
. . . . . . . . . . . . . . . . . . . . .  
4.5.3 
Shift-Reduce Parsing 
236 
. . . . . . . . . . .  
4.5.4 
Conflicts During Shift-Reduce Parsing 
238 
. . . . . . . . . . . . . . . . . . .  
4.5.5 
Exercises for Section 4.5 
240 
. . . . . . . . . . . . . .  
4.6 Introduction to LR Parsing: Simple LR 
241 
. . . . . . . . . . . . . . . . . . . . . . .  
4.6.1 
Why LR Parsers? 
241 
. . . . . . . . . . . . . .  
4.6.2 
Items and the LR(0) Automaton 
242 
. . . . . . . . . . . . . . . . .  
4.6.3 
The LR-Parsing Algorithm 
248 
. . . . . . . . . . . . . .  
4.6.4 
Constructing SLR-Parsing Tables 
252 
4.6.5 
Viable Prefixes . . . . . . . . . . . . . . . . . . . . . . . .  
256 
4.6.6 
Exercisesfor Section 4.6 . . . . . . . . . . . . . . . . . . .  
257 
4.7 More Powerful LR Parsers . . . . . . . . . . . . . . . . . . . . . .  
259 
4.7.1 
Canonical LR(1) Items . . . . . . . . . . . . . . . . . . . .  
260 
4.7.2 
Constructing LR(1) Sets of Items . . . . . . . . . . . . . .  
261 
4.7.3 
Canonical LR(1) Parsing Tables . . . . . . . . . . . . . .  
265 
4.7.4 
Constructing LALR Parsing Tables . . . . . . . . . . . . .  
266 
4.7.5 
Efficient Construction of LALR Parsing Tables . . . . . .  
270 
4.7.6 
Compaction of LR Parsing Tables . . . . . . . . . . . . .  
275 
4.7.7 Exercises for Section 4.7 . . . . . . . . . . . . . . . . . . .  
277 
4.8 Using Ambiguous Grammars . . . . . . . . . . . . . . . . . . . .  
278 
4.8.1 
Precedence and Associativity to Resolve Conflicts . . . .  
279 
4.8.2 
The "Dangling-Else" Ambiguity . . . . . . . . . . . . . .  
281 
4.8.3 Error Recovery in LR Parsing . . . . . . . . . . . . . . . .  
283 
4.8.4 
Exercises for Section 4.8 . . . . . . . . . . . . . . . . . . .  
285 
4.9 Parser Generators . . . . . . . . . . . . . . . . . . . . . . . . . .  
287 
4.9.1 
The Parser Generator Yacc . . . . . . . . . . . . . . . . .  
287 
4.9.2 
Using Yacc 
with Ambiguous Grammars . . . . . . . . . .  
291 
4.9.3 
Creating Yacc Lexical Analyzers with Lex . . . . . . . . .  
294 
4.9.4 
Error Recovery in Yacc . . . . . . . . . . . . . . . . . . .  
295 
4.9.5 
Exercises for Section 4.9 . . . . . . . . . . . . . . . . . . .  
297 
4.10 Summary of Chapter 4 . . . . . . . . . . . . . . . . . . . . . . . .  
297 
4.11 References for Chapter 4 . . . . . . . . . . . . . . . . . . . . . . .  
300 
xiv 
TABLE OF CONTENTS 
5 Syntax-Directed Translation 
303 
5.1 Syntax-Directed Definitions . . . . . . . . . . . . . . . . . . . . .  
304 
5.1.1 Inherited and Synthesized Attributes . . . . . . . . . . . .  
304 
5.1.2 
Evaluating an SDD at the Nodes of a Parse Tree . . . . .  
306 
5.1.3 
Exercises for Section 5.1 . . . . . . . . . . . . . . . . . . .  
309 
5.2 Evaluation Orders for SDD's . . . . . . . . . . . . . . . . . . . .  
310 
5.2.1 
Dependency Graphs . . . . . . . . . . . . . . . . . . . . .  
310 
5.2.2 
Ordering the Evaluation of Attributes . . . . . . . . . . .  
312 
5.2.3 S-Attributed Definitions . . . . . . . . . . . . . . . . . . .  
312 
5.2.4 
L-Attributed Definitions . . . . . . . . . . . . . . . . . . .  
313 
5.2.5 
Semantic Rules with Controlled Side Effects . . . . . . . .  
314 
5.2.6 
Exercises for Section 5.2 . . . . . . . . . . . . . . . . . . .  
317 
5.3 Applications of Synt 
ax-Directed Translation . . . . . . . . . . . .  
318 
5.3.1 
Construction of Syntax Trees . . . . . . . . . . . . . . . .  
318 
. . . . . . . . . . . . . . . . . . .  
5.3.2 
The Structure of a Type 
321 
. . . . . . . . . . . . . . . . . . .  
5.3.3 Exercises for Section 5.3 
323 
. . . . . . . . . . . . . . . .  
5.4 Syntax-Directed Translation Schemes 
324 
. . . . . . . . . . . . . . . . .  
5.4.1 
Postfix Translation Schemes 
324 
. . . . . .  
5.4.2 
Parser-Stack Implementation of Postfix SDT's 
325 
. . . . . . . . . .  
5.4.3 SDT's With Actions Inside Productions 
327 
. . . . . . . . . .  
5.4.4 
Eliminating Left Recursion From SDT's 
328 
. . . . . . . . . . . . .  
5.4.5 
SDT's for L-Attributed Definitions 
331 
. . . . . . . . . . . . . . . . . . .  
5.4.6 
Exercises for Section 5.4 
336 
. . . . . . . . . . . . . . . . .  
5.5 Implementing L- 
Attributed SDD's 
337 
. . . . . .  
5.5.1 
Translation During Recursive-Descent Parsing 
338 
. . . . . . . . . . . . . . . .  
5.5.2 
On-The-Fly Code Generation 
340 
. . . . . . . . . . . .  
5.5.3 
L-Attributed SDD's and LL Parsing 
343 
. . . . . . . .  
5.5.4 
Bottom-Up Parsing of L-Attributed SDD's 
348 
. . . . . . . . . . . . . . . . . . .  
5.5.5 
Exercises for Section 5.5 
352 
. . . . . . . . . . . . . . . . . . . . . . . .  
5.6 Summary of Chapter 5 
353 
. . . . . . . . . . . . . . . . . . . . . . .  
5.7 References for Chapter 5 
354 
6 Intermediate-Code Generation 
357 
. . . . . . . . . . . . . . . . . . . . . . .  
6.1 Variants of Syntax Trees 
358 
. . . . . . . . . .  
6.1.1 
Directed Acyclic Graphs for Expressions 
359 
6.1.2 
The Value-Number Method for Constructing DAG's . . .  
360 
. . . . . . . . . . . . . . . . . . .  
6.1.3 Exercises for Section 6.1 
362 
. . . . . . . . . . . . . . . . . . . . . . . . .  
6.2 Three-Address Code 
363 
. . . . . . . . . . . . . . . . .  
6.2.1 
Addresses and Instructions 
364 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
6.2.2 
Quadruples 
366 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.2.3 
Triples 
367 
. . . . . . . . . . . . . . .  
6.2.4 
Static Single- 
Assignment Form 
369 
. . . . . . . . . . . . . . . . . . .  
6.2.5 
Exercises for Section 6.2 
370 
. . . . . . . . . . . . . . . . . . . . . . .  
6.3 Types and Declarations 
370 
. . . . . . . . . . . . . . . . . . . . . . .  
6.3.1 
Type Expressions 
371 
TABLE OF CONTENTS 
xv 
. . . . . . . . . . . . . . . . . . . . . . .  
6.3.2 
Type Equivalence 
372 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
6.3.3 
Declarations 
373 
. . . . . . . . . . . . . .  
6.3.4 
Storage Layout for Local Names 
373 
. . . . . . . . . . . . . . . . . .  
6.3.5 
Sequences of Declarations 
376 
. . . . . . . . . . . . . . . .  
6.3.6 
Fields in Records and Classes 
376 
. . . . . . . . . . . . . . . . . . .  
6.3.7 
Exercises for Section 6.3 
378 
. . . . . . . . . . . . . . . . . . . . . .  
6.4 Translation of Expressions 
378 
. . . . . . . . . . . . . . .  
6.4.1 
Operations Within Expressions 
378 
. . . . . . . . . . . . . . . . . . .  
6.4.2 
Incremental Translation 
380 
. . . . . . . . . . . . . . . . .  
6.4.3 Addressing Array Elements 
381 
. . . . . . . . . . . . . . .  
6.4.4 
Translation of Array References 
383 
. . . . . . . . . . . . . . . . . . .  
6.4.5 
Exercises for Section 6.4 
384 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.5 Type Checking 
386 
. . . . . . . . . . . . . . . . . . .  
6.5.1 
Rules for Type Checking 
387 
. . . . . . . . . . . . . . . . . . . . . .  
6.5.2 
Type Conversions 
388 
. . . . . . . . . .  
6.5.3 
Overloading of Functions and Operators 
390 
. . . . . . . .  
6.5.4 
Type Inference and Polymorphic Functions 
391 
. . . . . . . . . . . . . . . .  
6.5.5 
An Algorithm for Unification 
395 
. . . . . . . . . . . . . . . . . . .  
6.5.6 
Exercises for Section 6.5 
398 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.6 Control Flow 
399 
. . . . . . . . . . . . . . . . . . . . .  
6.6.1 
Boolean Expressions 
399 
. . . . . . . . . . . . . . . . . . . . . .  
6.6.2 
Short-circuit Code 
400 
. . . . . . . . . . . . . . . . .  
6.6.3 
Flow-of- 
Control Statements 
401 
6.6.4 
Control-Flow Translation of Boolean Expressions . . . . .  
403 
6.6.5 
Avoiding Redundant Gotos . . . . . . . . . . . . . . . . .  
405 
6.6.6 
Boolean Values and Jumping Code . . . . . . . . . . . . .  
408 
6.6.7 
Exercises for Section 6.6 . . . . . . . . . . . . . . . . . . .  
408 
6.7 Backpatching . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
410 
6.7.1 
One-Pass Code Generation Using Backpatching . . . . . .  
410 
6.7.2 
Backpatching for Boolean Expressions . . . . . . . . . . .  
411 
6.7.3 Flow-of-Control Statements . . . . . . . . . . . . . . . . .  
413 
6.7.4 
Break-, Continue-, and Goto-Statements . . . . . . . . . .  
416 
6.7.5 
Exercises for Section 6.7 . . . . . . . . . . . . . . . . . . .  
417 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
6.8 Switch-Statements 
418 
6.8.1 
Translationof Switch-Statements . . . . . . . . . . . . . .  
419 
6.8.2 
Syntax-Directed Translation of Switch-Statements . . . .  
420 
6.8.3 
Exercises for Section 6.8 . . . . . . . . . . . . . . . . . . .  
421 
6.9 Intermediate Code for Procedures . . . . . . . . . . . . . . . . . .  
422 
6.10 Summary of Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . .  
424 
6.11 References for Chapter 6 . . . . . . . . . . . . . . . . . . . . . . .  
425 
xvi 
TABLE OF CONTENTS 
7 Run-Time Environments 
427 
. . . . . . . . . . . . . . . . . . . . . . . . .  
7.1 Storage Organization 
427 
7.1.1 
Static Versus Dynamic Storage Allocation . . . . . . . . .  
429 
. . . . . . . . . . . . . . . . . . . . . .  
7.2 Stack Allocation of Space 
430 
7.2.1 Activation Trees . . . . . . . . . . . . . . . . . . . . . . .  
430 
7.2.2 
Activation Records . . . . . . . . . . . . . . . . . . . . . .  
433 
7.2.3 
Calling Sequences . . . . . . . . . . . . . . . . . . . . . .  
436 
7.2.4 Variable-Length Data on the Stack . . . . . . . . . . . . .  
438 
7.2.5 Exercises for Section 7.2 . . . . . . . . . . . . . . . . . . .  
440 
. . . . . . . . . . . . . . .  
7.3 Access to Nonlocal Data on the Stack 
441 
. . . . . . . . . .  
7.3.1 
Data Access Without Nested Procedures 
442 
. . . . . . . . . . . . . . .  
7.3.2 
Issues With Nested Procedures 
442 
7.3.3 A Language With Nested Procedure Declarations . . . . .  
443 
. . . . . . . . . . . . . . . . . . . . . . . .  
7.3.4 
Nesting Depth 
443 
. . . . . . . . . . . . . . . . . . . . . . . . .  
7.3.5 
Access Links 
445 
. . . . . . . . . . . . . . . . .  
7.3.6 Manipulating Access Links 
447 
7.3.7 
Access Links for Procedure Parameters . . . . . . . . . .  
448 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .  
7.3.8 
Displays 
449 
. . . . . . . . . . . . . . . . . . .  
7.3.9 
Exercises for Section 7.3 
451 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
7.4 Heap Management 
452 
. . . . . . . . . . . . . . . . . . . .  
7.4.1 The Memory Manager 
453 
. . . . . . . . . . .  
7.4.2 The Memory Hierarchy of a Computer 
454 
. . . . . . . . . . . . . . . . . . . . .  
7.4.3 Locality in Programs 
455 
. . . . . . . . . . . . . . . . . . .  
7.4.4 Reducing Fragmentation 
457 
. . . . . . . . . . . . . . .  
7.4.5 
Manual Deallocation Requests 
460 
. . . . . . . . . . . . . . . . . . .  
7.4.6 
Exercises for Section 7.4 
463 
. . . . . . . . . . . . . . . . .  
7.5 Introduction to Garbage Collection 
463 
. . . . . . . . . . . .  
7.5.1 Design Goals for Garbage Collectors 
464 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
7.5.2 
Reachability 
466 
. . . . . . . . . .  
7.5.3 Reference Counting Garbage Collectors 
468 
. . . . . . . . . . . . . . . . . . .  
7.5.4 
Exercises for Section 7.5 
470 
. . . . . . . . . . . . . . .  
7.6 Introduction to Trace-Based Collection 
470 
. . . . . . . . . . . . .  
7.6.1 A Basic Mark-and-Sweep Collector 
471 
. . . . . . . . . . . . . . . . . . . . . .  
7.6.2 
Basic Abstraction 
473 
. . . . . . . . . . . . . . . .  
7.6.3 
Optimizing Mark-and-Sweep 
475 
. . . . . . . . . .  
7.6.4 
Mark-and-Compact Garbage Collectors 
476 
. . . . . . . . . . . . . . . . . . . . . .  
7.6.5 
Copying collectors 
478 
. . . . . . . . . . . . . . . . . . . . . . .  
7.6.6 
Comparing Costs 
482 
. . . . . . . . . . . . . . . . . . .  
7.6.7 Exercises for Section 7.6 
482 
. . . . . . . . . . . . . . . . . . .  
7.7 Short-Pause Garbage Collection 
483 
. . . . . . . . . . . . . . .  
7.7.1 Incremental Garbage Collection 
483 
. . . . . . . . . . . . .  
7.7.2 
Incremental Reachability Analysis 
485 
. . . . . . . . . . . . . . . . . . .  
7.7.3 Partial-Collection Basics 
487 
. . . . . . . . . . . . . .  
7.7.4 
Generational Garbage Collection 
488 
. . . . . . . . . . . . . . . . . . . . .  
7.7.5 
The Train Algorithm 
490 
TABLE OF CONTENTS 
xvii 
. . . . . . . . . . . . . . . . . . .  
7.7.6 
Exercises for Section 7.7 
493 
. . . . . . . . . . . . . .  
7.8 Advanced Topics in Garbage Collection 
494 
. . . . . . . .  
7.8.1 Parallel and Concurrent Garbage Collection 
495 
. . . . . . . . . . . . . . . . . .  
7.8.2 
Partial Object Relocation 
497 
. . . . . . .  
7.8.3 
Conservative Collection for Unsafe Languages 
498 
. . . . . . . . . . . . . . . . . . . . . . .  
7.8.4 Weak References 
498 
. . . . . . . . . . . . . . . . . . .  
7.8.5 Exercises for Section 7.8 
499 
. . . . . . . . . . . . . . . . . . . . . . . .  
7.9 Summary of Chapter 7 
500 
. . . . . . . . . . . . . . . . . . . . . . .  
7.10 References for Chapter 7 
502 
8 Code Generation 
505 
. . . . . . . . . . . . .  
8.1 Issues in the Design of a Code Generator 
506 
. . . . . . . . . . . . . . . .  
8.1.1 Input to the Code Generator 
507 
. . . . . . . . . . . . . . . . . . . . .  
8.1.2 
The Target Program 
507 
. . . . . . . . . . . . . . . . . . . . .  
8.1.3 
Instruction Selection 
508 
. . . . . . . . . . . . . . . . . . . . . .  
8.1.4 
Register Allocation 
510 
. . . . . . . . . . . . . . . . . . . . . . .  
8.1.5 Evaluation Order 
511 
. . . . . . . . . . . . . . . . . . . . . . . .  
8.2 The Target Language 
512 
. . . . . . . . . . . . . .  
8.2.1 A Simple Target Machine Model 
512 
. . . . . . . . . . . . . . .  
8.2.2 
Program and Instruction Costs 
515 
. . . . . . . . . . . . . . . . . . .  
8.2.3 Exercises for Section 8.2 
516 
. . . . . . . . . . . . . . . . . . . .  
8.3 Addresses in the Target Code 
518 
8.3.1 
Static Allocation . . . . . . . . . . . . . . . . . . . . . . .  
518 
8.3.2 
Stack Allocation . . . . . . . . . . . . . . . . . . . . . . .  
520 
8.3.3 Run-Time Addresses for Names . . . . . . . . . . . . . . .  
522 
8.3.4 Exercises for Section 8.3 . . . . . . . . . . . . . . . . . . .  
524 
8.4 Basic Blocks and Flow Graphs . . . . . . . . . . . . . . . . . . .  
525 
8.4.1 Basic Blocks . . . . . . . . . . . . . . . . . . . . . . . . .  
526 
8.4.2 Next-Use Information . . . . . . . . . . . . . . . . . . . .  
528 
8.4.3 Flow Graphs . . . . . . . . . . . . . . . . . . . . . . . . .  
529 
8.4.4 
Representation of Flow Graphs . . . . . . . . . . . . . . .  
530 
8.4.5 Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
531 
8.4.6 
Exercises for Section 8.4 . . . . . . . . . . . . . . . . . . .  
531 
8.5 Optimization of Basic Blocks . . . . . . . . . . . . . . . . . . . .  
533 
8.5.1 
The DAG Representation of Basic Blocks . . . . . . . . .  
533 
8.5.2 
Finding Local Common Subexpressions . . . . . . . . . .  
534 
8.5.3 Dead Code Elimination . . . . . . . . . . . . . . . . . . .  
535 
8.5.4 
The Use of Algebraic Identities . . . . . . . . . . . . . . .  
536 
8.5.5 Representation of Array References . . . . . . . . . . . . .  
537 
8.5.6 
Pointer Assignments and Procedure Calls . . . . . . . . .  
539 
8.5.7 
Reassembling Basic Blocks From DAG's . . . . . . . . . .  
539 
8.5.8 Exercises for Section 8.5 . . . . . . . . . . . . . . . . . . .  
541 
8.6 A Simple Code Generator . . . . . . . . . . . . . . . . . . . . . .  
542 
8.6.1 Register and Address Descriptors . . . . . . . . . . . . . .  
543 
8.6.2 
The Code-Generation Algorithm . . . . . . . . . . . . . .  
544 
xviii 
TABLE OF CONTENTS 
8.6.3 Design of the Function getReg . . . . . . . . . . . . . . . .  
547 
8.6.4 Exercises for Section 8.6 . . . . . . . . . . . . . . . . . . .  
548 
. . . . . . . . . . . . . . . . . . . . . . . .  
8.7 Peephole Optimization 
549 
8.7.1 Eliminating Redundant Loads and Stores . . . . . . . . .  
550 
8.7.2 
Eliminating Unreachable Code . . . . . . . . . . . . . . .  
550 
. . . . . . . . . . . . . . .  
8.7.3 Flow-of-Control Optimizations 
551 
8.7.4 Algebraic Simplification and Reduction in Strength . . . .  
552 
. . . . . . . . . . . . . . . . . . . .  
8.7.5 Use of Machine Idioms 
552 
8.7.6 Exercises for Section 8.7 . . . . . . . . . . . . . . . . . . .  
553 
. . . . . . . . . . . . . . . .  
8.8 Register Allocation and Assignment 
553 
. . . . . . . . . . . . . . . . . .  
8.8.1 Global Register Allocation 
553 
. . . . . . . . . . . . . . . . . . . . . . . . .  
8.8.2 
Usage Counts 
554 
. . . . . . . . . . .  
8.8.3 Register Assignment for Outer Loops 
556 
. . . . . . . . . . .  
8.8.4 Register Allocation by Graph Coloring 
556 
. . . . . . . . . . . . . . . . . . .  
8.8.5 Exercises for Section 8.8 
557 
. . . . . . . . . . . . . .  
8.9 Instruction Selection by Tree Rewriting 
558 
. . . . . . . . . . . . . . . . . .  
8.9.1 Tree-Translation Schemes 
558 
. . . . . . . . .  
8.9.2 
Code Generation by Tiling an Input Tree 
560 
. . . . . . . . . . . . . . . .  
8.9.3 Pattern Matching by Parsing 
563 
. . . . . . . . . . . . . .  
8.9.4 Routines for Semantic Checking 
565 
. . . . . . . . . . . . . . . . . . . .  
8.9.5 General Tree Matching 
565 
. . . . . . . . . . . . . . . . . . .  
8.9.6 
Exercises for Section 8.9 
567 
. . . . . . . . . . . . .  
8.10 Optimal Code Generation for Expressions 
567 
. . . . . . . . . . . . . . . . . . . . . . .  
8.10.1 Ershov Numbers 
567 
. . . . .  
8.10.2 Generating Code From Labeled Expression Trees 
568 
8.10.3 Evaluating Expressions with an Insufficient Supply of Reg- 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
isters 
570 
. . . . . . . . . . . . . . . . . .  
8.10.4 Exercises for Section 8.10 
572 
. . . . . . . . . . . . . .  
8.11 Dynamic Programming Code-Generation 
573 
. . . . . . . . . . . . . . . . . . . .  
8.11.1 Contiguous Evaluation 
574 
. . . . . . . . . . .  
8.11.2 The Dynamic Programming Algorithm 
575 
. . . . . . . . . . . . . . . . . .  
8.1 
1.3 Exercises for Section 8.11 
577 
. . . . . . . . . . . . . . . . . . . . . . . .  
8.12 Summary of Chapter 8 
578 
. . . . . . . . . . . . . . . . . . . . . . .  
8.13 References for Chapter 8 
579 
9 Machine-Independent Optimizations 
583 
. . . . . . . . . . . . . . .  
9.1 The Principal Sources of Optimization 
584 
. . . . . . . . . . . . . . . . . . . .  
9.1.1 Causes of Redundancy 
584 
. . . . . . . . . . . . . . .  
9.1.2 
A Running Example: Quicksort 
585 
. . . . . . . . . . .  
9.1.3 
Semantics-Preserving Transformations 
586 
. . . . . . . . . . . . . .  
9.1.4 
Global Common Subexpressions 
588 
. . . . . . . . . . . . . . . . . . . . . .  
9.1.5 
Copy Propagation 
590 
. . . . . . . . . . . . . . . . . . .  
9.1.6 
Dead-Code Elimination 
591 
. . . . . . . . . . . . . . . . . . . . . . . . .  
9.1.7 
Code Motion 
592 
. . . . . .  
9.1.8 Induction Variables and Reduction in Strength 
592 
TABLE OF CONTENTS 
xix 
. . . . . . . . . . . . . . . . . . .  
9.1.9 
Exercises for Section 9.1 
596 
. . . . . . . . . . . . . . . .  
9.2 Introduction to Data-Flow Analysis 
597 
9.2.1 
The Data-Flow Abstraction . . . . . . . . . . . . . . . . .  
597 
9.2.2 
The Data-Flow Analysis Schema . . . . . . . . . . . . . .  
599 
. . . . . . . . . . . .  
9.2.3 Data-Flow Schemas on Basic Blocks 
600 
. . . . . . . . . . . . . . . . . . . . .  
9.2.4 
Reaching Definitions 
601 
. . . . . . . . . . . . . . . . . . . .  
9.2.5 
Live-Variable Arlalysis 
608 
. . . . . . . . . . . . . . . . . . . .  
9.2.6 
Available Expressions 
610 
. . . . . . . . . . . . . . . . . . . . . . . . . . .  
9.2.7 
Summary 
614 
. . . . . . . . . . . . . . . . . . .  
9.2.8 Exercises for Section 9.2 
615 
. . . . . . . . . . . . . . . . .  
9.3 Foundations of Data-Flow Analysis 
618 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
9.3.1 Semilattices 
618 
. . . . . . . . . . . . . . . . . . . . . .  
9.3.2 
Transfer Functions 
623 
. . . . .  
9.3.3 
The Iterative Algorithm for General Frameworks 
626 
. . . . . . . . . . . . . .  
9.3.4 
Meaning of a Data-Flow Solution 
628 
. . . . . . . . . . . . . . . . . . .  
9.3.5 Exercises for Section 9.3 
631 
. . . . . . . . . . . . . . . . . . . . . . . .  
9.4 Constant Propagation 
632 
9.4.1 Data-Flow Values for the Constant-Propagation Frame- 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
work 
633 
9.4.2 
The Meet for the Constant-Propagation Framework . . .  
633 
9.4.3 Transfer Functions for the Constant-Propagation Frame- 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
work 
634 
9.4.4 
Monotonicity of the Constant-Propagation Framework . . 635 
9.4.5 
Nondistributivity of the Constant-Propagation Framework 635 
9.4.6 
Interpretation of the Results . . . . . . . . . . . . . . . .  
637 
9.4.7 
Exercises for Section 9.4 . . . . . . . . . . . . . . . . . . .  
637 
9.5 Partial-Redundancy Elimination . . . . . . . . . . . . . . . . . .  
639 
9.5.1 
The Sources of Redundancy . . . . . . . . . . . . . . . . .  
639 
9.5.2 
Can All Redundancy Be Eliminated? . . . . . . . . . . . .  
642 
9.5.3 
The Lazy-Code-Motion Problem . . . . . . . . . . . . . .  
644 
9.5.4 
Anticipation of Expressions . . . . . . . . . . . . . . . . .  
645 
9.5.5 
The Lazy-Code-Motion Algorithm . . . . . . . . . . . . .  
646 
9.5.6 
Exercises for Section 9.5 . . . . . . . . . . . . . . . . . . .  
655 
9.6 Loops in Flow Graphs . . . . . . . . . . . . . . . . . . . . . . . .  
655 
9.6.1 Dominators . . . . . . . . . . . . . . . . . . . . . . . . . .  
656 
9.6.2 
Depth-First Ordering . . . . . . . . . . . . . . . . . . . .  
660 
9.6.3 Edges in a Depth-First Spanning Tree . . . . . . . . . . .  
661 
9.6.4 
Back Edges and Reducibility . . . . . . . . . . . . . . . .  
662 
9.6.5 Depth of a Flow Graph . . . . . . . . . . . . . . . . . . .  
665 
9.6.6 
Natural Loops . . . . . . . . . . . . . . . . . . . . . . . .  
665 
9.6.7 
Speed of Convergence of Iterative Data-Flow Algorithms . 667 
9.6.8 
Exercises for Section 9.6 . . . . . . . . . . . . . . . . . . .  
669 
. . . . . . . . . . . . . . . . . . . . . . . .  
9.7 Region-Based Analysis 
672 
9.7.1 Regions . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
672 
9.7.2 
Region Hierarchies for Reducible Flow Graphs . . . . . .  
673 
TABLE OF CONTENTS 
9.7.3 Overview of a Region-Based Analysis . . . . . . . . . . .  
676 
9.7.4 
Necessary Assumptions About Transfer Functions . . . .  
678 
9.7.5 
An Algorithm for Region-Based Analysis . . . . . . . . .  
680 
9.7.6 
Handling Nonreducible Flow Graphs . . . . . . . . . . . .  
684 
9.7.7 Exercises for Section 9.7 . . . . . . . . . . . . . . . . . . .  
686 
9.8 Symbolic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . .  
686 
9.8.1 Affine Expressions of Reference Variables . . . . . . . . .  
687 
9.8.2 
Data-Flow Problem Formulation . . . . . . . . . . . . . .  
689 
9.8.3 Region-Based Symbolic Analysis . . . . . . . . . . . . . .  
694 
9.8.4 Exercises for Section 9.8 . . . . . . . . . . . . . . . . . . .  
699 
. . . . . . . . . . . . . . . . . . . . . . . .  
9.9 Summary of Chapter 9 
700 
. . . . . . . . . . . . . . . . . . . . . . .  
9.10 References for Chapter 9 
703 
10 Instruct 
ion-Level Parallelism 
707 
. . . . . . . . . . . . . . . . . . . . . . .  
10.1 Processor Architectures 
708 
10.1.1 Instruction Pipelines and Branch Delays . . . . . . . . . .  
708 
. . . . . . . . . . . . . . . . . . . . .  
10.1.2 Pipelined Execution 
709 
10.1.3 Multiple Instruction Issue . . . . . . . . . . . . . . . . . .  
710 
. . . . . . . . . . . . . . . . . . . .  
10.2 Code-Scheduling Constraints 
710 
. . . . . . . . . . . . . . . . . . . . . . .  
10.2.1 Data Dependence 
711 
10.2.2 Finding Dependences Among Memory Accesses . . . . . .  
712 
10.2.3 Tradeoff Between Register Usage and Parallelism . . . . .  
713 
10.2.4 Phase Ordering Between Register Allocation and Code 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
Scheduling 
716 
. . . . . . . . . . . . . . . . . . . . .  
10.2.5 Control Dependence 
716 
. . . . . . . . . . . . . . .  
10.2.6 Speculative Execution Support 
717 
. . . . . . . . . . . . . . . . . . .  
10.2.7 A Basic Machine Model 
719 
. . . . . . . . . . . . . . . . . .  
10.2.8 Exercises for Section 10.2 
720 
. . . . . . . . . . . . . . . . . . . . . . . .  
10.3 Basic-Block Scheduling 
721 
. . . . . . . . . . . . . . . . . .  
10.3.1 Data-Dependence Graphs 
722 
. . . . . . . . . . . . . . .  
10.3.2 List Scheduling of Basic Blocks 
723 
. . . . . . . . . . . . . . .  
10.3.3 Prioritized Topological Orders 
725 
. . . . . . . . . . . . . . . . . .  
10.3.4 Exercises for Section 10.3 
726 
. . . . . . . . . . . . . . . . . . . . . . .  
10.4 Global Code Scheduling 
727 
. . . . . . . . . . . . . . . . . . .  
10.4.1 Primitive Code Motion 
728 
. . . . . . . . . . . . . . . . . . . .  
10.4.2 Upward Code Motion 
730 
. . . . . . . . . . . . . . . . . . .  
10.4.3 Downward Code Motion 
731 
. . . . . . . . . . . . . . . .  
10.4.4 Updating Data Dependences 
732 
. . . . . . . . . . . . . . . .  
10.4.5 Global Scheduling Algorithms 
732 
. . . . . . . . . . . . .  
10.4.6 Advanced Code Motion Techniques 
736 
. . . . . . . . . . . .  
10.4.7 Interaction with Dynamic Schedulers 
737 
. . . . . . . . . . . . . . . . . .  
10.4.8 Exercises for Section 10.4 
737 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
10.5 Software Pipelining 
738 
. . . . . . . . . . . . . . . . . . . . . . . . .  
10.5.1 Introduction 
738 
. . . . . . . . . . . . . . . .  
10.5.2 Software Pipelining of Loops 
740 
TABLE OF CONTENTS 
xxi 
. . . . . . . . .  
10.5.3 Register Allocation and Code Generation 
743 
. . . . . . . . . . . . . . . . . . . . . . .  
10.5.4 Do-Across Loops 
743 
. . . . . . .  
10.5.5 Goals and Constraints of Software Pipelining 
745 
. . . . . . . . . . . . . .  
10.5.6 A Software-Pipelining Algorithm 
749 
. . . . . . .  
10.5.7 Scheduling Acyclic Data-Dependence Graphs 
749 
. . . . . . . . . . .  
10.5.8 Scheduling Cyclic Dependence Graphs 
751 
. . . . . . . .  
10.5.9 Improvements to the Pipelining Algorithms 
758 
. . . . . . . . . . . . . . . .  
10.5.10 
Modular Variable Expansion 
758 
. . . . . . . . . . . . . . . . . . .  
10.5.11 
Conditional Statements 
761 
. . . . . . . . .  
10.5.12 
Hardware Support for Software Pipelining 
762 
. . . . . . . . . . . . . . . . . .  
10.5.13 
Exercises for Section 10.5 
763 
. . . . . . . . . . . . . . . . . . . . . . .  
10.6 Summary of Chapter 10 
765 
. . . . . . . . . . . . . . . . . . . . . .  
10.7 References for Chapter 10 
766 
11 Optimizing for Parallelism and Locality 
769 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .  
11.1 Basic Concepts 
771 
. . . . . . . . . . . . . . . . . . . . . . . .  
. 
11.1 1 Multiprocessors 
772 
. . . . . . . . . . . . . . . . .  
11.1.2 Parallelism in Applications 
773 
11.1.3 Loop-Level Parallelism . . . . . . . . . . . . . . . . . . . .  
775 
11.1.4 Data Locality . . . . . . . . . . . . . . . . . . . . . . . . .  
777 
. . . . . . . . .  
11.1.5 Introduction to Affine Transform Theory 
778 
. . . . . . . . . . . . . .  
11.2 Matrix Multiply: An In-Depth Example 
782 
. . . . . . . . . . .  
11.2.1 The Matrix-Multiplication Algorithm 
782 
. . . . . . . . . . . . . . . . . . . . . . . . .  
11.2.2 Optimizations 
785 
11.2.3 Cache Interference . . . . . . . . . . . . . . . . . . . . . .  
788 
. . . . . . . . . . . . . . . . . .  
11.2.4 Exercises for Section 11.2 
788 
11.3 Iteration Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
788 
11.3.1 Constructing Iteration Spaces from Loop Nests . . . . . .  
788 
11.3.2 Execution Order for Loop Nests . . . . . . . . . . . . . .  
791 
11.3.3 Matrix Formulation of Inequalities . . . . . . . . . . . . .  
791 
11.3.4 Incorporating Symbolic Constants . . . . . . . . . . . . .  
793 
11.3.5 Controlling the Order of Execution . . . . . . . . . . . . .  
793 
11.3.6 Changing Axes . . . . . . . . . . . . . . . . . . . . . . . .  
798 
11.3.7 Exercises for Section 11.3 . . . . . . . . . . . . . . . . . .  
799 
11.4 Affine Array Indexes . . . . . . . . . . . . . . . . . . . . . . . . .  
801 
11.4.1 Affine Accesses . . . . . . . . . . . . . . . . . . . . . . . .  
802 
11.4.2 Affine and Nonaffine Accesses in Practice . . . . . . . . .  
803 
11.4.3 Exercises for Section 11.4 . . . . . . . . . . . . . . . . . .  
804 
11.5 Data Reuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
804 
11.5.1 Types of Reuse . . . . . . . . . . . . . . . . . . . . . . . .  
805 
11.5.2 Self Reuse . . . . . . . . . . . . . . . . . . . . . . . . . . .  
806 
11.5.3 Self-spatial Reuse . . . . . . . . . . . . . . . . . . . . . .  
809 
11.5.4 Group Reuse . . . . . . . . . . . . . . . . . . . . . . . . .  
811 
11.5.5 Exercises for Section 11.5 . . . . . . . . . . . . . . . . . .  
814 
11.6 Array Data-Dependence Analysis . . . . . . . . . . . . . . . . . .  
815 
xxii 
TABLE OF CONTENTS 
11.6.1 Definition of Data Dependence of Array Accesses . . . . .  
816 
11.6.2 Integer Linear Programming . . . . . . . . . . . . . . . .  
817 
. . . . . . . . . . . . . . . . . . . . . . . .  
11.6.3 The GCD Test 
818 
11.6.4 Heuristics for Solving Integer Linear Programs . . . . . .  
820 
11.6.5 Solving General Integer Linear Programs . . . . . . . . .  
823 
11.6.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . .  
825 
. . . . . . . . . . . . . . . . . .  
11.6.7 Exercises for Section 11.6 
826 
11.7 Finding Synchronization-Free Parallelism . . . . . . . . . . . . .  
828 
11.7.1 An Introductory Example . . . . . . . . . . . . . . . . . .  
828 
. . . . . . . . . . . . . . . . . . . .  
11.7.2 Affine Space Partitions 
830 
11.7.3 Space-Partition Constraints . . . . . . . . . . . . . . . . .  
831 
11.7.4 Solving Space-Partition Constraints . . . . . . . . . . . .  
835 
11.7.5 A Simple Code-Generation Algorithm . . . . . . . . . . .  
838 
11.7.6 Eliminating Empty Iterations . . . . . . . . . . . . . . . .  
841 
11.7.7 Eliminating Tests from Innermost Loops . . . . . . . . . .  
844 
11.7.8 Source-Code Transforms . . . . . . . . . . . . . . . . . . .  
846 
11.7.9 Exercises for Section 11.7 . . . . . . . . . . . . . . . . . .  
851 
11.8 Synchronization Between Parallel Loops . . . . . . . . . . . . . .  
853 
11.8.1 A Constant Number of Synchronizations . . . . . . . . . .  
853 
11.8.2 Program-Dependence Graphs . . . . . . . . . . . . . . . .  
854 
11.8.3 Hierarchical Time . . . . . . . . . . . . . . . . . . . . . .  
857 
11.8.4 The Parallelization Algorithm . . . . . . . . . . . . . . . .  
859 
. . . . . . . . . . . . . . . . . .  
11.8.5 Exercises for Section 11.8 
860 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
11.9 Pipelining 
861 
. . . . . . . . . . . . . . . . . . . . .  
11.9.1 What is Pipelining? 
861 
. . . . .  
11.9.2 Successive Over-Relaxation (SOR): An Example 
863 
. . . . . . . . . . . . . . . . . . .  
11.9.3 Fully Permutable Loops 
864 
. . . . . . . . . . . . .  
11.9.4 Pipelining Fully Permutable Loops 
864 
. . . . . . . . . . . . . . . . . . . . . . . .  
11.9.5 General Theory 
867 
. . . . . . . . . . . . . . . . .  
11.9.6 Time-Partition Constraints 
868 
11.9.7 Solving Time-Partition Constraints by Farkas' Lemma . . 872 
. . . . . . . . . . . . . . . . . . . .  
11.9.8 Code Transformations 
875 
. . . . . . . .  
11.9.9 Parallelism With Minimum Synchronization 
880 
. . . . . . . . . . . . . . . . . .  
11.9.10 
Exercises for Section 11.9 
882 
. . . . . . . . . . . . . . . . . . . . . . .  
11.10 Locality Optimizations 
884 
. . . . . . . . . . .  
11.10.1 
Temporal Locality of Computed Data 
885 
. . . . . . . . . . . . . . . . . . . . . .  
11.10.2 
Array Contraction 
885 
. . . . . . . . . . . . . . . . . . . .  
11.10.3 
Partition Interleaving 
887 
. . . . . . . . . . . . . . . . . . .  
11.10.4 
Putting it All Together 
890 
. . . . . . . . . . . . . . . . . .  
11.10.5 
Exercises for Section 11.10 
892 
. . . . . . . . . . . . . . . . . .  
11.11 Other Uses of Affine Transforms 
893 
. . . . . . . . . . . . . . . .  
I1 
.1 
1.1 
Distributed memory machines 
894 
. . . . . . . . . . . . .  
11.11.2 
Multi-Instruction-Issue Processors 
895 
. . . . . . . . . . . . . . .  
11 
.l 
1.3 
Vector and SIMD Instructions 
895 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
11.11.4 
Prefetching 
896 
TABLE OF CONTENTS 
xxiii 
. . . . . . . . . . . . . . . . . . . . . . .  
11.12 Summary of Chapter 11 
897 
. . . . . . . . . . . . . . . . . . . . . .  
11.13 References for Chapter 11 
899 
12 
Interprocedural Analysis 
903 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .  
12.1 Basic Concepts 
904 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
12.1.1 Call Graphs 
904 
. . . . . . . . . . . . . . . . . . . . . .  
12.1.2 Context Sensitivity 
906 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
12.1.3 Call Strings 
908 
. . . . . . . . .  
12.1.4 Cloning-Based Context-Sensitive Analysis 
910 
. . . . . . . .  
12.1.5 Summary-Based Context-Sensitive Analysis 
911 
. . . . . . . . . . . . . . . . . .  
12.1.6 Exercises for Section 12.1 
914 
. . . . . . . . . . . . . . . . . . .  
12.2 Why Interprocedural Analysis? 
916 
. . . . . . . . . . . . . . . . .  
12.2.1 Virtual Method Invocation 
916 
. . . . . . . . . . . . . . . . . . . .  
12.2.2 Pointer Alias Analysis 
917 
. . . . . . . . . . . . . . . . . . . . . . . .  
12.2.3 Parallelization 
917 
. . . . .  
12.2.4 Detection of Software Errors and Vulnerabilities 
917 
. . . . . . . . . . . . . . . . . . . . . . . . .  
12.2.5 SQL Injection 
918 
. . . . . . . . . . . . . . . . . . . . . . . .  
12.2.6 Buffer Overflow 
920 
. . . . . . . . . . . . . . .  
12.3 A Logical Representation of Data Flow 
921 
. . . . . . . . . . . . . . . . . . .  
12.3.1 Introduction to Datalog 
921 
. . . . . . . . . . . . . . . . . . . . . . . . .  
12.3.2 Datalog Rules 
922 
. . . . . . . . . . .  
12.3.3 Intensional and Extensional Predicates 
924 
. . . . . . . . . . . . . . .  
12.3.4 Execution of Datalog Programs 
927 
. . . . . . .  
12.3.5 Incremental Evaluation of Datalog Programs 
928 
. . . . . . . . . . . . . . . . .  
12.3.6 Problematic Datalog Rules 
930 
12.3.7 Exercises for Section 12.3 . . . . . . . . . . . . . . . . . .  
932 
12.4 A Simple Pointer-Analysis Algorithm . . . . . . . . . . . . . . . .  
933 
12.4.1 Why is Pointer Analysis Difficult . . . . . . . . . . . . . .  
934 
12.4.2 A Model for Pointers and References . . . . . . . . . . . .  
935 
12.4.3 Flow Insensitivity . . . . . . . . . . . . . . . . . . . . . .  
936 
12.4.4 The Formulation in Datalog . . . . . . . . . . . . . . . . .  
937 
12.4.5 Using Type Information . . . . . . . . . . . . . . . . . . .  
938 
12.4.6 Exercises for Section 12.4 . . . . . . . . . . . . . . . . . .  
939 
12.5 Context-Insensitive Interprocedural Analysis . . . . . . . . . . . .  
941 
12.5.1 Effects of a Method Invocation . . . . . . . . . . . . . . .  
941 
12.5.2 Call Graph Discovery in Datalog . . . . . . . . . . . . . .  
943 
12.5.3 Dynamic Loading and Reflection . . . . . . . . . . . . . .  
944 
. . . . . . . . . . . . . . . . . .  
12.5.4 Exercises for Section 12.5 
945 
12.6 Context-Sensitive Pointer Analysis . . . . . . . . . . . . . . . . .  
945 
12.6.1 Contexts and Call Strings . . . . . . . . . . . . . . . . . .  
946 
. . . . . . . . . . . . . .  
12.6.2 Adding Context to Datalog Rules 
949 
12.6.3 Additional Observations About Sensitivity . . . . . . . . .  
949 
. . . . . . . . . . . . . . . . . .  
12.6.4 Exercises for Section 12.6 
950 
12.7 Datalog Implementation by BDD's . . . . . . . . . . . . . . . . .  
951 
12.7.1 Binary Decision Diagrams . . . . . . . . . . . . . . . . . .  
951 
TABLE OF CONTENTS 
12.7.2 Transformations on BDD7s . . . . . . . . . . . . . . . . .  
953 
12.7.3 Representing Relations by BDD7s 
. . . . . . . . . . . . . .  
954 
. . . . . . . . .  
12.7.4 Relational Operations as BDD Operations 
954 
. . . . . . . . . . . .  
12.7.5 Using BDD7s 
for Points-to Analysis 
957 
. . . . . . . . . . . . . . . . . .  
12.7.6 Exercises for Section 12.7 
958 
. . . . . . . . . . . . . . . . . . . . . . .  
12.8 Summary of Chapter 12 
958 
. . . . . . . . . . . . . . . . . . . . . .  
12.9 References for Chapter 12 
961 
A A Complete Front End 
965 
. . . . . . . . . . . . . . . . . . . . . . . .  
A.l The Source Language 
965 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
A.2 Main 
966 
. . . . . . . . . . . . . . . . . . . . . . . . . . .  
A.3 Lexical Analyzer 
967 
. . . . . . . . . . . . . . . . . . . . . .  
A.4 Symbol Tables and Types 
970 
. . . . . . . . . . . . . . . . .  
A.5 Intermediate Code for Expressions 
971 
. . . . . . . . . . . . . .  
A.6 Jumping Code for Boolean Expressions 
974 
. . . . . . . . . . . . . . . . .  
A.7 Intermediate Code for Statements 
978 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
A.8 Parser 
981 
. . . . . . . . . . . . . . . . . . . . . . .  
A.9 Creating the Front End 
986 
B Finding Linearly Independent Solutions 
989 
Index 
993 
Chapter 1 
Introduction 
Programming languages are notations for describing computations to people 
and to machines. The world as we know it depends on programming languages, 
because all the software running on all the computers was written in some 
programming language. But, before a program can be run, it first must be 
translated into a form in which it can be executed by a computer. 
The software systems that do this translation are called compilers. 
This book is about how to design and implement compilers. We shall dis- 
cover that a few basic ideas can be used to construct translators for a wide 
variety of languages and machines. Besides compilers, the principles and tech- 
niques for compiler design are applicable to so many other domains that they 
are likely to be reused many times in the career of a computer scientist. The 
study of compiler writing touches upon programming languages, machine ar- 
chitecture, language theory, algorithms, and software engineering. 
In this preliminary chapter, we introduce the different forms of language 
translators, give a high level overview of the structure of a typical compiler, 
and discuss the trends in programming languages and machine architecture 
that are shaping compilers. We include some observations on the relationship 
between compiler design and computer-science theory and an outline of the 
applications of compiler technology that go beyond compilation. We end with 
a brief outline of key programming-language concepts that will be needed for 
our study of compilers. 
1.1 Language Processors 
Simply stated, a compiler is a program that can read a program in one lan- 
guage - 
the source language - 
and translate it into an equivalent program in 
another language - 
the target language; see Fig. 1.1. 
An important role of the 
compiler is to report any errors in the source program that it detects during 
the translation process. 
CHAPTER 2. INTRODUCTION 
source program 
Compiles 
h 
+ 
target program 
Figure 1.1 
: A compiler 
If the target program is an executable machine-language program, it can 
then be called by the user to process inputs and produce outputs; see Fig. 1.2. 
Target Program 
output 
t
-
 
Figure 1.2: Running the target program 
An interpreter is another common kind of language processor. Instead of 
producing a target program as a translation, an interpreter appears to directly 
execute the operations specified in the source program on inputs supplied by 
the user, as shown in Fig. 1.3. 
source program 1 
Interpreter t- output 
input 
Figure 1.3: An interpreter 
The machine-language target program produced by a compiler is usually 
much faster than an interpreter at mapping inputs to outputs . An interpreter, 
however, can usually give better error diagnostics than a compiler, because it 
executes the source program statement by statement. 
Example 1.1 
: 
Java language processors combine compilation and interpreta- 
tion, as shown in Fig. 1.4. A Java source program may first be compiled into 
an intermediate form called bytecodes. The bytecodes are then interpreted by a 
virtual machine. A benefit of this arrangement is that bytecodes compiled on 
one machine can be interpreted on another machine, perhaps across a network. 
In order to achieve faster processing of inputs to outputs, some Java compil- 
ers, called just-in-time compilers, translate the bytecodes into machine language 
immediately before they run the intermediate program to process the input. 
1.1. LANGUAGE PROCESSORS 
source program 
Translator 
intermediate program 
input 
Figure 1.4: A hybrid compiler 
In addition to a compiler, several other programs may be required to create 
an executable target program, as shown in Fig. 1.5. A source program may be 
divided into modules stored in separate files. The task of collecting the source 
program is sometimes entrusted to a separate program, called a preprocessor. 
The preprocessor may also expand shorthands, called macros, into source lan- 
guage st 
at 
ements. 
The modified source program is then fed to a compiler. The compiler may 
produce an assembly-language program as its output, because assembly lan- 
guage is easier to produce as output and is easier to debug. The assembly 
language is then processed by a program called an assembler that produces 
relocatable machine code as its output. 
Large programs are often compiled in pieces, so the relocatable machine 
code may have to be linked together with other relocatable object files and 
library files into the code that actually runs on the machine. The linker resolves 
external memory addresses, where the code in one file may refer to a location 
in another file. The loader then puts together all of the executable object files 
into memory for execution. 
1.11 Exercises for Section 1.1 
Exercise 1.1.1 
: 
What is the difference between a compiler and an interpreter? 
Exercise 1.1.2 
: 
What are the advantages of (a) a compiler over an interpreter 
(b) an interpreter over a compiler? 
Exercise 1.1.3 : 
What advantages are there to a language-processing system in 
which the compiler produces assembly language rather than machine language? 
Exercise 1.1.4 
: 
A compiler that translates a high-level language into another 
high-level language is called a source-to-source translator. What advantages are 
there to using C as a target language for a compiler? 
Exercise 1.1.5 : Describe some of the tasks that an assembler needs to per- 
form. 
CHAPTER 1. INTRODUCTION 
source program 
i 
Preprocessor 
J 
t 
modified source program 
I 
Compiler 
fi 
t 
target assembly program 
i 
/ 
Assembler 
1 
i 
relocatable machine code 
library files 
relocatable obiect files 
t 
target machine code 
Figure 1.5: A language-processing system 
1.2 The Structure of a Compiler 
Up to this point we have treated a compiler as a single box that maps a source 
program into a semantically equivalent target program. If we open up this box 
a little, we see that there are two parts to this mapping: analysis and synthesis. 
The analysis part breaks up the source program into constituent pieces and 
imposes a grammatical structure on them. It then uses this structure to cre- 
ate an intermediate representation of the source program. If the analysis part 
detects that the source program is either syntactically ill formed or semanti- 
cally unsound, then it must provide informative messages, so the user can take 
corrective action. The analysis part also collects information about the source 
program and stores it in a data structure called a symbol table, which is passed 
along with the intermediate representation to the synthesis part. 
The synthesis part constructs the desired target program from the interme- 
diate representation and the information in the symbol table. The analysis part 
is often called the front end of the compiler; the synthesis part is the back end. 
If we examine the compilation process in more detail, we see that it operates 
as a sequence of phases, each of which transforms one representation of the 
source program to another. A typical decomposition of a compiler into phases 
is shown in Fig. 1.6. In practice, several phases may be grouped together, 
and the intermediate representations between the grouped phases need not be 
constructed explicitly. The symbol table, which stores information about the 
1.2. THE STRUCTURE O F A  COMPILER 
Symbol Table 
E l  
, 
characte; stream 
, 
/ 
Lexical Analyzer 
1 
token Atream 
f 
Syntax Analyzer 
syntax tree 
+ 
1 
Semantic Analyzer 
I Intermediate Code Generator I 
I 
I 
I 
intermediate represent ation 
i 
Machine-Independent 
intermediate representation 
i 
1 
Code Generator 
I 
I 
I 
I 
target-machine code 
C 
Machine-Dependent 
I 
Code Optimizer 
I 
, 
I 
t 
arget-machine code 
t 
Figure 1.6: Phases of a compiler 
entire source program, is used by all phases of the compiler. 
Some compilers have a machine-independent optimization phase between 
the front end and the back end. The purpose of this optimization phase is to 
perform transformations on the intermediate representation, so that the back 
end can produce a better target program than it would have otherwise pro- 
duced from an unoptimized intermediate representation. Since optimization is 
optional, one or the other of the two optimization phases shown in Fig. 1.6 may 
be missing. 
1.2.1 Lexical Analysis 
The first phase of a compiler is called lexical analysis or scanning. The lex- 
ical analyzer reads the stream of characters making up the source program 
6 
CHAPTER 1. INTRODUCTION 
and groups the characters into meaningful sequences called lexemes. For each 
lexeme, the lexical analyzer produces as output a token of the form 
(token-name, attribute-value) 
that it passes on to the subsequent phase, syntax analysis. In the token, the 
first component token-name is an abstract symbol that is used during syntax 
analysis, and the second component attribute-value points to an entry in the 
symbol table for this token. Information from the symbol-table entry 
'is needed 
for semantic analysis and code generation. 
For example, suppose a source program contains the assignment statement 
position = i n i t i a l  + r a t e  * 60 
(1.1) 
The characters in this assignment could be grouped into the following lexemes 
and mapped into the following tokens passed on to the syntax analyzer: 
1. posit 
ion is a lexeme that would be mapped into a token (id, I), where id 
is an abstract symbol standing for identifier and 1 
points to the symbol- 
table entry for position. The symbol-table entry for an identifier holds 
information about the identifier, such as its name and type. 
2. The assignment symbol = is a lexeme that is mapped into the token (=). 
Since this token needs no attribute-value, we have omitted the second 
component. We could have used any abstract symbol such as assign for 
the token-name, but for notational convenience we have chosen to use the 
lexeme itself as the name of the abstract symbol. 
3. i n i t i a l  is a lexeme that is mapped into the token (id, 
2), where 2 points 
to the symbol-table entry for i n i t i a l .  
4. + is a lexeme that is mapped into the token (+). 
5. r a t e  is a lexeme that is mapped into the token (id, 
3), where 3 points to 
the symbol-table entry for r a t e .  
6. * is a lexeme that is mapped into the token (*) . 
7. 60 is a lexeme that is mapped into the token (60) .' 
Blanks separating the lexemes would be discarded by the lexical analyzer. 
Figure 1.7 shows the representation of the assignment statement (1.1) after 
lexical analysis as the sequence of tokens 
In this representation, the token names =, +, and * are abstract symbols for 
the assignment, addition, and multiplication operators, respectively. 
'Technically speaking, for the lexeme 60 we should make up a token like (number,4), 
where 4 points to the symbol table for the internal representation of integer 60 but we shall 
defer the discussion of tokens for numbers until Chapter 2. Chapter 3 discusses techniques 
for building lexical analyzers. 
1.2. THE STRUCTURE OF A COMPILER 
;m 
3 r a t e  
position = i n i t i a l  + r a t e  * 60 
t 
Lexical Analyzer 
t 
(id, 
1) 
(=) (id, 
2) 
(+) 
(id, 
3) 
(*) (60) 
t 
Syntax Analyzer 
(id, 
2
)
/
 
JF 
(
i
d
,
 
3
)
/
 
\ 
60 
Semantic Analyzer 
s 
\
+
,
 
(id, 
2
)
'
 
* \ 
(id, 
3
)
'
 
i
n
t
 
t 
ofloat 
t 
I 
60 
I Intermediate Code Generator I 
t 
tl = i n t t o f l o a t ( 6 0 )  
t 2  = i d 3  * ti 
t 3  = id2 + t 2  
i d 1  = t 3  
tl = i d 3  * 60.0 
i d 1  = id2 + t1 
LDF 
R2, i d 3  
M
U
L
F
 R2, R2, #60.0 
LDF 
R1, id2 
A
D
D
F
 R1, R1, R2 
STF 
i d l y  R l  
Figure 1.7: Translation of an assignment statement 
8 
CHAPTER 1. INTRODUCTION 
1.2.2 Syntax Analysis 
The second phase of the compiler is syntax analysis or parsing. The parser uses 
the first components of the tokens produced by the lexical analyzer to create 
a tree-like intermediate representation that depicts the grammatical structure 
of the token stream. A typical representation is a syntax tree in which each 
interior node represents an operation and the children of the node represent the 
arguments of the operation. A syntax tree for the token stream (1.2) is shown 
as the output of the syntactic analyzer in Fig. 1.7. 
This tree shows the order in which the operations in the assignment 
position = i n i t i a l  + r a t e  * 60 
are to be performed. The tree has an interior node labeled * with (id, 
3) as 
its left child and the integer 60 as its right child. The node (id, 
3) represents 
the identifier rate. The node labeled * makes it explicit that we must first 
multiply the value of r a t e  by 60. The node labeled + indicates that we must 
add the result of this multiplication to the value of i n i t i a l .  The root of the 
tree, labeled =, indicates that we must store the result of this addition into the 
location for the identifier p o s i t  
ion. This ordering of operations is consistent 
with the usual conventions of arithmetic which tell us that multiplication has 
higher precedence than addition, and hence that the multiplication is to be 
performed before the addition. 
The subsequent phases of the compiler use the grammatical structure to help 
analyze the source program and generate the target program. In Chapter 4 
we shall use context-free grammars to specify the grammatical structure of 
programming languages and discuss algorithms for constructing efficient syntax 
analyzers automatically from certain classes of grammars. In Chapters 2 and 5 
we shall see that syntax-directed definitions can help specify the translation of 
programming language constructs. 
1.2.3 Semantic Analysis 
The semantic analyzer uses the syntax tree and the information in the symbol 
table to check the source program for semantic consistency with the language 
definition. It also gathers type information and saves it in either the syntax tree 
or the symbol table, for subsequent use during intermediate-code generation. 
An important part of semantic analysis is type checking, where the compiler 
checks that each operator has matching operands. For example, many program- 
ming language definitions require an array index to be an integer; the compiler 
must report an error if a floating-point number is used to index an array. 
The language specification may permit some type conversions called coer- 
cions. For example, a binary arithmetic operator may be applied to either a 
pair of integers or to a pair of floating-point numbers. I
f
 the operator is applied 
to a floating-point number and an integer, the compiler may convert or coerce 
the integer into a floating-point number. 
1.2. THE STRUCTURE OF A COMPILER 
9 
Such a coercion appears in Fig. 1.7. Suppose that position, i n i t i a l ,  and 
r a t e  have been declared to be floating-point numbers, and that the lexeme 60 
by itself forms an integer. The type checker in the semantic analyzer in Fig. 1.7 
discovers that the operator * is applied to a floating-point number r a t e  and 
an integer 60. In this case, the integer may be converted into a floating-point 
number. In Fig. 1.7, notice that the output of the semantic analyzer has an 
extra node for the operator inttofloat, which explicitly converts its integer 
argument into a floating-point number. Type checking and semantic analysis 
are discussed in Chapter 6. 
1.2.4 Intermediate Code Generation 
In the process of translating a source program into target code, a compiler may 
construct one or more intermediate representations, which can have a variety 
of forms. Syntax trees are a form of intermediate representation; they are 
commonly used during syntax and semantic analysis. 
After syntax and semantic analysis of the source program, many compil- 
ers generate an explicit low-level or machine-like intermediate representation, 
which we can think of as a program for an abstract machine. This intermedi- 
ate representation should have two important properties: it should be easy to 
produce and it should be easy to translate into the target machine. 
In Chapter 6, we consider an intermediate form called three-address code, 
which consists of a sequence of assembly-like instructions with three operands 
per instruction. Each operand can act like a register. The output of the inter- 
mediate code generator in Fig. 1.7 consists of the three-address code sequence 
tl = i n t t o f l o a t  
(60) 
t 2  = id3 * tl 
t 3  = id2 + t 2  
i d 1  = t 3  
There are several points worth noting about three-address instructions. 
First, each three-address assignment instruction has at most one operator on the 
right side. Thus, these instructions f
i
x
 the order in which operations are to be 
done; the multiplication precedes the addition in the source program (1.1). Sec- 
ond, the compiler must generate a temporary name to hold the value computed 
by a three-address instruction. Third, some "three-address instructions" like 
the first and last in the sequence (1.3), above, have fewer than three operands. 
In Chapter 6, we cover the principal intermediate representations used in 
compilers. Chapters 5 introduces techniques for syntax-directed translation 
that are applied in Chapter 6 to type checking and intermediate-code generation 
for typical programming language constructs such as expressions, flow-of-control 
constructs, and procedure calls. 
10 
CHAPTER 1. INTRODUCTION 
1.2.5 Code Optimization 
The machine-independent code-optimization phase attempts to improve the 
intermediate code so that better target code will result. Usually better means 
faster, but other objectives may be desired, such as shorter code, or target code 
that consumes less power. For example, a straightforward algorithm generates 
the intermediate code (1.3), using an instruction for each operator in the tree 
representation that comes from the semantic analyzer. 
A simple intermediate code generation algorithm followed by code optimiza- 
tion is a reasonable way to generate good target code. The optimizer can deduce 
that the conversion of 60 from integer to floating point can be done once and for 
all at compile time, so the inttofloat operation can be eliminated by replacing 
the integer 60 by the floating-point number 60.0. Moreover, t3 
is used only 
once to transmit its value to id1 so the optimizer can transform (1.3) into the 
shorter sequence 
There is a great variation in the amount of code optimization different com- 
pilers perform. In those that do the most, the so-called "optimizing compilers," 
a significant amount of time is spent on this phase. There are simple opti- 
mizations that significantly improve the running time of the target program 
without slowing down compilation too much. The chapters from 8 on discuss 
machine-independent and machine-dependent optimizations in detail. 
1.2.6 Code Generation 
The code generator takes as input an intermediate representation of the source 
program and maps it into the target language. If the target language is machine 
code, registers or memory locations are selected for each of the variables used by 
the program. Then, the intermediate instructions are translated into sequences 
of machine instructions that perform the same task. A crucial aspect of code 
generation is the judicious assignment of registers to hold variables. 
For example, using registers R 1  and R2, 
the intermediate code in (1.4) might 
get translated into the machine code 
LDF 
R2, 
id3 
MULF R2, 
R2, #60.0 
LDF 
R l ,  
id2 
ADDF R l ,  
R l ,  R2 
STF 
i d l ,  R l  
The first operand of each instruction specifies a destination. The F in each 
instruction tells us that it deals with floating-point numbers. The code in 
1.2. THE STRUCTURE OF A COMPILER 
11 
(1.5) loads the contents of address id3 into register R2, then multiplies it with 
floating-point constant 60.0. The # signifies that 60.0 is to be treated as an 
immediate constant. The third instruction moves id2 into register R 1  and the 
fourth adds to it the value previously computed in register R2. Finally, the value 
in register R1 
is stored into the address of idl, 
so the code correctly implements 
the assignment statement (1.1). Chapter 8 covers code generation. 
This discussion of code generation has ignored the important issue of stor- 
age allocation for the identifiers in the source program. As we shall see in 
Chapter 7, the organization of storage at run-time depends on the language be- 
ing compiled. Storage-allocation decisions are made either during intermediate 
code generation or during code generation. 
1.2.7 Symbol-Table Management 
An essential function of a compiler is to record the variable names used in the 
source program and collect information about various attributes of each name. 
These attributes may provide information about the storage allocated for a 
name, its type, its scope (where in the program its value may be used), and 
in the ca,se of procedure names, such things as the number and types of its 
arguments, the method of passing each argument (for example, by value or by 
reference), and the type returned. 
The symbol table is a data structure containing a record for each variable 
name, with fields for the attributes of the name. The data structure should be 
designed to allow the compiler to find the record for each name quickly and to 
store or retrieve data from that record quickly. Symbol tables are discussed in 
Chapter 2. 
1.2.8 The Grouping of Phases into Passes 
The discussion of phases deals with the logical organization of a compiler. In 
an implementation, activities from several phases may be grouped together 
into a pass that reads an input file and writes an output file. For example, 
the front-end phases of lexical analysis, syntax analysis, semantic analysis, and 
intermediate code generation might be grouped together into one pass. Code 
optimization might be an optional pass. Then there could be a back-end pass 
consisting of code generation for a particular target machine. 
Some compiler collections have been created around carefully designed in- 
termediate representations that allow the front end for a particular language to 
interface with the back end for a certain target machine. With these collections, 
we can produce compilers for different source languages for one target machine 
by combining different front ends with the back end for that target machine. 
Similarly, we can produce compilers for different target machines, by combining 
a front end with back ends for different target machines. 
12 
CHAPTER 1. INTRODUCTION 
1.2.9 Compiler-Construction Tools 
The compiler writer, like any software developer, can profitably use modern 
software development environments containing tools such as language editors, 
debuggers, version managers, profilers, test harnesses, and so on. In addition 
to these general software-development tools, other more specialized tools have 
been created to help implement various phases of a compiler. 
These tools use specialized languages for specifying and implementing spe- 
cific components, and many use quite sophisticated algorithms. The most suc- 
cessful tools are those that hide the details of the generation algorithm and 
produce components that can be easily integrated into the remainder of the 
compiler. Some commonly used compiler-construction tools include 
1. Parser generators that automatically produce syntax analyzers from a 
grammatical description of a programming language. 
2. Scanner generators that produce lexical analyzers from a regular-expres- 
sion description of the tokens of a language. 
3. Syntax-directed translation engines that produce collections of routines 
for walking a parse tree and generating intermediate code. 
4. Code-generator generators that produce a code generator from a collection 
of rules for translating each operation of the intermediate language into 
the machine language for a target machine. 
5. Data-flow analysis engines that facilitate the gathering of information 
about how values are transmitted from one part of a program to each 
other part. Data-flow analysis is a key part of code optimization. 
6. Compiler-construction toolk2ts that provide an integrated set of routines 
for constructing various phases of a compiler. 
We shall describe many of these tools throughout this book. 
1
.
3
 The Evolution of Programming Languages 
The first electronic computers appeared in the 1940's and were programmed in 
machine language by sequences of 0's and 1's that explicitly told the computer 
what operations to execute and in what order. The operations themselves 
were very low level: move data from one location to another, add the contents 
of two registers, compare two values, and so on. Needless to say, this kind 
of programming was slow, tedious, and error prone. And once written, the 
programs were hard to understand and modify. 
1.3. THE EVOLUTION OF PROGRAMMING LANGUAGES 
1.3.1 The Move to Higher-level Languages 
The first step towards more people-friendly programming languages was the 
development of mnemonic assembly languages in the early 1950's. Initially, 
the instructions in an assembly language were just mnemonic representations 
of machine instructions. Later, macro instructions were added to assembly 
languages so that a programmer could define parameterized shorthands for 
frequently used sequences of machine instructions. 
A major step towards higher-level languages was made in the latter half of 
the 1950's with the development of Fortran for scientific computation, Cobol 
for business data processing, and Lisp for symbolic computation. The philos- 
ophy behind these languages was to create higher-level notations with which 
programmers could more easily write numerical computations, business appli- 
cations, and symbolic programs. These languages were so successful that they 
are still in use today. 
In the following decades, many more languages were created with innovative 
features to help make programming easier, more natural, and more robust. 
Later in this chapter, we shall discuss some key features that are common to 
many modern programming languages. 
Today, there are thousands of programming languages. They can be classi- 
fied in a variety of ways. One classification is by generation. First-generation 
languages are the machine languages, second-generation the assembly languages, 
and third-generation the higher-level languages like Fortran, Cobol, Lisp, C, 
C++, C#, and Java. Fourth-generation languages are languages designed 
for specific applications like NOMAD for report generation, SQL for database 
queries, and Postscript for text formatting. The term fifth-generation language 
has been applied to logic- and constraint-based languages like Prolog and OPS5. 
Another classification of languages uses the term imperative for languages 
in which a program specifies how a computation is to be done and declarative 
for languages in which a program specifies what computation is to be done. 
Languages such as C, C++, C#, and Java are imperative languages. In imper- 
ative languages there is a notion of program state and statements that change 
the state. Functional languages such as ML and Haskell and constraint logic 
languages such as Prolog are often considered to be declarative languages. 
The term von Neumann language is applied to programming languages 
whose computational model is based on the von Neumann computer archi- 
tecture. Many of today's languages, such as Fortran and C are von Neumann 
languages. 
An object-oriented language is one that supports object-oriented program- 
ming, a programming style in which a program consists of a collection of objects 
that interact with one another. Simula 67 and Smalltalk are the earliest major 
object-oriented languages. Languages such as C++, C#, Java, and Ruby are 
more recent ob 
ject-oriented languages. 
Scripting languages are interpreted languages with high-level operators de- 
signed for "gluing toget 
her" computations. These computations were originally 
14 
CHAPTER 1. INTRODUCTION 
called "scripts." Awk, JavaScript, Perl, PHP, Python, Ruby, and Tcl are pop- 
ular examples of scripting languages. Programs written in scripting languages 
are often much shorter than equivalent programs written in languages like C. 
1.3.2 
Impacts on Compilers 
Since the design of programming languages and compilers are intimately related, 
the advances in programming languages placed new demands on compiler writ- 
ers. They had to devise algorithms and representations to translate and support 
the new language features. Since the 1940's, computer architecture has evolved 
as well. Not only did the compiler writers have to track new language fea- 
tures, they also had to devise translation algorithms that would take maximal 
advantage of the new hardware capabilities. 
Compilers can help promote the use of high-level languages by minimizing 
the execution overhead of the programs written in these languages. Compilers 
are also critical in making high-performance computer architectures effective 
on users' applications. In fact, the performance of a computer system is so 
dependent on compiler technology that compilers are used as a tool in evaluating 
architectural concepts before a computer is built. 
Compiler writing is challenging. A compiler by itself is a large program. 
Moreover, many modern language-processing systems handle several source lan- 
guages and target machines within the same framework; that is, they serve as 
collections of compilers, possibly consisting of millions of lines of code. Con- 
sequently, good software-engineering techniques are essential for creating and 
evolving modern language processors. 
A compiler must translate correctly the potentially infinite set of programs 
that could be written in the source language. The problem of generating the 
optimal target code from a source program is undecidable in general; thus, 
compiler writers must evaluate tradeoffs about what problems to tackle and 
what heuristics to use to approach the problem of generating efficient code. 
A study of compilers is also a study of how theory meets practice, as we 
shall see in Section 1.4. 
The purpose of this text is to teach the methodology and fundamental ideas 
used in compiler design. It is not the intention of this text to teach all the 
algorithms and techniques that could be used for building a st 
ate-of-the-art 
language-processing system. However, readers of this text will acquire the basic 
knowledge and understanding to learn how to build a compiler relatively easily. 
1.3.3 Exercises for Section 1.3 
Exercise 1.3.1 
: 
Indicate which of the following terms: 
a) imperative 
b) declarative 
c) von Neumann 
d) object-oriented 
e) functional 
f) third-generation 
g) fourth-generation 
h) scripting 
1.4. THE SCIENCE OF BUILDING A COMPILER 
apply to which of the following languages: 
1) C 
2) C++ 
3) Cobol 
4) Fortran 
5) Java 
6) Lisp 
7) ML 
8) Per1 
9) Python 
10) VB. 
1.4 The Science of Building a Compiler 
Compiler design is full of beautiful examples where complicated real-world prob- 
lems are solved by abstracting the essence of the problem mathematically. These 
serve as excellent illustrations of how abstractions can be used to solve prob- 
lems: take a problem, formulate a mathematical abstraction that captures the 
key characteristics, and solve it using mathematical techniques. The problem 
formulation must be grounded in a solid understanding of the characteristics of 
computer programs, and the solution must be validated and refined empirically. 
A compiler must accept all source programs that conform to the specification 
of the language; the set of source programs is infinite and any program can be 
very large, consisting of possibly millions of lines of code. Any transformation 
performed by the compiler while translating a source program must preserve the 
meaning of the program being compiled. Compiler writers thus have influence 
over not just the compilers they create, but all the programs that their com- 
pilers compile. This leverage makes writing compilers particularly rewarding; 
however, it also makes compiler development challenging. 
1.4.1 Modeling in Compiler Design and Implementation 
The study of compilers is mainly a study of how we design the right mathe- 
matical models and choose the right algorithms, while balancing the need for 
generality and power against simplicity and efficiency. 
Some of most fundamental models are finite-state machines and regular 
expressions, which we shall meet in Chapter 3. These models are useful for de- 
scribing the lexical units of programs (keywords, identifiers, and such) and for 
describing the algorithms used by the compiler to recognize those units. Also 
among the most fundamental models are context-free grammars, used to de- 
scribe the syntactic structure of programming languages such as the nesting of 
parentheses or control constructs. We shall study grammars in Chapter 4. Sim- 
ilarly, trees are an important model for representing the structure of programs 
and their translation into object code, as we shall see in Chapter 5. 
1.4.2 The Science of Code Optimization 
The term "optimization" in compiler design refers to the attempts that a com- 
piler makes to produce code that is more efficient than the obvious code. "Op- 
timization" is thus a misnomer, since there is no way that the code produced 
by a compiler can be guaranteed to be as fast or faster than any other code 
that performs the same task. 
CHAPTER 1. INTRODUCTION 
In modern times, the optimization of code that a compiler performs has 
become both more important and more complex. It is more complex because 
processor architectures have become more complex, yielding more opportunities 
to improve the way code executes. It is more important because massively par- 
allel computers require substantial optimization, or their performance suffers by 
orders of magnitude. With the likely prevalence of multicore machines (com- 
puters with chips that have large numbers of processors on them), all compilers 
will have to face the problem of taking advantage of multiprocessor machines. 
It is hard, if not impossible, to build a robust compiler out of "hacks." 
Thus, an extensive and useful theory has been built up around the problem of 
optimizing code. The use of a rigorous mathematical foundation allows us to 
show that an optimization is correct and that it produces the desirable effect 
for all possible inputs. We shall see, starting in Chapter 9, how models such 
as graphs, matrices, and linear programs are necessary if the compiler is to 
produce well optimized code. 
On the other hand, pure theory alone is insufficient. Like many real-world 
problems, there are no perfect answers. In fact, most of the questions that 
we ask in compiler optimization are undecidable. One of the most important 
skills in compiler design is the ability to formulate the right problem to solve. 
We need a good understanding of the behavior of programs to start with and 
thorough experimentation and evaluation to validate our intuitions. 
Compiler optimizations must meet the following design objectives: 
The optimization must be correct, that is, preserve the meaning of the 
compiled program, 
The optimization must improve the performance of many programs, 
The compilation time must be kept reasonable, and 
The engineering effort required must be manageable. 
It is impossible to overemphasize the importance of correctness. It is trivial 
to write a compiler that generates fast code if the generated code need not 
be correct! Optimizing compilers are so difficult to get right that we dare say 
that no optimizing compiler is completely error-free! Thus, the most important 
objective in writing a compiler is that it is correct. 
The second goal is that the compiler must be effective in improving the per- 
formance of many input programs. Normally, performance means the speed of 
the program execution. Especially in embedded applications, we may also wish 
to minimize the size of the generated code. And in the case of mobile devices, 
it is also desirable that the code minimizes power consumption. Typically, the 
same optimizations that speed up execution time also conserve power. Besides 
performance, usability aspects such as error reporting and debugging are also 
import 
ant. 
Third, we need to keep the compilation time short to support a rapid devel- 
opment and debugging cycle. This requirement has become easier to meet as 
1.5. APPLICATIONS OF COMPILER TECHNOLOGY 
17 
machines get faster. Often, a program is first developed and debugged without 
program optimizations. Not only is the compilation time reduced, but more 
importantly, unoptimized programs are easier to debug, because the optimiza- 
tions introduced by a compiler often obscure the relationship between the source 
code and the object code. Turning on optimizations in the compiler sometimes 
exposes new problems in the source program; thus testing must again be per- 
formed on the optimized code. The need for additional testing sometimes deters 
the use of optimizations in applications, especially if their performance is not 
critical. 
Finally, a compiler is a complex system; we must keep the system sim- 
ple to assure that the engineering and maintenance costs of the compiler are 
manageable. There is an infinite number of program optimizations that we 
could implement, and it takes a nontrivial amount of effort to create a correct 
and effective optimization. We must prioritize the optimizations, implementing 
only those that lead to the greatest benefits on source programs encountered in 
practice. 
Thus, in studying compilers, we learn not only how to build a compiler, but 
also the general methodology of solving complex and open-ended problems. The 
approach used in compiler development involves both theory and experimenta- 
tion. We normally start by formulating the problem based on our intuitions on 
what the important issues are. 
1.5 Applications of Compiler Technology 
Compiler design is not only about compilers, and many people use the technol- 
ogy learned by studying compilers in school, yet have never, strictly speaking, 
written (even part of) a compiler for a major programming language. Compiler 
technology has other important uses as well. Additionally, compiler design im- 
pacts several other areas of computer science. In this section, we review the 
most important interactions and applications of the technology. 
1.5.1 Implementation of High-Level Programming 
Languages 
A high-level programming language defines a programming abstraction: the 
programmer expresses an algorithm using the language, and the compiler must 
translate that program to the target language. Generally, higher-level program- 
ming languages are easier to program in, but are less efficient, that is, the target 
programs run more slowly. Programmers using a low-level language have more 
control over a computation and can, in principle, produce more efficient code. 
Unfortunately, lower-level programs are harder to write and - 
worse still - 
less portable, more prone to errors, and harder to maintain. Optimizing com- 
pilers include techniques to improve the performance of generated code, thus 
offsetting the inefficiency introduced by high-level abstractions. 
18 
CHAPTER 1. INTRODUCTION 
Example 1.2 
: The register keyword in the C programming language is an 
early example of the interaction between compiler technology and language evo- 
lution. When the C language was created in the mid 1970s, it was considered 
necessary to let a programmer control which program variables reside in regis- 
ters. This control became unnecessary as effective register-allocation techniques 
were developed, and most modern programs no longer use this language feature. 
In fact, programs that use the register keyword may lose efficiency, because 
programmers often are not the best judge of very low-level matters like register 
allocation. The optimal choice of register allocation depends greatly on the 
specifics of a machine architecture. Hardwiring low-level resource-management 
decisions like register allocation may in fact hurt performance, especially if the 
program is run on machines other than the one for which it was written. 
The many shifts in the popular choice of programming languages have been 
in the direction of increased levels of abstraction. C was the predominant 
systems programming language of the 80's; many of the new projects started 
in the 90's chose C++; Java, introduced in 1995, gained popularity quickly 
in the late 90's. The new programming-language features introduced in each 
round spurred new research in compiler optimization. In the following, we give 
an overview on the main language features that have stimulated significant 
advances in compiler technology. 
Practically all common programming languages, including C, Fortran and 
Cobol, support user-defined aggregate data types, such as arrays and structures, 
and high-level control flow, such as loops and procedure invocations. If we just 
take each high-level construct or data-access operation and translate it directly 
to machine code, the result would be very inefficient. A body of compiler 
optimizations, known as data-flow optimizations, has been developed to analyze 
the flow of data through the program and removes redundancies across these 
constructs. They are effective in generating code that resembles code written 
by a skilled programmer at a lower level. 
Object orientation was first introduced in Simula in 1967, and has been 
incorporated in languages such as Smalltalk, C++, C#, and Java. The key 
ideas behind object orientation are 
1. Data abstraction and 
2. Inheritance of properties, 
both of which have been found to make programs more modular and easier to 
maintain. Object-oriented programs are different from those written in many 
other languages, in that they consist of many more, but smaller, procedures 
(called methods in object-oriented terms). Thus, compiler optimizations must 
be able to perform well across the procedural boundaries of the source program. 
Procedure inlining, which is the replacement of a procedure call by the body 
of the procedure, is particularly useful here. Optimizations to speed up virtual 
met 
hod dispatches have also been developed. 
APPLICATIONS OF COMPILER TECHNOLOGY 
Java has many features that make programming easier, many of which have 
been introduced previously in other languages. The Java language is type-safe; 
that is, an object cannot be used as an object of an unrelated type. All array 
accesses are checked to ensure that they lie within the bounds of the array. 
Java has no pointers and does not allow pointer arithmetic. It has a built-in 
garbage-collection facility that automatically frees the memory of variables that 
are no longer in use. While all these features make programming easier, they 
incur a run-time overhead. Compiler optimizations have been developed to 
reduce the overhead, for example, by eliminating unnecessary range checks and 
by allocating objects that are not accessible beyond a procedure on the stack 
instead of the heap. Effective algorithms also have been developed to minimize 
the overhead of garbage collection. 
In addition, Java is designed to support portable and mobile code. Programs 
are distributed as Java bytecode, which must either be interpreted or compiled 
into native code dynamically, that is, at run time. Dynamic compilation has also 
been studied in other contexts, where information is extracted dynamically at 
run time and used to produce better-optimized code. In dynamic optimization, 
it is important to minimize the compilation time as it is part of the execution 
overhead. A common technique used is to only compile and optimize those 
parts of the program that will be frequently executed. 
1.5.2 Optimizations for Computer Architectures 
The rapid evolution of computer architectures has also led to an insatiable 
demand for new compiler technology. Almost all high-performance systems 
take advantage of the same two basic techniques: parallelism and memory hi- 
erarchies. Parallelism can be found at several levels: at the instruction level, 
where multiple operations are executed simultaneously and at the processor 
level, where different threads of the same application are run on different pro- 
cessors. Memory hierarchies are a response to the basic limitation that we can 
build very fast storage or very large storage, but not storage that is both fast 
and large. 
Parallelism 
All modern microprocessors exploit instruction-level parallelism. However, this 
parallelism can be hidden from the programmer. Programs are written as if all 
instructions were executed in sequence; the hardware dynamically checks for 
dependencies in the sequential instruction stream and issues them in parallel 
when possible. In some cases, the machine includes a hardware scheduler that 
can change the instruction ordering to increase the parallelism in the program. 
Whether the hardware reorders the instructions or not, compilers can rearrange 
the instructions to make instruction-level parallelism more effective. 
Instruction-level parallelism can also appear explicitly in the instruction set. 
VLIW (Very Long Instruction Word) machines have instructions that can issue 
CHAPTER 2. INTRODUCTION 
multiple operations in parallel. The Intel IA64 is a well-known example of such 
an architecture. All high-performance, general-purpose microprocessors also 
include instructions that can operate on a vector of data at the same time. 
Compiler techniques have been developed to generate code automatically for 
such machines from sequential programs. 
Multiprocessors have also become prevalent ; even personal computers of- 
ten have multiple processors. Programmers can write multithreaded code for 
multiprocessors, or parallel code can be automatically generated by a com- 
piler from conventional sequential programs. Such a compiler hides from the 
programmers the details of finding parallelism in a program, distributing the 
computation across the machine, and minimizing synchronization and com- 
munication among the processors. Many scientific-computing and engineering 
applications are computation-intensive and can benefit greatly from parallel 
processing. Parallelization techniques have been developed to translate auto- 
matically sequential scientific programs into multiprocessor code. 
Memory Hierarchies 
A memory hierarchy consists of several levels of storage with different speeds 
and sizes, with the level closest to the processor being the fastest but small- 
est. The average memory-access time of a program is reduced if most of its 
accesses are satisfied by the faster levels of the hierarchy. Both parallelism and 
the existence of a memory hierarchy improve the potential performance of a 
machine, but they must be harnessed effectively by the compiler to deliver real 
performance on an application. 
Memory hierarchies are found in all machines. A processor usually has 
a small number of registers consisting of hundreds of bytes, several levels of 
caches containing kilobytes to megabytes, physical memory containing mega- 
bytes to gigabytes, and finally secondary storage that contains gigabytes and 
beyond. Correspondingly, the speed of accesses between adjacent levels of the 
hierarchy can differ by two or three orders of magnitude. The performance of a 
system is often limited not by the speed of the processor but by the performance 
of the memory subsystem. While compilers traditionally focus on optimizing 
the processor execution, more emphasis is now placed on making the memory 
hierarchy more effective. 
Using registers effectively is probably the single most important problem in 
optimizing a program. Unlike registers that have to be managed explicitly in 
software, caches and physical memories are hidden from the instruction set and 
are managed by hardware. It has been found that cache-management policies 
implemented by hardware are not effective in some cases, especially in scientific 
code that has large data structures (arrays, typically). It is possible to improve 
the effectiveness of the memory hierarchy by changing the layout of the data, 
or changing the order of instructions accessing the data. We can also change 
the layout of code to improve the effectiveness of instruction caches. 
1.5. APPLICATIONS OF COMPILER TECHNOLOGY 
1
.
5
.
3
 Design of New Computer Architectures 
In the early days of computer architecture design, compilers were developed 
after the machines were built. That has changed. Since programming in high- 
level languages is the norm, the performance of a computer system is determined 
not by its raw speed but also by how well compilers can exploit its features. 
Thus, in modern computer architecture development, compilers are developed 
in the processor-design stage, and compiled code, running on simulators, is used 
to evaluate the proposed architectural features. 
RISC 
One of the best known examples of how compilers influenced the design of 
computer architecture was the invention of the RISC (Reduced Instruction-Set 
Computer) architecture. Prior to this invention, the trend was to develop pro- 
gressively complex instruction sets intended to make assembly programming 
easier; these architectures were known as CISC (Complex Instruction-Set Com- 
puter). For example, CISC instruction sets include complex memory-addressing 
modes to support data-structure accesses and procedure-invocation instructions 
that save registers and pass parameters on the stack. 
Compiler optimizations often can reduce these instructions to a small num- 
ber of simpler operations by eliminating the redundancies across complex in- 
structions. Thus, it is desirable to build simple instruction sets; compilers can 
use them effectively and the hardware is much easier to optimize. 
Most general-purpose processor architectures, including PowerPC, SPARC, 
MIPS, Alpha, and PA-RISC, are based on the RISC concept. Although the 
x86 architecture-the 
most popular microprocessor-has 
a CISC instruction 
set, many of the ideas developed for RISC machines are used in the imple- 
mentation of the processor itself. Moreover, the most effective way to use a 
high-performance x86 machine is to use just its simple instructions. 
Specialized Architectures 
Over the last three decades, many architectural concepts have been proposed. 
They include data flow machines, vector machines, VLIW (Very Long Instruc- 
tion Word) machines, SIMD (Single Instruction, Multiple Data) arrays of pro- 
cessors, systolic arrays, multiprocessors with shared memory, and multiproces- 
sors with distributed memory. The development of each of these architectural 
concepts was accompanied by the research and development of corresponding 
compiler technology. 
Some of these ideas have made their way into the designs of embedded 
machines. Since entire systems can fit on a single chip, processors need no 
longer be prepackaged commodity units, but can be tailored to achieve better 
cost-effectiveness for a particular application. Thus, in contrast to general- 
purpose processors, where economies of scale have led computer architectures 
22 
CHAPTER 1. INTRODUCTION 
to converge, application-specific processors exhibit a diversity of computer ar- 
chitectures. Compiler technology is needed not only to support programming 
for these architectures, but also to evaluate proposed architectural designs. 
1.5.4 Program Translations 
While we normally think of compiling as a translation from a high-level lan- 
guage to the machine level, the same technology can be applied to translate 
between different kinds of languages. The following are some of the important 
applications of program-translation techniques. 
Binary Translation 
Compiler technology can be used to translate the binary code for one machine 
to that of another, allowing a machine to run programs originally compiled for 
another instruction set. Binary translation technology has been used by various 
computer companies to increase the availability of software for their machines. 
In particular, because of the domination of the x86 personal-computer mar- 
ket, most software titles are available as x86 code. Binary translators have 
been developed to convert x86 code into both Alpha and Sparc code. Binary 
translation was also used by Transmeta Inc. in their implementation of the x86 
instruction set. Instead of executing the complex x86 instruction set directly in 
hardware, the Transmeta Crusoe processor is a VLIW processor that relies on 
binary translation to convert x86 code into native VLIW code. 
Binary translation can also be used to provide backward compatibility. 
When the processor in the Apple Macintosh was changed from the Motorola MC 
68040 to the PowerPC in 1994, binary translation was used to allow PowerPC 
processors run legacy MC 68040 code. 
Hardware Synthesis 
Not only is most software written in high-level languages; even hardware de- 
signs are mostly described in high-level hardware description languages like 
Verilog and VHDL (Very high-speed integrated circuit Hardware Description 
Language). Hardware designs are typically described at the register trans- 
fer level (RTL), where variables represent registers and expressions represent 
combinational logic. Hardware-synthesis tools translate RTL descriptions auto- 
matically into gates, which are then mapped to transistors and eventually to a 
physical layout. Unlike compilers for programming languages, these tools often 
take hours optimizing the circuit. Techniques to translate designs at higher 
levels, such as the behavior or functional level, also exist. 
Database Query Interpreters 
Besides specifying software and hardware, languages are useful in many other 
applications. For example, query languages, especially SQL (Structured Query 
1.5. APPLICATIONS OF COMPILER TECHNOLOGY 
Language), are used to search databases. Database queries consist of predicates 
containing relational and boolean operators. They can be interpreted or com- 
piled into commands to search a database for records satisfying that predicate. 
Compiled Simulation 
Simulation is a general technique used in many scientific and engineering disci- 
plines to understand a phenomenon or to validate a design. Inputs to a simula- 
tor usually include the description of the design and specific input parameters 
for that particular simulation run. Simulations can be very expensive. We typi- 
cally need to simulate many possible design alternatives on many different input 
sets, and each experiment may take days to complete on a high-performance 
machine. Instead of writing a simulator that interprets the design, it is faster 
to compile the design to produce machine code that simulates that particular 
design natively. Compiled simulation can run orders of magnitude faster than 
an interpreter-based approach. Compiled simulation is used in many state-of- 
the-art tools that simulate designs written in Verilog or VHDL. 
1.5.5 Software Productivity Tools 
Programs are arguably the most complicated engineering artifacts ever pro- 
duced; they consist of many many details, every one of which must be correct 
before the program will work completely. As a result, errors are rampant in 
programs; errors may crash a system, produce wrong results, render a system 
vulnerable to security attacks, or even lead to catastrophic failures in critical 
systems. Testing is the primary technique for locating errors in programs. 
An interesting and promising complementary approach is to use data-flow 
analysis to locate errors statically (that is, before the program is run). Data- 
flow analysis can find errors along all the possible execution paths, and not 
just those exercised by the input data sets, as in the case of program testing. 
Many of the data-flow-analysis techniques, originally developed for compiler 
optimizations, can be used to create tools that assist programmers in their 
software engineering tasks. 
The problem of finding all program errors is undecidable. A data-flow analy- 
sis may be designed to warn the programmers of all possible statements violating 
a particular category of errors. But if most of these warnings are false alarms, 
users will not use the tool. Thus, practical error detectors are often neither 
sound nor complete. That is, they may not find all the errors in the program, 
and not all errors reported are guaranteed to be real errors. Nonetheless, var- 
ious static analyses have been developed and shown to be effective in finding 
errors, such as dereferencing null or freed pointers, in real programs. The fact 
that error detectors may be unsound makes them significantly different from 
compiler optimizations. Optimizers must be conservative and cannot alter the 
semantics of the program under any circumstances. 
24 
CHAPTER 1. INTRODUCTION 
In the balance of this section, we shall mention several ways in which pro- 
gram analysis, building upon techniques originally developed to optimize code 
in compilers, have improved software productivity. Of special importance are 
techniques that detect statically when a program might have a security vulner- 
ability. 
Type Checking 
Type checking is an effective and well-established technique to catch inconsis- 
tencies in programs. It can be used to catch errors, for example, where an 
operation is applied to the wrong type of object, or if parameters passed to a 
procedure do not match the signature of the procedure. Program analysis can 
go beyond finding type errors by analyzing the flow of data through a program. 
For example, if a pointer is assigned n u l l  and then immediately dereferenced, 
the program is clearly in error. 
The same technology can be used to catch a variety of security holes, in 
which an attacker supplies a string or other data that is used carelessly by the 
program. A user-supplied string can be labeled with a type "dangerous." If 
this string is not checked for proper format, then it remains "dangerous," and 
if a string of this type is able to influence the control-flow of the code at some 
point in the program, then there is a potential security flaw. 
Bounds Checking 
It is easier to make mistakes when programming in a lower-level language than 
a higher-level one. For example, many security breaches in systems are caused 
by buffer overflows in programs written in C. Because C does not have array- 
bounds checks, it is up to the user to ensure that the arrays are not accessed 
out of bounds. Failing to check that the data supplied by the user can overflow 
a buffer, the program may be tricked into storing user data outside of the 
buffer. An attacker can manipulate the input data that causes the program to 
misbehave and compromise the security of the system. Techniques have been 
developed to find buffer overflows in programs, but with limited success. 
Had the program been written in a safe language that includes automatic 
range checking, this problem would not have occurred. The same data-flow 
analysis that is used to eliminate redundant range checks can also be used to 
locate buffer overflows. The major difference, however, is that failing to elimi- 
nate a range check would only result in a small run-time cost, while failing to 
identify a potential buffer overflow may compromise the security of the system. 
Thus, while it is adequate to use simple techniques to optimize range checks, so- 
phisticated analyses, such as tracking the values of pointers across procedures, 
are needed to get high-quality results in error detection tools. 
1.6. PROGRAMMING LANGUAGE BASICS 
Memory-Management Tools 
Garbage collection is another excellent example of the tradeoff between effi- 
ciency and a combination of ease of programming and software reliability. Au- 
tomatic memory management obliterates all memory-management errors (e.g., 
"memory leaks"), which are a major source of problems in C and C++ pro- 
grams. Various tools have been developed to help programmers find memory 
management errors. For example, Purify is a widely used tool that dynamically 
catches memory management errors as they occur. Tools that help identify 
some of these problems statically have also been developed. 
1.6 Programming Language Basics 
In this section, we shall cover the most important terminology and distinctions 
that appear in the study of programming languages. It is not our purpose to 
cover all concepts or all the popular programming languages. We assume that 
the reader is familiar with at least one of C, C++, C#, or Java, and may have 
encountered other languages as well. 
1.6.1 The Static/Dynarnic Distinction 
Among the most important issues that we face when designing a compiler for 
a language is what decisions can the compiler make about a program. If a 
language uses a policy that allows the compiler to decide an issue, then we say 
that the language uses a static policy or that the issue can be decided at compile 
time. On the other hand, a policy that only allows a decision to be made when 
we execute the program is said to be a dynamic policy or to require a decision 
at run time. 
One issue on which we shall concentrate is the scope of declarations. The 
scope of a declaration of x is the region of the program in which uses of x refer to 
this declaration. A language uses static scope or lexical scope if it is possible to 
determine the scope of a declaration by looking only at the program. Otherwise, 
the language uses dynamic scope. With dynamic scope, as the program runs, 
the same use of x could refer to any of several different declarations of x. 
Most languages, such as C and Java, use static scope. We shall discuss static 
scoping in Section 1.6.3. 
Example 1.3 
: 
As another example of the staticldynamic distinction, consider 
the use of the term "static" as it applies to data in a Java class declaration. In 
Java, a variable is a name for a location in memory used to hold a data value. 
Here, "static" refers not to the scope of the variable, but rather to the ability of 
the compiler to determine the location in memory where the declared variable 
can be found. A declaration like 
public s t a t i c  i n t  x; 
CHAPTER 1. INTRODUCTION 
makes x a class variable and says that there is only one copy of x, no matter how 
many objects of this class are created. Moreover, the compiler can determine a 
location in memory where this integer x will be held. In contrast, had "static" 
been omitted from this declaration, then each object of the class would have its 
own location where x would be held, and the compiler could not determine all 
these places in advance of running the program. 
1.6.2 Environments and States 
Another important distinction we must make when discussing programming 
languages is whether changes occurring as the program runs affect the values of 
data elements or affect the interpretation of names for that data. For example, 
the execution of an assignment such as x = 
y + 1 changes the value denoted by 
the name x. More specifically, the assignment changes the value in whatever 
location is denoted by x. 
It may be less clear that the location denoted by x can change at run time. 
For instance, as we discussed in Example 1.3, if x is not a static (or "class") 
variable, then every object of the class has its own location for an instance 
of variable x. In that case, the assignment to x can change any of those "in- 
stance" variables, depending on the object to which a method containing that 
assignment is applied. 
environment 
state 
names n 
n 
locations 
values 
(variables) 
Figure 1.8: Two-stage mapping from names to values 
The association of names with locations in memory (the store) and then 
with values can be described by two mappings that change as the program runs 
(see Fig. 1.8): 
1. The environment is a mapping from names to locations in the store. Since 
variables refer to locations ('L1-values" 
in the terminology of C), we could 
alternatively define an environment as a mapping from names to variables. 
2. The state is a mapping from locations in store to their values. That is, the 
state maps 1-values to their corresponding r-values, in the terminology of 
C. 
Environments change according to the scope rules of a language. 
Example 1.4: Consider the C program fragment in Fig. 1.9. Integer i is 
declared a global variable, and also declared as a variable local to function f .  
When f is executing, the environment adjusts so that name i refers to the 
1.6. PROGRAMMING LANGUAGE BASICS 
i n t  i ;  
... 
void f(.--) 
{ 
i n t  i ;  
/* global i 
*/ 
/* local i 
*/ 
/* use of local i */ 
x = i + I ;  
/* use of global i */ 
Figure 1.9: Two declarations of the name i 
location reserved for the i that is local to f ,  and any use of i, such as the 
assignment i = 3 shown explicitly, refers to that location. Typically, the local 
i is given a place on the run-time stack. 
Whenever a function g other than f is executing, uses of i cannot refer to 
the i that is local to f .  Uses of name i in g must be within the scope of some 
other declaration of i. An example is the explicitly shown statement x = i+l, 
which is inside some procedure whose definition is not shown. The i in i + 1 
presumably refers to the global i. As in most languages, declarations in C must 
precede their use, so a function that comes before the global i cannot refer to 
it. 
The environment and state mappings in Fig. 1.8 are dynamic, but there are 
a few exceptions: 
1. Static versus dynamic binding of names to locations. Most binding of 
names to locations is dynamic, and we discuss several approaches to this 
binding throughout the section. Some declarations, such as the global i 
in Fig. 1.9, can be given a location in the store once and for all, as the 
compiler generates object code.2 
2. Static versus dynamic binding of locations to values. The binding of lo- 
cations to values (the second stage in Fig. 1.8), is generally dynamic as 
well, since we cannot tell the value in a location until we run the program. 
Declared constants are an exception. For instance, the C definition 
#define ARRAYSIZE 1000 
-- 
-
-
 
2~echnically, 
the C compiler will assign a location in virtual memory for the global i, 
leaving it to the loader and the operating system to determine where in the physical memory 
of the machine i will be located. However, we shall not worry about "relocation" issues such 
as these, which have no impact on compiling. Instead, we treat the address space that the 
compiler uses for its output code as if it gave physical memory locations. 
28 
CHAPTER I .  INTRODUCTION 
Names, Identifiers, and Variables 
Although the terms "name" and "variable," often refer to the same thing, 
we use them carefully to distinguish between compile-time names and the 
run-time locations denoted by names. 
An identifier is a string of characters, typically letters or digits, that 
refers to (identifies) an entity, such as a data object, a procedure, a class, 
or a type. All identifiers are names, but not all names are identifiers. 
Names can also be expressions. For example, the name x.y 
might denote 
the field y 
of a structure denoted by x. Here, x and y 
are identifiers, while 
x.y 
is a name, but not an identifier. Composite names like x.y are called 
qualified names. 
A variable refers to a particular location of the store. It is common for 
the same identifier to be declared more than once; each such declaration 
introduces a new variable. Even if each identifier is declared just once, an 
identifier local to a recursive procedure will refer to different locations of 
the store at different times. 
binds the name ARRAYSIZE 
to the value 1000 statically. We can determine 
this binding by looking at the statement, and we know that it is impossible 
for this binding to change when the program executes. 
1.6.3 Static Scope and Block Structure 
Most languages, including C and its family, use static scope. The scope rules 
for C are based on program structure; the scope of a declaration is determined 
implicitly by where the declaration appears in the program. Later languages, 
such as C++, Java, and C#, also provide explicit control over scopes through 
the use of keywords like public, private, and protected. 
In this section we consider static-scope rules for a language with blocks, 
where a block is a grouping of declarations and statements. C uses braces I and 
) 
to delimit a block; the alternative use of begin and end for the same purpose 
dates back to Algol. 
Example 1.5 : 
To a first approximation, the C static-scope policy is as follows: 
1. A C program consists of a sequence of top-level declarations of variables 
and functions. 
2. Functions may have variable declarations within them, where variables 
include local variables and parameters. The scope of each such declaration 
is restricted to the function in which it appears. 
1.6. PROGRAMMING LANGUAGE BASICS 
29 
Procedures, Functions, and Methods 
To avoid saying "procedures, functions, or methods," each time we want 
to talk about a subprogram that may be called, we shall usually refer to 
all of them as "procedures." The exception is that when talking explicitly 
of programs in languages like C that have only functions, we shall refer 
to them as "functions." Or, if we are discussing a language like Java that 
has only methods, we shall use that term instead. 
A function generally returns a value of some type (the "return type"), 
while a procedure does not return any value. C and similar languages, 
which have only functions, treat procedures as functions that have a special 
return type "void," to signify no return value. Object-oriented languages 
like Java and C++ use the term "methods." These can behave like either 
functions or procedures, but are associated with a particular class. 
3. The scope of a top-level declaration of a name x consists of the entire 
program that follows, with the exception of those statements that lie 
within a function that also has a declaration of x. 
The additional detail regarding the C static-scope policy deals with variable 
declarations within statements. We examine such declarations next and in 
Example 1.6. 
In C, the syntax of blocks is given by 
1. One type of statement is a block. Blocks can appear anywhere that other 
types of statements, such as assignment statements, can appear. 
2. A block is a sequence of declarations followed by a sequence of statements, 
all surrounded by braces. 
Note that this syntax allows blocks to be nested inside each other. This 
nesting property is referred to as block structure. The C family of languages 
has block structure, except that a function may not be defined inside another 
function. 
We say that a declaration D "belongs" to a block B if B is the most closely 
nested block containing D; that is, D is located within B, but not within any 
block that is nested within B. 
The static-scope rule for variable declarations in a block-structured lan- 
guages is as follows. If declaration D of name x belongs to block B, then the 
scope of D is all of B, except for any blocks B' nested to any depth within B, 
in which x is redeclared. Here, x is redeclared in B' if some other declaration 
D' of the same name x belongs to B'. 
30 
CHAPTER 1. INTRODUCTION 
An equivalent way to express this rule is to focus on a use of a name x. 
Let B1, B2, 
. 
. . , 
Bk be all the blocks that surround this use of x, with Bk the 
smallest, nested within Bk-1, which is nested within Bk-2, and so on. Search 
for the largest i such that there is a declaration of x belonging to Bi. This use 
of x refers to the declaration in Bi. Alternatively, this use of x is within the 
scope of the declaration in Bi. 
'
i
n
t
 
b = 2; 
\ 
.€ 
B2 
int a = 3; 
cout << a << b; 
3 
int b = 4; 
cout << a << b; 
3 
,cout << a << b; 
J 
cout << a << b; 
1 
Figure 1.10: Blocks in a C++ program 
Example 1.6 : The C++ program in Fig. 1.10 has four blocks, with several 
definitions of variables a and b. As a memory aid, each declaration initializes 
its variable to the number of the block to which it belongs. 
For instance, consider the declaration int a = I in block B1. Its scope 
is all of B1, except for those blocks nested (perhaps deeply) within B1 that 
have their own declaration of a. B2, nested immediately within B1, does not 
have a declaration of a, but B3 does. B4 does not have a declaration of a, so 
block B3 is the only place in the entire program that is outside the scope of the 
declaration of the name a that belongs to B1. That is, this scope includes fi 
and all of B2 except for the part of B2 that is within B3. The scopes of all five 
declarations are summarized in Fig. 1.11. 
From another point of view, let us consider the output statement in block 
B4 and bind the variables a and b used there to the proper declarations. The 
list of surrounding blocks, in order of increasing size, is Bq 
, 
B2, 
B1. Note that 
B3 does not surround the point in question. B4 has a declaration of b, so it 
is to this declaration that this use of b refers, and the value of b printed is 4. 
However, B4 does not have a declaration of a, so we next look at B2. That 
block does not have a declaration of a either, so we proceed to B1. Fortunately, 
1.6. PROGRAMMING LANGUAGE BASICS 
DECLARATION 
int a = 1; 
int b = 1; 
int b = 2; 
int a = 3; 
int b = 4; 
Figure 1.11: Scopes of declarations in Example 1.6 
there is a declaration int a = 1 belonging to that block, so the value of a 
printed is I. Had there been no such declaration, the program would have been 
erroneous. 
C1 
1.6.4 Explicit Access Control 
Classes and structures introduce a new scope for their members. If p is an 
object of a class with a field (member) x, then the use of x in p.x refers to 
field x in the class definition. In analogy with block structure, the scope of a 
member declaration x in a class C extends to any subclass C', except if C' has 
a local declaration of the same name x. 
Through the use of keywords like public, private, and protected, object- 
oriented languages such as C++ or Java provide explicit control over access 
to member names in a superclass. These keywords support encapsulation by 
restricting access. Thus, private names are purposely given a scope that includes 
only the method declarations and definitions associated with that class and any 
"friend" classes (the C++ term). Protected names are accessible to subclasses. 
Public names are accessible from outside the class. 
In C++, a class definition may be separated from the definitions of some 
or all of its methods. Therefore, a name x associated with the class C may 
have a region of the code that is outside its scope, followed by another region (a 
method definition) that is within its scope. In fact, regions inside and outside 
the scope may alternate, until all the methods have been defined. 
1.6.5 Dynamic Scope 
Technically, any scoping policy is dynamic if it is based on factor(s) that can 
be known only when the program executes. The term dynamic scope, however, 
usually refers to the following policy: a use of a name x refers to the declaration 
of x in the most recently called procedure with such a declaration. Dynamic 
scoping of this type appears only in special situations. We shall consider two ex- 
amples of dynamic policies: macro expansion in the C preprocessor and method 
resolution in ob 
ject-oriented programming. 
32 
CHAPTER 1. INTRODUCTION 
Declarations and Definitions 
The apparently similar terms "declaration" and "definition" for program- 
ming-language concepts are actually quite different. Declarations tell us 
about the types of things, while definitions tell us about their values. Thus, 
int i 
is a declaration of i, while i = I is a definition of i. 
The difference is more significant when we deal with methods or other 
procedures. In C++, a method is declared in a class definition, by giving 
the types of the arguments and result of the method (often called the 
signature for the method. The method is then defined, i.e., the code for 
executing the method is given, in another place. Similarly, it is common 
to define a C function in one file and declare it in other files where the 
function is used. 
Example 1.7 
: In the C program of Fig. 1.12, identifier a is a macro that 
stands for expression (x + I). But what is x? We cannot resolve x statically, 
that is, in terms of the program text. 
int x = 2; 
void b() ( int x = I ; printf 
(ll%d\nll, 
a) 
; 3 
void c ( )  ( printf("%d\nI1, 
a); 
void main() ( b(); c ( ) ;  3 
Figure 1.12: A macro whose names must be scoped dynamically 
In fact, in order to interpret x, we must use the usual dynamic-scope rule. 
We examine all the function calls that are currently active, and we take the most 
recently called function that has a declaration of x. It is to this declaration that 
the use of x refers. 
In the example of Fig. 1.12, the function main first calls function b. As b 
executes, it prints the value of the macro a. Since (x + 
1) must be substituted 
for a, we resolve this use of x to the declaration int x=l in function b. The 
reason is that b has a declaration of x, so the (x + 
1) in the printf in b refers 
to this x. Thus, the value printed is 1. 
After b finishes, and c is called, we again need to print the value of macro 
a. However, the only x accessible to c is the global x. The printf statement 
in c 
thus refers to this declaration of x, and value 2 is printed. 
Dynamic scope resolution is also essential for polymorphic procedures, those 
that have two or more definitions for the same name, depending only on the 
1.6. PROGRAMMING LANGUAGE BASICS 
Analogy Between Static and Dynamic Scoping 
While there could be any number of static or dynamic policies for scoping, 
there is an interesting relationship between the normal (block-structured) 
static scoping rule and the normal dynamic policy. In a sense, the dynamic 
rule is to time as the static rule is to space. While the static rule asks us to 
find the declaration whose unit (block) most closely surrounds the physical 
location of the use, the dynamic rule asks us to find the declaration whose 
unit (procedure invocation) most closely surrounds the time of the use. 
types of the arguments. In some languages, such as ML (see Section 7.3.3), it 
is possible to determine statically types for all uses of names, in which case the 
compiler can replace each use of a procedure name p by a reference to the code 
for the proper procedure. However, in other languages, such as Java and C++, 
there are times when the compiler cannot make that determination. 
Example 1.8 : 
A distinguishing feature of object-oriented programming is the 
ability of each object to invoke 
the appropriate method in response to a message. 
In other words, the procedure called when x.m() is executed depends on the 
class of the object denoted by x at that time. A typical example is as follows: 
1. There is a class iC with a method named m(). 
2. D is a subclass of C, and D has its own method named m(). 
3. There is a use of m of the form x.m(), where x is an object of class C. 
Normally, it is impossible to tell at compile time whether x will be of class 
C or of the subclass D. If the method application occurs several times, it is 
highly likely that some will be on objects denoted by x that are in class C but 
not D, while others will be in class D. It is not until run-time that it can be 
decided which definition of rn is the right one. Thus, the code generated by the 
compiler must determine the class of the object x, and call one or the other 
method named m. 
1.6.6 
Parameter Passing Mechanisms 
All programming languages have a notion of a procedure, but they can differ 
in how these procedures get their arguments. In this section, we shall consider 
how the actual parameters (the parameters used in the call of a procedure) 
are associated with the formal parameters (those used in the procedure defi- 
nition). Which mechanism is used determines how the calling-sequence code 
treats parameters. The great majority of languages use either "call-by-value," 
or "call-by-reference," or both. We shall explain these terms, and another 
method known as "call-by-name," that is primarily of historical interest. 
CHAPTER 1. INTRODUCTION 
In call-by-value, the actual parameter is evaluated (if it is an expression) or 
copied (if it is a variable). The value is placed in the location belonging to 
the corresponding formal parameter of the called procedure. This method is 
used in C and Java, and is a common option in C++, as well as in most 
other languages. Call-by-value has the effect that all computation involving the 
formal parameters done by the called procedure is local to that procedure, and 
the actual parameters themselves cannot be changed. 
Note, however, that in C we can pass a pointer to a variable to allow that 
variable to be changed by the callee. Likewise, array names passed as param- 
eters in C, C++, or Java give the called procedure what is in effect a pointer 
or reference to the array itself. Thus, if a is the name of an array of the calling 
procedure, and it is passed by value to corresponding formal parameter x, then 
an assignment such as x[i] = 2 really changes the array element a[2]. The 
reason is that, although x gets a copy of the value of a, that value is really a 
pointer to the beginning of the area of the store where the array named a is 
located. 
Similarly, in Java, many variables are really references, or pointers, to the 
things they stand for. This observation applies to arrays, strings, and objects 
of all classes. Even though Java uses call-by-value exclusively, whenever we 
pass the name of an object to a called procedure, the value received by that 
procedure is in effect a pointer to the object. Thus, the called procedure is able 
to affect the value of the object itself. 
Call- 
by-Reference 
In call- 
b y-reference, the address of the actual parameter is passed to the callee as 
the value of the corresponding formal parameter. Uses of the formal parameter 
in the code of the callee are implemented by following this pointer to the location 
indicated by the caller. Changes to the formal parameter thus appear as changes 
to the actual parameter. 
If the actual parameter is an expression, however, then the expression is 
evaluated before the call, and its value stored in a location of its own. Changes 
to the formal parameter change this location, but can have no effect on the 
data of the caller. 
Call-by-reference is used for "ref" parameters in C++ and is an option in 
many other languages. It is almost essential when the formal parameter is a 
large object, array, or structure. The reason is that strict call-by-value requires 
that the caller copy the entire actual parameter into the space belonging to 
the corresponding formal parameter. This copying gets expensive when the 
parameter is large. As we noted when discussing call-by-value, languages such 
as Java solve the problem of passing arrays, strings, or other objects by copying 
only a reference to those objects. The effect is that Java behaves as if it used 
call-by-reference 
for anything other than a basic type such as an integer or real. 
1.6. PROGRAMMING LANGUAGE BASICS 
Call- 
by-Name 
A third mechanism - 
call-by-name - 
was used in the early programming 
language Algol 60. It requires that the callee execute as if the actual parameter 
were substituted literally for the formal parameter in the code of the callee, as 
if the formal parameter were a macro standing for the actual parameter (with 
renaming of local names in the called procedure, to keep them distinct). When 
the actual parameter is an expression rather than a variable, some unintuitive 
behaviors occur, which is one reason this mechanism is not favored today. 
1.6.7 Aliasing 
There is an interesting consequence of call-by-reference parameter passing or 
its simulation, as in Java, where references to objects are passed by value. It 
is possible that two formal parameters can refer to the same location; such 
variables are said to be aliases of one another. As a result, any two variables, 
which may appear to take their values from two distinct formal parameters, can 
become aliases of each other, as well. 
Example 1.9 
: Suppose a is an array belonging to a procedure p, and p calls 
another procedure q(x, 
y) with a call q(a, 
a). Suppose also that parameters 
are passed by value, but that array names are really references to the location 
where the array is stored, as in C or similar languages. Now, x and y have 
become aliases of each other. The important point is that if within q there is 
an assignment x [lo] = 2, then the value of y[10] also becomes 2. 
It turns out that understanding aliasing and the mechanisms that create it 
is essential if a compiler is to optimize a program. As we shall see starting in 
Chapter 9, there are many situations where we can only optimize code if we 
can be sure certain variables are not aliased. For instance, we might determine 
that x = 2 is the only place that variable x is ever assigned. If so, then we can 
replace a use of x by a use of 2; for example, replace a = x+3 by the simpler 
a = 5. But suppose there were another variable y that was aliased to x. Then 
an assignment y = 4 might have the unexpected effect of changing x. It might 
also mean that replacing a = x+3 by a = 5 was a mistake; the proper value of 
a could be 7 there. 
1.6.8 Exercises for Section 1.6 
Exercise 1.6.1 
: 
For the block-structured C code of Fig. 1.13(a), indicate the 
values assigned to w, x, y, and x. 
Exercise 1.6.2 
: 
Repeat Exercise 1.6.1 for the code of Fig. 1.13(b). 
Exercise 1.6.3 
: 
For the block-structured code of Fig. 1.14, assuming the usual 
static scoping of declarations, give the scope for each of the twelve declarations. 
CHAPTER 1. INTRODUCTION 
i n t  w, x, y, z; 
i n t  i = 4; i n t  j = 5; 
( 
i n t  j = 7 ;  
i = 6 ;  
w = i + j ;  
3 
x = i + j ;  
{ 
i n t i = 8 ;  
y = i + j ;  
i n t  w, x, y, z; 
i n t  i = 3; i n t  j = 4; 
( 
i n t i = 5 ;  
w = i + j ;  
3 
x = i + j ;  
( 
i n t  j = 6 ;  
i = 7; 
y = i + j ;  
(a) Code for Exercise 1.6.1 
(b) Code for Exercise 1.6.2 
Figure 1.13: Block-structured code 
C 
i n t  w, x, y, z ;  
/* Block B 1  */ 
C 
i n t x , z ;  
/* Block B2 */ 
( 
i n t  w, x; 
/* Block B3 */ 3 
3 
{ i n t w , x ;  
/* Block B4 */ 
{ 
i n t  y, z ;  
/* Block B5 */ 3 
3 
3 
Figure 1.14: Block structured code for Exercise 1.6.3 
Exercise 1.6.4 
: 
What is printed by the following C code? 
#define a (x+l) 
i n t  x = 2; 
void b
(
)
 ( x = a ;  printf (ll%d\nlf, 
x ) ;  3 
void c ( )  ( i n t  x = 1; printf ("%d\n"), a ;  ) 
void main() ( b(); c ( ) ;  3 
1.7 Summary of Chapter 1 
+ Language Processors. An integrated software development environment 
includes many different kinds of language processors such as compilers, 
interpreters, assemblers, linkers, loaders, debuggers, profilers. 
+ Compiler Phases. A compiler operates as a sequence of phases, each of 
which transforms the source program from one intermediate representa- 
tion to another. 
1.7. SUMMARY OF CHAPTER 1 
+ Machine and Assembly Languages. Machine languages were the first- 
generation programming languages, followed by assembly languages. Pro- 
gramming in these languages was time consuming and error prone. 
+ Modeling in Compiler Design. Compiler design is one of the places where 
theory has had the most impact on practice. Models that have been found 
useful include automata, grammars, regular expressions, trees, and many 
others. 
+ Code Optimization. Although code cannot truly be "optimized," the sci- 
ence of improving the efficiency of code is both complex and very impor- 
tant. It is a major portion of the study of compilation. 
+ Higher-Level Languages. As time goes on, programming languages take 
on progressively more of the tasks that formerly were left to the program- 
mer, such as memory management, type-consistency checking, or parallel 
execution of code. 
+ Compilers and Computer Architecture. Compiler technology influences 
computer architecture, as well as being influenced by the advances in ar- 
chitecture. Many modern innovations in architecture depend on compilers 
being able to extract from source programs the opportunities to use the 
hardware capabilities effectively. 
+ Software Productivity and Software Security. The same technology that 
allows compilers to optimize code can be used for a variety of program- 
analysis tasks, ranging from detecting common program bugs to discov- 
ering that a program is vulnerable to one of the many kinds of intrusions 
that "hackers" have discovered. 
+ Scope Rules. The scope of a declaration of x is the context in which uses 
of x refer to this declaration. A language uses static scope or lexical scope 
if it is possible to determine the scope of a declaration by looking only at 
the program. Otherwise, the language uses dynamic scope. 
+ Environments. The association of names with locations in memory and 
then with values can be described in terms of environments, which map 
names to locations in store, and states, which map locations to their 
values. 
+ Block Structure. Languages that allow blocks to be nested are said to 
have block structure. A name x in a nested block B is in the scope of a 
declaration D of x in an enclosing block if there is no other declaration 
of x in an intervening block. 
+ Parameter Passing. Parameters are passed from a calling procedure to 
the callee either by value or by reference. When large objects are passed 
by value, the values passed are really references to the objects themselves, 
resulting in an effective call-by-reference. 
38 
CHAPTER 1. INTRODUCTION 
+ Aliasing. When parameters are (effectively) passed by reference, two for- 
mal parameters can refer to the same object. This possibility allows a 
change in one variable to change another. 
1.8 References for Chapter 1 
For the development of programming languages that were created and in use 
by 1967, including Fortran, Algol, Lisp, and Simula, see [7]. For languages that 
were created by 1982, including C, C++, Pascal, and Smalltalk, see [I]. 
The GNU Compiler Collection, gcc, is a popular source of open-source 
compilers for C, C-t- +, Fortran, Java, and other languages [2]. Phoenix is a 
compiler-construction toolkit that provides an integrated framework for build- 
ing the program analysis, code generation, and code optimization phases of 
compilers discussed in this book [3]. 
For more information about programming language concepts, we recom- 
mend [5,6]. For more on computer architecture and how it impacts compiling, 
we suggest [4]. 
1. Bergin, T. J. and R. G. Gibson, History of Programming Languages, ACM 
Press, New York, 1996. 
2. http: 
//gcc 
.gnu.org/ 
. 
4. Hennessy, J. L. and D. A. Patterson, Computer Organization and De- 
sign: The Hardware/Software Interface, Morgan-Kaufmann, San Fran- 
cisco, CA, 2004. 
5. Scott, M. L., Programming Language Pragmatics, second edition, Morgan- 
Kaufmann, San Francisco, CA, 2006. 
6. Sethi, R., Programming Languages: Concepts and Constructs, Addison- 
Wesley, 1996. 
7. Wexelblat, R. L., History o
f
 Programming Languages, Academic Press, 
New York, 1981. 
Chapter 2 
A Simple Syntax-Directed 
Translator 
This chapter is an introduction to the compiling techniques in Chapters 3 
through 6 of this book. It illustrates the techniques by developing a working 
Java program that translates representative programming language statements 
into three-address code, an intermediate representation. In this chapter, the 
emphasis is on the front end of a compiler, in particular on lexical analysis, 
parsing, and intermediate code generation. Chapters 7 and 8 show how to 
generate machine instructions from three-address code. 
We start small by creating a syntax-directed translator that maps infix arith- 
metic expressions into postfix expressions. We then extend this translator to 
map code fragments as shown in Fig. 2.1 into three-address code of the form 
in Fig. 2.2. 
The working Java translator appears in Appendix A. The use of Java is 
convenient, but not essential. In fact, the ideas in this chapter predate the 
creation of both Java and C. 
< 
i n t  i ;  i n t  j ;  float[100] a ;  f l o a t  v; f l o a t  x; 
while ( t r u e  ) ( 
do i = i + l ;  while ( a[i] < v ) ;  
do j = j-I; while ( a[j] > v ) ;  
i f  ( i >= j ) break; 
x = a [ i l ;  a [ i l  = a[j]; a[j] = x; 
Figure 2.1: A code fragment to be translated 
39 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Figure 2.2: Simplified intermediate code for the program fragment in Fig. 2.1 
2.1 Introduction 
The analysis phase of a compiler breaks up a source program into constituent 
pieces and produces an internal representation for it, called intermediate code. 
The synthesis phase translates the intermediate code into the target program. 
Analysis is organized around the "syntax" of the language to be compiled. 
The syntax of a programming language describes the proper form of its pro- 
grams, while the semantics of the language defines what its programs mean; that 
is, what each program does when it executes. For specifying syntax, we present 
a widely used notation, called context-free grammars or BNF (for Backus-Naur 
Form) in Section 2.2. With the notations currently available, the semantics of 
a language is much more difficult to describe than the syntax. For specifying 
semantics, we shall therefore use informal descriptions and suggestive examples. 
Besides specifying the syntax of a language, a context-free grammar can be 
used to help guide the translation of programs. In Section 2.3, we introduce 
a grammar-oriented compiling technique known as syntax-directed translation. 
Parsing or syntax analysis is introduced in Section 2.4. 
The rest of this chapter is a quick tour through the model of a compiler 
front end in Fig. 2.3. We begin with the parser. For simplicity, we consider the 
syntax-directed translation of infix expressions to postfix form, a notation in 
which operators appear after their operands. For example, the postfix form of 
the expression 9 - 
5 + 
2 is 95 - 
2+. Translation into postfix form is rich enough 
to illustrate syntax analysis, yet simple enough that the translator is shown in 
full in Section 2.5. The simple translator handles expressions like 9 - 
5 + 2, 
consisting of digits separated by plus and minus signs. One reason for starting 
with such simple expressions is that the syntax analyzer can work directly with 
the individual characters for operators and operands. 
2.1. INTRODUCTION 
Symbol 
1 
Table 
1 
Figure 2.3: A model of a compiler front end 
t 
hree-address 
code 
A lexical analyzer allows a translator to handle multicharacter constructs 
like identifiers, which are written as sequences of characters, but are treated 
as units called tokens during syntax analysis; for example, in the expression 
count 
+ 1, 
the identifier count is treated as a unit. The lexical analyzer in 
Section 2.6 allows numbers, identifiers, and "white space" (blanks, tabs, and 
newlines) to appear within expressions. 
Next, we consider intermediate-code generation. Two forms of intermedi- 
ate code are illustrated in Fig. 2.4. One form, called abstract syntax trees or 
simply syntax trees, represents the hierarchical syntactic structure of the source 
program. In the model in Fig. 2.3, the parser produces a syntax tree, that 
is further translated into three-address code. Some compilers combine parsing 
and intermediate-code generation into one component. 
body 
I 
syntax 
tree 
Parser 
source 
prograz 
assign 
/ \ 
Intermediate 
Code 
Generator 
Figure 2.4: Intermediate code for "do i 
= i 
+ 1 
; while 
( 
a 
[il 
< v) ; 
" 
Y 
~
~
~
i
~
~
l
 
Analyzer 
The root of the abstract syntax tree in Fig. 2.4(a) represents an entire do- 
while loop. The left child of the root represents the body of the loop, which 
consists of only the assignment i 
= i 
+ 1 
; 
. The right child of the root repre- 
sents the condition a 
Cil < v. An implementation of syntax trees appears in 
Section 2.8(a). 
The other common intermediate representation, shown in Fig. 2.4(b), is a 
tokens 
c- 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
sequence of "three-address" instructions; a more complete example appears in 
Fig. 2.2. This form of intermediate code takes its name from instructions of 
the form x = 
y op z, where op is a binary operator, y and z the are addresses 
for the operands, and x is the address for the result of the operation. A three- 
address instruction carries out at most one operation, typically a computation, 
a comparison, or a branch. 
In Appendix A, we put the techniques in this chapter together to build a 
compiler front end in Java. The front end translates statements into assembly- 
level instructions. 
2.2 Syntax Definition 
In this section, we introduce a notation - 
the "context-free grammar," or 
"grammar" for short - 
that is used to specify the syntax of a language. Gram- 
mars will be used throughout this book to organize compiler front ends. 
A grammar naturally describes the hierarchical structure of most program- 
ming language constructs. For example, an if-else statement in Java can have 
the form 
if ( expression ) statement else statement 
That is, an if-else statement is the concatenation of the keyword if, an open- 
ing parenthesis, an expression, a closing parenthesis, a statement, the keyword 
else, and another statement. Using the variable expr to denote an expres- 
sion and the variable stmt to denote a statement, this structuring rule can be 
expressed as 
stmt -+ if ( expr ) stmt else stmt 
in which the arrow may be read as "can have the form." Such a rule is called a 
production. In a production, lexical elements like the keyword if and the paren- 
theses are called terminals. Variables like expr and stmt represent sequences of 
terminals and are called nonterminals. 
2.2.1 Definition of Grammars 
A context-free grammar has four components: 
1. A set of terminal symbols, sometimes referred to as "tokens." The termi- 
nals are the elementary symbols of the language defined by the grammar. 
2. A set of nonterminals, sometimes called "syntactic variables." Each non- 
terminal represents a set of strings of terminals, in a manner we shall 
describe. 
3. A set of productions, where each production consists of a nonterminal, 
called the head or left side of the production, an arrow, and a sequence of 
2.2. SYNTAX DEFINITION 
43 
Tokens Versus Terminals 
In a compiler, the lexical analyzer reads the characters of the source pro- 
gram, groups them into lexically meaningful units called lexemes, and pro- 
duces as output tokens representing these lexemes. A token consists of two 
components, a token name and an attribute value. The token names are 
abstract symbols that are used by the parser for syntax analysis. Often, 
we shall call these token names terminals, since they appear as terminal 
symbols in the grammar for a programming language. The attribute value, 
if present, is a pointer to the symbol table that contains additional infor- 
mation about the token. This additional information is not part of the 
grammar, so in our discussion of syntax analysis, often we refer to tokens 
and terminals synonymously. 
terminals and/or nonterminals, called the body or right side of the produc- 
tion. The intuitive intent of a production is to specify one of the written 
forms of a construct; if the head nonterminal represents a construct, then 
the body represents a written form of the construct. 
4. A designation of one of the nonterminals as the start symbol. 
We specify grammars by listing their productions, with the productions 
for the start symbol listed first. We assume that digits, signs such as < and 
<=, and boldface strings such as while are terminals. An italicized name is a 
nonterminal, and any nonitalicized name or symbol may be assumed to be a 
terminal.' For notational convenience, productions with the same nonterminal 
as the head can have their bodies grouped, with the alternative bodies separated 
by the symbol 1 ,  which we read as "or." 
Example 2.1 : Several examples in this chapter use expressions consisting of 
digits and plus and minus signs; e.g., strings such as 9-5+2, 3-1, or 7. Since a 
plus or minus sign must appear between two digits, we refer to such expressions 
as "lists of digits separated by plus or minus signs." The following grammar 
describes the syntax of these expressions. The productions are: 
list -+ list + digit 
list -+ 
list - digit 
list -+ 
digit 
digit -+ 0 1 1 1 2 ) 3 ) 4 ( 5 1 6 1 7 1 8 1 9  
l~ndividual 
italic letters will be used for additional purposes, especially when grammars 
are studied in detail in Chapter 4. For example, we shall use X, Y, and Z to talk about a 
symbol that is either a terminal or a nonterminal. However, any italicized name containing 
two or more characters will continue to represent a nonterminal. 
44 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
The bodies of the three productions with nonterminal list as head equiva- 
lently can be grouped: 
list + list + digit 1 list - digit I digit 
According to our conventions, the terminals of the grammar are the symbols 
The nonterminals are the italicized names list and digit, with list being the start 
symbol because its productions are given first. 
We say a production is for a nonterminal if the nonterminal is the head of 
the production. A string of terminals is a sequence of zero or more terminals. 
The string of zero terminals, written as E ,  is called the empty string2 
2.2.2 Derivations 
A grammar derives strings by beginning with the start symbol and repeatedly 
replacing a nonterminal by the body of a production for that nonterminal. The 
terminal strings that can be derived from the start symbol form the language 
defined by the grammar. 
Example 2.2 : 
The language defined by the grammar of Example 2.1 consists 
of lists of digits separated by plus and minus signs. The ten productions for the 
nonterminal digit allow it to stand for any of the terminals 0,1,. 
. 
. ,9. From 
production (2.3), a single digit by itself is a list. Productions (2.1) and (2.2) 
express the rule that any list followed by a plus or minus sign and then another 
digit makes up a new list. 
Productions (2.1) to (2.4) are all we need to define the desired language. 
For example, we can deduce that 9-5+2 is a list as follows. 
a) 9 is a list by production (2.3), since 9 is a digit. 
b) 9-5 is a list by production (2.2), since 9 is a list and 5 is a digit. 
c) 9-5+2 is a list by production (2.1), since 9-5 is a list and 2 is a digit. 
Example 2.3 : 
A somewhat different sort of list is the list of parameters in a 
function call. In Java, the parameters are enclosed within parentheses, as in 
the call max(x, y) of function max with parameters x and y. One nuance of such 
lists is that an empty list of parameters may be found between the terminals 
( and ). We may start to develop a grammar for such sequences with the 
productions: 
2~echnically, 
e can be a string of zero symbols from any alphabet (collection of symbols). 
2.2. SYNTAX DEFINITION 
call + id ( optparams ) 
optparams 
-+ 
params I 6 
params 
-+ 
params , 
param I param 
Note that the second possible body for optpamms ("optional parameter list") 
is t, which stands for the empty string of symbols. That is, optparams can be 
replaced by the empty string, so a call can consist of a function name followed 
by the two-terminal string () 
. Notice that the productions for params are 
analogous to those for dist in Example 2.1, with comma in place of the arithmetic 
operator + or -, and param in place of digit. We have not shown the productions 
for param, since parameters are really arbitrary expressions. Shortly, we shall 
discuss the appropriate productions for the various language constructs, such 
as expressions, statements, and so on. 
Parsing is the problem of taking a string of terminals and figuring out how 
to derive it from the start symbol of the grammar, and if it cannot be derived 
from the start symbol of the grammar, then reporting syntax errors within the 
string. Parsing is one of the most fundamental problems in all of compiling; 
the main approaches to parsing are discussed in Chapter 4. In this chapter, for 
simplicity, we begin with source programs like 9-5+2 in which each character 
is a terminal; in general, a source program has multicharacter lexemes that are 
grouped by the lexical analyzer into tokens, whose first components are the 
terminals processed by the parser. 
2.2.3 Parse Trees 
A parse tree pictorially shows how the start symbol of a grammar derives a 
string in the language. If nonterminal A has a production A -+ XYZ, then a 
parse tree may have an interior node labeled A with three children labeled X, 
Y, and Z, from left to right: 
/ 
1 \ 
X
Y
Z
 
Formally, given a context-free grammar, a parse tree according to the gram- 
mar is a tree with the following properties: 
1. The root is labeled by the start symbol. 
2. Each leaf is labeled by a terminal or by e. 
3. Each interior node is labeled by a nonterminal. 
4. If A is the nonterminal labeling some interior node and XI, 
Xz, 
. 
. . , 
Xn are 
the labels of the children of that node from left to right, then there must 
be a production A -+ X1X2 . . 
Xn. Here, X1, X2, 
. 
. 
. , 
X, each stand 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Tree Terminology 
Tree data structures figure prominently in compiling. 
A tree consists of one or more nodes. Nodes may have labels, which 
in this book typically will be grammar symbols. When we draw a 
tree, we often represent the nodes by these labels only. 
Exactly one node is the root. All nodes except the root have a unique 
parent; the root has no parent. When we draw trees, we place the 
parent of a node above that node and draw an edge between them. 
The root is then the highest (top) node. 
If node N is the parent of node M ,  then M is a child of N. The 
children of one node are called siblings. They have an order, from 
the left, and when we draw trees, we order the childen of a given 
node in this manner. 
A node with no children is called a leaf. Other nodes - 
those with 
one or more children - 
are interior nodes. 
A descendant of a node N is either N itself, a child of N, a child of 
a child of N, and so on, for any number of levels. We say node N is 
an ancestor of node M if M is a descendant of N. 
for a symbol that is either a terminal or a nonterminal. As a special case, 
if A -+ c is a production, then a node labeled A may have a single child 
labeled E .  
Example 2.4: The derivation of 9-5+2 in Example 2.2 is illustrated by the 
tree in Fig. 2.5. Each node in the tree is labeled by a grammar symbol. An 
interior node and its children correspond to a production; the interior node 
corresponds to the head of the production, the children to the body. 
In Fig. 2.5, the root is labeled list, the start symbol of the grammar in 
Example 2.1. The children of the root are labeled, from left to right, list, +, 
and digit. Note that 
list -+ 
list + digit 
is a production in the grammar of Example 2.1. The left child of the root is 
similar to the root, with a child labeled - instead of +. The three nodes labeled 
digit each have one child that is labeled by a digit. 
From left to right, the leaves of a parse tree form the yield of the tree, which 
is the string generated or derived from the nonterminal at the root of the parse 
2.2. SYNTAX DEFINITION 
list 
list -----' 1 ' d i g i t  
Figure 2.5: Parse tree for 9-5+2 according to the grammar in Example 2.1 
list / 
I 
digit 
I 
tree. In Fig. 2.5, the yield is 9-5+2; for convenience, all the leaves are shown 
at the bottom level. Henceforth, we shall not necessarily line up the leaves in 
this way. Any tree imparts a natural left-to-right order to its leaves, based on 
the idea that if X and Y are two children with the same parent, and X is to 
the left of Y, then all descendants of X are to the left of descendants of Y. 
Another definition of the language generated by a grammar is as the set of 
strings that can be generated by some parse tree. The process of finding a parse 
tree for a given string of terminals is called parsing that string. 
\ 
digit 
2.2.4 Ambiguity 
We have to be careful in talking about the structure of a string according to a 
grammar. A grammar can have more than one parse tree generating a given 
string of terminals. Such a grammar is said to be ambiguous. To show that a 
grammar is ambiguous, all we need to do is find a terminal string that is the 
yield of more than one parse tree. Since a string with more than one parse tree 
usually has more than one meaning, we need to design unambiguous grammars 
for compiling applications, or to use ambiguous grammars with additional rules 
to resolve the ambiguities. 
Example 2.5 : Suppose we used a single nonterminal string and did not dis- 
tinguish between digits and lists, as in Example 2.1. We could have written the 
grammar 
string -+ string + string I s t k g  - string 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 / 9 
Merging the notion of digit and list into the nonterminal string makes superficial 
sense, because a single digit is a special case of a list. 
However, Fig. 2.6 shows that an expression like 9-5+2 has more than one 
parse tree with this grammar. The two trees for 9-5+2 correspond to the two 
ways of parenthesizing the expression: (9-5) +2 and 9- (5+2) 
. This second 
parenthesization gives the expression the unexpected value 2 rather than the 
customary value 6. The grammar of Example 2.1 does not permit this inter- 
pretation. 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
string 
/ I \  
string 
+ 
string 
/ 
I \ 
I 
string 
- 
string 
2 
I 
I 
9 
5 
string 
- 
string 
I 
/
I
\
 
9 
string 
+ 
string 
I  
I 
5 
2 
Figure 2.6: Two parse trees for 9-5+2 
2.2.5 Associativity of Operators 
By convention, 9+5+2 
is equivalent to (9+5)+2 
and 9-5-2 
is equivalent to 
(9-5)-2. When an operand like 5 
has operators to its left and right, con- 
ventions are needed for deciding which operator applies to that operand. We 
say that the operator + associates 
to the left, because an operand with plus signs 
on both sides of it belongs to the operator to its left. In most programming 
languages the four arithmetic operators, addition, subtraction, multiplication, 
and division are left-associative. 
Some common operators such as exponentiation are right-associative. As 
another example, the assignment operator = in C and its descendants is right- 
associative; that is, the expression a=b=c 
is treated in the same way as the 
expression a= 
(b=c) 
. 
Strings like a=b=c 
with a right-associative operator are generated by the 
following grammar: 
right + letter = right I letter 
letter 
-+ 
a 
I b 
I . 
1 z 
The contrast between a parse tree for a left-associative operator like - and 
a parse tree for a right-associative operator like = is shown by Fig. 2.7. Note 
that the parse tree for 9-5-2 
grows down towards the left, whereas the parse 
tree for a=b=c 
grows down towards the right. 
2.2.6 Precedence of Operators 
Consider the expression 9+5*2. 
There are two possible interpretations of this 
expression: (9+5) 
*2 
or 9+ 
(5*2). The associativity rules for + and * apply to 
occurrences of the same operator, so they do not resolve this ambiguity. Rules 
defining the relative precedence of operators are needed when more than one 
kind of operator is present. 
We say that * has higher precedence than + if * takes its operands before + 
does. In ordinary arithmetic, multiplication and division have higher precedence 
than addition and subtraction. Therefore, 5 
is taken by * in both 9+5*2 
and 
9*5+2; 
i.e., the expressions are equivalent to 9+ 
(5*2) 
and (9*5) 
+2, 
respectively. 
2.2. SYNTAX DEFINITION 
list 
- 
digit 
/ 
I \ 
I 
list 
- 
digit 
2 
I 
I 
digit 
5 
I 
9 
letter 
= 
right 
I 
/ 
I \ 
a 
letter 
= 
right 
I 
I 
b 
letter 
Figure 2.7: Parse trees for left- and right-associative grammars 
Example 2.6 : 
A grammar for arithmetic expressions can be constructed from 
a table showing the associativity and precedence of operators. We start with 
the four common arithmetic operators and a precedence table, showing the 
operators in order of increasing precedence. Operators on the same line have 
the same associativity and precedence: 
left-associative: 
+ - 
left-associative: * / 
We create two nonterminals expr and term for the two levels of precedence, 
and an extra nonterminal factor for generating basic units in expressions. The 
basic units in expressions are presently digits and parenthesized expressions. 
factor + digit I ( expr ) 
Now consider the binary operators, * and /, that have the highest prece- 
dence. Since these operators associate to the left, the productions are similar 
to those for lists that associate to the left. 
term + term * factor 
I 
term / factor 
I 
factor 
Similarly, expr generates lists of terms separated by the additive operators. 
expr + expr + term 
I 
expr - term 
I 
term 
The resulting grammar is therefore 
expr 
-+ 
expr + term ( expr - term ( term 
term -+ 
term * factor I term / factor 
( factor 
factor 
--+ 
digit 1 ( expr ) 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Generalizing the Expression Grammar of Example 2.6 
We can think of a factor as an expression that cannot be "torn apart" by 
any operator. By "torn apart," we mean that placing an operator next 
to any factor, on either side, does not cause any piece of the factor, other 
than the whole, to become an operand of that operator. If the factor is a 
parenthesized expression, the parentheses protect against such "tearing," 
while if the factor is a single operand, it cannot be torn apart. 
A term (that is not also a factor) is an expression that can be torn 
apart by operators of the highest precedence: * and /, but not by the 
lower-precedence operators. An expression (that is not a term or factor) 
can be torn apart by any operator. 
We can generalize this idea to any number n of precedence levels. We 
need n+ 1 
nonterminals. The first, like factor in Example 2.6, can never be 
torn apart. Typically, the production bodies for this nonterminal are only 
single operands and parenthesized expressions. Then, for each precedence 
level, there is one nonterminal representing expressions that can be torn 
apart only by operators at that level or higher. Typically, the productions 
for this nonterminal have bodies representing uses of the operators at that 
level, plus one body that is just the nonterminal for the next higher level. 
With this grammar, an expression is a list of terms separated by either + or 
- signs, and a term is a list of factors separated by * or / signs. Notice that 
any parenthesized expression is a factor, so with parentheses we can develop 
expressions that have arbitrarily deep nesting (and arbitrarily deep trees). 
Example 2.7: Keywords allow us to recognize statements, since most state- 
ment begin with a keyword or a special character. Exceptions to this rule 
include assignments and procedure calls. The statements defined by the (am- 
biguous) grammar in Fig. 2.8 are legal in Java. 
In the first production for stmt, the terminal id represents any identifier. 
The productions for expression are not shown. The assignment statements 
specified by the first production are legal in Java, although Java treats = as an 
assignment operator that can appear within an expression. For example, Java 
allows a 
= 
b = c, which this grammar does not. 
The nonterminal stmts generates a possibly empty list of statements. The 
second production for stmts generates the empty list E .  The first production 
generates a possibly empty list of statements followed by a statement. 
The placement of semicolons is subtle; they appear at the end of every body 
that does not end in stmt. This approach prevents the build-up of semicolons 
after statements such as if- and while-, which end with nested substatements. 
When the nested substatement is an assignment or a do-while, a semicolon will 
be generated as part of the substatement. 
2.2. SYNTAX DEFINITION 
stmt -+ 
id = expression ; 
( 
if ( expression ) stmt 
I 
if ( expression ) stmt else stmt 
I 
while ( expression ) stmt 
I 
do stmt while ( expression ) ; 
I 
C stmts 3 
stmts 
-+ 
stmts stmt 
I 
t
:
 
Figure 2.8: A grammar for a subset of Java statements 
2.2.7 Exercises for Section 2.2 
Exercise 2.2 
.I : 
Consider the context-free grammar 
S -+ S S +  
I S S *  1 a 
a) Show how the string aa+a* can be generated by this grammar. 
b) Construct a parse tree for this string. 
c) What language does this grammar generate? Justify your answer. 
Exercise 2.2.2 : 
What language is generated by the following grammars? In 
each case justify your answer. 
Exercise 2.2.3 : 
Which of the grammars in Exercise 2.2.2 are ambiguous? 
Exercise 2.2.4 : Construct unambiguous context-free grammars for each of 
the following languages. In each case show that your grammar is correct. 
a) Arithmetic expressions in postfix notation. 
b) Left-associative lists of identifiers separated by commas. 
c) Right-associative lists of identifiers separated by commas. 
d) Arithmetic expressions of integers and identifiers with the four binary 
operators +, -, *, /. 
52 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
! 
e) Add unary plus and minus to the arithmetic operators of (d). 
Exercise 2.2.5 : 
a) Show that all binary strings generated by the following grammar have 
values divisible by 3. Hint. Use induction on the number of nodes in a 
parse tree. 
num += 
I1 I 1001 I numO I numnum 
b) Does the grammar generate all binary strings with values divisible by 3? 
Exercise 2.2.6 : 
Construct a context-free grammar for roman numerals. 
2.3 Syntax-Directed Translation 
Syntax-directed translation is done by attaching rules or program fragments to 
productions in a grammar. For example, consider an expression expr generated 
by the production 
expr += expr, + term 
Here, expr is the sum of the two subexpressions expr, and term. (The subscript 
in expr, is used only to distinguish the instance of expr in the production body 
from the head of the production). We can translate expr by exploiting its 
structure, as in the following pseudo-code: 
translate expr, ; 
translate term; 
handle +; 
Using a variant of this pseudocode, we shall build a syntax tree for expr in 
Section 2.8 by building syntax trees for exprl and term and then handling + by 
constructing a node for it. For convenience, the example in this section is the 
translation of infix expressions into postfix notation. 
This section introduces two concepts related to syntax-directed translation: 
Attributes. An attribute is any quantity associated with a programming 
construct. Examples of attributes are data types of expressions, the num- 
ber of instructions in the generated code, or the location of the first in- 
struction in the generated code for a construct, among many other pos- 
sibilities. Since we use grammar symbols (nonterminals and terminals) 
to represent programming constructs, we extend the notion of attributes 
from constructs to the symbols that represent them. 
2.3. SYNTAX-DIRECTED TRANSLATION 
53 
(Syntax-directed) translation schemes. A translation scheme is a notation 
for attaching program fragments to the productions of a grammar. The 
program fragments are executed when the production is used during syn- 
tax analysis. The combined result of all these fragment executions, in 
the order induced by the syntax analysis, produces the translation of the 
program to which this analysis/synthesis process is applied. 
Syntax-directed translations will be used throughout this chapter to trans- 
late infix expressions into postfix notation, to evaluate expressions, and to build 
syntax trees for programming constructs. A more detailed discussion of syntax- 
directed formalisms appears in Chapter 5 .  
2.3.1 Postfix Notation 
The examples in this section deal with translation into postfix notation. The 
postfix notation for an expression E can be defined inductively as follows: 
1. If E is a variable or constant, then the postfix notation for E is E itself. 
2. If E is an expression of the form El op E2, 
where op is any binary 
operator, then the postfix notation for E is Ei E
h
 op, where Ei and E
h
 
are the postfix notations for El and E2, 
respectively. 
3. If E is a parenthesized expression of the form (El), then the postfix 
notation for E is the same as the postfix notation for El. 
Example 2.8 : 
The postfix notation for (9-5)+2 is 95-2+. That is, the trans- 
lations of 9, 5, and 2 are the constants themselves, by rule (1). Then, the 
translation of 9-5 is 95- by rule (2). The translation of (9-5) is the same 
by rule (3). Having translated the parenthesized subexpression, we may apply 
rule (2) to the entire expression, with (9-5) in the role of El and 2 in the role 
of E2, 
to get the result 95-2+. 
As another example, the postfix notation for 9- (5+2) is 952+-. That is, 5+2 
is first translated into 52+, and this expression becomes the second argument 
of the minus sign. 
No parentheses are needed in postfix notation, because the position and 
arity (number of arguments) of the operators permits only one decoding of a 
postfix expression. The "trick" is to repeatedly scan the postfix string from the 
left, until you find an operator. Then, look to the left for the proper number 
of operands, and group this operator with its operands. Evaluate the operator 
on the operands, and replace them by the result. Then repeat the process, 
continuing to the right and searching for another operator. 
Example 2.9 : Consider the postfix expression 952+-3*. Scanning from the 
left, we first encounter the plus sign. Looking to its left we find operands 5 and 
2. Their sum, 7, replaces 52+, and we have the string 97-3*. Now, the leftmost 
54 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
operator is the minus sign, and its operands are 9 and 7. Replacing these by 
the result of the subtraction leaves 23*. Last, the multiplication sign applies to 
2 and 3, giving the result 6. 
2.3.2 
Synthesized Attributes 
The idea of associating quantities with programming constructs-for 
example, 
values and types with expressions-can be expressed in terms of grammars. We 
associate attributes with nonterminals and terminals. Then, we attach rules to 
the productions of the grammar; these rules describe how the attributes are 
computed at those nodes of the parse tree where the production in question is 
used to relate a node to its children. 
A syntax-directed definition associates 
1. With each grammar symbol, a set of attributes, and 
2. With each production, a set of semantic rules for computing the values of 
the attributes associated with the symbols appearing in the production. 
Attributes can be evaluated as follows. For a given input string x, construct 
a parse tree for x. Then, apply the semantic rules to evaluate attributes at each 
node in the parse tree, as follows. 
Suppose a node N in a parse tree is labeled by the grammar symbol X .  We 
write X.a to denote the value of attribute a of X at that node. A parse tree 
showing the attribute values at each node is called an annotated parse tree. For 
example, Fig. 2.9 shows an annotated parse tree for 9-5+2 with an attribute 
t associated with the nonterminals expr and term. The value 95-2+ of the 
attribute at the root is the postfix notation for 9-5+2. We shall see shortly how 
these expressions are computed. 
Figure 2.9: Attribute values at nodes in a parse tree 
An attribute is said to be synthesized 
if its value at a parse-tree node N is de- 
termined from attribute values at the children of N and at N itself. Synthesized 
2.3. SYNTAX-DIRECTED TRANSLATION 
55 
attributes have the desirable property that they can be evaluated during a sin- 
gle bottom-up traversal of a parse tree. In Section 5.1.1 
we shall discuss another 
important kind of attribute: the "inherited" attribute. Informally, inherited at- 
tributes have their value at a parse-tree node determined from attribute values 
at the node itself, its parent, and its siblings in the parse tree. 
Example 2.10 : 
The annotated parse tree in Fig. 2.9 is based on the syntax- 
directed definition in Fig. 2.10 for translating expressions consisting of digits 
separated by plus or minus signs into postfix notation. Each nonterminal has a 
string-valued attribute t that represents the postfix notation for the expression 
generated by that nonterminal in a parse tree. The symbol 1 1  in the semantic 
rule is the operator for string concatenation. 
expr -+ exprl + term 
expr -+ exprl - term 
expr -+ term 
term -+ 0 
term -+ I 
. 
. . 
term -+ 9 
expr.t = exprl.t 1 1  term.t 1 1  '+I 
expr.t = exprl.t )I term.t 1 1  I-' 
expr.t = term.t 
term.t = '0' 
term.t = 'I' 
. . 
. 
term.t = '9' 
Figure 2.10: Syntax-directed definition for infix to postfix translation 
The postfix form of a digit is the digit itself; e.g., the semantic rule associ- 
ated with the production term -+ 9 defines term.t to be 9 itself whenever this 
production is used at a node in a parse tree. The other digits are translated 
similarly. As another example, when the production expr -+ term is applied, 
the value of term.t becomes the value of expr.t. 
The production expr -+ 
exprl + term derives an expression containing a plus 
operator.3 The left operand of the plus operator is given by expr, and the right 
operand by term. The semantic rule 
associated with this production constructs the value of attribute expr.t by con- 
catenating the postfix forms expr, 
.t and term.t of the left and right operands, 
respectively, and then appending the plus sign. This rule is a formalization of 
the definition of "postfix expression." 
3 ~ n  
this and many other rules, the same nonterminal (expr, here) appears several times. 
The purpose of the subscript 1 in exprl is to distinguish the two occurrences of expr in the 
production; the "1" is not part of the nonterminal. See the box on "Convention Distinguishing 
Uses of a Nonterminal" for more details. 
56 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Convention Distinguishing Uses of a Nonterminal 
In rules, we ~ftem 
have a need to distinguish among several uses of the 
same nonterminal in the head and/or body of a production; e.g., see Ex- 
ample 2.10. The reason is that in the parse tree, different nodes labeled 
by the same nonterminal usually have different values for their transla- 
tions. We shall adopt the following convention: the nonterminal appears 
unsubscripted in the head and with distinct subscripts in the body. These 
are all occurrences of the same nonterminal, and the subscript is not part 
of its name. However, the reader should be alert to the difference be- 
tween examples of specific translations, where this convention is used, and 
generic productions like A -+ X1X2,. 
. 
. , 
X,, where the subscripted X's 
represent an arbitrary list of grammar symbols, and are not instances of 
one particular nonterminal called X. 
2.3.3 
Simple Syntax-Directed Definitions 
The syntax-directed definition in Example 2.10 has the following important 
property: the string representing the translation of the nonterminal at the head 
of each production is the concatenation of the translations of the nonterminals 
in the production body, in the same order as in the production, with some 
optional additional strings interleaved. A syntax-directed definition with this 
property is termed simple. 
Example 2.1 
1 : 
Consider the first production and semantic rule from Fig. 2.10: 
PRODUCTION 
SEMANTIC 
RULE 
expr -+ 
exprl + term 
expr.t = expr, .t I I term.t I I '+' 
(2.5) 
Here the translation expr.t is the concatenation of the translations of expr, and 
term, followed by the symbol +. Notice that exprl and term appear in the 
same order in both the production body and the semantic rule. There are no 
additional symbols before or between their translations. In this example, the 
only extra symbol occurs at the end. 
When translation schemes are discussed, we shall see that a simple syntax- 
directed definition can be implemented by printing only the additional strings, 
in the order they appear in the definition. 
2.3.4 
Tree Traversals 
Tree traversals will be used for describing attribute evaluation and for specifying 
the execution of code fragments in a translation scheme. A traversal of a tree 
starts at the root and visits each node of the tree in some order. 
2.3. SYNTAX-DIRE 
CTED TRANSLATION 
A depth-first traversal starts at the root and recursively visits the children 
of each node in any order, not necessarily from left to right. It is called "depth- 
first" because it visits an unvisited child of a node whenever it can, so it visits 
nodes as far away from the root (as "deep") as quickly as it can. 
The procedure visit(N) in Fig. 2.11 is a depth first traversal that visits the 
children of a node in left-to-right order, as shown in Fig. 2.12. In this traversal, 
we have included the action of evaluating translations at each node, just before 
we finish with the node (that is, after translations at the children have surely 
been computed). In general, the actions associated with a traversal can be 
whatever we choose, or nothing at all. 
procedure visit(node N) { 
for ( each child C of N, from left to right ) { 
visit (C) 
; 
1 
evaluate semantic rules at node N; 
Figure 2.11: A depth-first traversal of a tree 
Figure 2.12: Example of a depth-first traversal of a tree 
A syntax-directed definition does not impose any specific order for the eval- 
uation of attributes on a parse tree; any evaluation order that computes an 
attribute a after all the other attributes that a depends on is acceptable. Syn- 
thesized attributes can be evaluated during any bottom-up traversal, that is, a 
traversal that evaluates attributes at a node after having evaluated attributes 
at its children. In general, with both synthesized and inherited attributes, the 
matter of evaluation order is quite complex; see Section 5.2. 
2.3.5 Translation Schemes 
The syntax-directed definition in Fig. 2.10 builds up a translation by attaching 
strings as attributes to the nodes in the parse tree. We now consider an alter- 
native approach that does not need to manipulate strings; it produces the same 
translation increment ally, by executing program fragments. 
58 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Preorder and Postorder Traversals 
Preorder and postorder traversals are two important special cases of depth- 
first traversals in which we visit the children of each node from left to right. 
Often, we traverse a tree to perform some particular action at each 
node. If the action is done when we first visit a node, then we may refer 
to the traversal as a preorder traversal. Similarly, if the action is done 
just before we leave a node for the last time, then we say it is a postorder 
traversal of the tree. The procedure visit(N) in Fig. 2.11 is an example of 
a postorder traversal. 
Preorder and postorder traversals define corresponding orderings on 
nodes, based on when the action at a node would be performed. The 
preorder of a (sub)tree rooted at node N consists of N, followed by the 
preorders of the subtrees of each of its children, if any, from the left. The 
postorder of a (sub)tree rooted at N consists of the postorders of each of 
the subtrees for the children of N, if any, from the left, followed by N 
itself. 
A syntax-directed translation scheme is a notation for specifying a transla- 
tion by attaching program fragments to productions in a grammar. A transla- 
tion scheme is like a syntax-directed definition, except that the order of evalu- 
ation of the semantic rules is explicitly specified. 
Program fragments embedded within production bodies are called semantic 
actions. The position at which an action is to be executed is shown by enclosing 
it between curly braces and writing it within the production body, as in 
rest -+ + term {print('+')) restl 
We shall see such rules when we consider an alternative form of grammar for 
expressions, where the nonterminal rest represents "everything but the first 
term of an expression." This form of grammar is discussed in Section 2.4.5. 
Again, the subscript in restl distinguishes this instance of nonterminal rest in 
the production body from the instance of rest at the head of the production. 
When drawing a parse tree for a translation scheme, we indicate an action 
by constructing an extra child for it, connected by a dashed line to the node 
that corresponds to the head of the production. For example, the portion of 
the parse tree for the above production and action is shown in Fig. 2.13. The 
node for a semantic action has no children, so the action is performed when 
that node is first seen. 
Example 2.12: The parse tree in Fig. 2.14 has print statements at extra 
leaves, which are attached by dashed lines to interior nodes of the parse tree. 
The translation scheme appears in Fig. 2.15. The underlying grammar gen- 
erates expressions consisting of digits separated by plus and minus signs. The 
2.3. SYNTAX-DIRECTED TRANSLATION 
Figure 2.13: An extra leaf is constructed for a semantic action 
actions embedded in the production bodies translate such expressions into post- 
fix notation, provided we perform a left-to-right depth-first traversal of the tree 
and execute each print statement when we visit its leaf. 
xprK. 
\ \ . 
\ 
+ 
term 
iprint 
('+I)) 
A e x 7 - -  
---- 
-- . 
/ \ \ \ \  
expr 
- 
term 
{print 
(I-') 
2 
{print ('2')) 
I 
/ \ \ \ \  
term 
5 
{print 
('5')) 
/ \ \ \ \  
9 
{print 
('9')) 
Figure 2.14: Actions translating 9-5+2 into 95-2+ 
expr 
-+ 
expr, + term 
{print('+')) 
expr -+ expr, - term 
{print('-')) 
e
x
 + term 
term + 0 
{print 
('0')) 
term + 1 
{print 
(' 1')) 
... 
term 
-+ 
9 
{print 
('9')) 
Figure 2.15: Actions for translating into postfix notation 
The root of Fig. 2.14 represents the first production in Fig. 2.15. In a 
postorder traversal, we first perform all the actions in the leftmost subtree of 
the root, for the left operand, also labeled expr like the root. We then visit the 
leaf + at which there is no action. We next perform the actions in the subtree 
for the right operand term and, finally, the semantic action {print('+') ) at the 
extra node. 
Since the productions for term have only a digit on the right side, that digit 
is printed by the actions for the productions. No output is necessary for the 
production expr + term, and only the operator needs to be printed in the 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
action for each of the first two productions. When executed during a postorder 
traversal of the parse tree, the actions in Fig. 2.14 print 95-2+. 
Note that although the schemes in Fig. 2.10 and Fig. 2.15 produce the same 
translation, they construct it differently; Fig. 2.10 attaches strings as attributes 
to the nodes in the parse tree, while the scheme in Fig. 2.15 prints the translation 
incrementally, through semantic actions. 
The semantic actions in the parse tree in Fig. 2.14 translate the infix ex- 
pression 9-5+2 into 95-2+ by printing each character in 9-5+2 exactly once, 
without using any storage for the translation of subexpressions. When the out- 
put is created incrementally in this fashion, the order in which the characters 
are printed is significant. 
The implementation of a translation scheme must ensure that semantic ac- 
tions are performed in the order they would appear during a postorder traversal 
of a parse tree. The implementation need not actually construct a parse tree 
(often it does not), as long as it ensures that the semantic actions are per- 
formed as if we constructed a parse tree and then executed the actions during 
a postorder traversal. 
2.3.6 Exercises for Section 2.3 
Exercise 2.3.1 : Construct a syntax-directed translation scheme that trans- 
lates arithmetic expressions from infix notation into prefix notation in which an 
operator appears before its operands; e.g., -xy is the prefix notation for x - 
y. 
Give annotated parse trees for the inputs 9-5+2 and 9-5*2. 
Exercise 2.3.2 : Construct a syntax-directed translation scheme that trans- 
lates arithmetic expressions from postfix notation into infix notation. Give 
annotated parse trees for the inputs 95-2* and 952*-. 
Exercise 2.3.3 : Construct a syntax-directed translation scheme that trans- 
lates integers into roman numerals. 
Exercise 2.3.4 : Construct a syntax-directed translation scheme that trans- 
lates roman numerals into integers. 
Exercise 2.3.5 : Construct a syntax-directed translation scheme that trans- 
lates postfix arithmetic expressions into equivalent infix arithmetic expressions. 
2.4 Parsing 
Parsing is the process of determining how a string of terminals can be generated 
by a grammar. In discussing this problem, it is helpful to think of a parse tree 
being constructed, even though a compiler may not construct one, in practice. 
However, a parser must be capable of constructing the tree in principle, or else 
the translation cannot be guaranteed correct. 
2.4. PARSING 
This section introduces a parsing method called "recursive descent," which 
can be used both to parse and to implement syntax-directed translators. A com- 
plete Java program, implementing the translation scheme of Fig. 2.15, appears 
in the next section. A viable alternative is to use a software tool to generate 
a translator directly from a translation scheme. Section 4.9 describes such a 
tool - 
Yacc; it can implement the translation scheme of Fig. 2.15 without 
modification. 
For any context-free grammar there is a parser that takes at most O(n3) 
time to parse a string of n terminals. But cubic time is generally too expen- 
sive. Fortunately, for real programming languages, we can generally design a 
grammar that can be parsed quickly. Linear-time algorithms suffice to parse 
essentially all languages that arise in practice. Programming-language parsers 
almost always make a single left-to-right scan over the input, looking ahead one 
terminal at a time, and constructing pieces of the parse tree as they go. 
Most parsing methods fall into one of two classes, called the top-down and 
bottom-up methods. These terms refer to the order in which nodes in the parse 
tree are constructed. In top-down parsers, construction starts at the root and 
proceeds towards the leaves, while in bottom-up parsers, construction starts at 
the leaves and proceeds towards the root. The popularity of top-down parsers 
is due to the fact that efficient parsers can be constructed more easily by hand 
using top-down methods. Bottom-up parsing, however, can handle a larger class 
of grammars and translation schemes, so software tools for generating parsers 
directly from grammars often use bottom-up methods. 
2.4.1 Top-Down Parsing 
We introduce top-down parsing by considering a grammar that is well-suited 
for this class of methods. Later in this section, we consider the construction 
of top-down parsers in general. The grammar in Fig. 2.16 generates a subset 
of the statements of C or Java. We use the boldface terminals if and for for 
the keywords "if" and "for", respectively, to emphasize that these character 
sequences are treated as units, i.e., as single terminal symbols. Further, the 
terminal expr represents expressions; a more complete grammar would use a 
nonterminal expr and have productions for nonterminal expr. Similarly, other 
is a terminal representing other statement constructs. 
The top-down construction of a parse tree like the one in Fig. 2.17, is done 
by starting with the root, labeled with the starting nonterminal stmt, and re- 
peatedly performing the following two steps. 
1. At node N, labeled with nonterminal A, select one of the productions for 
A and construct children at N for the symbols in the production body. 
2. Find the next node at which a subtree is to be constructed, typically the 
leftmost unexpanded nonterminal of the tree. 
For some grammars, the above steps can be implemented during a single 
left-to-right scan of the input string. The current terminal being scanned in the 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
stmt + expr ; 
1 
if ( expr ) stmt 
I 
for ( optexpr ; optexpr ; optexpr ) st& 
1 
other 
optexpr + 
E: 
I 
expr 
Figure 2.16: A grammar for some statements in C and Java 
A
t
%
\
\
 
for 
( 
optexpr ; optexpr ; optexpr 
stmt 
E 
expr 
expr 
other 
Figure 2.17: A parse tree according to the grammar in Fig. 2.16 
input is frequently referred to as the lookahead symbol. Initially, the lookahead 
symbol is the first, i.e., leftmost, terminal of the input string. Figure 2.18 
illustrates the construction of the parse tree in Fig. 2.17 for the input string 
for ( ; expr ; expr ) other 
Initially, the terminal for is the lookahead symbol, and the known part of the 
parse tree consists of the root, labeled with the starting nonterminal stmt in 
Fig. 2.18(a). The objective is to construct the remainder of the parse tree in 
such a way that the string generated by the parse tree matches the input string. 
For a match to occur, the nonterminal stmt in Fig. 2.18(a) must derive a 
string that starts with the lookahead symbol for. In the grammar of Fig. 2.16, 
there is just one production for stmt that can derive such a string, so we select it, 
and construct the children of the root labeled with the symbols in the production 
body. This expansion of the parse tree is shown in Fig. 2.18(b). 
Each of the three snapshots in Fig. 2.18 has arrows marking the lookahead 
symbol in the input and the node in the parse tree that is being considered. 
Once children are constructed at a node, we next consider the leftmost child. In 
Fig. 2.18(b), children have just been constructed at the root, and the leftmost 
child labeled with for is being considered. 
When the node being considered in the parse tree is for a terminal, and 
the terminal matches the lookahead symbol, then we advance in both the parse 
tree and the input. The next terminal in the input becomes the new lookahead 
symbol, and the next child in the parse tree is considered. In Fig. 2.18(c), the 
arrow in the parse tree has advanced to the next child of the root, and the arrow 
2.4. PARSING 
63 
PARSE 
stmt 
TREE 
$
.
 
(4 
INPUT 
for 
( 
; 
expr 
; 
expr 
) 
other 
4 
PARSE 
TREE 
4 
for 
( 
optexpr ; optexpr ; optexpr 
) 
stmt 
INPUT 
for 
( 
; 
expr 
; 
expr 
) 
other 
4 
PARSE 
TREE 
optexpr ; optexpr ; optexpr 
stmt 
INPUT 
; 
expr 
; 
expr 
other 
Figure 2.18: Top-down parsing while scanning the input from left to right 
in the input has advanced to the next terminal, which is (. A further advance 
will take the arrow in the parse tree to the child labeled with nonterminal 
optexpr and take the arrow in the input to the terminal ; 
. 
At the nonterminal node labeled optexpr, we repeat the process of selecting a 
production for a nonterminal. Productions with t as the body ( “e-productions7') 
require special treatment. For the moment, we use them as a default when 
no other production can be used; we return to them in Section 2.4.3. With 
nonterminal optexpr and lookahead ; 
, the €-production is used, since ; does 
not match the only other production for optexpr, which has terminal expr as 
its body. 
In general, the selection of a production for a nonterminal may involve trial- 
and-error; that is, we may have to try a production and backtrack to try another 
production if the first is found to be unsuitable. A production is unsuitable 
if, after using the production, we cannot complete the tree to match the input 
64 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
string. Backtracking is not needed, however, in an important special case called 
predictive parsing, which we discuss next. 
2.4.2 
Predictive Parsing 
Recursive-descent parsing is a top-down method of syntax analysis in which 
a set of recursive procedures is used to process the input. One procedure is 
associated with each nonterminal of a grammar. Here, we consider a simple form 
of recursive-descent parsing, called predictive parsing, in which the lookahead 
symbol unambiguously determines the flow of control through the procedure 
body for each nonterminal. The sequence of procedure calls during the analysis 
of an input string implicitly defines a parse tree for the input, and can be used 
to build an explicit parse tree, if desired. 
The predictive parser in Fig. 2.19 consists of procedures for the nontermi- 
nals stmt and optexpr of the grammar in Fig. 2.16 and an additional procedure 
match, used to simplify the code for stmt and optexpr. Procedure match(t) com- 
pares its argument t with the lookahead symbol and advances to the next input 
terminal if they match. Thus match changes the value of variable lookahead, a 
global variable that holds the currently scanned input terminal. 
Parsing begins with a call of the procedure for the starting nonterminal stmt. 
With the same input as in Fig. 2.18, lookahead is initially the first terminal for. 
Procedure stmt executes code corresponding to the production 
stmt -+ 
for ( optexpr ; optexpr ; optexpr ) stmt 
In the code for the production body - 
that is, the for case of procedure stmt - 
each terminal is matched with the lookahead symbol, and each nonterminal 
leads to a call of its procedure, in the following sequence of calls: 
match(for) 
; match(' (I); 
O P ~ ~ X P ~ ( ) ;  
match(';'); optexpr(); match(';'); optexpr(); 
match ('1 
') ; stmt 
() 
; 
Predictive parsing relies on information about the first symbols that can be 
generated by a production body. More precisely, let a be a string of grammar 
symbols (terminals and/or nonterminals). We define  FIRST(^) tb be the set of 
terminals that appear as the first symbols of one or more strings of terminals 
generated from a .  If a is E or can generate t, then t is also in FIRST(&). 
The details of how one computes  FIRST(^) are in Section 4.4.2. Here, we 
shall just use ad hoc reasoning to deduce the symbols in  FIRST(^); typically, a 
will either begin with a terminal, which is therefore the only symbol in  FIRST(^), 
or a will begin with a nonterminal whose production bodies begin with termi- 
nals, in which case these terminals are the only members of  FIRST(^). 
For example, with respect to the grammar of Fig. 2.16, the following are 
correct calculations of FIRST. 
2.4. PARSING 
void stmt() { 
switch ( lookahead ) { 
case expr: 
match(expr); match(' ; 
'); break; 
case if: 
match(if); match(' (I); 
match (expr); match(') '); stmt 
(); 
break; 
case for: 
match (for); match (' (') ; 
optexpr (); match(' ; 
I ) ;  optexpr(); match(' ; 
'); optexpr(); 
match(') '); stmt 
(); break; 
case other; 
match (ot 
her) 
; break; 
default: 
report ("synt 
ax error"); 
) 
1 
void optexpro { 
if ( lookahead == expr ) match(expr); 
1 
void match(termina1 t) { 
if ( Eookahead == t ) Eookahead = nextTermina1; 
else report ("syntax error 
If); 
1 
Figure 2.19: Pseudocode for a predictive parser 
 FIRST(^^^^) 
= {expr, if, for, other) 
FIRST(expr ; 
) 
= {expr} 
The FIRST sets must be considered if there are two productions A -+ a, and 
A -+ p. Ignoring E-productions for the moment, predictive parsing requires 
FIRST@!) and  FIRST(,^) to be disjoint. The lookahead symbol can then be used 
to decide which production to use; if the lookahead symbol is in FIRST(Q), 
then 
a, is used. Otherwise, if the lookahead symbol is in FIRST@), 
then /3 is used. 
2.4.3 When to Use c-Productions 
Our predictive parser uses an r-production as a default when no other produc- 
tion can be used. With the input of Fig. 2.18, after the terminals for and ( are 
matched, the lookahead symbol is ; 
. At this point procedure optexpr is called, 
and the code 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
if ( lookahead == expr ) match(expr); 
in its body is executed. Nonterminal optexpr has two productions, with bodies 
expr and E .  The lookahead symbol ";" 
does not match the terminal expr, so 
the production with body expr cannot apply. In fact, the procedure returns 
without changing the lookahead symbol or doing anything else. Doing nothing 
corresponds to applying an e-production. 
More generally, consider a variant of the productions in Fig. 2.16 where 
optexpr generates an expression nonterminal instead of the terminal expr: 
optexpr + expr 
I 
E 
Thus, optexpr either generates an expression using nonterminal expr or it gen- 
erates e. While parsing optexpr, if the lookahead symbol is not in F I R S T ( ~ X ~ ~ ) ,  
then the €-production is used. 
For more on when to use e-productions, see the discussion of LL(1) grammars 
in Section 4.4.3. 
2.4.4 
Designing a Predictive Parser 
We can generalize the technique introduced informally in Section 2.4.2, to apply 
to any grammar that has disjoint FIRST sets for the production bodies belonging 
to any nonterminal. We shall also see that when we have a translation scheme - 
that is, a grammar with embedded actions - 
it is possible to execute those 
actions as part of the procedures designed for the parser. 
Recall that a predictive parser is a program consisting of a procedure for 
every nonterminal. The procedure for nonterminal A does two things. 
1. It decides which A-production to use by examining the lookahead symbol. 
The production with body a (where a is not E ,  the empty string) is used 
if the lookahead symbol is in FIRST(&). If there is a conflict between 
two nonempty bodies for any lookahead symbol, then we cannot use this 
parsing method on this grammar. In addition, the E-production 
for A, if 
it exists, is used if the lookahead symbol is not in the FIRST set for any 
other production body for A. 
2. The procedure then mimics the body of the chosen production. That 
is, the symbols of the body are "executed" in turn, from the left. A 
nonterrninal is "executed" by a call to the procedure for that nonterminal, 
and a terminal matching the lookahead symbol is "executed" by reading 
the next input symbol. If at some point the terminal in the body does 
not match the lookahead symbol, a syntax error is reported. 
Figure 2.19 is the result of applying these rules to the grammar in Fig. 2.16. 
2.4. PARSING 
67 
Just as a translation scheme is formed by extending a grammar, a syntax- 
directed translator can be formed by extending a predictive parser. An algo- 
rithm for this purpose is given in Section 5.4. The following limited construction 
suffices for the present: 
1. Construct a predictive parser, ignoring the actions in productions. 
2. Copy the actions from the translation scheme into the parser. If an action 
appears after grammar symbol X in production p, then it is copied after 
the implementation of X in the code for p. Otherwise, if it appears at the 
beginning of the production, then it is copied just before the code for the 
production body. 
We shall construct such a translator in Section 2.5. 
2.4.5 
Left Recursion 
It is possible for a recursive-descent parser to loop forever. A problem arises 
with "left-recursive" productions like 
expr -+ expr + term 
where the leftmost symbol of the body is the same as the nonterminal at the 
head of the production. Suppose the procedure for expr decides to apply this 
production. The body begins with expr so the procedure for expr is called 
recursively. Since the lookahead symbol changes only when a terminal in the 
body is matched, no change to the input took place between recursive calls of 
expr. As a result, the second call to expr does exactly what the first call did, 
which means a third call to expr, and so on, forever. 
A left-recursive production can be eliminated by rewriting the offending 
production. Consider a nonterminal A with two productions 
where a and ,
8
 are sequences of terminals and nonterminals that do not start 
with A. For example, in 
e
x
 
+ expr + term 1 term 
nonterminal A = expr, string a = 
+ term, and string ,6 = term. 
The nonterminal A and its production are said to be left recurs2ve, because 
the production A -+ Aa has A itself as the leftmost symbol on the right side.4 
Repeated application of this production builds up a sequence of a's to the right 
of A, as in Fig. 2.20(a). When A is finally replaced by P, we have a ,8 followed 
by a sequence of zero or more a's. 
The same effect can be achieved, as in Fig. 2.20(b), by rewriting the pro- 
ductions for A in the following manner, using a new nonterminal R: 
4 ~ n  
a general left-recursive grammar, instead of a production A -+ 
Aa, the nonterminal A 
may derive Aa through intermediate productions. 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Figure 2.20: Left- and right-recursive ways of generating a string 
Nonterminal R and its production R --+ c
x
R
 are right recursive because this pro- 
duction for R has R itself as the last symbol on the right side. Right-recursive 
productions lead to trees that grow down towards the right, as in Fig. 2.20(b). 
Trees growing down to the right make it harder to translate expressions con- 
taining left-associative operators, such as minus. In Section 2.5.2, however, we 
shall see that the proper translation of expressions into postfix notation can 
still be attained by a careful design of the translation scheme. 
In Section 4.3.3, we shall consider more general forms of left recursion and 
show how all left recursion can be eliminated from a grammar. 
2.4.6 Exercises for Section 2.4 
Exercise 2.4.1 : 
Construct recursive-descent parsers, starting with the follow- 
ing grammars: 
2.5 
A Translator for Simple Expressions 
Using the techniques of the last three sections, we now construct a syntax- 
directed translator, in the form of a working Java program, that translates 
arithmetic expressions into postfix form. To keep the initial program manage- 
ably small, we start with expressions consisting of digits separated by binary 
plus and minus signs. We extend the program in Section 2.6 to translate ex- 
pressions that include numbers and other operators. It is worth studying the 
2.5. A TRANSLATOR FOR SIMPLE EXPRESSIONS 
69 
translation of expressions in detail, since they appear as a construct in so many 
languages. 
A syntax-directed translation scheme often serves as the specification for 
a translator. The scheme in Fig. 2.21 (repeated from Fig. 2.15) defines the 
translation to be performed here. 
expr -+ expr + term { print('+') ) 
I 
expr - term { print('-') } 
I 
term 
term 3 0 
{ print ('0') ) 
I
1
 
{ print('I1) ) 
. 
. . 
9 
{ print ('9') ) 
Figure 2.21: Actions for translating into postfix notation 
Often, the underlying grammar of a given scheme has to be modified before 
it can be parsed with a predictive parser. In particular, the grammar underlying 
the scheme in Fig. 2.21 is left recursive, and as we saw in the last section, a 
predictive parser cannot handle a left-recursive grammar. 
We appear to have a conflict: on the one hand we need a grammar that 
facilitates translation, on the other hand we need a significantly different gram- 
mar that facilitates parsing. The solution is to begin with the grammar for 
easy translation and carefully transform it to facilitate parsing. By eliminating 
the left recursion in Fig. 2.21, we can obtain a grammar suitable for use in a 
predictive recursive-descent translator. 
2.5.1 
Abstract and Concrete Syntax 
A useful starting point for designing a translator is a data structure called 
an abstract syntax tree. In an abstract syntax tree for an expression, each 
interior node represents an operator; the children of the node represent the 
operands of the operator. More generally, any programming construct can be 
handled by making up an operator for the construct and treating as operands 
the semantically meaningful components of that construct. 
In the abstract syntax tree for 9-5+2 in Fig. 2.22, the root represents the 
operator +. The subtrees of the root represent the subexpressions 9-5 and 
2. The grouping of 9-5 as an operand reflects the left-to-right evaluation of 
operators at the same precedence level. Since - 
and + have the same precedence, 
9-5+2 is equivalent to (9-5)+2. 
Abstract syntax trees, or simply syntax trees, resemble parse trees to an 
extent. However, in the syntax tree, interior nodes represent programming 
constructs while in the parse tree, the interior nodes represent nonterminals. 
Many nonterminals of a grammar represent programming constructs, but others 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Figure 2.22: Syntax tree for 9-5+2 
are "helpers" of one sort of another, such as those representing terms, factors, 
or other variations of expressions. In the syntax tree, these helpers typically are 
not needed and are hence dropped. To emphasize the contrast, a parse tree is 
sometimes called a concrete syntax tree, and the underlying grammar is called 
a concrete syntax for the language. 
In the syntax tree in Fig. 2.22, each interior node is associated with an 
operator, with no "helper" nodes for single productions (a production whose 
body consists of a single nonterminal, and nothing else) like expr --+ term or for 
E-productions 
like rest --+ 
E .  
It is desirable for a translation scheme to be based on a grammar whose parse 
trees are as close to syntax trees as possible. The grouping of subexpressions 
by the grammar in Fig. 2.21 is similar to their grouping in syntax trees. For 
example, subexpressions of the addition operator are given by expr and term in 
the production body expr+ term. 
2.5.2 
Adapting the Translation Scheme 
The left-recursion-elimination technique sketched in Fig. 2.20 can also be ap- 
plied to productions containing semantic actions. First, the technique extends 
to multiple productions for A. In our example, A is expr, and there are two left- 
recursive productions for expr and one that is not left recursive. The technique 
transforms the productions A --+ Aa I AP 1 y into 
Second, we need to transform productions that have embedded actions, not 
just terminals and nonterminals. Semantic actions embedded in the productions 
are simply carried along in the transformation, as if they were terminals. 
Example 2.13 : 
Consider the translation scheme of Fig. 2.21. Let 
A = expr 
a = + term { print ('+') ) 
/
3
 
= - term { print('-') ) 
Y
=
 
term 
2.5. A TRANSLATOR FOR SIMPLE EXPRESSIONS 
71 
Then the left-recursion-eliminating transformation produces the translation 
scheme in Fig. 2.23. The expr productions in Fig. 2.21 have been transformed 
into the productions for expr, and a new nonterminal rest plays the role of R. 
The productions for term are repeated from Fig. 2.21. Figure 2.24 shows how 
9-5+2istranslatedusingthegrammarinFig. 2.23. 
e
x
 -+ 
term rest 
rest 
-+ 
+ term { print('+') } rest 
I 
- term { print('-') } rest 
I 
l
5
 
term 
-+ 
0 { print('ol) ) 
( 
1 { print ('1') } 
... 
1 
9 { print(I9') ) 
Figure 2.23: Translation scheme after left-recursion elimination 
/Tt 
\\ 
9 
{print 
('9')) 
- j.~\ 
{prini('-')) 
/
T
t
\
\
 
5 
{print('5')) 
+ 7 
, 
{ p i t ( + ) }  
rest 
\ 
\ 
\ 
I 
2 
{print 
('2')) 
E 
Figure 2.24: Translation of 9-5+2 to 95-2+ 
Left-recursion elimination must be done carefully, to ensure that we preserve 
the ordering of semantic actions. For example, the transformed scheme in 
Fig. 2.23 has the actions { print('+') } and { print('-') } in the middle of 
a production body, in each case between nonterminals term and rest. If the 
actions were to be moved to the end, after rest, then the translations would 
become incorrect. We leave it to the reader to show that 9-5+2 would then be 
translated incorrectly into 952+-, the postfix notation for 9- (5+2), instead of 
the desired 95-2+, the postfix notation for (9-5)+2. 
72 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
2.5.3 Procedures for the Nonterminals 
Functions expr, rest, and term in Fig. 2.25 implement the syntax-directed trans- 
lation scheme in Fig. 2.23. These functions mimic the production bodies of 
the corresponding nonterminals. Function expr implements the production 
expr -+ term rest by the calls term() followed by rest (). 
void expro { 
term () 
; rest 
() 
; 
1 
void rest() { 
if ( lookahead == '+I 
) { 
match('+') ; term () 
; print 
( ' + I ) ;  rest () 
; 
1 
else if ( lookahead == I-' 
) { 
match('-'); 
term(); print (I-'); 
rest (); 
1 
else { } /* 
do nothing with the input */ ; 
1 
void term() ( 
if ( lookahead is a digit ) { 
t = lookahead, match(1ookahead); print (t); 
1 
else report (" 
syntax error") 
; 
1 
Figure 2.25: Pseudocode for nonterminals expr, rest, and term. 
Function rest implements the three productions for nonterminal rest in 
Fig. 2.23. It applies the first production if the lookahead symbol is a plus 
sign, the second production if the lookahead symbol is a minus sign, and the 
production rest -+ t in all other cases. The first two productions for rest are 
implemented by the first two branches of the if-statement in procedure rest. 
If the lookahead symbol is +, the plus sign is matched by the call match('+'). 
After the call term(), the semantic action is implemented by writing a plus 
character. The second production is similar, with - instead of +. Since the 
third production for rest has t as its right side, the last else-clause in function 
rest does nothing. 
The ten productions for t e n  
generate the ten digits. Since each of these 
productions generates a digit and prints it, the same code in Fig. 2.25 imple- 
ments them all. If the test succeeds, variable t saves the digit represented by 
lookahead so it can be written after the call to match. Note that match changes 
2.5. A TRANSLATOR FOR SIMPLE EXPRESSIONS 
the lookahead symbol, so the digit needs to be saved for later printing.5 
2.5.4 
Simplifying the Translator 
Before showing a complete program, we shall make two simplifying transfor- 
mations to the code in Fig. 2.25. The simplifications will fold procedure rest 
into procedure expr. When expressions with multiple levels of precedence are 
translated, such simplifications reduce the number of procedures needed. 
First, certain recursive calls can be replaced by iterations. When the last 
statement executed in a procedure body is a recursive call to the same proce- 
dure, the call is said to be tail recursive. For example, in function rest, the 
calls of rest() 
with lookahead + and - are tail recursive because in each of these 
branches, the recursive call to rest is the last statement executed by the given 
call of rest. 
For a procedure without parameters, a tail-recursive call can be replaced 
simply by a jump to the beginning of the procedure. The code for rest can be 
rewritten as the pseudocode of Fig. 2.26. As long as the lookahead symbol is 
a plus or a minus sign, procedure rest matches the sign, calls term to match 
a digit, and continues the process. Otherwise, it breaks out of while loop and 
returns from rest. 
void rest() { 
while( true ) { 
i f (  
lookahead == I+' 
) { 
match('+'); 
term(); 
print ('+I); 
continue; 
} 
else i f  
( lookahead == '-' 
) { 
match('-'); term(); 
print (I-'); 
continue; 
1 
break ; 
1 
1 
Figure 2.26: Eliminating tail recursion in the procedure rest of Fig. 2.25. 
Second, the complete Java program will include one more change. Once 
the tail-recursive calls to rest in Fig. 2.25 are replaced by iterations, the only 
remaining call to rest is from within procedure expr. The two procedures can 
therefore be integrated into one, by replacing the call rest() by the body of 
procedure rest. 
5
~
s
 
a minor optimization, we could print before calling match to avoid the need to save 
the digit. In general, changing the order of actions and grammar symbols is risky, since it 
could change what the translation does. 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
2.5.5 
The Complete Program 
The complete Java program for our translator appears in Fig. 2.27. The first 
line of Fig. 2.27, beginning with import, provides access to the package j ava. i o  
for system input and output. The rest of the code consists of the two classes 
Parser and Postf ix. Class Parser contains variable lookahead and functions 
Parser, expr, term, and match. 
Execution begins with function main, which is defined in class Postfix. 
Function main creates an instance parse of class Parser and calls its function 
expr to parse an expression. 
The function Parser, with the same name as its class, is a constructor; 
it is called automatically when an object of the class is created. Notice from 
its definition at the beginning of class Parser that the constructor Parser 
initializes variable lookahead by reading a token. Tokens, consisting of single 
characters, are supplied by the system input routine read, which reads the next 
character from the input file. Note that lookahead is declared to be an integer, 
rather than a character, to anticipate the fact that additional tokens other than 
single characters will be introduced in later sections. 
Function expr is the result of the simplifications discussed in Section 2.5.4; 
it implements nonterminals expr and rest in Fig. 2.23. The code for expr 
in Fig. 2.27 calls term and then has a while-loop that forever tests whether 
lookahead matches either ' 
+' or ' 
- 
' 
. Control exits from this while-loop when 
it reaches the return statement. Within the loop, the input/output facilities of 
the System class are used to write a character. 
Function term uses the routine isDigit from the Java class Character 
to test if the lookahead symbol is a digit. The routine isDigit expects to 
be applied to a character; however, lookahead is declared to be an integer, 
anticipating future extensions. The construction (char) 
lookahead casts or 
coerces lookahead to be a character. In a small change from Fig. 2.25, the 
semantic action of writing the lookahead character occurs before the call to 
match. 
The function match checks terminals; it reads the next input terminal if the 
lookahead symbol is matched and signals an error otherwise by executing 
throw new Error(I1syntax 
error") ; 
This code creates a new exception of class Error and supplies it the string 
syntax error as an error message. Java does not require Error exceptions 
to be declared in a throws clause, since they are meant to be used only for 
abnormal events that should never occur.6 
"rror 
handling can be streamlined using the exception-handling facilities of Java. One ap- 
proach is to define a new exception, say SyntaxError, that extends the system class Exception. 
Then, throw SyntaxError instead of Error when an error is detected in either term or match. 
Further, handle the exception in main by enclosing the call parse. 
expr 
() within a try state- 
ment that catches exception SyntaxError, writes a message, and terminates. We would need 
to add a class SyntaxError to the program in Fig. 2.27. To complete the extension, in addition 
to IOException, functions match and term must now declare that they can throw SyntaxError. 
Function expr, which calls them, must also declare that it can throw SyntaxError. 
2.5. A TRANSLATOR FOR SIMPLE EXPRESSIONS 
import java.io.*; 
class 
Parser ( 
static int lookahead; 
public Parser() throws IOException 
( 
lookahead 
= System.in.read(); 
3 
void expro throws IOException 
( 
term() ; 
while 
(true) ( 
if 
( lookahead 
== '+' ) ( 
match('+') ; term() ; System.out 
.write('+)) ; 
3 
else if 
( lookahead 
== '-' ) ( 
m
a
t
c
h
(
'
-
'
)
;
 
t
e
r
m
(
)
;
 System.out 
.
w
r
i
t
e
(
'
-
'
)
;
 
J 
else return; 
> 
3 
void t
e
r
m
(
)
 throws IOException 
( 
if 
( Character. 
isDigit 
(
(
c
h
a
r
)
 lookahead) ) ( 
System. 
out. 
write( (char) 
lookahead) 
; match(1ookahead) ; 
3 
else throw 
new Error("syntax error") 
; 
void match(int t) throws IOException 
( 
if 
( lookahead 
== t ) lookahead 
= System. 
in.read() ; 
else throw 
new Error("syntax error") 
; 
public class Postfix ( 
public static void main(StringC1 args) throws IOException 
( 
Parser 
parse = new Parser(); 
parse. 
expro 
; System. 
out. 
write()\n)) ; 
3 
J 
Figure 2.27: Java program to translate infix expressions into postfix form 
76 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
A Few Salient Features of Java 
Those unfamiliar with Java may find the following notes on Java helpful 
in reading the code in Fig. 2.27: 
A class in Java consists of a sequence of variable and function defi- 
nitions. 
Parentheses enclosing function parameter lists are needed even if 
there are no parameters; hence we write expr 
() and term(). These 
functions are actually procedures, because they do not return values, 
signified by the keyword void 
before the function name. 
Functions communicate either by passing parameters "by value" 
or by accessing shared data. For example, the functions expro 
and term() examine the lookahead symbol using the class variable 
lookahead 
that they can all access since they all belong to the same 
class Parser. 
Like C, Java uses = for assignment, == for equality, and ! 
= for in- 
equality. 
The clause "throws 
IOExcept 
ion" in the definition of term() de- 
clares that an exception called IOExcept 
ion 
can occur. Such an 
exception occurs if there is no input to be read when the function 
match 
uses the routine read. 
Any function that calls match 
must also 
declare that an IOException 
can occur during its own execution. 
2.6 Lexical Analysis 
A lexical analyzer reads characters from the input and groups them into "token 
objects." Along with a terminal symbol that is used for parsing decisions, 
a token object carries additional information in the form of attribute values. 
So far, there has been no need to distinguish between the terms "token" and 
"terminal," since the parser ignores the attribute values that are carried by a 
token. In this section, a token is a terminal along with additional information. 
A sequence of input characters that comprises a single token is called a 
lexeme. Thus, we can say that the lexical analyzer insulates a parser from the 
lexeme representation of tokens. 
The lexical analyzer in this section allows numbers, identifiers, and "white 
space" (blanks, tabs, and newlines) to appear within expressions. It can be used 
to extend the expression translator of the previous section. Since the expression 
grammar of Fig. 2.21 must be extended to allow numbers and identifiers, we 
2.6. LEXICAL ANALYSIS 
77 
shall take this opportunity to allow multiplication and division as well. The 
extended translation scheme appears in Fig. 2.28. 
expr 
expr + term 
{ print (I+') 
} 
I 
expr - term 
{ print('-') } 
1 
term 
term -+ 
term * factor 
{ print('*') } 
I 
term / factor { print ('/I) 
} 
I 
factor 
factor -+ 
( expr ) 
I 
num 
{ print (num. 
value) } 
I 
id 
{ print (id. 
lexeme) } 
Figure 2.28: Actions for translating into postfix notation 
In Fig. 2.28, the terminal num is assumed to have an attribute num.value, 
which gives the integer value corresponding to this occurrence of num. Termi- 
nal id has a string-valued attribute written as id.lexeme; we assume this string 
is the actual lexeme comprising this instance of the token id. 
The pseudocode fragments used to illustrate the workings of a lexical ana- 
lyzer will be assembled into Java code at the end of this section. The approach 
in this section is suitable for hand-written lexical analyzers. Section 3.5 de- 
scribes a tool called Lex that generates a lexical analyzer from a specification. 
Symbol tables or data structures for holding information about identifiers are 
considered in Section 2.7. 
2.6.1 Removal of White Space and Comments 
The expression translator in Section 2.5 sees every character in the input, so 
extraneous characters, such as blanks, will cause it to fail. Most languages 
allow arbitrary amounts of white space to appear between tokens. Comments 
are likewise ignored during parsing, so they may also be treated as white space. 
I
f
 white space is eliminated by the lexical analyzer, the parser will never 
have to consider it. The alternative of modifying the grammar to incorporate 
white space into the syntax is not nearly as easy to implement. 
The pseudocode in Fig. 2.29 skips white space by reading input characters 
as long as it sees a blank, a tab, or a newline. Variable peek holds the next 
input character. Line numbers and context are useful within error messages to 
help pinpoint errors; the code uses variable line to count newline characters in 
the input. 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
for ( ; ; 
peek = next input character ) { 
if ( peek is a blank or a tab ) do nothing; 
else if ( peek is a newline ) line = line+-I; 
else break; 
I 
Figure 2.29: Skipping white space 
2.6.2 Reading Ahead 
A lexical analyzer may need to read ahead some characters before it can decide 
on the token to be returned to the parser. For example, a lexical analyzer for 
C or Java must read ahead after it sees the character >. If the next character 
is =, then > is part of the character sequence >=, the lexeme for the token for 
the "greater than or equal to" operator. Otherwise > itself forms the "greater 
than" operator, and the lexical analyzer has read one character too many. 
A general approach to reading ahead on the input, is to maintain an input 
buffer from which the lexical analyzer can read and push back characters. Input 
buffers can be justified on efficiency grounds alone, since fetching a block of 
characters is usually more efficient than fetching one character at a time. A 
pointer keeps track of the portion of the input that has been analyzed; pushing 
back a character is implemented by moving back the pointer. Techniques for 
input buffering are discussed in Section 3.2. 
One-character read-ahead usually suffices, so a simple solution is to use a 
variable, say peek, to hold the next input character. The lexical analyzer in 
this section reads ahead one character while it collects digits for numbers or 
characters for identifiers; e.g., it reads past 1 
to distinguish between 1 
and 10, 
and it reads past t to distinguish between t and true. 
The lexical analyzer reads ahead only when it must. An operator like * can 
be identified without reading ahead. In such cases, peek is set to a blank, which 
will be skipped when the lexical analyzer is called to find the next token. The 
invariant assertion in this section is that when the lexical analyzer returns a 
token, variable peek either holds the character beyond the lexeme for the current 
token, or it holds a blank. 
2.6.3 Constants 
Anytime a single digit appears in a grammar for expressions, it seems reasonable 
to allow an arbitrary integer constant in its place. Integer constants can be 
allowed either by creating a terminal symbol, say num, for such constants or 
by incorporating the syntax of integer constants into the grammar. The job 
of collecting characters into integers and computing their collective numerical 
value is generally given to a lexical analyzer, so numbers can be treated as single 
units during parsing and translation. 
2.6. LEXICAL ANALYSIS 
79 
When a sequence of digits appears in the input stream, the lexical analyzer 
passes to the parser a token consisting of the terminal num along with an 
integer-valued attribute computed from the digits. If we write tokens as tuples 
enclosed between ( ), the input 31 + 28 + 59 is transformed into the sequence 
(num,31) (+) (num,28) (+) (num,59) 
Here, the terminal symbol + has no attributes, so its tuple is simply (+). The 
pseudocode in Fig. 2.30 reads the digits in an integer and accumulates the value 
of the integer using variable u. 
if ( peek holds a digit ) { 
u = 0; 
do { 
v = v * 10 + integer value of digit peek 
peek = next input character; 
) while ( peek holds a digit ) 
; 
return token (num, v); 
1 
Figure 2.30: Grouping digits into integers 
2.6.4 Recognizing Keywords and Identifiers 
Most languages use fixed character strings such as for, do, and i f ,  
as punctua- 
tion marks or to identify constructs. Such character strings are called keywords. 
Character strings are also used as identifiers to name variables, arrays, func- 
tions, and the like. Grammars routinely treat identifiers as terminals to sim- 
plify the parser, which can then expect the same terminal, say id, each time 
any identifier appears in the input. For example, on input 
count = count + increment; 
(2.6) 
the parser works with the terminal stream id = id + id. The token for id has 
an attribute that holds the lexeme. Writing tokens as tuples, we see that the 
tuples for the input stream (2.6) are 
(id, 
llcount 
'I) (=) (id, "count ") (+) (id, 
"increment ") ( ; 
) 
Keywords generally satisfy the rules for forming identifiers, so a mechanism 
is needed for deciding when a lexeme forms a keyword and when it forms an 
identifier. The problem is easier to resolve if keywords are reserved; i.e., if they 
cannot be used as identifiers. Then, a character string forms an identifier only 
if it is not a keyword. 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
The lexical analyzer in this section solves two problems by using a table to 
hold character strings: 
Single Representation. A string table can insulate the rest of the compiler 
from the representation of strings, since the phases of the compiler can 
work with references or pointers to the string in the table. References can 
also be manipulated more efficiently than the strings themselves. 
Reserved Words. Reserved words can be implemented by initializing the 
string table with the reserved strings and their tokens. When the lexical 
analyzer reads a string or lexeme that could form an identifier, it first 
checks whether the lexeme is in the string table. If so, it returns the 
token from the table; otherwise, it returns a token with terminal id. 
In Java, a string table can be implemented as a hash table using a class 
called Hashtable. The declaration 
Hashtable words = new Hashtable(); 
sets up words as a default hash table that maps keys to values. We shall use it 
to map lexemes to tokens. The pseudocode in Fig. 2.31 uses the operation get 
to look up reserved words. 
if ( peek holds a letter ) { 
collect letters or digits into a buffer b; 
s = string formed from the characters in b; 
w = 
token returned by words.get(s); 
if ( w is not null ) return w; 
else ( 
Enter the key-value pair (s, (id, s)) into words 
return token (id, s); 
1 
1 
Figure 2.31: Distinguishing keywords from identifiers 
This pseudocode collects from the input a string s consisting of letters and 
digits beginning with a letter. We assume that s is made as long as possible; 
i.e., the lexical analyzer will continue reading from the input as long as it 
encounters letters and digits. When something other than a letter or digit, e.g., 
white space, is encountered, the lexeme is copied into a buffer b. If the table 
has an entry for s, then the token retrieved by words.get is returned. Here, s 
could be either a keyword, with which the words table was initially seeded, or 
it could be an identifier that was previously entered into the table. Otherwise, 
token id and attribute s are installed in the table and returned. 
2.6. LEXICAL ANALYSIS 
81 
2.6.5 
A Lexical Analyzer 
The pseudocode fragments so far in this section fit together to form a function 
scan that returns token objects, as follows: 
Token scan() { 
skip white space, as in Section 2.6.1; 
handle numbers, as in Section 2.6.3; 
handle reserved words and identifiers, as in Section 2.6.4; 
/* if we get here, treat read-ahead character peek as a token */ 
Token t = new Token(peelc); 
peek = blank /* 
initialization, as discussed in Section 2.6.2 */ ; 
return t; 
The rest of this section implements function scan as part of a Java package 
for lexical analysis. The package, called lexer has classes for tokens and a class 
Lexer containing function scan. 
The classes for tokens and their fields are illustrated in Fig. 2.32; their 
methods are not shown. Class Token has a field t a g  that is used for parsing 
decisions. Subclass Num adds a field value for an integer value. Subclass Word 
adds a field lexeme that is used for reserved words and identifiers. 
class Token 
class N u m  
class Ward 
1 int ualue 
I string lexeme 
I 
Figure 2.32: Class Token and subclasses Nurn and Word 
Each class is in a file by itself. The file for class Token is as follows: 
1) package lexer; 
// File Token.java 
2) public class Token ( 
3) 
public f i n a l  i n t  tag; 
4 )  
public Token(int t )  ( t a g  = t ;  ) 
5 )  3 
Line 1 identifies the package lexer. Field t a g  is declared on line 3 to be f i n a l  
so it cannot be changed once it is set. The constructor Token on line 4 is used 
to create token objects, as in 
new Token('+') 
which creates a new object of class Token and sets its field t a g  to an integer 
representation of ' 
+ ' 
. (For brevity, we omit the customary method tostring, 
which would return a string suitable for printing.) 
82 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Where the pseudocode had terminals like num and id, the Java code uses 
integer constants. Class Tag 
implements such constants: 
1) package lexer; 
// File Tag.java 
2) public class 
Tag ( 
3
)
 
public final static int 
4) 
NUM = 256, 
I D  = 257, 
TRUE 
= 258, 
FALSE 
= 259; 
5) 3 
In addition to the integer-valued fields NUM 
and I D ,  this class defines two addi- 
tional fields, TRUE 
and FALSE, 
for future use; they will be used to illustrate the 
treatment of reserved keywords.7 
The fields in class Tag 
are public, 
so they can be used outside the package. 
They are static, 
so there is just one instance or copy of these fields. The 
fields are final, 
so they can be set just once. In effect, these fields represent 
constants. A similar effect is achieved in C by using define-statements to allow 
names such as NUM 
to be used as symbolic constants, e.g.: 
#define 
NUM 256 
The Java code refers to Tag. 
NUM 
and Tag. 
I D  
in places where the pseudocode 
referred to terminals num and id. The only requirement is that Tag. 
NUM and 
Tag. 
I D  must be initialized with distinct values that differ from each other and 
from the constants representing single-character tokens, such as ' 
+ ' or ' 
* ' 
. 
1) package lexer; 
// File Num.java 
2) public class 
Num extends 
Token { 
3) 
public final int value; 
4) 
public N
u
m
(
i
n
t
 v
)
 { super(Tag.NUM) ; value = v; 
3 
5) 3 
1) package lexer; 
// File Word.java 
2) public class 
Word extends 
Token { 
3) 
public final 
String lexeme; 
4) 
public Word(iqt t, 
String s
)
 ( 
5) 
super(t) ; lexeme 
= new String(s) ; 
6) 
1 
7) 3 
Figure 2.33: Subclasses Num 
and Word 
of Token 
Classes Num and Word appear in Fig. 2.33. Class Num extends Token 
by 
declaring an integer field value 
on line 3. The constructor Num 
on line 4 
calls 
super 
(Tag. 
NUM) 
, 
which sets field tag 
in the superclass Token 
to Tag. 
NUM. 
7~~~~~ 
characters are typically converted into integers between 0 and 255. We therefore 
use integers greater than 255 for terminals. 
2.6. LEXICAL ANALYSIS 
I) package lexer; 
// File Lexer.java 
2) import j 
ava. 
io 
. 
* ; import j ava. 
ut 
il 
. 
* ; 
3) public class 
Lexer I 
4) 
public int line = I; 
5) 
private char 
peek = ) 
) ;  
6) 
private Hashtable 
words = new Hashtable() ; 
7) 
void reserve(Word t
)
 { words.put 
(
t
 
. 
lexeme, 
t) 
; 3 
8) 
public Lexer() ( 
9) 
reserve( new Word(Tag.TRUE, "true") ) ; 
10) 
reserve 
( new Word(Tag .FALSE, 
"false") ) ; 
11) 
3 
12) 
public Token scan() throws IOException 
I 
I31 
for( ; ; peek = (char)System. in.read() ) { 
14) 
if 
( peek == ) 
) I I peek == ) \t 
) ) continue 
; 
15) 
else i
f
(
 peek == )\n) 
) line = line + 1; 
16) 
else break; 
17) 
3 
/* continues in Fig. 2.35 */ 
Figure 2.34: Code for a lexical analyzer, part 1 of 2 
Class Word 
is used for both reserved words and identifiers, so the constructor 
Word 
on line 4 expects two parameters: a lexeme and a corresponding integer 
value for tag. 
An object for the reserved word true 
can be created by executing 
new Word(Tag . 
TRUE, 
"true") 
which creates a new object with field tag 
set to Tag. 
TRUE 
and field lexeme 
set 
to the string "true". 
Class Lexer 
for lexical analysis appears in Figs. 2.34 and 2.35. The integer 
variable line 
on line 4 counts input lines, and character variable peek 
on line 5 
holds the next input character. 
Reserved words are handled on lines 6 through 11. The table words is 
declared on line 6. The helper function reserve 
on line 7 puts a string-word 
pair in the table. Lines 9 and 10 in the constructor Lexer 
initialize the table. 
They use the constructor Word 
to create word objects, which are passed to the 
helper function reserve. 
The table is therefore initialized with reserved words 
"truef1 
and "false" 
before the first call of scan. 
The code for scan 
in Fig. 2.34-2.35 implements the pseudocode fragments 
in this section. The for-statement on lines 13 through 17 skips blank, tab, 
and newline characters. Control leaves the for-statement with peek 
holding a 
non-white-space character. 
The code for reading a sequence of digits is on lines 18 through 25. The 
function isDigit 
is from the built-in Java class Character. It is used on 
line 18 to check whether peek 
is a digit. If so, the code on lines 19 through 24 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
if 
( Character. 
isDigit 
(peek) ) ( 
int v = 0; 
do ( 
v = 1O*v + Character.digit(peek, 1
0
)
;
 
peek = (char) 
System. 
in. 
r
e
a
d
(
)
 ; 
) while 
( Character. 
isDigit 
(peek) ) ; 
return 
new N
u
m
(
v
)
 ; 
1 
if 
( Character. 
isLetter 
(peek) ) ( 
StringBuffer 
b = new StringBufferO; 
do ( 
b 
. 
append 
(peek) 
; 
peek = (char)System. in. 
r
e
a
d
(
)
 ; 
) while( ~haracter.is~etterOr~igit(peek) 
); 
String s = b.toString(); 
Word w = (Word) 
words. 
get 
(
s
)
 
; 
if 
( w ! 
= null ) return 
w; 
w = new Word(Tag. ID, 
s
)
 
; 
words 
.put 
(s, 
w
)
 
; 
return 
w; 
3 
Token 
t = new Token(peek) ; 
peek = ' ' 
; 
return 
t; 
Figure 2.35: Code for a lexical analyzer, part 2 of 2 
accumulates the integer value of the sequence of digits in the input and returns 
a new Num 
object. 
Lines 26 through 38 analyze reserved words and identifiers. Keywords true 
and false have already been reserved on lines 9 and 10. Therefore, line 35 is 
reached if string s 
is not reserved, so it must be the lexeme for an identifier. 
Line 35 therefore returns a new word object with lexeme 
set to s 
and tag 
set 
to Tag. 
ID. 
Finally, lines 39 through 41 return the current character as a token 
and set peek 
to a blank that will be stripped the next time scan 
is called. 
2.6.6 Exercises for Section 2.6 
Exercise 2.6.1 : 
Extend the lexical analyzer in Section 2.6.5 to remove com- 
ments, defined as follows: 
2.7. SYMBOL TABLES 
85 
a) A comment begins with // and includes all characters until the end of 
that line. 
b) A comment begins with /* and includes all characters through the next 
occurrence of the character sequence */. 
Exercise 2.6.2 : Extend the lexical analyzer in Section 2.6.5 to recognize the 
relational operators <, <=, ==, !=, >=, >. 
Exercise 2.6.3 : 
Extend the lexical analyzer in Section 2.6.5 to recognize float- 
ing point numbers such as 2. , 3.14, and .5. 
2.7 
Symbol Tables 
Symbol tables are data structures that are used by compilers to hold information 
about source-program constructs. The information is collected incrementally by 
the analysis phases of a compiler and used by the synthesis phases to generate 
the target code. Entries in the symbol table contain information about an 
identifier such as its character string (or lexeme) 
, 
its type, its position in storage, 
and any other relevant information. Symbol tables typically need to support 
multiple declarations of the same identifier within a program. 
From Section 1.6.1, the scope of a declaration is the portion of a program 
to which the declaration applies. We shall implement scopes by setting up a 
separate symbol table for each scope. A program block with declarations8 will 
have its own symbol table with an entry for each declaration in the block. This 
approach also works for other constructs that set up scopes; for example, a class 
would have its own table, with an entry for each field and method. 
This section contains a symbol-table module suitable for use with the Java 
translator fragments in this chapter. The module will be used as is when we 
put together the translator in Appendix A. Meanwhile, for simplicity, the main 
example of this section is a stripped-down language with just the key constructs 
that touch symbol tables; namely, blocks, declarations, and factors. All of the 
other statement and expression constructs are omitted so we can focus on the 
symbol-table operations. A program consists of blocks with optional declara- 
tions and "statements" consisting of single identifiers. Each such statement 
represents a use of the identifier. Here is a sample program in this language: 
The examples of block structure in Section 1.6.3 dealt with the definitions and 
uses of names; the input (2.7) consists solely of definitions and uses of names. 
The task we shall perform is to print a revised program, in which the decla- 
rations have been removed and each "statement" has its identifier followed by 
a colon and its type. 
'1n C, for instance, program blocks are either functions or sections of functions that are 
separated by curly braces and that have one or more declarations within them. 
86 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Who Creates Symbol-Table Entries? 
Symbol-table entries are created and used during the analysis phase by the 
lexical analyzer, the parser, and the semantic analyzer. In this chapter, 
we have the parser create entries. With its knowledge of the syntactic 
structure of a program, a parser is often in a better position than the 
lexical analyzer to distinguish among different declarations of an identifier. 
In some cases, a lexical analyzer can create a symbol-table entry as 
soon as it sees the characters that make up a lexeme. More often, the 
lexical analyzer can only return to the parser a token, say id, along with 
a pointer to the lexeme. Only the parser, however, can decide whether to 
use a previously created symbol-table entry or create a new one for the 
identifier. 
Example 2.14 : 
On the above input (2.7), the goal is to produce: 
The first x and y are from the inner block of input (2.7). Since this use of x 
refers to the declaration of x in the outer block, it is followed by int, the type 
of that declaration. The use of y in the inner block refers to the declaration of 
y in that very block and therefore has boolean type. We also see the uses of x 
and y in the outer block, with their types, as given by declarations of the outer 
block: integer and character, respectively. 
2.7.1 Symbol Table Per Scope 
The term "scope of identifier 2
'
 
really refers to the scope of a particular dec- 
laration of x. The term scope by itself refers to a portion of a program that is 
the scope of one or more declarations. 
Scopes are important, because the same identifier can be declared for differ- 
ent purposes in different parts of a program. Common names like i 
and x often 
have multiple uses. As another example, subclasses can redeclare a method 
name to override a method in a superclass. 
If blocks can be nested, several declarations of the same identifier can appear 
within a single block. The following syntax results in nested blocks when stmts 
can generate a block: 
block -+ '(I 
decls stmts '3' 
(We quote curly braces in the syntax to distinguish them from curly braces for 
semantic actions.) With the grammar in Fig. 2.38, decls generates an optional 
sequence of declarations and stmts generates an optional sequence of statements. 
2.7. SYMBOL TABLES 
87 
Optimization of Symbol Tables for Blocks 
Implementations of symbol tables for blocks can take advantage of the 
most-closely nested rule. Nesting ensures that the chain of applicable 
symbol tables forms a stack. At the top of the stack is the table for 
the current block. Below it in the stack are the tables for the enclosing 
blocks. Thus, symbol tables can be allocated and deallocated in a stack- 
like fashion. 
Some compilers maintain a single hash table of accessible entries; that 
is, of entries that are not hidden by a declaration in a nested block. Such 
a hash table supports essentially constant-time lookups, at the expense of 
inserting and deleting entries on block entry and exit. Upon exit from a 
block B, the compiler must undo any changes to the hash table due to 
declarations in block B. It can do so by using an auxiliary stack to keep 
track of changes to the hash table while block B is processed. 
Moreover, a statement can be a block, so our language allows nested blocks, 
where an identifier can be redeclared. 
The most-closely nested rule for blocks is that an identifier x is in the scope 
of the most-closely nested declaration of x; that is, the declaration of x found 
by examining blocks inside-out, starting with the block in which x appears. 
Example 2.15 : The following pseudocode uses subscripts to distinguish a- 
mong distinct declarations of the same identifier: 
1) 
{ 
i
n
t
 xl; i
n
t
 yl; 
2) 
{ 
i
n
t
 w2; 
boo1 y2; i
n
t
 zz; 
3) 
. 
. 
. w2 ...; ... XI 
...; ... 
y2 '." ... 
, 
22 "'; 
4) 
1 
The subscript is not part of an identifier; it is in fact the line number of the 
declaration that applies to the identifier. Thus, all occurrences of x are within 
the scope of the declaration on line 1. The occurrence of y on line 3 is in the 
scope of the declaration of y on line 2 since y is redeclared within the inner block. 
The occurrence of y on line 5, however, is within the scope of the declaration 
of y on line 1. 
The occurrence of w 
on line 5 
is presumably within the scope of a declaration 
of w 
outside this program fragment; its subscript 0 denotes a declaration that 
is global or external to this block. 
Finally, z is declared and used within the nested block, but cannot be used 
on line 5, since the nested declaration applies only to the nested block. 
CHAPTER 2. A SIMPLE SYNTAX-DIRE 
CTED TRANSLATOR 
The most-closely nested rule for blocks can be implemented by chaining 
symbol tables. That is, the table for a nested block points to the table for its 
enclosing block. 
Example 2.16 : 
Figure 2.36 shows symbol tables for the pseudocode in Exam- 
ple 2.15. B1 is for the block starting on line 1 
and B2 
is for the block starting at 
line 2. At the top of the figure is an additional symbol table Bo for any global 
or default declarations provided by the language. During the time that we are 
analyzing lines 2 through 4, the environment is represented by a reference to 
the lowest symbol table - 
the one for B2. When we move to line 5 ,  the symbol 
table for B2 becomes inaccessible, and the environment refers instead to the 
symbol table for B1, from which we can reach the global symbol table, but not 
the table for B2. 
Figure 2.36: Chained symbol tables for Example 2.15 
Bo: 
The Java implementation of chained symbol tables in Fig. 2.37 defines a 
class Env, short for env~ronrnent.~ 
Class Env supports three operations: 
W I  ... 
Create a new symbol table. The constructor Env (p) on lines 6 through 
8 of Fig. 2.37 creates an Env object with a hash table named table. 
The object is chained to the environment-valued parameter p by setting 
field next to p. Although it is the Env objects that form a chain, it is 
convenient to talk of the tables being chained. 
Put a new entry in the current table. The hash table holds key-value 
pairs, where: 
- 
The key is a string, or rather a reference to a string. We could 
alternatively use references to token objects for identifiers as keys. 
- 
The value is an entry of class Symbol. The code on lines 9 through 
11 does not need to know the structure of an entry; that is, the code 
is independent of the fields and methods in class Symbol. 
9''Environment" is another term for the collection of symbol tables that are relevant at a 
point in the program. 
2.7. SYMBOL TABLES 
1) package symbols; 
2) import j ava. 
u t  
il 
. 
* ; 
3) public c l a s s  Env { 
4) 
private Hashtable t a b l e  
; 
5) 
protected Env prev; 
// File Env.java 
6) 
publicEnv(Envp) i 
7) 
t a b l e  = new Hashtable() ; prev = p; 
8) 
3 
9) 
public void put (String s ,  Symbol sym) { 
10) 
t a b l e .  
put (s 
, sym) ; 
11) 
1 
12) 
public Symbol get(String s )  i 
l3) 
f o r (  Env e = t h i s ;  e != n u l l ;  e = e.prev ) C 
14) 
Symbol found = (Symbol) (e 
.table. 
get 
(s) 
) ; 
I51 
i f  ( found != n u l l  ) return found; 
16) 
3 
17) 
return n u l l ;  
18) 
1 
19) 1 
Figure 2.37: Class Env implements chained symbol tables 
Get an entry for an identifier by searching the chain of tables, starting 
with the table for the current block. The code for this operation on lines 
12 through 18 returns either a symbol-table entry or null. 
Chaining of symbol tables results in a tree structure, since more than one 
block can be nested inside an enclosing block. The dotted lines in Fig. 2.36 are 
a reminder that chained symbol tables can form a tree. 
2.7.2 The Use of Symbol Tables 
In effect, the role of a symbol table is to pass information from declarations to 
uses. A semantic action "puts" information about identifier x into the symbol 
table, when the declaration of x is analyzed. Subsequently, a semantic action 
associated with a production such as factor +- 
id "gets" information about 
the identifier from the symbol table. Since the translation of an expression 
El o p  E2, 
for a typical operator op, depends only on the translations of El and 
Ez, 
and does not directly depend on the symbol table, we can add any number 
of operators without changing the basic flow of information from declarations 
to uses, through the symbol table. 
Example 2.17 : 
The translation scheme in Fig. 2.38 illustrates how class Env 
can be used. The translation scheme concentrates on scopes, declarations, and 
CHAPTER 2. A SIlMPLE SYNTAX-DIRECTED TRANSLATOR 
uses. It implements the translation described in Example 2.14. As noted earlier, 
on input 
program 
--+ 
{ top = null; ) 
block 
block 
--+ 
'(I 
{ saved = top; 
top = new Enu(top); 
print (" 
( I t )  ; 
} 
decls stmts '3' 
{ top = saved; 
print 
( I 1  3 I t )  ; 
) 
decls + decls decl 
I 
decl + t y p e  id ; 
stmts + stmts stmt 
I
6
 
strnt + block 
I 
factor ; 
factor + id 
{ s = n e w  Symbol; 
s.type = 
type.lexeme 
top.put (id. 
lexeme, s); ) 
{ print (" ; 
{ s = top.get(id.lexeme); 
print 
(id. 
lexeme) 
; 
print (" : 
I t )  ; 
) 
print (s. 
type) 
; 
Figure 2.38: The use of symbol tables for translating a language with blocks 
( int x; char y; ( boo1 y; X; 
J T ;  3 x; y; 3 
the translation scheme strips the declarations and produces 
Notice that the bodies of the productions have been aligned in Fig. 2.38 
so that all the grammar symbols appear in one column, and all the actions in 
a second column. As a result, components of the body are often spread over 
several lines. 
Now, consider the semantic actions. The translation scheme creates and 
discards symbol tables upon block entry and exit, respectively. Variable top 
denotes the top table, at the head of a chain of tables. The first production of 
2.8. INTERMEDIATE CODE GENERATION 
91 
the underlying grammar is program -+ block. The semantic action before block 
initializes top to null, with no entries. 
The second production, block -+ '(I 
declsstmts')', has actions upon block 
entry and exit. On block entry, before decls, a semantic action saves a reference 
to the current table using a local variable saved. Each use of this production 
has its own local variable saved, distinct from the local variable for any other 
use of this production. In a recursive-descent parser, saved would be local to 
the procedure for block. The treatment of local variables of a recursive function 
is discussed in Section 7.2. The code 
top = new Env(top); 
sets variable top to a newly created new table that is chained to the previous 
value of top just before block entry. Variable top is an object of class Env; the 
code for the constructor Env appears in Fig. 2.37. 
On block exit, after I)', 
a semantic action restores top to its value saved on 
block entry. In effect, the tables form a stack; restoring top to its saved value 
pops the effect of the declarations in the block.1° Thus, the declarations in the 
block are not visible outside the block. 
A declaration, decls -+ type id results in a new entry for the declared iden- 
tifier. We assume that tokens type and id each have an associated attribute, 
which is the type and lexeme, respectively, of the declared identifier. We shall 
not go into all the fields of a symbol object s, but we assume that there is a 
field type that gives the type of the symbol. We create a new symbol object s 
and assign its type properly by s.type = type.lexeme. The complete entry is 
put into the top symbol table by top.put(id.lexeme, s). 
The semantic action in the production factor -+ id uses the symbol table 
to get the entry for the identifier. The get operation searches for the first entry 
in the chain of tables, starting with top. The retrieved entry contains any 
information needed about the identifier, such as the type of the identifier. 
2.8 
Intermediate Code Generation 
The front end of a compiler constructs an intermediate representation of the 
source program from which the back end generates the target program. In 
this section, we consider intermediate representations for expressions and state- 
ments, and give tutorial examples of how to produce such representations. 
2.8.1 Two Kinds of Intermediate Representations 
As was suggested in Section 2.1 and especially Fig. 2.4, the two most important 
intermediate representations are: 
1°1nstead of explicitly saving and restoring tables, we could alternatively add static opera- 
tions push and pop to class Env. 
92 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Trees, including parse trees and (abstract) syntax trees. 
Linear representations, especially "three-address code." 
Abstract-syntax trees, or simply syntax trees, were introduced in Section 
2.5.1, and in Section 5.3.1 they will be reexamined more formally. During 
parsing, syntax-tree nodes are created to represent significant programming 
constructs. As analysis proceeds, information is added to the nodes in the form 
of attributes associated with the nodes. The choice of attributes depends on 
the translation to be performed. 
Three-address code, on the other hand, is a sequence of elementary program 
steps, such as the addition of two values. Unlike the tree, there is no hierarchical 
structure. As we shall see in Chapter 9, we need this representation if we are 
to do any significant optimization of code. In that case, we break the long 
sequence of three-address statements that form a program into "basic blocks," 
which are sequences of statements that are always executed one-after-the-other, 
with no branching. 
In addition to creating an intermediate representation, a compiler front end 
checks that the source program follows the syntactic and semantic rules of the 
source language. This checking is called static checking; in general "static" 
means "done by the compiler." 
l1 Static checking assures that certain kinds 
of programming errors, including type mismatches, are detected and reported 
during compilation. 
It is possible that a compiler will construct a syntax tree at the same time 
it emits steps of three-address code. However, it is common for compilers to 
emit the three-address code while the parser "goes through the motions" of 
constructing a syntax tree, without actually constructing the complete tree 
data structure. Rather, the compiler stores nodes and their attributes needed 
for semantic checking or other purposes, along with the data structure used for 
parsing. By so doing, those parts of the syntax tree that are needed to construct 
the three-address code are available when needed, but disappear when no longer 
needed. We take up the details of this process in Chapter 5. 
2.8.2 Construction of Syntax Trees 
We shall first give a translation scheme that constructs syntax trees, and later, 
in .Section 2.8.4, show how the scheme can be modified to emit three-address 
code, along with, or instead of, the syntax tree. 
Recall from Section 2.5.1 that the syntax tree 
lllts opposite, "dynamic," means "while the program is running." Many languages also 
make certain dynamic checks. For instance, an object-oriented language like Java sometimes 
must check types during program execution, since the method applied to an object may 
depend on thk-particulaFsubGass of the object. 
2.8. INTERMEDIATE CODE GENERATION 
represents an expression formed by applying the operator op to the subexpres- 
sions represented by El and E2. Syntax trees can be created for any construct, 
not just expressions. Each construct is represented by a node, with children 
for the semantically meaningful components of the construct. For example, the 
semantically meaningful components of a C while-statement: 
while ( expr ) stmt 
are the expression expr and the statement stmt.12 The syntax-tree node for such 
a while-statement has an operator, which we call while, and two children-the 
syntax trees for the expr and the stmt. 
The translation scheme in Fig. 2.39 constructs syntax trees for a repre- 
sentative, but very limited, language of expressions and statements. All the 
nonterminals in the translation scheme have an attribute n, which is a node of 
the syntax tree. Nodes are implemented as objects of class Node. 
Class Node has two immediate subclasses: Expr for all kinds of expressions, 
and Stmt for all kinds of statements. Each type of statement has a corresponding 
subclass of Stmt; for example, operator while corresponds to subclass While. 
A syntax-tree node for operator while with children x and y is created by the 
pseudocode 
new While (x, 
y ) 
which creates an object of class While by calling constructor function While, 
with the same name as the class. Just as constructors correspond to operators, 
constructor parameters correspond to operands in the abstract syntax. 
When we study the detailed code in Appendix A, we shall see how methods 
are placed where they belong in this hierarchy of classes. In this section, we 
shall discuss only a few of the methods, informally. 
We shall consider each of the productions and rules of Fig. 2.39, in turn. 
First, the productions defining different types of statements are explained, fol- 
lowed by the productions that define our limited types of expressions. 
Syntax Trees for Statements 
For each statement construct, we define an operator in the abstract syntax. For 
constructs that begin with a keyword, we shall use the keyword for the operator. 
Thus, there is an operator while for while-statements and an operator do for 
do-while statements. Conditionals can be handled by defining two operators 
1 2 ~ h e  
right parenthesis serves only to separate the expression from the statement. The left 
parenthesis actually has no meaning; it is there only to please the eye, since without it, C 
would allow unbalanced parentheses. 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
program --+ block 
{ return blockn; } 
block --+ '{I 
stmts '3' 
{ b1ock.n = stmts.n; } 
stmts --+ stmtsl stmt 
{ stmts.n = new Seq (stmtsl 
.n, 
stmt.n); } 
I 
{ stmts.n = null; } 
stmt --+ expr ; 
{ stmt.n = new Eva1 (expr.n); 
} 
I 
i f  ( expr ) stmtl 
{ stmt.n = new If (expr.n, 
stmtl .n); 
} 
I 
while ( expr ) stmtl 
{ stmt.n = new While 
(expr.n, 
stmtl .n); 
} 
I 
do stmtl while ( expr ) ; 
{ stmt.n = new Do (stmtl 
.n, expr.n); 
} 
I block 
{ stmt.n = b1ock.n; } 
expr --+ r
e
1
 = exprl 
{ expr.n = new Assign ( ' = I ,  reLn, expr, .n); 
} 
I r
e
1
 
{ expr.n = re1.n; } 
re1 --+ r
e
1
1
 < add 
{ re1.n = new Re1 ('<I, 
r
e
1
1
 .n, add.n); 
} 
I r
e
1
1
 <= add 
{ re1.n = new Re1 ('st, 
reh .n, add.n); 
} 
I 
add 
{ re1.n = add.n; } 
add --+ addl + term 
{ add.n = new Op ( I + ' ,  add1 .n, term.n); 
1 
I term 
{ add.n = term.n; } 
term --+ terml * factor 
{ term.n = new Op ( I * ' ,  terml.n,factor.n); 
} 
I factor 
{ term.n = 
fact0r.n; } 
factor -+ ( expr ) 
{ fact0r.n = expr.n; } 
I 
num 
{ fact0r.n = new Num (num.value); 
} 
Figure 2.39: Construction of syntax trees for expressions and statements 
2.8. INTERMEDIATE CODE GENERATION 
95 
ifelse and if for if-statements with and without an else part, respectively. In our 
simple example language, we do not use else, and so have only an if-statement. 
Adding else presents some parsing issues, which we discuss in Section 4.8.2. 
Each statement operator has a corresponding class of the same name, with 
a capital first letter; e.g., class I
f
 corresponds to if. In addition, we define 
the subclass Seq, which represents a sequence of statements. This subclass 
corresponds to the nonterminal stmts of the grammar. Each of these classes are 
subclasses of Stmt, which in turn is a subclass of Node. 
The translation scheme in Fig. 2.39 illustrates the construction of syntax- 
tree nodes. A typical rule is the one for if-statements: 
stmt -+ if ( expr ) stmtl 
{ stmt.n = new If(expr.n, 
stmtl .n); } 
The meaningful components of the if-statement are expr and stmtl . The se- 
mantic action defines the node stmt.n as a new object of subclass If. 
The code 
for the constructor I
f
 is not shown. It creates a new node labeled if with the 
nodes expr.n and stmt1.n as children. 
Expression statements do not begin with a keyword, so we define a new op- 
erator eval and class Eval, which is a subclass of Stmt, to represent expressions 
that are statements. The relevant rule is: 
stmt -+ expr ; { stmt.n = new Eval (expr.n); } 
Representing Blocks in Syntax Trees 
The remaining statement construct in Fig. 2.39 is the block, consisting of a 
sequence of statements. Consider the rules: 
stmt -+ 
block 
{ stmt.n = b1ock.n; } 
block -+ '
C
'
 stmts ' ) I  
{ b1ock.n = stmts.n; } 
The first says that when a statement is a block, it has the same syntax tree as 
the block. The second rule says that the syntax tree for nonterminal block is 
simply the syntax tree for the sequence of statements in the block. 
For simplicity, the language in Fig. 2.39 does not include declarations. Even 
when declarations are included in Appendix A, we shall see that the syntax 
tree for a block is still the syntax tree for the statements in the block. Since 
information from declarations is incorporated into the symbol table, they are 
not needed in the syntax tree. Blocks, with or without declarations, therefore 
appear to be just another statement construct in intermediate code. 
A sequence of statements is represented by using a leaf null for an empty 
statement and a operator seq for a sequence of statements, as in 
stmts -t stmtsl stmt 
{ stmts.n = new Seq(stmtsl.n, 
stmt.n); } 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
Example 2.18 : 
In Fig. 2.40 we see part of a syntax tree representing a block 
or statement list. There are two statements in the list, the first an if-statement 
and the second a while-statement. We do not show the portion of the tree 
above this statement list, and we show only as a triangle each of the necessary 
subtrees: two expression trees for the conditions of the if- and while-statements, 
and two statement trees for their substatements. 
null 
Figure 2.40: Part of a syntax tree for a statement list consisting of an if- 
statement and a while-statement 
Syntax Trees for Expressions 
Previously, we handled the higher precedence of * over + by using three non- 
terminals expr, term, and factor. The number of nonterminals is precisely one 
plus the number of levels of precedence in expressions, as we suggested in Sec- 
tion 2.2.6. In Fig. 2.39, we have two comparison operators, < and <= at one 
precedence level, as well as the usual + and * operators, so we have added one 
additional nonterminal, called add. 
Abstract syntax allows us to group "similar" operators to reduce the number 
of cases arid subclasses of nodes in an implementation of expressions. In this 
chapter, we take "similar" to mean that the type-checking and code-generation 
rules for the operators are similar. For example, typically the operators + and * 
can be grouped, since they can be handled in the same way - 
their requirements 
regarding the types of operands are the same, and they each result in a single 
three-address instruction that applies one operator to two values. In general, 
the grouping of operators in the abstract syntax is based on the needs of the 
later phases of the compiler. The table in Fig. 2.41 specifies the correspondence 
between the concrete and abstract syntax for several of the operators of Java. 
In the concrete syntax, all operators are left associative, except the assign- 
ment operator =, which is right associative. The operators on a line have the 
2.8. INTERMEDIATE CODE GENERATION 
CONCRETE 
SYNTAX ABSTRACT 
SYNTAX 
- 
- 
assign 
I I 
cond 
&& 
cond 
-- 
-- I =  
re1 
< <= >= > 
re1 
+ - 
O P 
* / %  
0 
P 
! 
not 
- 
unary 
minus 
C 1 
access 
Figure 2.41: Concrete and abstract syntax for several Java operators 
same precedence; that is, == and != have the same precedence. The lines are 
in order of increasing precedence; e.g., == has higher precedence than the oper- 
ators && and =. The subscript unary in -,,ary 
is solely to distinguish a leading 
unary minus sign, as in -2, from a binary minus sign, as in 2-a. The operator 
[ I  represents array access, as in aCil . 
The abstract-syntax column specifies the grouping of operators. The assign- 
ment operator = is in a group by itself. The group cond contains the conditional 
boolean operators && and I I .  The group re1 contains the relational comparison 
operators on the lines for == and <. The group op contains the arithmetic 
operators like + and *. Unary minus, boolean negation, and array access are in 
groups by themselves. 
The mapping between concrete and abstract syntax in Fig. 2.41 can be 
implemented by writing a translation scheme. The productions for nonterminals 
expr, rel, add, term, and factor in Fig. 2.39 specify the concrete syntax for a 
representative subset of the operators in Fig. 2.41. The semantic actions in 
these productions create syntax-tree nodes. For example, the rule 
term + terml * factor 
{ term.n = new Op (I*', 
terml .n, 
fact0r.n); } 
creates a node of class Op, which implements the operators grouped under op 
in Fig. 2.41. The constructor 0p has a parameter I*' 
to identify the actual 
operator, in addition to the nodes term1.n and fact0r.n for the subexpressions. 
2.8.3 Static Checking 
Static checks are consistency checks that are done during compilation. Not only 
do they assure that a program can be compiled successfully, 
but they also have 
the potential for catching programming errors early, before a program is run. 
Static checking includes: 
Syntactic Checking. There is more to syntax than grammars. For ex- 
ample, constraints such as an identifier being declared at most once in a 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
scope, or that a break statement must have an enclosing loop or switch 
statement, are syntactic, although they are not encoded in, or enforced 
by, a grammar used for parsing. 
Type Checking. The type rules of a language assure that an operator or 
function is applied to the right number and type of operands. If conversion 
between types is necessary, e.g., when an integer is added to a float, then 
the type-checker can insert an operator into the syntax tree ta represent 
that conversion. We discuss type conversion, using the common term 
"coercion," below. 
L-values and R-values 
We now consider some simple static checks that can be done during the con- 
struction of a syntax tree for a source program. In general, complex static checks 
may need to be done by first constructing an intermediate representation and 
then analyzing it. 
There is a distinction between the meaning of identifiers on the left and 
right sides of an assignment. In each of the assignments 
the right side specifies an integer value, while the left side specifies where the 
value is to be stored. The terms 1-value and r-value refer to values that are 
appropriate on the left and right sides of an assigfiment, respectively. That is, 
r-values are what we usually think of as "values," while bvalues are locations. 
Static checking must assure that the left side of an assignment denotes an 
1-value. An identifier like i has an 1-value, as does an array access like aC21. 
But a constant like 2 is not appropriate on the left side of an assignment, since 
it has an r-value, but not an Cvalue. 
Type Checking 
Type checking assures that the type of a construct matches that expected by 
its context. For example, in the if-statement 
if ( expr ) stmt 
the expression expr is expected to have type boolean. 
Type checking rules follow the operator/operand structure of the abstract 
syntax. Assume the operator re1 represents relational operators such as <=. 
The type rule for the operator group re1 is that its two operands must have the 
same type, and the result has type boolean. Using attribute type for the type 
of an expression, let E consist of re1 applied to El and Ez. The type of E can 
be checked when its node is constructed, by executing code like the following: 
2.8. INTERMEDIATE CODE GENERATION 
if ( El 
.type == E2 
.type ) E.type = boolean; 
else error; 
The idea of matching actual with expected types continues to apply, even 
in the following situations: 
Coercions. A coercion occurs if the type of an operand is automatically 
converted to the type expected by the operator. In an expression like 
2 * 3.14, the usual transformation is to convert the integer 2 into an 
equivalent floating-point number, 2.0, and then perform a floating-point 
operation on the resulting pair of floating-point operands. The language 
definition specifies the allowable coercions. For example, the actual rule 
for re1 discussed above might be that El .type and E2.type are convertible 
to the same type. In that case, it would be legal to compare, say, an 
integer with a float. 
Overloading. The operator + in Java represents addition when applied 
to integers; it means concatenation when applied to strings. A symbol is 
said to be overloaded if it has different meanings depending on its context. 
Thus, + is overloaded in Java. The meaning of an overloaded operator is 
determined by considering the known types of its operands and results. 
For example, we know that the + in z = 
x + 
y is concatenation if we know 
that any of x, y, or z is of type string. However, if we also know that 
another one of these is of type integer, then we have a type error and 
there is no meaning to this use of +. 
2.8.4 Three-Address Code 
Once syntax trees are constructed, further analysis and synthesis can be done 
by evaluating attributes and executing code fragments at nodes in the tree. 
We illustrate the possibilities by walking syntax trees to generate three-address 
code. Specifically, we show how to write functions that process the syntax tree 
and, as a side-effect, emit the necessary three-address code. 
Three- 
Address Instructions 
Three-address code is a sequence of instructions of the form 
x =  
y o p x  
where x, y, and z are names, constants, or compiler-generated temporaries; and 
op stands for an operator. 
Arrays will be handled by using the following two variants of instructions: 
100 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
The first puts the value of z in the location x[y], and the second puts the value 
of y[x] in the location x. 
Three-address instructions are executed in numerical sequence unless forced 
to do otherwise by a conditional or unconditional jump. We choose the following 
instructions for control flow: 
i f F a l s e  x goto L 
if x is false, next execute the instruction labeled L 
ifTrue x goto L 
if x is true, next execute the instruction labeled L 
goto L 
next execute the instruction labeled L 
A label L can be attached to any instruction by prepending a prefix L:. An 
instruction can have more than one label. 
Finally, we need instructions that copy a value. The following three-address 
instruction copies the value of y into x: 
Translation of Statements 
Statements are translated into three-address code by using jump instructions 
to implement the flow of control through the statement. The layout in Fig. 2.42 
illustrates the translation of if expr then stmtl. The jump instruction in the 
layout 
i f  
False x goto after 
jumps over the translation of stmtl if expr evaluates to false. Other statement 
constructs are similarly translated using appropriate jumps around the code for 
their components. 
code to compute 
expr into x 
ifFalse x goto after 
code for stmtl 
Figure 2.42: Code layout for if-statements 
For concreteness, we show the pseudocode for class 1
'
 
in Fig. 2.43. Class 
I
f
 is a subclass of Stmt, as are the classes for the other statement constructs. 
Each subclass of Stmt has a constructor - 
I
f
 in this case - 
and a function gen 
that is called to generate three-address code for this kind of statement. 
2.8. INTERMEDIATE CODE GENERATION 
class If extends Stmt { 
Expr E; Stmt S; 
public If(Expr x, Stmt y )  { E = x; 
S = 
y; after = newlabel(); 
} 
public void gen() { 
Expr n = E.rvalue(); 
emit( "ifFalse " + n.toString() + " goto " + after); 
S-genO; 
emit (after + " 
: 
" 
) ; 
Figure 2.43: Function gen in class If generates three-address code 
The constructor If in Fig. 2.43 creates syntax-tree nodes for if-statements. 
It is called with two parameters, an expression node x and a statement node 
y ,  which it saves as attributes E and S. The constructor also assigns attribute 
after a unique new label, by calling function newlabel(). 
The label will be used 
according to the layout in Fig. 2.42. 
Once the entire syntax tree for a source program is constructed, the function 
gen is called at the root of the syntax tree. Since a program is a block in 
our simple language, the root of the syntax tree represents the sequence of 
statements in the block. All statement classes contain a function gen. 
The pseudocode for function gen of class If in Fig. 2.43 is representative. It 
calls E.rvalue() to translate the expression E (the boolean-valued expression 
that is part of the if-statements) and saves the result node returned by E. 
Translation of expressions will be discussed shortly. Function gen then emits a 
conditional jump and calls S.gen() 
to translate the substatement S. 
Translation of Expressions 
We now illustrate the translation of expressions by considering expressions con- 
taining binary operators op, array accesses, and assignments, in addition to 
constants and identifiers. For simplicity, in an array access y [x], we require that 
y be an identifier.13 For a detailed discussion of intermediate code generation 
for expressions, see Section 6.4. 
We shall take the simple approach of generating one three-address instruc- 
tion for each operator node in the syntax tree for an expression. No code is 
generated for identifiers and constants, since they can appear as addresses in 
instructions. If a node x of class Expr has operator op, then an instruction is 
emitted to compute the value at node x into a compiler generated "temporary" 
name, say t. Thus, i-j+k translates into two instructions 
13This simple language supports aCa 
C
n
l
 
I , but not a 
[ml 
[nl 
. Note that a 
[a 
[nl 
I has the 
form a [El, 
where E 
is a C
n
l
 . 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
With array accesses and assignments comes the need to distinguish between 
1-values and r-values. For example, 2*a [il can be translated by computing the 
r-value of a [i] into a temporary, as in 
But, we cannot simply use a temporary in place of a[i], if a[i] appears on 
the left side of an assignment. 
The simple approach uses the two functions lualue and rualue, which appear 
in Fig. 2.44 and 2.45, respectively. When function rualue is applied to a nonleaf 
node x, it generates instructions to compute x into a temporary, and returns 
a new node representing the temporary. When function lualue is applied to a 
nonleaf, it also generates instructions to compute the subtrees below x, and 
returns a node representing the "address" for x. 
We describe function lualue first, since it has fewer cases. When applied 
to a node x, function lualue simply returns x if it is the node for an identifier 
(i.e., if x is of class Id). In our simple language, the only other case where 
an expression has an I-value occurs when x represents an array access, such as 
a[il. In this case, x will have the form Access(y, 
x), where class Access is a 
subclass of Expr, y represents the name of the accessed array, and x represents 
the offset (index) of the chosen element in that array. From the pseudo-code 
in Fig. 2.44, function lualue calls rualue(z) to generate instructions, if needed, 
to compute the r-value of x. It then con.structs and returns a new Access node 
with children for the array name y and the r-value of x. 
Expr lvalue(x : 
Expr) { 
if ( x is an Id node ) return x; 
else if ( x is an Access (y, z) node and y is an Id node ) { 
return new Access (y 
, 
ruaIue(z)) 
; 
1 
else error; 
Figure 2.44: Pseudocode for function lualue 
Example 2.19: When node x represents the array access a[2*k], the call 
lualue(x) generates an instruction 
and returns a new node x1 representing the 1-value act], where t is a new 
temporary name. 
In detail, the code fragment 
2.8. INTERMEDIATE CODE GENERATION 
return new Access (y 
, 
rvalue(z)); 
is rea,ched 
with y being the node for a and z being the node for expression 2*k. 
The call rvalue(z) generates code for the expression 2*k (i.e., the three-address 
statement t = 2 * k) and returns the new node z' representing the temporary 
name t. That node x' becomes the value of the second field in the new Access 
node x' that is created. 
Expr rvalue(x : 
Expr) { 
if ( x is an Id or a Constant node ) return x; 
else if ( x is an Op (op, 
y 
, 
x) or a Re1 
(op, 
y 
, 
x) node ) { 
t = new temporary; 
emit string for t = rvalue(y) op rvalue(x); 
return a new node for t; 
1 
else if ( x is an Access (y, z) node ) { 
t = 
new temporary; 
call lvalue(x), which returns Access (y 
,xl); 
emit string for t = Access (y, 
z'); 
return a new node for t; 
1 
else if ( x is an Assign (y, 
x) node ) { 
Z' = rvalue(x); 
emit string for lvalue(y) = x'; 
return x'; 
1 
1 
Figure 2.45: Pseudocode for function rvalue 
Function rvalue in Fig. 2.45 generates instructions and returns a possibly 
new node. When x represents an identifier or a constant, rvalue returns x itself. 
In all other cases, it returns an Id node for a new temporary t. The cases are 
as follows: 
When x represents y op z, the code first computes y' = rvalue(y) and 
x' = rvalue(z). It creates a new temporary t and generates an instruc- 
tion t = y' o p  z' (more precisely, an instruction formed from the string 
representations of t, y', op, and 2'). It returns a node for identifier t. 
When x represents an array access y Czl, we can reuse function lvalue. 
The call lvalue(x) returns an access y Cz'l , 
where z' represents an identifier 
holding the offset for the array access. The code creates a new temporary 
t, generates an instruction based on t = y Cx'l , 
and returns a node for t. 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
When s represents y = z, then the code first computes x
'
 = rvalue(z). It 
generates an instruction based on lvalue(y) = x' and returns the node x'. 
Example 2.20 : 
When applied to the syntax tree for 
function rvalue generates 
That is, the root is an Assign node with first argument a [i] and second ar- 
gument 2*a C j -kl 
. Thus, the third case applies, and function rvalue recursively 
evaluates 2*a [ 
j 
-kl 
. The root of this subtree is the Op node for *, which causes 
a new temporary t 
1 
to be created, before the left operand, 2 is evaluated, and 
then the right operand. The constant 2 generates no three-address code, and 
its r-value is returned as a Constant node with value 2. 
The right operand a [ 
j 
-k] is an Access node, which causes a new temporary 
t 2  to be created, before function lvalue is called on this node. Recursively, 
rvalue is called on the expression j 
-k. As a side-effect of this call, the three- 
address statement t 3  = j - k is generated, after the new temporary t 3  is 
created. Then, returning to the call of lvalue on a [
j
 
-k] 
, the temporary t 
2 is 
assigned the r-value of the entire access-expression, that is, t 2  = a [ t 3  1. 
Now, we return to the call of rvalue on the Op node 2*a [j 
-k] 
, 
which earlier 
created temporary t 
I. A three-address statement t 
1 = 2 * t 2  
is generated as 
a side-effect, to evaluate this multiplication-expression. Last, the call to rvalue 
on the whole expression completes by calling lvalue on the left side a h 1  
and 
then generating a three-address instruction a [ i 1 = ti, 
in which the right 
side of the assignment is assigned to the left side. 
Better Code for Expressions 
We can improve on function rvalue in Fig. 2.45 and generate fewer three-address 
instructions, in several ways: 
Reduce the number of copy instructions in a subsequent optimization 
phase. For example, the pair of instructions t = i+l 
and i = t can be 
combined into i 
= i + l ,  
if there are no subsequent uses of t. 
Generate fewer instructions in the first place by taking context into ac- 
count. For example, if the left side of a three-address assignment is an 
array access a [t] 
, 
then the right side must be a name, a constant, or a 
temporary, all of which use just one address. But if the left side is a name 
x, then the right side can be an operation y op z that uses two addresses. 
2.9. SUMMARY OF CHAPTER 2 
105 
We can avoid some copy instructions by modifying the translation functions 
generate a partial instruction that computes, say j+k, but does not commit 
where the result is to be placed, signified by a null address for the result: 
null = j + k 
(2.8) 
The null result address is later replaced by either an identifier or a temporary, 
as appropriate. It is replaced by an identifier if j+k is on the right side of an 
assigriment, as in i= 
j +k ; 
, 
in which case (2.8) becomes 
But, if j+k is a subexpression, as in j+k+l, then the null result address in (2.8) 
is replaced by a new temporary t, 
and a new partial instruction is generated 
t = j + k  
null = t + 1 
Many compilers make every effort to generate code that is as good as or bet- 
ter than hand-written assembly code produced by experts. If code-optimization 
techniques, such as the ones in Chapter 9 are used, then an effective strategy 
may well be to use a simple approach for intermediate code generation, and 
rely on the code optimizer to eliminate unnecessary instructions. 
2.8.5 
Exercises for Section 2.8 
Exercise 2.8.1 : 
For-statements in C and Java have the form: 
f o r  ( exprl ; expr2 ; expr3 ) stmt 
The first expression is executed before the loop; it is typically used for initializ- 
ing the loop index. The second expression is a test made before each iteration 
of the loop; the loop is exited if the expression becomes 0. The loop itself can be 
thought of as the statement Cstrnt expr3 ; 
1. The third expression is executed 
at the end of each iteration; it is typically used to increment the loop index. 
The meaning of the for-statement is similar to 
exprl ; while ( expr2 ) (stmt exprs ; ) 
Define a class For for for-statements, similar to class I
f
 in Fig. 2.43. 
Exercise 2.8.2 : 
The programming language C does not have a boolean type. 
Show how a C compiler might translate an if-statement into three-address code. 
2.9 Summary of Chapter 2 
The syntax-directed techniques in this chapter can be used to construct compiler 
front ends, such as those illustrated in Fig. 2.46. 
CHAPTER 2. A SIMPLE SYNTAX-DIRECTED TRANSLATOR 
if( peek == '\n' 1 line = line + 1; 
r 
Lexical Analyzer 
(
i
f
'
)
 
(() (id, 
"peek") 
(eq) 
(const, 
'\nY) 
()) 
(id, 
"line") 
(assign) 
(id, 
"line") (+) (num, 
1) (;) 
Syntax-Directed Translator 
/if\ 
1: tl = (int) '\nY 
2: ifFalse peek == ti goto 4 
/""\ 
assi n 
/ B 
3: line = line + I 
4
:
 
peek 
(int) 
line 
I 
/+\ 
' 
\n 
' 
line 
1 
Figure 2.46: Two possible translations of a statement 
+ The starting point for a syntax-directed translator is a grammar for the 
source language. A grammar describes the hierarchical structure of pro- 
grams. It is defined in terms of elementary symbols called terminals and 
variable symbols called nonterminals. These symbols represent language 
constructs. The rules or productions of a grammar consist of a nonterminal 
called the head or left side of a production and a sequence of terminals 
and nonterminals called the body or right side of the production. One 
nonterminal is designated as the start symbol. 
+ In specifying a translator, it is helpful to attach attributes to programming 
construct, where an attribute is any quantity associated with a construct. 
Since constructs are represented by grammar symbols, the concept of 
attributes extends to grammar symbols. Examples of attributes include 
an integer value associated with a terminal nurn representing numbers, 
and a string associated with a terminal i
d
 representing identifiers. 
+ A lexical analyzer reads the input one character at a time and produces 
as output a stream of tokens, where a token consists of a terminal symbol 
along with additional information in the form of attribute values. In 
Fig. 2.46, 
tokens are written as tuples enclosed between ( ). The token 
(id, 
"peek") 
consists of the terminal i
d
 and a pointer to the symbol-table 
entry containing the string "peek". 
The translator uses the table to keep 
2.9. SUMMARY OF CHAPTER 2 
track of reserved words and identifiers that have already been seen. 
+ Parsing is the problem of figuring out how a string of terminals can be 
derived from the start symbol of the grammar by repeatedly replacing a 
nonterminal by the body of one of its productions. Conceptually, a parser 
builds a parse tree in which the root is labeled with the start symbol, 
each nonleaf corresponds to a production, and each leaf is labeled with 
a terminal or the empty string E-. The parse tree derives the string of 
terminals at the leaves, read from left to right. 
+ Efficient parsers can be built by hand, using a top-down (from the root to 
the leaves of a parse tree) method called predictive parsing. A predictive 
parser has a procedure for each nonterminal; procedure bodies mimic the 
productions for nonterminals; and, the flow of control through the pro- 
cedure bodies can be determined unambiguously by looking one symbol 
ahead in the input stream. See Chapter 4 for other approaches to parsing. 
+ Syntax-directed translation is done by attaching either rules or program 
fragments to productions in a grammar. In this chapter, we have consid- 
ered only synthesized attributes - 
the value of a synthesized attribute at 
any node x can depend only on attributes at the children of x, if any. A 
syntax-directed definition attaches rules to productions; the rules compute 
attribute vales. A translation scheme embeds program fragments called 
semantic actions in production bodies. The actions are executed in the 
order that productions are used during syntax analysis. 
+ The result of syntax analysis is a representation of the source program, 
called intermediate code. Two primary forms of intermediate code are il- 
lustrated in Fig. 2.46. An abstract syntax tree has nodes for programming 
constructs; the children of a node give the meaningful subconstructs. Al- 
ternatively, three-address code is a sequence of instructions in which each 
instruction carries out a single operation. 
+ Symbol tables are data structures that hold information about identifiers. 
Information is put into the symbol table when the declaration of an iden- 
tifier is analyzed. A semantic action gets information from the symbol 
table when the identifier is subsequently used, for example, as a factor in 
an expression. 
Chapter 3 
Lexical Analysis 
In this chapter we show how to construct a lexical analyzer. To implement a 
lexical analyzer by hand, it helps to start with a diagram or other description for 
the lexemes of each token. We can then write code to identify each occurrence of 
each lexeme on the input and to return information about the token identified. 
We can also produce a lexical analyzer automatically by specifying the lex- 
eme patterns to a lexical-analyzer generator and compiling those patterns into 
code that functions as a lexical analyzer. This approach makes it easier to mod- 
ify a lexical analyzer, since we have only to rewrite the affected patterns, not 
the entire program. It also speeds up the process of implementing the lexical 
analyzer, since the programmer specifies the software at the very high level of 
patterns and relies on the generator to produce the detailed code. We shall 
introduce in Section 3.5 a lexical-analyzer generator called Lex (or Flex in a 
more recent embodiment). 
We begin the study of lexical-analyzer generators by introducing regular 
expressions, a convenient notation for specifying lexeme patterns. We show 
how this notation can be transformed, first into nondeterministic automata 
and then into deterministic automata. The latter two notations can be used as 
input to a "driver," that is, code which simulates these automata and uses them 
as a guide to determining the next token. This driver and the specification of 
the automaton form the nucleus of the lexical analyzer. 
3.1 The Role of the Lexical Analyzer 
As the first phase of a compiler, the main task of the lexical analyzer is to 
read the input characters of the source program, group them into lexemes, and 
produce as output a sequence of tokens for each lexeme in the source program. 
The stream of tokens is sent to the parser for syntax analysis. It is common 
for the lexical analyzer to interact with the symbol table as well. When the 
lexical analyzer discovers a lexeme constituting an identifier, it needs to enter 
that lexeme into the symbol table. In some cases, information regarding the 
110 
CHAPTER 3. LEXICAL ANALYSIS 
kind of identifier may be read from the symbol table by the lexical analyzer to 
assist it in determining the proper token it must pass to the parser. 
These interactions are suggested in Fig. 3.1. Commonly, the interaction is 
implemented by having the parser call the lexical analyzer. The call, suggested 
by the getNextToken command, causes the lexical analyzer to read characters 
from its input until it can identify the next lexeme and produce for it the next 
token, which it returns to the parser. 
Symbol 
Table 
source 
program -t 
Figure 3.1: Interactions between the lexical analyzer and the parser 
Since the lexical analyzer is the part of the compiler that reads the source 
text, it may perform certain other tasks besides identification of lexemes. One 
such task is stripping out comments and whitespace (blank, newline, tab, and 
perhaps other characters that are used to separate tokens in the input). Another 
task is correlating error messages generated by the compiler with the source 
program. For instance, the lexical analyzer may keep track of the number 
of newline characters seen, so it can associate a line number with each error 
message. In some compilers, the lexical analyzer makes a copy of the source 
program with the error messages inserted at the appropriate positions. I
f
 the 
source program uses a macro-preprocessor, the expansion of macros may also 
be performed by the lexical analyzer. 
Sometimes, lexical analyzers are divided into a cascade of two processes: 
Lexical 
Analyzer 
a) Scanning consists of the simple processes that do not require tokenization 
of the input, such as deletion of comments and compaction of consecutive 
whitespace characters into one. 
b) Lexical analysis proper is the more complex portion, where the scanner 
produces the sequence of tokens as output. 
token 
b 
+ 
getNextToken 
3.1.1 
Lexical Analysis Versus Parsing 
There are a number of reasons why the analysis portion of a compiler is normally 
separated into lexical analysis and parsing (syntax analysis) phases. 
Parser 
to semantic 
-t analysis 
3.1. THE ROLE OF THE LEXICAL ANALYZER 
1. Simplicity of design is the most important consideration. The separation 
of lexical and syntactic analysis often allows us to simplify at least one 
of these tasks. For example, a parser that had to deal with comments 
and whitespace as syntactic units would be considerably more complex 
than one that can assume comments and whitespace have already been 
removed by the lexical analyzer. If we are designing a new language, 
separating lexical and syntactic concerns can lead to a cleaner overall 
language design. 
2. Compiler efficiency is improved. A separate lexical analyzer allows us to 
apply specialized techniques that serve only the lexical task, not the job 
of parsing. In addition, specialized buffering techniques for reading input 
characters can speed up the compiler significantly. 
3. Compiler portability is enhanced. Input-device-specific peculiarities can 
be restricted to the lexical analyzer. 
3.1.2 
Tokens, Patterns, and Lexemes 
When discussing lexical analysis, we use three related but distinct terms: 
A token is a pair consisting of a token name and an optional attribute 
value. The token name is an abstract symbol representing a kind of 
lexical unit, e.g., a particular keyword, or a sequence of input characters 
denoting an identifier. The token names are the input symbols that the 
parser processes. In what follows, we shall generally write the name of a 
token in boldface. We will often refer to a token by its token name. 
A pattern is a description of the form that the lexemes of a token may take. 
In the case of a keyword as a token, the pattern is just the sequence of 
characters that form the keyword. For identifiers and some other tokens, 
the pattern is a more complex structure that is matched by many strings. 
A lexeme is a sequence of characters in the source program that matches 
the pattern for a token and is identified by the lexical analyzer as an 
ihstance of that token. 
Example 3.1 : 
Figure 3.2 gives some typical tokens, their informally described 
patterns, and some sample lexemes. To see how these concepts are used in 
practice, in the C statement 
printf ("Total = %d\nI1, 
score) ; 
both printf and score are lexemes matching the pattern for token id, and 
"Total = %d\nI1 
is a lexeme matching literal. 
In many programming languages, the following classes cover most or all of 
the tokens: 
112 
CHAPTER 3. LEXICAL ANALYSIS 
else 
1 characters e, 1, 
s, e 
I else 
TOKEN 
if 
lit 
era1 
1 anything but ", surrounded by If's 1 "core dumped" 
INFORMAL 
DESCRIPTION 
characters i, 
f 
comparison 
id 
number 
Figure 3.2: Examples of tokens 
SAMPLE LEXEMES 
i f  
1. One token for each keyword. The pattern for a keyword is the same as 
the keyword itself. 
< or > or <= or >= or == or ! 
= 
letter followed by letters and digits 
any numeric constant 
2. Tokens for thd operators, either individually or in classes such as the token 
comparison rhentioned in Fig. 3.2. 
<=, != 
pi, score, D2 
3.14159, 0, 6.02e23 
3. One token representing all identifiers. 
4. One or more tokens representing constants, such as numbers and literal 
strings. 
5. Tokens for each punctuation symbol, such as left and right parentheses, 
comma, and semicolon. 
3.1.3 Attributes for Tokens 
When more than one lexeme can match a pattern, the lexical analyzer must 
provide the subsequent compiler phases additional information about the par- 
ticular lexeme that matched. For example, the pattern for token number 
matches both 0 and 1, but it is extremely important for the code generator to 
know which lexeme was found in the source program. Thus, in many cases the 
lexical analyzer returns to the parser not only a token name, but an attribute 
value that describes the lexeme represented by the token; the token name in- 
fluences parsing decisions, while the attribute value influences translation of 
tokens after the parse. 
We shall assume that tokens have at most one associated attribute, although 
this attribute may have a structure that combines several pieces of information. 
The most important example is the token id, where we need to associate with 
the token a great deal of information. Normally, information about an identi- 
fier - 
e.g., its lexeme, its type, and the location at which it is first found (in 
case an error message about that identifier must be issued) - 
is kept in the 
symbol table. Thus, the appropriate attribute value for an identifier is a pointer 
to the symbol-table entry for that identifier. 
3.1. THE ROLE OF THE LEXICAL ANALYZER 
113 
Tricky Problems When Recognizing Tokens 
Usually, given the pattern describing the lexemes of a token, it is relatively 
simple to recognize matching lexemes when they occur on the input. How- 
ever, in some languages it is not immediately apparent when we have seen 
an instance of a lexeme corresponding to a token. The following example 
is taken from Fortran, in the fixed-format still allowed in Fortran 90. In 
the statement 
DO 5 I = 1.25 
it is not apparent that the first lexeme is D051, an instance of the identifier 
token, until we see the dot following the 1. Note that blanks in fixed-format 
Fortran are ignored (an archaic convention). Had we seen a comma instead 
of the dot, we would have had a do-statement 
DO 5 I = 1,25 
in which the first lexeme is the keyword DO. 
Example 3.2 : The token names and associated attribute values for the For- 
tran statement 
are written below as a sequence of pairs. 
<id, 
pointer to symbol-table entry for E> 
< 
assign-op > 
<id, pointer to symbol-table entry for M> 
<mult 
-op> 
<id, pointer to symbol-table entry for C> 
<exp-op> 
<number 
, 
integer value 2 > 
Note that in certain pairs, especially operators, punctuation, and keywords, 
there is no need for an attribute value. In this example, the token number has 
been given an integer-valued attribute. In practice, a typical compiler would 
instead store a character string representing the constant and use as an attribute 
value for number a pointer to that string. 
I3 
3.1.4 Lexical Errors 
It is hard for a lexical analyzer to tell, without the aid of other components, 
that there is a source-code error. For instance, if the string f i is encountered 
for the first time in a C program in the context: 
CHAPTER 3. LEXICAL ANALYSIS 
a lexical analyzer cannot tell whether f i is a misspelling of the keyword i f  or 
an undeclared function identifier. Since f i is a valid lexeme for the token id, 
the lexical analyzer must return the token id to the parser and let some other 
phase of the compiler - 
probably the parser in this case - 
handle an error 
due to transposition of the letters. 
However, suppose a situation arises in which the lexical analyzer is unable 
to proceed because none of the patterns for tokens matches any prefix of the 
remaining input. The simplest recovery strategy is "panic mode" recovery. We 
delete successive characters from the remaining input, until the lexical analyzer 
can find a well-formed token at the beginning of what input is left. This recovery 
technique may confuse the parser, but in an interactive computing environment 
it may be quite adequate. 
Other possible error-recovery actions are: 
1. Delete one character from the remaining input. 
2. Insert a missing character into the remaining input. 
3. Replace a character by another character. 
4. Transpose two adjacent characters. 
Transformations like these may be tried in an attempt to repair the input. The 
simplest such strategy is to see whether a prefix of the remaining input can 
be transformed into a valid lexeme by a single transformation. This strategy 
makes sense, since in practice most lexical errors involve a single character. A 
more general correction strategy is to find the smallest number of transforma- 
tions needed to convert the source program into one that consists only of valid 
lexemes, but this approach is considered too expensive in practice to be worth 
the effort. 
3.1.5 Exercises for Section 3.1 
Exercise 3.1.1 : 
Divide the following C + 
+ program: 
f l o a t  lirnitedSquare(x) f l o a t  x ( 
/* returns x-squared, but never more than 100 */ 
return (x<=-10.01 ~x>=lO.O)?iOO:x*x; 
> 
into appropriate lexemes, using the discussion of Section 3.1.2 
as a guide. Which 
lexemes should get associated lexical values? What should those values be? 
! 
Exercise 3.1.2 : 
Tagged languages like HTML or XML are different from con- 
ventional programming languages in that the punctuation (tags) are either very 
numerous (as in HTML) or a user-definable set (as in XML). Further, tags can 
often have parameters. Suggest how to divide the following HTML document: 
3.2. INPUT BUFFERING 
Here is a photo of <B>my 
house</B>: 
<P><IMG 
SRC = "house. 
gif 
I1><BR> 
See <A HREF = "morePix. 
htmll'>More 
Pictures</A> if you 
liked that one. 
<P> 
into appropriate lexemes. Which lexemes should get associated lexical values, 
and what should those values be? 
3.2 
Input Buffering 
Before discussing the problem of recognizing 
lexemes in the input, let us examine 
some ways that the simple but important task of reading the source program 
can be speeded. This task is made difficult by the fact that we often have 
to look one or more characters beyond the next lexeme before we can be sure 
we have the right lexeme. The box on "Tricky Problems When Recognizing 
Tokens" in Section 3.1 gave an extreme example, but there are many situations 
where we need to look at least one additional character ahead. For instance, 
we cannot be sure we've seen the end of an identifier until we see a character 
that is not a letter or digit, and therefore is not part of the lexeme for id. In 
C, single-character operators like -, =, or < could also be the beginning of a 
two-character operator like ->, ==, or <=. Thus, we shall introduce a two-buffer 
scheme that handles large lookaheads safely. We then consider an improvement 
involving "sentinels" that saves time checking for the ends of buffers. 
3.2.1 Buffer Pairs 
Because of the amount of time taken to process characters and the large number 
of characters that must be processed during the compilation of a large source 
program, specialized buffering techniques have been developed to reduce the 
amount of overhead required to process a single input character. An impor- 
tant scheme involves two buffers that are alternately reloaded, as suggested in 
Fig. 3.3. 
I 
forward 
Figure 3.3: Using a pair of input buffers 
Each buffer is of the same size N, and N is usually the size of a disk block, 
e.g., 4096 bytes. Using one system read command we can read N characters 
inio a buffer, rather than using one system call per character. If fewer than N 
characters remain in the input file, then a special character, represented by eof, 
116 
CHAPTER 3. LEXICAL ANALYSIS 
marks the end of the source file and is different from any possible character of 
the source program. 
Two pointers to the input are maintained: 
I. Pointer lexemeBegin, marks the beginning of the current lexeme, whose 
extent we are attempting to determine. 
2. Pointer forward scans ahead until a pattern match is found; the exact 
strategy whereby this determination is made will be covered in the balance 
of this chapter. 
Once the next lexeme is determined, forward is set to the character at its right 
end. Then, after the lexeme is recorded as an attribute value of a token returned 
to the parser, 1exemeBegin is set to the character immediately after the lexeme 
just found. In Fig. 3.3, we see forward has passed the end of the next lexeme, 
** (the Fortran exponentiation operator), and must be retracted one position 
to its left. 
Advancing forward requires that we first test whether we have reached the 
end of one of the buffers, and if so, we must reload the other buffer from the 
input, and move forward to the beginning of the newly loaded buffer. As long 
as we never need to look so far ahead of the actual lexeme that the sum of the 
lexeme's length plus the distance we look ahead is greater than N, we shall 
never overwrite the lexeme in its buffer before determining it. 
3.2.2 Sentinels 
If we use the scheme of Section 3.2.1 as described, we must check, each time we 
advance forward, that we have not moved off one of the buffers; if we do, then 
we must also reload the other buffer. Thus, for each character read, we make 
two tests: one for the end of the buffer, and one to determine what character 
is read (the latter may be a multiway branch). We can combine the buffer-end 
test with the test for the current character if we extend each buffer to hold a 
sentinel character at the end. The sentinel is a special character that cannot 
be part of the source program, and a natural choice is the character eof. 
Figure 3.4 shows the same arrangement as Fig. 3.3, but with the sentinels 
added. Note that eof retains its use as a marker for the end of the entire input. 
Any eof that appears other than at the end of a buffer means that the input 
is at an end. Figure 3.5 summarizes the algorithm for advancing forward. 
Notice how the first test, which can be part of a multiway branch based on the 
character pointed to by forward, is the only test we make, except in the case 
where we actually are at the end of a buffer or the end of the input. 
3.3 Specification of Tokens 
Regular expressions are an important notation for specifying lexeme patterns. 
While they cannot express all possible patterns, they are very effective in spec- 
3.3. SPECIFICATION OF TOKENS 
117 
Can We Run Out of Buffer Space? 
In most modern languages, lexemes are short, and one or two characters 
of lookahead is sufficient. Thus a buffer size N in the thousands is ample, 
and the double-buffer scheme of Section 3.2.1 works without problem. 
However, there are some risks. For example, if character strings can be 
very long, extending over many lines, then we could face the possibility 
that a lexeme is longer than N. To avoid problems with long character 
strings, we can treat them as a concatenation of components, one from 
each line over which the string is written. For instance, in Java it is 
conventional to represent long strings by writing a piece on each line and 
concatenating pieces with a + operator at the end of each piece. 
A more difficult problem occurs when arbitrarily long lookahead may 
be needed. For example, some languages like PL/I do not treat key- 
words as reserved; that is, you can use identifiers with the same name as 
a keyword like D
E
C
L
A
R
E
.
 If the lexical analyzer is presented with text of a 
PL/I program that begins D
E
C
L
A
R
E
 ( ARGI, ARG2,. . 
. it cannot be sure 
whether D
E
C
L
A
R
E
 is a keyword, and A
R
G
I
 and so on are variables being de- 
clared, or whether D
E
C
L
A
R
E
 is a procedure name with its arguments. For 
this reason, modern languages tend to reserve their keywords. However, if 
not, one can treat a keyword like D
E
C
L
A
R
E
 as an ambiguous identifier, and 
let the parser resolve the issue, perhaps in conjunction with symbol-table 
lookup. 
ifying those types of patterns that we actually need for tokens. In this section 
we shall study the formal notation for regular expressions, and in Section 3.5 
we shall see how these expressions are used in a lexical-analyzer generator. 
Then, Section 3.7 shows how to build the lexical analyzer by converting regular 
expressions to automata that perform the recognition of the specified tokens. 
3.3.1 
Strings and Languages 
An alphabet is any finite set of symbols. Typical examples of symbols are let- 
ters, digits, and punctuation. The set {0,1) is the binary alphabet. ASCII is an 
important example of an alphabet; it is used in many software systems. Uni- 
Figure 3.4: Sentinels at the end of each buffer 
'
'
.
'
.
a
'
'
.
.
 
'
"
.
'
'
'
 
- 
: 
: 
: 
: E :  
-
:
 
"
.
"
a
'
.
"
 
'
.
'
.
'
.
'
.
I
'
 
.
.
#
.
.
,
 
: ~ ~ * ~ e o f ~ ~ * ! * ~ 2 ! e o f :  
: 
: 
: 
: 
:eof 
"
"
'
*
"
'
.
 
forward 
lexemeBegin 
CHAPTER 3. LEXICAL ANALYSIS 
switch ( *forward++ ) { 
case eof: 
if (forward is at end of first buffer ) { 
reload second buffer; 
forward = beginning of second buffer; 
1 
else if (forward is at end of second buffer ) { 
reload first buffer; 
forward = beginning of first buffer; 
1 
else /* 
eof within a buffer marks the end of input */ 
terminate lexical analysis; 
break; 
Cases for the other characters 
1 
Figure 3.5: Lookahead code with sentinels 
Implementing Multiway Branches 
We might imagine that the switch in Fig. 3.5 requires many steps to exe- 
cute, and that placing the case eof first is not a wise choice. Actually, it 
doesn't matter in what order we list the cases for each character. In prac- 
tice, a multiway branch depending on the input character is be made in 
one step by jumping to an address found in an array of addresses, indexed 
by characters. 
code, which includes approximately 100,000 characters from alphabets around 
the world, is another important example of an alphabet. 
A string over an alphabet is a finite sequence of symbols drawn from that 
alphabet. In language theory, the terms "sentence" and "word" are often used 
as synonyms for "string." The length of a string s, usually written Isl, is the 
number of occurrences of symbols in s. For example, banana is a string of 
length six. The empty string, denoted 6, is the string of length zero. 
A language is any countable set of strings over some fixed alphabet. This 
definition is very broad. Abstract languages like 0, 
the empty set, or (€1, 
the 
set containing only the empty string, are languages under this definition. So 
too are the set of all syntactically well-formed C programs and the set of all 
grammatically correct English sentences, although the latter two languages are 
difficult to specify exactly. Note that the definition of "language" does not 
require that any meaning be ascribed to the strings in the language. Methods 
for defining the "meaning" of strings are discussed in Chapter 5. 
3.3. SPECIFICATION OF TOKENS 
119 
Terms for Parts of Strings 
The following string-related terms are commonly used: 
1. A prefix of string s is any string obtained by removing zero or more 
symbols from the end of s. For example, ban, banana, and E are 
prefixes of banana. 
2. A sufix of string s is any string obtained by removing zero or more 
symbols from the beginning of s. For example, nana, banana, and E 
are suffixes of banana. 
3. A substring of s is obtained by deleting any prefix and any suffix 
from s. For instance, banana, nan, and E are substrings of banana. 
4. The proper prefixes, suffixes, and substrings of a string s are those, 
prefixes, suffixes, and substrings, respectively, of s that are not E or 
not equal to s itself. 
5 .  A subsequence of s is any string formed by deleting zero or more 
not necessarily consecutive positions of s. For example, baan is a 
subsequence of banana. 
If x and y are strings, then the concatenation of x and y, denoted xy, is the 
string formed by appending y to x. For example, if x = dog and y = house, 
then xy = doghouse. The empty string is the identity under concatenation; 
that is, for any string s, ES = 
SE = 
s. 
If we think of concatenation as a product, we can define the 'kxponentiation" 
of strings as follows. Define so to be E, and for all i > 0, define si to be si-ls. 
Since ES = 
S, it follows that s1 = s. Then s2 = ss, s3 = sss, and so on. 
3.3.2 Operations on Languages 
In lexical analysis, the most important operations on languages are union, con- 
catenation, and closure, which are defined formally in Fig. 3.6. Union is the 
familiar operation on sets. The concatenation of languages is all strings formed 
by taking a string from the first language and a string from the second lan- 
guage, in all possible ways, and concatenating them. The (Kleene) closure of a 
language L, denoted L*, is the set of strings you get by concatenating L zero 
or more times. Note that Lo, 
the "concatenation of L zero times," is defined to 
be {E), and inductively, L~ is Li-'L. Finally, the positive closure, denoted L+, 
is the same as the Kleene closure, but without the term Lo. That is, E will not 
be in L+ unless it is in L itself. 
CHAPTER 3. LEXICAL ANALYSIS 
OPERATION , 
Union of L and M 
Figure 3.6: Definitions of operations on languages 
DEFINITION 
AND NOTATION 
L U M = {s ( s is in L or s is in M )  
Concatenation of L and M 
Kleene closure of L 
Positive closure of L 
Example 3.3 
'
:
 Let L be the set of letters {A, B, 
. . 
. , 
Z, a, 
b, 
. 
. . 
, 
z )  and let D 
be the set of digits {0,1,. 
. 
.9). We may think of L and D in two, essentially 
equivalent, ways. One way is that L and D are, respectively, the alphabets of 
uppercase and lowercase letters and of digits. The second way is that L and D 
are languages, all of whose strings happen to be of length one. Here are some 
other languages that can be constructed from languages L and D, using the 
operators of Fig. 3.6: 
LM = {st I s is in L and t is in M )  
L* = U F O  Li 
L f  =U& 
L~ 
1
.
 L U D is the set of letters and digits - 
strictly speaking the language 
with 62 strings of length one, each of which strings is either one letter or 
one digit. 
2. LD is the set df 520 strings of length two, each consisting of one letter 
followed by one digit. 
3. L4 is the set of all 4-letter strings. 
4. L* is the set of ail strings of letters, including e, the empty string. 
5. L(L 
U D)* is the set of all strings of letters and digits beginning with a 
letter. 
6. D+ is the set of all strings of one or more digits. 
3.3.3 Regular Expressions 
Suppose we wanted to describe the set of valid C identifiers. It is almost ex- 
actly the language described in item (5) above; the only difference is that the 
underscore is included among the letters. 
In Example 3.3, we were able to describe identifiers by giving names to sets 
of letters and digits and using the language operators union, concatenation, 
and closure. This process is so useful that a notation called regular expressions 
has come into common use for describing all the languages that can be built 
from these operators applied to the symbols of some alphabet. In this notation, 
if letter- is established to stand for any letter or the underscore, and digit- is 
3.3. SPECIFICATION OF TOKENS 
121 
established to stand for any digit, then we could describe the language of C 
identifiers by: 
letter- ( letter- I digit )* 
The vertical bar above means union, the parentheses are used to group subex- 
pressions, the star means "zero or more occurrences of," and the juxtaposition 
of letter- with the remainder of the expression signifies concatenation. 
The regular expressions are built recursively out of smaller regular expres- 
sions, using the rules described below. Each regular expression r denotes a 
language L(r), 
which is also defined recursively from the languages denoted by 
r's subexpressions. Here are the rules that define the regular expressions over 
some alphabet C and the languages that those expressions denote. 
BASIS: There are two rules that form the basis: 
1. E is a regular expression, and L 
(E) 
is {E) , 
that is, the language whose sole 
member is the empty string. 
2. If a is a symbol in C, then a is a regular expression, and L(a) = {a), that 
is, the language with one string, of length one, with a in its one position. 
Note that by convention, we use italics for symbols, and boldface for their 
corresponding regular expression.' 
INDUCTION: There are four parts to the induction whereby larger regular 
expressions are built from smaller ones. Suppose r and s are regular expressions 
denoting languages L(r) and L(s), respectively. 
1. (r) 
1 (9) is a regular expression denoting the language L(r) U L(s). 
2. (r) 
(s) is a regular expression denoting the language L(r) 
L(s) 
. 
3. (r) 
* is a regular expression denoting (L 
(r)) 
* . 
4. (r) is a regular expression denoting L(r). This last rule says that we can 
add additional pairs of parentheses around expressions without changing 
the language they denote. 
As defined, regular expressions often contain unnecessary pairs of paren- 
theses. We may drop certain pairs of parentheses if we adopt the conventions 
that: 
a) The unary operator * has highest precedence and is left associative. 
b) Concatenation has second highest precedence and is left associative. 
 o ow ever, when talking about specific characters from the ASCII character set, we shall 
generally use teletype font for both the character and its regular expression. 
122 
CHAPTER 3. LEXICAL ANALYSIS 
c) I has lowest precedence and is left associative. 
Under these conventions, for example, we may replace the regular expression 
(a) 
I ((b) 
* (c)) 
by a/ 
b*c. Both expressions denote the set of strings that are either 
a single a or are zero or more b's followed by one c. 
Example 3.4: Let C = {a, b}. 
1. The regular expression a
1
 b denotes the language {a, 
b}. 
2. (a1 
b) 
(alb) 
denotes {aa, 
ab, ba, bb), the language of all strings of length two 
over the alphabet C. Another regular expression for the same language is 
aalablbal 
bb. 
3. a* 
denotes the language consisting of all strings of zero or more a's, that 
is, {E, a, 
aa, 
aaa, . 
. 
. 
}. 
4. (alb)* 
denotes the set of all strings consisting of zero or more instances 
of a or b, that is, all strings of a's and b's: {e, 
a, 
b, aa, 
ab, ba, bb, aaa, . 
. .}. 
Another regular expression for the same language is (a*b*)*. 
5. ala*b denotes the language {a, 
b, ab, aab,aaab,. 
. .), 
that is, the string a 
and all strings consisting of zero or more a's and ending in b. 
A language that can be defined by a regular expression is called a regular 
set. If two regular expressions r and s denote the same regular set, we say they 
are equivalent and write r = 
s. For instance, (alb) = (bla). 
There are a number 
of algebraic laws for regular expressions; each law asserts that expressions of 
two different forms are equivalent. Figure 3.7 shows some of the algebraic laws 
that hold for arbitrary regular expressions r, s, and t. 
Figure 3.7: Algebraic laws for regular expressions 
rIs = slr 
rI(sIt) = (rIs>It 
r(st) = (rs)t 
r(slt) = 
rslrt; (slt)r 
= 
srltr 
E
r
 = 
re = 
r 
r* = 
(TIE)* 
r** 
= 
r* 
I is commutative 
I is associative 
Concatenation is associative 
Concatenation distributes over I 
E is the identity for concatenation 
r: is guaranteed in a closure 
* is idempotent 
3.3. SPECIFICATION OF TOKENS 
123 
3.3.4 Regular Definitions 
For notational convenience, we may wish to give names to certain regular ex- 
pressions and use those names in subsequent expressions, as if the names were 
themselves symbols. If C is an alphabet of basic symbols, then a regular defi- 
nition is a sequence of definitions of the form: 
where: 
1. Each di is a new symbol, not in C and not the same as any other of the 
d's, and 
2. Each ri is a regular expression over the alphabet C U {dl, 
d2,. 
. 
. , 
di-l). 
By restricting ri to C and the previously defined d's, we avoid recursive defini- 
tions, and we can construct a regular expression over C alone, for each ri. We 
do so by first replacing uses of dl in r2 (which cannot use any of the d's except 
for dl), then replacing uses of dl and d2 in r3 by rl and (the substituted) 7-2, 
and so on. Finally, in rn we replace each di, for i = 1,2,. 
. 
. ,n - 
1, by the 
substituted version of ri, each of which has only symbols of C. 
Example 3.5 : 
C identifiers are strings of letters, digits, and underscores. Here 
is a regular definition for the language of C identifiers. We shall conventionally 
use italics for the symbols defined in regular definitions. 
letter- + A ( B I . - . [  
Z 1 a 1 b l . - - l  
z 1 - 
digit 
-+ 
0 1  1 1 - - . 1 9  
id + letter- ( letter- I digit )* 
Example 3.6 : Unsigned numbers (integer or floating point) are strings such 
as 5280, 0.01234, 6.336E4, or 1.89E-4. The regular definition 
digit + 0 I 1 
( . - . (  
9 
digits 
-+ 
digit digit* 
optionalFraction + . digits 1 c 
optionalExponent 
-+ 
( E ( + ( - [ c ) digits ) 1 c 
number + digits optionalFraction optionalExponent 
is a precise specification for this set of strings. That is, an optionalFraction is 
either a decimal point (dot) followed by one or more digits, or it is missing (the 
empty string). An optionalExponent, if not missing, is the letter E followed by 
an optional + 
or - 
sign, followed by one or more digits. Note that at least one 
digit must follow the dot, so number does not match I., 
but does match 1.0. 
124 
CHAPTER 3. LEXICAL ANALYSIS 
3.3.5 Extensions of Regular Expressions 
Since Kleene introduced regular expressions with the basic operators for union, 
concatenation, and Kleene closure in the 1950s, many extensions have been 
added to regular expressions to enhance their ability to specify string patterns. 
Here we mention a few notational extensions that were first incorporated into 
Unix utilities such as Lex that are particularly useful in the specification lexical 
analyzers. The references to this chapter contain a discussion of some regular- 
expression variants in use today. 
1. One or more instances. The unary, postfix operator + represents the 
positive closure of a regular expression and its language. That is, if r is a 
regular expression, then (r)+ 
denotes the language ( ~ ( r ) ) ' .  
The operator 
has the same precedence and associativity as the operator *. Two useful 
algebraic laws, r* = r+Jc 
and r f  = rr* = r*r relate the Kleene closure 
and positive closure. 
2. Zero or one instance. The unary postfix operator ? means "zero or one 
occurrence." That is, r? is equivalent to rlc, or put another way, L(r?) 
= 
L(r) 
U (€1. The ? operator has the same precedence and associativity as 
* and +. 
3. Character classes. A regular expression allazl. 
.. 
lan, where the ai's 
are each symbols of the alphabet, can be replaced by the shorthand 
[ala2 
. 
. . 
a,]. 
More importantly, when a1 
, 
a2, . 
. . , 
a, form a logical se- 
quence, e.g., consecutive uppercase letters, lowercase letters, or digits, we 
can replace them by al-a,, that is, just the first and last separated by 
a hyphen. Thus, [abc] is shorthand for alblc, and [a-z] is shorthand for 
aJbJ 
. 
. 
. 
Jz. 
Example 3.7 : 
Using these shorthands, we can rewrite the regular definition 
of Example 3.5 as: 
letter- + [A-Za-z-] 
digit + [O-91 
id -+ letter- ( letter 1 digit )* 
The regular definition of Example 3.6 can also be simplified: 
digit + [o-91 
digits += digit+ 
number + digits ( . digits)? ( E 
[+-I? digits )? 
3.3. SPECIFICATION OF TOKENS 
3.3.6 Exercises for Section 3.3 
Exercise 3
.
3
.
1
 : 
Consult the language reference manuals to determine (i) the 
sets of characters that form the input alphabet (excluding those that may only 
appear in character strings or comments), (ii) the lexical form of numerical 
constants, and (iii) the lexical form of identifiers, for each of the following 
languages: (a) C (b) C++ (c) C# (d) Fortran (e) Java (f) Lisp (g) SQL. 
! 
Exercise 3
.
3
.
2
 
: 
Describe the languages denoted by the following regular ex- 
pressions: 
d) a* 
ba* 
ba* 
ba* 
. 
Exercise 3
.
3
.
3
 
: 
In a string of length n, how many of the following are there? 
a) Prefixes. 
b) Suffixes. 
c) Proper prefixes. 
! 
d) Substrings. 
! 
e) Subsequences. 
Exercise 3
.
3
.
4
 
: 
Most languages are case sensitive, so keywords can be written 
only one way, and the regular expressions describing their lexeme is very simple. 
However, some languages, like SQL, are case insensitive, so a keyword can be 
written either in lowercase or in uppercase, or in any mixture of cases. Thus, 
the SQL keyword SELECT can also be written select, Select, or sElEcT, for 
instance. Show how to write a regular expression for a keyword in a case- 
insensitive language. Illustrate the idea by writing the expression for "select" 
in SQL. 
! 
Exercise 3
.
3
.
5
 
: 
Write regular definitions for the following languages: 
a) All strings of lowercase letters that contain the five vowels in order. 
b) All strings of lowercase letters in which the letters are in ascending lexi- 
cographic order. 
c) Comments, consisting of a string surrounded by /* and */, without an 
intervening */, unless it is inside double-quotes ("). 
126 
CHAPTER 3. LEXICAL ANALYSIS 
!! 
d) All strings of digits with no repeated digits. Hint: Try this problem first 
with a few digits, such as {O,1,2). 
!! 
e) All strings of digits with at most one repeated digit. 
!! 
f) All strings of a's and b's with an even number of a's and an odd number 
of b's. 
g) The set of Chess moves, in the informal notation, such as p-k4 or kbp x qn. 
!! 
h) All strings of a's and b's that do not contain the substring abb. 
i) All strings of a's and b's that do not contain the subsequence abb. 
Exercise 3.3.6 : 
Write character classes for the following sets of characters: 
a) The first ten letters (up to "j" ) in either upper or lower case. 
b) The lowercase consonants. 
c) The "digits" in a hexadecimal number (choose either upper or lower case 
for the "digits" above 9). 
d) The characters that can appear at the end of a legitimate English sentence 
(e.g., exclamation point). 
The following exercises, up to and including Exercise 3.3.10, discuss the 
extended regular-expression notation from Lex (the lexical-analyzer generator 
that we shall discuss extensively in Section 3.5). The extended notation is listed 
in Fig. 3.8. 
Exercise 3.3.7 : 
Note that these regular expressions give all of the following 
symbols (operator characters) a special meaning: 
Their special meaning must be turned off if they are needed to represent them- 
selves in a character string. We can do so by quoting the character within a 
string of length one or more; e.g., the regular expression It 
**It matches the string 
**. We can also get the literal meaning of an operator character by preceding 
it by a backslash. Thus, the regular expression \*\* also matches the string 
**. Write a regular expression that matches the string "\. 
Exercise 3.3.8 : 
In Lex, a complemented character class represents any char- 
acter except the ones listed in the character class. We denote a complemented 
class by using * as the first character; this symbol (caret) is not itself part of 
the class being complemented, unless it is listed within the class itself. Thus, 
[^ A-Za-z] matches any character that is not an uppercase or lowercase letter, 
and [^\^I represents any character but the caret (or newline, since newline 
cannot be in any character class). Show that for every regular expression with 
complemented character classes, there is an equivalent regular expression with- 
out complemented character classes. 
3.3. SPECIFICATION OF TOKENS 
the one non-operator character c 
character c literally 
string s literally 
any character but newline 
beginning of a line 
end of a line 
any one of the characters in string s 
any one character not in string s 
zero or more strings matching r 
one or more strings matching r 
zero or one r 
between m and n occurrences of r 
an r l  followed by an r2 
an r1 or an r
2
 
same as r 
r l  when followed by 7-2 
Figure 3.8: Lex regular expressions 
! Exercise 3.3.9 : The regular expression r{m, 
n }  matches from m to n occur- 
rences of the pattern r. For example, a [I, 
51 matches a string of one to five a's. 
Show that for every regular expression containing repetition operators of this 
form, there is an equivalent regular expression without repetition operators. 
! Exercise 3.3.10 : 
The operator A matches the left end of a line, and $ matches 
the right end of a line. The operator A is also used to introduce complemented 
character classes, but the context always makes it clear which meaning is in- 
tended. For example, ^ CAaeioul 
*$ matches any complete line that does not 
contain a lowercase vowel. 
a) How do you tell which meaning of 
A is intended? 
b) Can you always replace a regular expression using the A and $ operators 
by an equivalent expression that does not use either of these operators? 
! Exercise 3.3.1 
1 : 
The UNIX shell command sh 
uses the operators in Fig. 3.9 
in filename expressions to describe sets of file names. For example, the filename 
expression * . 
o 
matches all file names ending in . 
o; sort 
1. 
? matches all file- 
names of the form sort 
. 
c, where c is any character. Show how sh 
filename 
CHAPTER 3. LEXICAL ANALYSIS 
Figure 3.9: Filename expressions used by the shell command sh 
EXPRESSION 
I 
I 
s 
\ 
* 
? 
[s] 
expressions can be replaced by equivalent regular expressions using only the 
basic union, concatenation, and closure operators. 
! 
Exercise 3.3.12 : SQL allows a rudimentary form of patterns in which two 
characters have special meaning: underscore (-) stands for any one character 
and percent-sign (%) 
stands for any string of 0 or more characters. In addition, 
the programmer may define any character, say e, to be the escape character, so 
e preceding an e preceding -, %, 
or another e gives the character that follows its 
literal meaning. Show how to express any SQL pattern as a regular expression, 
given that we know which character is the escape character. 
MATCHES 
string s literally 
character c literally 
any string 
any character 
any character in s 
3.4 Recognition of Tokens 
EXAMPLE 
J\J 
\ 
*.o 
sort 
1. 
? 
sort 
1. 
[cso] 
In the previous section we learned how to express patterns using regular expres- 
sions. Now, we must study how to take the patterns for all the needed tokens 
and build a piece of code that examines the input string and finds a prefix that 
is a lexeme matching one of the patterns. Our discussion will make use of the 
following running example. 
stmt + if expr then stmt 
I 
if expr then stmt else stmt 
I 
E. 
expr + term relop term 
I 
term 
term + id 
I 
number 
Figure 3.10: A grammar for branching statements 
Example 3.8 : The grammar fragment of Fig. 3.10 describes a simple form 
of branching statements and conditional expressions. This syntax is similar to 
that of the language Pascal, in that then appears explicitly after conditions. 
3.4. RECOGNITION OF TOKENS 
129 
For relop, we use the comparison operators of languages like Pascal or SQL, 
where = is "equals" and <> is "not equals," because it presents an interesting 
structure of lexemes. 
The terminals of the grammar, which are if, then, else, relop, id, and 
number, are the names of tokens as far as the lexical analyzer is concerned. The 
patterns for these tokens are described using regular definitions, as in Fig. 3.11. 
The patterns for id and number are similar to what we saw in Example 3.7. 
digit 
digits 
number 
letter 
id 
if 
then 
else 
relop 
Lo-91 
digit+ 
digits ( . digits)? ( E [+-I? 
digits )? 
[A-~a-z] 
letter ( letter I  
digit )* 
i f  
then 
else 
< I > I < = I > = I = ) < >  
Figure 3.11: Patterns for tokens of Example 3.8 
For this language, the lexical analyzer will recognize the keywords i f ,  
then, 
and else, as well as lexemes that match the patterns for relop, id, and number. 
To simplify matters, we make the common assumption that keywords are also 
reserved words: that is, they are not identifiers, even though their lexemes 
match the pattern for identifiers. 
In addition, we assign the lexical analyzer the job of stripping out white- 
space, by recognizing the "token" ws defined by: 
ws -+ ( blank I  
tab ( newline )+ 
Here, blank, tab, and newline are abstract symbols that we use to express 
the ASCII characters of the same names. Token ws is different from the other 
tokens in that, when we recognize it, we do not return it to the parser, but rather 
restart the lexical analysis from the character that follows the whitespace. It is 
the following token that gets returned to the parser. 
Our goal for the lexical analyzer is summarized in Fig. 3.12. That table 
shows, for each lexeme or family of lexemes, which token name is returned to 
the parser and what attribute value, as discussed in Section 3.1.3, is returned. 
Note that for the six relational operators, symbolic constants LT, LE, and so 
on are used as the attribute value, in order to indicate which instance of the 
token relop we have found. The particular operator found will influence the 
code that is output from the compiler. 
CHAPTER 3. LEXICAL ANALYSIS 
Figure 3.12: Tokens, their patterns, and attribute values 
LEXEMES 
Any ws 
i f  
then 
else 
Any id 
Any number 
< 
<= 
- 
- 
<> 
> 
>= 
3.4.1 Transition Diagrams 
As an intermediate step in the construction of a lexical analyzer, we first convert 
patterns into stylized flowcharts, called "transition diagrams." In this section, 
we perform the conversion from regular-expression patterns to transition dia- 
grams by hand, but in Section 3.6, we shall see that there is a mechanical way 
to construct these diagrams from collections of regular expressions. 
Transition diagrams have a collection of nodes or circles, called states. Each 
state represents a condition that could occur during the process of scanning 
the input looking for a lexeme that matches one of several patterns. We may 
think of a state as summarizing all we need to know about what characters we 
have seen between the lexemeBegin pointer and the forward pointer (as in the 
situation of Fig. 3.3). 
Edges are directed from one state of the transition diagram to another. 
Each edge is labeled by a symbol or set of symbols. If we are in some state 
s, and the next input symbol is a, we look for an edge out of state s labeled 
by a (and perhaps by other symbols, as well). If we find such an edge, we 
advance the forward pointer arid enter the state of the transition diagram to 
which that edge leads. We shall assume that all our transition diagrams are 
deterministic, meaning that there is never more than one edge out of a given 
state with a given symbol among its labels. Starting in Section 3.5, we shall 
relax the condition of determinism, making life much easier for the designer 
of a lexical analyzer, although trickier for the implementer. Some important 
conventions about transition diagrams are: 
TOKEN 
NAME 
- 
if 
then 
else 
id 
number 
relop 
relop 
relop 
relop 
relop 
relop 
1. Certain states are said to be accepting, or final. These states indicate that 
a lexeme has been found, although the actual lexeme may not consist of 
all positions between the ZexemeBegin and forward pointers. We always 
ATTRIBUTE 
VALUE 
- 
- 
- 
- 
Pointer to table entry 
Pointer to table entry 
LT 
LE 
EQ 
NE 
GT 
GE 
3.4. RECOGNITION OF TOKENS 
131 
indicate an accepting state by a double circle, and if there is an action 
to be taken - 
typically returning a token and an attribute value to the 
parser - 
we shall attach that action to the accepting state. 
2. In addition, if it is necessary to retract the forward pointer one position 
(i.e., the lexeme does not include the symbol that got us to the accepting 
state), then we shall additionally place a * near that accepting state. In 
our example, it is never necessary to retract forward by more than one 
position, but if it were, we could attach any number of *'s to the accepting 
state. 
3. One state is designated the start state, or initial state; it is indicated by 
an edge, labeled "start 
," entering from nowhere. The transition diagram 
always begins in the start state before any input symbols have been read. 
Example 3.9 : 
Figure 3.13 is a transition diagram that recognizes the lexemes 
matching the token relop. We begin in state 0, the start state. If we see < as the 
first input symbol, then among the lexemes that match the pattern for relop 
we can only be looking at <, <>, or <=. We therefore go to state 1, and look at 
the next character. If it is =, then we recognize lexeme <=, enter state 2, and 
return the token relop with attribute LE, the symbolic constant representing 
this particular comparison operator. If in state 1 
the next character is >, then 
instead we have lexeme <>, and enter state 3 to return an indication that the 
not-equals operator has been found. On any other character, the lexeme is <, 
and we enter state 4 to return that information. Note, however, that state 4 
has a * to indicate that we must retract the input one position. 
return( relop, LE) 
return( relop, NE) 
return( relop, LT) 
' 
return relop, GE) 
o
*
 
return ( relop, GT) 
Figure 3.13: Transition diagram for relop 
On the other hand, if in state 0 the first character we see is =, then this one 
character must be the lexeme. We immediately return that fact from state 5. 
132 
CHAPTER 3. LEXICAL ANALYSIS 
The remaining possibility is that the first character is >. Then, we must enter 
state 6 and decide, on the basis of the next character, whether the lexeme is >= 
(if we next see the = sign), or just > (on any other character). Note that if, in 
state 0, we see any character besides C, =, or >, we can not possibly be seeing 
a relop lexeme, so this transition diagram will not be used. 
3.4.2 Recognition of Reserved Words and Identifiers 
Recognizing keywords and identifiers presents a problem. Usually, keywords like 
i f  or then are reserved (as they are in our running example), so they are not 
identifiers even though they look like identifiers. Thus, although we typically 
use a transition diagram like that of Fig. 3.14 to search for identifier lexemes, 
this diagram will also recognize the keywords i f ,  
then, and else 
of our running 
example. 
letter or digit 
* 
@) return (getToken( ), installZD 
( )) 
Figure 3.14: A transition diagram for id's and keywords 
There are two ways that we can handle reserved words that look like iden- 
tifiers: 
1. Install the reserved words in the symbol table initially. A field of the 
symbol-table entry indicates that these strings are never ordinary identi- 
fiers, and tells which token they represent. We have supposed that this 
method is in use in Fig. 3.14. When we find an identifier, a call to installID 
places it in the symbol table if it is not already there and returns a pointer 
to the symbol-table entry for the lexeme found. Of course, any identifier 
not in the symbol table during lexical analysis cannot be a reserved word, 
so its token is id. The function getToken examines the symbol table entry 
for the lexeme found, and returns whatever token name the symbol table 
says this lexeme represents - 
either id or one of the keyword tokens that 
was initially installed in the table. 
2. Create separate transition diagrams for each keyword; an example for 
the keyword then is shown in Fig. 3.15. Note that such a transition 
diagram consists of states representing the situation after each successive 
letter of the keyword is seen, followed by a test for a "nonletter-or-digit," 
i.e., any character that cannot be the continuation of an identifier. It is 
necessary to check that the identifier has ended, or else we would return 
token then in situations where the correct token was id, with a lexeme 
like thenextvalue that has then as a proper prefix. If we adopt this 
approach, then we must prioritize the tokens so that the reserved-word 
3.4. RECOGNITION OF TOKENS 
133 
tokens are recognized in preference to id, when the lexeme matches both 
patterns. We do not use this approach in our example, which is why the 
states in Fig. 3.15 are unnumbered. 
Figure 3.15: Hypothetical transition diagram for the keyword then 
3.4.3 Completion of the Running Example 
The transition diagram for id's that we saw in Fig. 3.14 has a simple structure. 
Starting in state 9, it checks that the lexeme begins with a letter and goes to 
state 10 if so. We stay in state 10 as long as the input contains letters and digits. 
When we first encounter anything but a letter or digit, we go to state 1
1
 and 
accept the lexeme found. Since the last character is not part of the identifier, 
we must retract the input one position, and as discussed in Section 3.4.2, we 
enter what we have found in the symbol table and determine whether we have 
a keyword or a true identifier. 
The transition diagram for token number is shown in Fig. 3.16, and is so 
far the most complex diagram we have seen. Beginning in state 12, if we see a 
digit, we go to state 13. In that state, we can read any number of additional 
digits. However, if we see anything but a digit or a dot, we have seen a number 
in the form of an integer; 123 is an example. That case is handled by entering 
state 20, where we return token number and a pointer to a table of constants 
where the found lexeme is entered. These mechanics are not shown on the 
diagram but are analogous to the way we handled identifiers. 
digit 
digit 
digit 
Figure 3.16: A transition diagram for unsigned numbers 
If we instead see a dot in state 13, then we have an "optional fraction." 
State 14 is entered, and we look for one or more additional digits; state 15 is 
used for that purpose. If we see an E, then we have an "optional exponent," 
whose recognition is the job of states 16 through 19. Should we, in state 15, 
instead see anything but E or a digit, then we have come to the end of the 
fraction, there is no exponent, and we return the lexeme found, via state 21. 
134 
CHAPTER 3. LEXICAL ANALYSIS 
The final transition diagram, shown in Fig. 3.17, is for whitespace. In that 
diagram, we look for one or more "whitespace" characters, 
represented by delim 
in that diagram - 
typically these characters would be blank, tab, newline, and 
perhaps other characters that are not considered by the language design to be 
part of any token. 
delim 
start 
22 
-
:
,
 
delim 8 
23 
other @ 
* 
Figure 3.17: A transition diagram for whitespace 
Note that in state 24, we have found a block of consecutive whitespace 
characters, followed by a nonwhitespace character. We retract the input to 
begin at the nonwhitespace, but we do not return to the parser. Rather, we 
must restart the process of lexical analysis after the whitespace. 
3.4.4 Architecture of a Transition-Diagram-Based Lexical 
Analyzer 
There are several ways that a collection of transition diagrams can be used 
to build a lexical analyzer. Regardless of the overall strategy, each state is 
represented by a piece of code. We may imagine a variable s t a t e  holding the 
number of the current state for a transition diagram. A switch based on the 
value of s t a t e  takes us to code for each of the possible states, where we find 
the action of that state. Often, the code for a state is itself a switch statement 
or multiway branch that determines the next state by reading and examining 
the next input character. 
Example 3.10: In Fig. 3.18 we see a sketch of getRelop(), a C++ function 
whose job is to simulate the transition diagram of Fig. 3.13 and return an object 
of type T
O
K
E
N
,
 that is, a pair consisting of the token name (which must be relop 
in this case) and an attribute value (the code for one of the six comparison 
operators in this case). getRelop() first creates a new object retToken and 
initializes its first component to RELOP, 
the symbolic code for token relop. 
We see the typical behavior of a state in case 0, the case where the current 
state is 0. A function nextchar() obtains the next character from the input 
and assigns it to local variable c. We then check c for the three characters we 
expect to find, making the state transition dictated by the transition diagram 
of Fig. 3.13 in each case. For example, if the next input character is =, we go 
to state 5. 
If the next input character is not one that can begin a comparison operator, 
then a function f a i l  
() is called. What fail 
() does depends on the global error- 
recovery strategy of the lexical analyzer. It should reset the forward pointer 
to lexemeBegin, in order to allow another transition diagram to be applied to 
3.4. RECOGNITION OF TOKENS 
T
O
K
E
N
 getRelop() 
T
O
K
E
N
 retToken = new(REL0P); 
while(1) ( /* repeat character processing u n t i l  a r e t u r n  
o r  f a i l u r e  occurs */ 
switch(state) ( 
case 0: c = nextchar(); 
i f  ( c == ' 0  ) s t a t e  = 1 ;  
e l s e  i f  ( c == ) = )  ) s t a t e  = 5; 
e l s e  i f  ( c == ) > )  ) s t a t e  = 6; 
e l s e  f a i l ( ) ;  /* lexeme is not a relop */ 
break; 
case 1: . 
. 
. 
... 
case 8: r e t r a c t  () ; 
retToken.attribute = GT; 
return(retToken) ; 
1 
Figure 3.18: Sketch of implementation of relop transition diagram 
the true beginning of the unprocessed input. It might then change the value 
of s t a t e  
to be the start state for another transition diagram, which will search 
for another token. Alternatively, if there is no other transition diagram that 
remains unused, f a i l  
() could initiate an error-correction phase that will try 
to repair the input and find a lexeme, as discussed in Section 3.1.4. 
We also show the action for state 8 in Fig. 3.18. Because state 8 bears a *, 
we must retract the input pointer one position (i.e., put c back on the input 
stream). That task is accomplished by the function r e t r a c t  
() 
. Since state 8 
represents the recognition of lexeme >=, we set the second component of the 
returned object, which we suppose is named a t t r i b u t e ,  to GT, the code for this 
operator. 
CI 
To place the simulation of one transition diagram in perspective, let us 
consider the ways code like Fig. 3.18 could fit into the entire lexical analyzer. 
I. We could arrange for the transition diagrams for each token to be tried se- 
quentially. Then, the function f a i l  
( 
) of Example 3.10 resets the pointer 
forward and starts the next transition diagram, each time it is called. 
This method allows us to use transition diagrams for the individual key- 
words, like the one suggested in Fig. 3.15. We have only to use these 
before we use the diagram for id, in order for the keywords to be reserved 
words. 
136 
CHAPTER 3. LEXICAL ANALYSIS 
2. We could run the various transition diagrams "in parallel," feeding the 
next input character to all of them and allowing each one to make what- 
ever transitions it required. If we use this strategy, we must be careful 
to resolve the case where one diagram finds a lexeme that matches its 
pattern, while one or more other diagrams are still able to process input. 
The normal strategy is to take the longest prefix of the input that matches 
any pattern. That rule allows us to prefer identifier thenext to keyword 
then, or the operator -> to -, for example. 
3. The preferred approach, and the one we shall take up in the following 
sections, is to combine all the transition diagrams into one. We allow the 
transition diagram to read input until there is no possible next state, and 
then take the longest lexeme that matched any pattern, as we discussed 
in item (2) above. In our running example, this combination is easy, 
because no two tokens can start with the same character; i.e., the first 
character immediately tells us which token we are looking for. Thus, we 
could simply combine states 0, 9, 12, and 22 into one start state, leaving 
other transitions intact. However, in general, the problem of combining 
transition diagrams for several tokens is more complex, as we shall see 
shortly. 
3.4.5 Exercises for Section 3.4 
Exercise 3.4.1 : 
Provide transition diagrams to recognize the same languages 
as each of the regular expressions-in 
Exercise 3.3.2. 
Exercise 3.4.2 : 
Provide transition diagrams to recognize the same languages 
as each of the regular expressions in Exercise 3.3.5. 
The following exercises, up to Exercise 3.4.12, introduce the Aho-Corasick 
algorithm for recognizing a collection of keywords in a text string in time pro- 
portional to the length of the text and the sum of the length of the keywords. 
This algorithm uses a special form of transition diagram called a trie. A trie is 
a tree-structured transition diagram with distinct labels on the edges leading 
from a node to its children. Leaves of the trie represent recognized keywords. 
Knuth, Morris, and Pratt presented an algorithm for recognizing a single 
keyword blb2.. b, in a text string. Here the trie is a transition diagram with 
n states, 0 through n. State 0 is the initial state, and state n represents ac- 
ceptance, that is, discovery of the keyword. From each state s from 0 through 
n - 
1, 
there is a transition to state s + 
1, 
labeled by symbol b,+~. For example, 
the trie for the keyword ababaa 
is: 
In order to process text strings rapidly and search those strings for a key- 
word, it is useful to define, for keyword blb2 - . . 
b, and position s in that keyword 
(corresponding to state s of its trie), a failure function, f 
(s), computed as in 
3.4. RECOGNITION OF TOKENS 
137 
Fig. 3.19. The objective is that blbz.. 
- bf(,) is the longest proper prefix of 
bl b2 . 
- . 
b, that is also a suffix of bl bz . 
. . 
b,. The reason f (s) 
is important is that 
if we are trying to match a text string for bl b2 - - . 
b,, and we have matched the 
first s positions, but we then fail (i.e., the next position of the text string does 
not hold b,+l), then f (s) 
is the longest prefix of bl b2 . . bn that could possibly 
match the text string up to the point we are at. Of course, the next character of 
the text string must be bf 
or else we still have problems and must consider 
a yet shorter prefix, which will be bf 
( 
1) t = 0; 
2) 
f (1) 
= 0; 
3) for (s= 1; s <n; 
s + + )  { 
4) 
while (t > 0 && b,+l ! = bt+l) t = f (t); 
5 )  
if (b,+l == bt+l) { 
6) 
t = t + l ;  
7 )  
f ( s  + 
1) = t; 
1 
8) 
else f ( s  
+ 
1) = 0; 
I 
Figure 3.19: Algorithm to compute the failure function for keyword blb2 . . 
. 
bn 
As an example, the failure function for the trie constructed above for ababaa 
is: 
For instance, states 3 and 1 represent prefixes aba and a, respectively. f (3) 
= 1 
because a is the longest proper prefix of aba that is also a suffix of aba. Also, 
f 
(2) = 0, because the longest proper prefix of ab that is also a suffix is the 
empty string. 
Exercise 3
.
4
.
3
 
: 
Construct the failure function for the strings: 
a) abababaab. 
b) aaaaaa. 
c) abbaabb. 
! 
Exercise 3
.
4
.
4
 
: Prove, by induction on s, that the algorithm of Fig. 3.19 
correctly computes the failure function. 
!! Exercise 3
.
4
.
5
 
: 
Show that the assignment t = 
f (t) 
in line (4) 
of Fig. 3.19 is 
executed at most n times. Show that therefore, the entire algorithm takes only 
0 
(n) 
time on a keyword of length n. 
138 
CHAPTER 3. LEXICAL ANALYSIS 
Having computed the failure function for a keyword blb2 . . 
. 
b,, we can scan 
a string ala2 .. 
-a, in time O(m) to tell whether the keyword occurs in the 
string. The algorithm, shown in Fig. 3.20, slides the keyword along the string, 
trying to make progress by matching the next character of the keyword with the 
next character of the string. If it cannot do so after matching s characters, then 
it "slides" the keyword right s - 
f (s) positions, so only the first f (s) characters 
of the keyword are considered matched with the string. 
1) s = 0; 
2) 
for (i = I; 
i 5 m; i++) { 
3 
while (S > 0 && ai ! = bs+l) s = 
f 
(s); 
4) 
if (ai == bs+l) s = s + 1; 
5 
if (9 == n) return "yes" ; 
I 
6) 
return "no"; 
Figure 3.20: The KMP algorithm tests whether string ala2 . . 
a, 
contains a 
single keyword bl b2 . . 
. 
bn as a substring in O(m + 
n) time 
Exercise 3
.
4
.
6
:
 
Apply Algorithm KMP 
to test whether keyword ababaa is a 
substring of: 
a) abababaab. 
b) abababbaa. 
!! Exercise 3
.
4
.
7
 
: Show that the algorithm of Fig. 3.20 correctly tells whether 
the keyword is a substring of the given string. Hint: proceed by induction on 
i. Show that for all i, the value of s after line (4) is the length of the longest 
prefix of the keyword that is a suffix of a1 
a2 . ai. 
!! 
Exercise 3
.
4
.
8
 
: 
Show that the algorithm of Fig. 3.20 runs in time O(m + 
n) 
, 
assuming that function f is already computed and its values stored in an array 
indexed by s. 
Exercise 3
.
4
.
9
 
: 
The Fibonacci strings are defined as follows: 
For example, sy = ab, s4 = aba, and ss = abaab. 
a) What is the length of s,? 
3.4. RECOGNITION OF TOKENS 
b) Construct the failure function for se. 
c) Construct the failure function for 37. 
!! 
d) Show that the failure function for any s, can be expressed by f 
(I) = 
f ( 2 )  = 0, and for 2 < j 5 Is,[, f(j) is j - 
 IS^-^^, where k is the largest 
integer such that IsIc 
1 5 j + 1. 
!! 
e) In the KMP algorithm, what is the largest number of consecutive applica- 
tions of the failure function, when we try to determine whether keyword 
sk appears in text string sk+1? 
Aho and Corasick generalized the KMP algorithm to recognize any of a 
set of keywords in a text string. In this case, the trie is a true tree, with 
branching from the root. There is one state for every string that is a prefix 
(not necessarily proper) of any keyword. The parent of a state corresponding 
to string bl b2 . bh is the state that corresponds to bl b2 . . 
- bk-1. 
A state is 
accepting if it corresponds to a complete keyword. For example, Fig. 3.21 
shows the trie for the keywords he, she, his, and hers. 
Figure 3.21: Trie for keywords he, she, his, hers 
The failure function for the general trie is defined as follows. Suppose s 
is the state that corresponds to string blb2 
. 
bn. Then f (s) 
is the state that 
corresponds to the longest proper suffix of blb2 - .  
. 
b, that is also a prefix of 
some keyword. For example, the failure function for the trie of Fig. 3.21 is: 
! 
Exercise 3.4.10 : Modify the algorithm of Fig. 3.19 to compute the failure 
function for general tries. Hint: The major difference is that we cannot simply 
test for equality or inequality of b,+l and bt+1 in lines (4) and (5) 
of Fig. 3.19. 
Rather, from any state there may be several transitions out on several charac- 
ters, as there are transitions on both e and i from state 1 in Fig. 3.21. Any of 
140 
CHAPTER 3. LEXICAL ANALYSIS 
those transitions could lead to a state that represents the longest suffix that is 
also a prefix. 
Exercise 3.4.11 : 
Construct the tries and compute the failure function for the 
following sets of keywords: 
a) aaa, abaaa, and ababaaa. 
b) a l l ,  f a l l ,  
fatal, llama, and lame. 
c) pipe, pet, item, temper, and perpetual. 
! 
Exercise 3.4.12 : 
Show that your algorithm from Exercise 3.4.10 still runs in 
time that is linear in the sum of the lengths of the keywords. 
3.5 
The Lexical- 
Analyzer Generator Lex 
In this section, we introduce a tool called Lex, or in a more recent implemen- 
tation Flex, that allows one to specify a lexical analyzer by specifying regular 
expressions to describe patterns for tokens. The input notation for the Lex tool 
is referred to as the Lex language and the tool itself is the Lex compiler. Behind 
the scenes, the Lex compiler transforms the input patterns into a transition 
diagram and generates code, in a file called lex . 
yy . 
c, that simulates this tran- 
sition diagram. The mechanics of how this translation from regular expressions 
to transition diagrams occurs is the subject of the next sections; here we only 
learn the Lex language. 
3
.
5
.
1
 Use of Lex 
Figure 3.22 suggests how Lex is used. An input file, which we call lex.1, is 
written in the Lex language and describes the lexical analyzer to be generated. 
The Lex compiler transforms lex. 
1 
to a C program, in a file that is always 
named lex. 
yy . 
c. The latter file is compiled by the C compiler into a file called 
a. 
out, as always. The C-compiler output is a working lexical analyzer that can 
take a stream of input characters and produce a stream of tokens. 
The normal use of the compiled C program, referred to as a. 
out in Fig. 3.22, 
is as a subroutine of the parser. It is a C function that returns an integer, which 
is a code for one of the possible token names. The attribute value, whether it 
be another numeric code, a pointer to the symbol table, or nothing, is placed 
in a global variable yylval,2 
which is shared between the lexical analyzer and 
parser, thereby making it simple to return both the name and an attribute value 
of a token. 
2~ncidentally, 
the yy that appears in yylval and lex. 
yy . 
c refers to the Yacc parser- 
generator, which we shall describe in Section 4.9, and which is commonly used in conjunction 
with Lex. 
3.5. THE LEXICAL-ANALYZER GENERATOR LEX 
Lex source program 
compiler 
Input stream 
a. 
out 
- I t -  
* 
a. 
out 
Sequence of tokens 
C 
compiler 
Figure 3.22: Creating a lexical analyzer with Lex 
* 
3.5.2 Structure of Lex Programs 
A Lex 
program has the following form: 
declarations 
%% 
translation rules 
%% 
auxiliary functions 
The declarations section includes declarations of variables, manifest constants 
(identifiers declared to stand for a constant, e.g., the name of a token), and 
regular definitions, in the style of Section 3.3.4. 
The translation rules each have the form 
Pattern { Action ) 
Each pattern is a regular expression, which may use the regular definitions of 
the declaration section. The actions are fragments of code, typically written in 
C, although many variants of Lex 
using other languages have been created. 
The third section holds whatever additional functions are used in the actions. 
Alternatively, these functions can be compiled separately and loaded with the 
lexical analyzer. 
The lexical analyzer created by Lex behaves in concert with the parser as 
follows. When called by the parser, the lexical analyzer begins reading its 
remaining input, one character at a time, until it finds the longest prefix of the 
input that matches one of the patterns Pi. 
It then executes the associated action 
Ai. 
Typically, Ai will return to the parser, but if it does not (e.g., because Pi 
describes whitespace or comments), then the lexical analyzer proceeds to find 
additional lexemes, until one of the corresponding actions causes a return to 
the parser. The lexical analyzer returns a single value, the token name, to 
the parser, but uses the shared, integer variable yylval to pass additional 
information about the lexeme found, if needed. 
142 
CHAPTER 3. LEXICAL ANALYSIS 
Example 3.1 
1 : Figure 3.23 is a Lex program that recognizes the tokens of 
Fig. 3.12 and returns the token found. A few observations about this code will 
introduce us to many of the important features of Lex. 
In the declarations section we see a pair of special brackets, %( and %). 
Anything within these brackets is copied directly to the file lex . 
yy . 
c, and is 
not treated as a regular definition. It is common to place there the definitions of 
the manifest constants, using C #define statements to associate unique integer 
codes with each of the manifest constants. In our example, we have listed in a 
comment the names of the manifest constants, LT, IF, and so on, but have not 
shown them defined to be particular  integer^.^ 
Also in the declarations section is a sequence of regular definitions. These 
use the extended notation for regular expressions described in Section 3.3.5. 
Regular definitions that are used in later definitions or in the patterns of the 
translation rules are surrounded by curly braces. Thus, for instance, delim is 
defined to be a shorthand for the character class consisting of the blank, the 
tab, and the newline; the latter two are represented, as in all UNIX commands, 
by backslash followed by t or n, respectively. Then, ws is defined to be one or 
more delimiters, by the regular expression (del 
im)+. 
Notice that in the definition of id and number, parentheses are used as 
grouping metasymbols and do not stand for themselves. In contrast, E 
in the 
definition of number stands for itself. If we wish to use one of the Lex meta- 
symbols, such as any of the parentheses, +, *, or ?, to stand for themselves, we 
may precede them with a backslash. For instance, we see \ . in the definition of 
number, to represent the dot, since that character is a metasymbol representing 
"any character," as usual in UNIX regular expressions. 
In the auxiliary-function section, we see two such functions, i n s t a l l I D 0  
and installNum() . Like the portion of the declaration section that appears 
between %(. 
. 
.%I, 
everything in the auxiliary section is copied directly to file 
lex. 
yy . 
c, but may be used in the actions. 
Finally, let us examine some of the patterns and rules in the middle section of 
Fig. 3.23. First, ws, an identifier declared in the first section, has an associated 
empty action. If we find whitespace, we do not return to the parser, but look 
for another lexeme. The second token has the simple regular expression pattern 
i f .  Should we see the two letters i f  on the input, and they are not followed 
by another letter or digit (which would cause the lexical analyzer to find a 
longer prefix of the input matching the pattern for id), 
then the lexical analyzer 
consumes these two letters from the input and returns the token name IF, that 
is, the integer for which the manifest constant IF stands. Keywords then and 
e l s e  are treated similarly. 
The fifth token has the pattern defined by id. Note that, although keywords 
like i f  match this pattern as well as an earlier pattern, Lex chooses whichever 
31f Lex is used along with Yacc, then it would be normal to define the manifest constants 
in the Yacc program and use them without definition in the Lex program. Since lex 
.yy. c is 
compiled with the Yacc output, the constants thus will be available to the actions in the Lex 
program. 
3.5. THE LEXICAL-ANALYZER GENERATOR LEX 
%( 
/* definitions 
o
f
 manifest constants 
LT, 
LE, 
EQ, 
NE, 
GT, 
GE, 
IF, 
THEN, 
ELSE, 
ID, 
NUMBER, 
RELOP */ 
%3 
/* regular 
definitions 
*/ 
delim 
[ \t\nl 
ws 
(
d
e
l
i
m
)
+
 
letter 
[A-Za-z] 
digit 
[o-91 
id 
(letter) (
(
l
e
t
t
e
r
)
 
1 (
d
i
g
i
t
)
)
 * 
number 
(
d
i
g
i
t
)
+
 (\ . 
(
d
i
g
i
t
)
+
)
?
 
(E 
[+-I ?
(
d
i
g
i
t
)
+
)
?
 
(ws3 
if 
then 
else 
(
i
d
)
 
(number) 
11 < 
11 
11 < = I 1  
11=11 
(/* no action and no return 
*/) 
(return(1F) ; 
) 
(return(THEN) ; 
) 
(return(ELSE) ; 
) 
(yylval = (int) installID(); return(1D);) 
(yylval = (int) installNum() ; return(NUMBER) ; 
) 
(yylval = LT; 
return(REL0P) ; 
) 
(yylval = LE; 
return(REL0P) ; 
) 
(yylval = EQ 
; return(REL0P) ; 
) 
(yylval = NE; 
return(REL0P);) 
(yylval = GT; 
return(REL0P);) 
Cyylval 
= GE; 
return(REL0P);) 
int installID0 
(/* function 
to install 
the lexeme, 
whose 
first character 
is pointed to by yytext, 
and whose length 
is yyleng, 
into the 
symbol 
table and return a 
pointer 
thereto */ 
3 
int installNum() (/* similar 
to installID, 
but puts numer- 
ical constants 
into a separate 
table */ 
3 
Figure 3.23: Lex 
program for the tokens of Fig. 3.12 
144 
CHAPTER 3. LEXICAL ANALYSIS 
pattern is listed first in situations where the longest matching prefix matches 
two or more patterns. The action taken when id is matched is threefold: 
I. Function installID0 
is called to place the lexeme found in the symbol 
table. 
2. This function returns a pointer to the symbol table, which is placed in 
global variable yylval, 
where it can be used by the parser or a later 
component of the compiler. Note that installID 
() has available to it 
two variables that are set automatically by the lexical analyzer that Lex 
generates: 
(a) yytext 
is a pointer to the beginning of the lexeme, analogous to 
1exemeBegin 
in Fig. 3
.
3
.
 
(b) yyleng 
is the length of the lexeme found. 
3. The token name I D  is returned to the parser. 
The action taken when a lexeme matching the pattern number is similar, using 
the auxiliary function ins 
t 
allNum 
( 
) 
. 1
7
 
3.5.3 Conflict Resolution in Lex 
We have alluded to the two rules that Lex 
uses to decide on the proper lexeme 
to select, when several prefixes of the input match one or more patterns: 
1. Always prefer a longer prefix to a shorter prefix. 
2. If the longest possible prefix matches two or more patterns, prefer the 
pattern listed first in the Lex 
program. 
Example 3.12 : 
The first rule tells us to continue reading letters and digits to 
find the longest prefix of these characters to group as an identifier. It also tells 
us to treat <= as a single lexeme, rather than selecting < as one lexeme and = 
as the next lexeme. The second rule makes keywords reserved, if we list the 
keywords before id in the program. For instance, if then 
is determined to be 
the longest prefix of the input that matches any pattern, and the pattern then 
precedes {id), as it does in Fig. 3.23, then the token THEN is returned, rather 
than ID. 
3.5.4 The Lookahead Operator 
Lex 
automatically reads one character ahead of the last character that forms 
the selected lexeme, and then retracts the input so only the lexeme itself is 
consumed from the input. However, sometimes, we want a certain pattern to 
be matched to the input only when it is followed by a certain other characters. 
If so, we may use the slash in a pattern to indicate the end of the part of the 
3.5. THE LEXICAL-ANALYZER GENERATOR LEX 
145 
pattern that matches the lexeme. What follows / is additional pattern that 
must be matched before we can decide that the token in question was seen, but 
what matches this second pattern is not part of the lexeme. 
Example 3.13 : In Fortran and some other languages, keywords are not re- 
served. That situation creates problems, such as a statement 
where IF is the name of an array, not a keyword. This statement contrasts with 
statements of the form 
IF 
( condition ) THEN . . . 
where IF is a keyword. Fortunately, we can be sure that the keyword IF is 
always followed by a left parenthesis, some text - 
the condition - 
that may 
contain parentheses, a right parenthesis and a letter. Thus, we could write a 
Lex rule for the keyword IF like: 
This rule says that the pattern the lexeme matches is just the two letters IF. 
The slash says that additional pattern follows but does not match the lexeme. 
In this pattern, the first character is the left parentheses. Since that character is 
a Lex metas~mbol, 
it must be preceded by a backslash to indicate that it has its 
literal meaning. The dot and star match "any string without a newline." Note 
that the dot is a Lex metasymbol meaning "any character except newline." It 
is followed by a right parenthesis, again with a backslash to give that character 
its literal meaning. The additional pattern is followed by the symbol letter, 
which is a regular definition representing the character class of all letters. 
Note that in order for this pattern to be foolproof, we must preprocess 
the input to delete whitespace. We have in the pattern neither provision for 
whitespace, nor can we deal with the possibility that the condition extends over 
lines, since the dot will not match a newline character. 
For instance, suppose this pattern is asked to match a prefix of input: 
the first two characters match IF, the next character matches \ 
(, the next nine 
characters match . 
*, and the next two match \) and letter. Note the fact that 
the first right parenthesis (after C) is not followed by a letter is irrelevant; we 
only need to find some way of matching the input to the pattern. We conclude 
that the letters IF constitute the lexeme, and they are an instance of token if. 
146 
CHAPTER 3. LEXICAL ANALYSIS 
3.5.5 Exercises for Section 3.5 
Exercise 3.5.1 : 
Describe how to make the following modifications to the Lex 
program of Fig. 3.23: 
a) Add the keyword while. 
b) Change the comparison operators to be the C operators of that kind. 
c) Allow the underscore (-) as an additional letter. 
! 
d) Add a new pattern with token STRING. The pattern consists of a double- 
quote ("), any string of characters and a final double-quote. However, 
if a double-quote appears in the string, it must be escaped by preceding 
it with a backslash (\), and therefore a backslash in the string must be 
represented by two backslashes. The lexical value, which is the string 
without the surrounding double-quotes, and with backslashes used to es- 
cape a character removed. Strings are to be installed in a table of strings. 
Exercise 3.5.2 : Write a Lex program that copies a file, replacing each non- 
empty sequence of white space by a single blank. 
Exercise 3.5.3 : 
Write a Lex program that copies a C program, replacing each 
instance of the keyword f l o a t  by double. 
! 
Exercise 3.5.4: Write a Lex program that converts a file to "Pig latin." 
Specifically, assume the file is a sequence of words (groups of letters) separated 
by whitespace. Every time you encounter a word: 
1. If the first letter is a consonant, move it to the end of the word and then 
add ay. 
2. If the first letter is a vowel, just add ay to the end of the word. 
All nonletters are copied intact to the output. 
! 
Exercise 3.5.5 : 
In SQL, keywords and identifiers are case-insensitive. Write 
a Lex program that recognizes the keywords SELECT, F
R
O
M
,
 and W
H
E
R
E
 (in any 
combination of capital and lower-case letters), and token I D ,  which for the 
purposes of this exercise you may take to be any sequence of letters and digits, 
beginning with a letter. You need not install identifiers in a symbol table, but 
tell how the "install" function would differ from that described for case-sensitive 
identifiers as in Fig. 3.23. 
3.6. FINITE AUTOMATA 
3.6 
Finite Automata 
We shall now discover how Lex turns its input program into a lexical analyzer. 
At the heart of the transition is the formalism known as finite automata. These 
are essentially graphs, like transition diagrams, with a few differences: 
1. Finite automata are recognizers; they simply say "yes" or "no" about each 
possible input string. 
2. Finite automata come in two flavors: 
(a) Nondeterministic finite automata (NFA) have no restrictions on the 
labels of their edges. A symbol can label several edges out of the 
same state, and E, the empty string, is a possible label. 
(b) Deterministic finite automata (DFA) have, for each state, and for 
each symbol of its input alphabet exactly one edge with that symbol 
leaving that state. 
Both deterministic and nondeterministic finite automata are capable of rec- 
ognizing the same languages. In fact these languages are exactly the same 
languages, called the regular languages, that regular expressions can de~cribe.~ 
3.6. I Nondeterministic Finite Automata 
A nondeterministic finite automaton (NFA) consists of: 
1. A finite set of states S. 
2. A set of input symbols C, the input alphabet. We assume that E, which 
stands for the empty string, is never a member of C. 
3. A transition function that gives, for each state, and for each symbol in 
C U (E) a set of next states. 
4. A state so from S that is distinguished as the start state (or initial state). 
5 .  A set of states F, a subset of S, that is distinguished as the accepting 
states (or final states). 
We can represent either an NFA or DFA by a transition graph, where the 
nodes are states and the labeled edges represent the transition function. There 
is an edge labeled a from state s to state t if and only if t is one of the next 
states for state s and input a. This graph is very much like a transition diagram, 
except: 
-
-
 
- 
4 ~ h e r e  
is a small lacuna: as we defined them, regular expressions cannot describe the 
empty language, since we would never want to use this pattern in practice. However, finite 
automata can define the empty language. In the theory, 0 is treated as an additional regular 
expression for the sole purpose of defining the empty language. 
148 
CHAPTER 3. LEXICAL ANALYSIS 
a) The same symbol can label edges from one state to several different states, 
and 
b) An edge may be labeled by c, the empty string, instead of, or in addition 
to, symbols from the input alphabet. 
Example 3.14: The transition graph for an NFA recognizing the language 
of regular expression (aJb)*abb 
is shown in Fig. 3.24. This abstract example, 
describing all strings of a's and b's ending in the particular string abb, will be 
used throughout this section. It is similar to regular expressions that describe 
languages of real interest, however. For instance, an expression describing all 
files whose name ends in .o is any* 
.o, where any stands for any printable 
character . 
Figure 3.24: A nondeterministic finite automaton 
Following our convention for transition diagrams, the double circle around 
state 3 indicates that this state is accepting. Notice that the only ways to get 
from the start state 0 to the accepting state is to follow some path that stays 
in state 0 for a while, then goes to states 1, 2, and 3 by reading abb from the 
input. Thus, the only strings getting to the accepting state are those that end 
in abb. 
3.6.2 
Transition Tables 
We can also represent an NFA by a transition table, whose rows correspond to 
states, and whose columns correspond to the input symbols and c. The entry for 
a given state and input is the value of the transition function applied to those 
arguments. If the transition function has no information about that state-input 
pair, we put Q) in the table for the pair. 
Example 3.15: The transition table for the NFA of Fig. 3.24 is shown in 
Fig. 3.25. 
The transition table has the advantage that we can easily find the transitions 
on a given state and input. Its disadvantage is that it takes a lot of space, when 
the input alphabet is large, yet most states do not have any moves on most of 
the input symbols. 
3.6. FINITE AUTOMATA 
Figure 3.25: Transition table for the NFA of Fig. 3.24 
3.6.3 Acceptance of Input Strings by Automata 
An NFA accepts input string x if and only if there is some path in the transition 
graph from the start state to one of the accepting states, such that the symbols 
along the path spell out x. Note that c labels along the path are effectively 
ignored, since the empty string does not contribute to the string constructed 
along the path. 
Example 3.16: The string aabb is accepted by the NFA of Fig. 3.24. The 
path labeled by aabb from state 0 to state 3 demonstrating this fact is: 
a 
a 
b 
b 
0
-
0
-
1
-
2
-
3
 
Note that several paths labeled by the same string may lead to different states. 
For instance, path 
a 
a 
b 
b 
0
-
0
-
0
-
0
-
0
 
is another path from state 0 labeled by the string aabb. This path leads to 
state 0, which is not accepting. However, remember that an NFA accepts a 
string as long as some path labeled by that string leads from the start state 
to an accepting state. The existence of other paths leading to a nonaccepting 
state is irrelevant. 
The language defined (or accepted) by an NFA is the set of strings labeling 
some path from the start to an accepting state. As was mentioned, the NFA of 
Fig. 3.24 defines the same language as does the regular expression (aJb)* 
abb, 
that is, all strings from the alphabet {a, 
b} that end in abb. We may use L(A) 
to stand for the language accepted by automaton A. 
Example 3.17 : 
Figure 3.26 is an NFA accepting L(aa* 
lbb*). String aaa is 
accepted because of the path 
E 
a 
a 
a 
0
 -
-
 
1 -------- 
2
 -------- 
2
 - 
2 
Note that E'S "disappear" in a concatenation, so the label of the path is aaa. 
3.6.4 Deterministic Finite Automata 
A deterministic finite automaton (DFA) is a special case of an NFA where: 
CHAPTER 3. LEXICAL ANALYSIS 
Figure 3.26: NFA accepting aa* 
1 bb* 
1. There are no moves on input E ,  and 
2. For each state s and input symbol a, there is exactly one edge out of s 
labeled a. 
If we are using a transition table to represent a DFA, then each entry is a single 
state. we may therefore represent this state without the curly braces that we 
use to form sets. 
While the NFA is an abstract representation of an algorithm to recognize 
the strings of a certain language, the DFA is a simple, concrete algorithm for 
recognizing strings. It is fortunate indeed that every regular expression and 
every NFA can be converted to a DFA accepting the same language, because it 
is the DFA that we really implement or simulate when building lexical analyzers. 
The following algorithm shows how to apply a DFA to a string. 
Algorithm 3.18 : 
Simulating a DFA. 
INPUT: An input string x terminated by an end-of-file character eof. A DFA 
D with start state so, accepting states F, and transition function moue. 
OUTPUT: Answer ''yes" if D accepts x; "no" otherwise. 
METHOD: Apply the algorithm in Fig. 3.27 to the input string x. The function 
moue(s, 
c) gives the state to which there is an edge from state s on input c. 
The function next Char returns the next character of the input string x. 
Example 3.19 : 
In Fig. 3.28 we see the transition graph of a DFA accepting 
the language (alb)*abb, 
the same as that accepted by the NFA of Fig. 3.24. 
Given the input string ababb, this DFA enters the sequence of states 0,1,2,1,2,3 
and returns "yes." 
3.6. FINITE AUTOMATA 
S = so; 
c = nextchar(); 
while ( c != 
eof ) { 
s = move(s, c); 
c = nextchar(); 
1 
if ( s is in F ) return "yes"; 
else return "no"; 
Figure 3.27: Simulating a DFA 
Figure 3.28: DFA accepting (aJb)*abb 
3
.
6
.
5
 Exercises for Section 3
.
6
 
! 
Exercise 3.6.1 : 
Figure 3.19 in the exercises of Section 3.4 computes the failure 
function for the KMP algorithm. Show how, given that failure function, we 
can construct, from a keyword blb2 . bn an n + I-state DFA that recognizes 
.* bl b2 . bn, where the dot stands for "any character." Moreover, this DFA can 
be constructed in O(n) time. 
Exercise 3.6.2 : Design finite automata (deterministic or nondeterministic) 
for each of the languages of Exercise 3.3.5. 
Exercise 3.6.3 : 
For the NFA of Fig. 3.29, indicate all the paths labeled aabb. 
Does the NFA accept aabb? 
start 8-83 
a 
a 
Figure 3.29: NFA for Exercise 3.6.3 
CHAPTER 3. LEXICAL ANALYSIS 
Figure 3.30: NFA for Exercise 3.6.4 
Exercise 3.6.4 : 
Repeat Exercise 3.6.3 for the NFA of Fig. 3.30. 
Exercise 3.6.5 : 
Give the transition tables for the NFA of: 
a) Exercise 3.6.3. 
b) Exercise 3.6.4. 
c) Figure 3.26. 
3.7 From Regular Expressions to Automata 
The regular expression is the notation of choice for describing lexical analyzers 
and other pattern-processing software, as was reflected in Section 3.5. How- 
ever, implementation of that software requires the simulation of a DFA, as in 
Algorithm 3.18, or perhaps simulation of an NFA. Because an NFA often has a 
choice of move on an input symbol (as Fig. 3.24 does on input a from state 0) 
or on e (as Fig. 3.26 does from state 0), or even a choice of making a transition 
on E
:
 or on a real input symbol, its simulation is less straightforward than for a 
DFA. Thus often it is important to convert an NFA to a DFA that accepts the 
same language. 
In this section we shall first show how to convert NFA's to DFA's. Then, we 
use this technique, known as "the subset construction," to give a useful algo- 
rit 
hm for simulating NFA's directly, in situations (other than lexical analysis) 
where the NFA-to-DFA conversion takes more time than the direct simulation. 
Next, we show how to convert regular expressions to NFA's, from which a DFA 
can be constructed if desired. We conclude with a discussion of the time-space 
tradeoffs inherent in the various methods for implementing regular expressions, 
and see how to choose the appropriate method for your application. 
3.7.1 Conversion of an NFA to a DFA 
The general idea behind the subset construction is that each state of the 
constructed DFA corresponds to a set of NFA states. After reading input 
3.7. FROM REGULAR EXPRESSIONS T O  AUTOMATA 
ala2 - - .  
a,, the DFA is in that state which corresponds to the set of states that 
the NFA can reach, from its start state, following paths labeled ala2 . 
. 
an. 
It is possible that the number of DFA states is exponential in the number 
of NFA states, which could lead to difficulties when we try to implement this 
DFA. However, part of the power of the automaton-based approach to lexical 
analysis is that for real languages, the NFA and DFA have approximately the 
same number of states, and the exponential behavior is not seen. 
Algorithm 3.20 : 
The subset construction of a DFA from an NFA. 
OUTPUT: A DFA D accepting the same language as N. 
METHOD: Our algorithm constructs a transition table Dtran for D. Each 
state of D is a set of NFA states, and we construct Dtran so D will simulate 
"in parallel" all possible moves N can make on a given input string. Our first 
problem is to deal with e-transitions of N properly. In Fig. 3.31 we see the 
definitions of several functions that describe basic computations on the states 
of N that are needed in the algorithm. Note that s is a single state of N, while 
T is a set of states of N. 
t-closure(s) I Set of NFA states reachable from NFA state s 
I in set T on €-transitions alone; = Us 
in T 
e-closure(s). 
e-closure(T) 
move(T,a) 
( Set of NFA states to which there is a transition on 
on e-transitions alone. 
Set of NFA states reachable from some NFA state s 
I input symbol a from some state s in T. 
Figure 3.31: Operations on NFA states 
We must explore those sets of states that N can be in after seeing some input 
string. As a basis, before reading the first input symbol, N can be in any of the 
states of E-closure(so), 
where so is its start state. For the induction, suppose 
that N can be in set of states T after reading input string x. If it next reads 
input a, 
then N can immediately go to any of the states in move(T, 
a). However, 
after reading a, it may also make several €-transitions; thus N could be in any 
state of e-closure(move(T, 
a)) after reading input xu. Following these ideas, the 
construction of the set of D's states, Dstates, and its transition function Dtran, 
is shown in Fig. 3.32. 
The start state of D is c-closure(so), and the accepting states of D are all 
those sets of N's states that include at least one accepting state of N. To 
complete our description of the subset construction, we need only to show how 
CHAPTER 3.- LEXICAL ANALYSIS 
initially, e-closure(so) is the only state in Dstates, and it is unmarked; 
while ( there is an unmarked state T in Dstates ) { 
mark T; 
for ( each input symbol a ) { 
U = 
E- closure(moue(~, 
a)) 
; 
if ( U is not in Dstates ) 
add U as an unmarked state to Dstates; 
Dtran[T, a] = U; 
3 
Figure 3.32: The subset construction 
E-closure(T) is computed for any set of NFA states T. This process, shown in 
Fig. 3.33, is a straightforward search in a graph from a set of states. In this 
case, imagine that only the €-labeled edges are available in the graph. 
push all states of T onto stack; 
initialize E- closure(T) to T; 
while ( stack is not empty ) { 
pop t, the top element, off stack; 
for ( each state u with an edge from t to u labeled e ) 
if ( u is not in e-closure(T) ) { 
add u to e-closure(T); 
push u onto stack 
1 
Figure 3.33: Computing E- closure(T) 
Example 3.21 : 
Figure 3.34 shows another NFA accepting (a1 
b) 
*abb; 
it hap- 
pens to be the one we shall construct directly from this regular expression in 
Section 3.7. Let us apply Algorithm 3.20 to Fig. 3.29. 
The start state A of the equivalent DFA is E-closure(O), 
or A = {0,1,2,4,7), 
since these are exactly the states reachable from state 0 via a path all of whose 
edges have label e. Note that a path can have zero edges, so state 0 is reachable 
from itself by an €-labeled path. 
The input alphabet is {a, 
b). Thus, our first step is to mark A and compute 
Dtran[A, 
a] = E-closure(moue(A, 
a)) and Dtran[A, 
b] = t- 
closure(moue(A, 
b)) . 
Among the states 0, 1, 2, 4, and 7, only 2 and 7 have transitions on a, to 
3 and 8, respectively. Thus, move(A, 
a) = {3,8). Also, t-closure({3,8) = 
{1,2,3,4,6,7,8), 
so we conclude 
3.7. FROM REGULAR EXPRESSIONS TO AUTOMATA 
Figure 3.34: NFA N for (alb)*abb 
Dtran[A, 
a] = 
e-closure(rnoue(A, 
a)) = 
e-closure({3, 8)) = 
{I, 
2,3,4,6,7,8) 
Let us call this set B, so Dtran[A, 
a] = 
B. 
Now, we must compute Dtran[A, 
b]. Among the states in A, only 4 has a 
transition on b, and it goes to 5. Thus, 
Let us call the above set C, so Dtran[A, 
b] = 
C. 
Figure 3.35: Transition table Dtran for DFA D 
If we continue this process with the unmarked sets B and C, we eventually 
reach a point where all the states of the DFA are marked. This conclusion is 
guaranteed, since there are "only" 2'' different subsets of a set of eleven NFA 
states. The five different DFA states we actually construct, their correspond- 
ing sets of NFA states, and the transition table for the DFA D are shown in 
Fig. 3.35, and the transition graph for D is in Fig. 3.36. State A is the start 
state, and state E, which contains state 10 of the NFA, is the only accepting 
state. 
Note that D has one more state than the DFA of Fig. 3.28 for the same lan- 
guage. States A and C have the same move function, and so can be merged. We 
discuss the matter of minimizing the number of states of a DFA in Section 3.9.6. 
CHAPTER 3. LEXICAL ANALYSIS 
Figure 3.36: Result of applying the subset construction to Fig. 3.34 
3.7.2 Simulationofan NFA 
A strategy that has been used in a number of text-editing programs is to con- 
struct an NF'A from a regular expression and then simulate the NFA using 
something like an on-the-fly subset construction. The simulation is outlined 
below. 
Algorit 
hrn 3.22 : 
Simulating an NFA. 
INPUT: An input string x terminated by an end-of-file character eof. An NFA 
N with start state so, accepting states F, 
and transition function moue. 
OUTPUT: Answer "yes7' 
if M accepts x; "no" otherwise. 
METHOD: The algorithm keeps a set of current states S, 
those that are reached 
from so following a path labeled by the inputs read so far. I
f
 c is the next input 
character, read by the function nextchar(), then we first compute move(S, 
c) 
and then close that set using E-closure(). 
The algorithm is sketched in Fig. 3.37. 
S = E-closure(so); 
c = nextchar(); 
while ( c != 
eof ) { 
S = E- 
closure 
(move(S, c)) 
; 
c = nextchar(); 
1 
if ( S n 
F != 0 
) return Ityesll; 
else return "no"; 
Figure 3.37: Simulating an NFA 
3.7. FROM REGULAR EXPRESSIONS TO AUTOMATA 
3.7.3 
Efficiency of NFA Simulation 
If carefully implemented, Algorithm 3.22 can be quite efficient. As the ideas 
involved are useful in a number of similar algorithms involving search of graphs, 
we shall look at this implementation in additional detail. The data structures 
we need are: 
I. Two stacks, each of which holds a set of NFA states. One of these stacks, 
oldstates, holds the "current" set of states, i.e., the value of S on the right 
side of line (4) in Fig. 3.37. The second, newstates, holds the "next" set 
of states - 
S on the left side of line (4). Unseen is a step where, as we 
go around the loop of lines (3) through (6), newstates is transferred to 
oldstates. 
2. A boolean array alreadyon, indexed by the NFA states, to indicate which 
states are in newstates. While the array and stack hold the same infor- 
mation, it is much faster to interrogate alreadyOn[s] than to search for 
state s on the stack newstates. It is for this efficiency that we maintain 
both representations. 
3. A two-dimensional array move[s, 
a] holding the transition table of the 
NFA. The entries in this table, which are sets of states, are represented 
by linked lists. 
To implement line (1) of Fig. 3.37, we need to set each entry in array al- 
readyon to FALSE, then for each state s in c-closure(so), 
push s onto oldstates 
and set alreadyOn[s] 
to TRUE. This operation on state s, and the implementation 
of line (4) as well, are facilitated by a function we shall call addState(s). This 
function pushes state s onto newstates, sets alreadyOn[s] to TRUE, and calls 
itself recursively on the states in move[s, 
€1 in order to further the computation 
of c-closure(s). However, to avoid duplicating work, we must be careful never 
to call addstate on a state that is already on the stack newstates. Figure 3.38 
sketches this function. 
9) 
addState(s) { 
10) 
push s onto newstates; 
l1> 
alreadyOn[s] = TRUE; 
12) 
for ( t on move[s, 
€1 ) 
13) 
if ( !alreadyOn(t) ) 
14) 
addState(t) 
; 
15) > 
Figure 3.38: Adding a new state s, which is known not to be on newstates 
We implement line (4) of Fig. 3.37 by looking at each state s on oldstates. 
We first find the set of states move[s, 
c], where c is the next input, and for each 
158 
CHAPTER 3. LEXICAL ANALYSIS 
of those states that is not already on newstates, we apply addstate to it. Note 
that addstate has the effect of computing the E-closure 
and adding all those 
states to newstates as well, if they were not already on. This sequence of steps 
is summarized in Fig. 3.39. 
16) for ( s on oldstates ) { 
17) 
for ( t on move[s, 
c] ) 
18) 
if ( !alreadyOn[t] ) 
19) 
addState(t) 
; 
20) 
pop s from oldstates; 
21) > 
22) 
for ( s on newstates ) { 
23) 
pop s from newstates; 
24) 
push s onto oldstates; 
25) 
alreadyOn[s] = 
FALSE; 
26) 
} 
Figure 3.39: Implementation of step (4) of Fig. 3.37 
Now, suppose that the NFA N has n states and m transitions; i.e., m is the 
sum over all states of the number of symbols (or E )  on which the state has a 
transition out. Not counting the call to addstate at line (19) of Fig. 3.39, the 
time spent in the loop of lines (16) through (21) is O(n). That is, we can go 
around the loop at most n times, and each step of the loop requires constant 
work, except for the time spent in addstate. The same is true of the loop of 
lines (22) through (26). 
During one execution of Fig. 3.39, i.e., of step (4) of Fig. 3.37, it is only 
possible to call addstate on a given state once. The reason is that whenever 
we call addState(s), we set alreadyOn[s] 
to T
R
U
E
 at line (11) of Fig. 3.39. Once 
alreadyOn[s] 
is TRUE, the tests at line (13) of Fig. 3.38 and line (18) of Fig. 3.39 
prevent another call. 
The time spent in one call to addstate, exclusive of the time spent in recur- 
sive calls at line (14), is O(1) for lines (10) and (11). For lines (12) and (13), 
the time depends on how many €-transitions there are out of state s. We do 
not know this number for a given state, but we know that there are at most m 
transitions in total, out of all states. As a result, the aggregate time spent in 
lines (11) over all calls to addstate during one execution of the code of Fig. 3.39 
is O(m). The aggregate for the rest of the steps of addstate is O(n), since it is 
a constant per call, and there are at most n calls. 
We conclude that, implemented properly, the time to execute line (4) of 
Fig. 3.37 is O(n + 
m). The rest of the while-loop of lines (3) through (6) takes 
O(1) time per iteration. If the input x is of length I F ,  then the total work in 
that loop is O((k(n + 
m)). Line (1) of Fig. 3.37 can be executed in O(n + 
m) 
time, since it is essentially the steps of Fig. 3.39 with oldstates containing only 
3.7. FROM REGULAR EXPRESSIONS TO AUTOMATA 
159 
Big-Oh Notation 
An expression like O(n) is a shorthand for "at most some constant times 
n." Technically, we say a function f 
(n), 
perhaps the running time of some 
step of an algorithm, is 0 
(g(n)) if there are constants c and no, such that 
whenever n 2 
no, it is true that f (n) < cg(n). A useful idiom is "0(1)," 
which means "some constant." The use of this big-oh notation enables 
us to avoid getting too far into the details of what we count as a unit of 
execution time, yet lets us express the rate at which the running time of 
an algorithm grows. 
the state so. Lines (2)) (7), and (8) each take O(1) time. Thus, the running 
time of Algorithm 3.22, properly implemented, is O((lc(n + 
m)). That is, the 
time taken is proportional to the length of the input times the size (nodes plus 
edges) of the transition graph. 
3.7.4 Construction of an NFA from a Regular Expression 
We now give an algorithm for converting any regular expression to an NFA 
that defines the same language. The algorithm is syntax-directed, in the sense 
that it works recursively up the parse tree for the regular expression. For each 
subexpression the algorithm constructs an NFA with a single accepting state. 
Algorithm 3.23 : 
The McNaughton-Yamada-Thompson algorithm to convert 
a regular expression to an NFA. 
INPUT: A regular expression r over alphabet C. 
OUTPUT: An NFA N accepting L(r). 
METHOD: Begin by parsing r into its constituent subexpressions. The rules 
for constructing an NFA consist of basis rules for handling subexpressions with 
no operators, and inductive rules for constructing larger NFA's from the NFA's 
for the immediate subexpressions of a given expression. 
BASIS: For expression e construct the NFA 
st 
a
-
 
Here, i is a new state, the start state of this NFA, and f is another new state, 
the accepting state for the NFA. 
For any subexpressiop a in C, construct the NFA 
160 
CHAPTER 3. LEXICAL ANALYSIS 
where again i and f are new states, the start and accepting states, respectively. 
Note that in both of the basis constructions, we construct a distinct NFA, with 
new states, for every occurrence of e or some a as a subexpression of r. 
INDUCTION: Suppose N(s) and N(t) are NFA's for regular expressions s and 
t, respectively. 
a) Suppose r = slt. Then N (r), 
the NFA for r, is constructed as in Fig. 3.40. 
Here, i and f are new states, the start and accepting states of N(r), 
respectively. There are €-transitions from i to the start states of N(s) 
and N(t), and each of their accepting states have €-transitions to the 
accepting state f .  
Note that the accepting states of N(s) and N(t) are 
not accepting in N(r). Since any path from i to f must pass through 
either N (s) or N (t) exclusively, and since the label of that path is not 
changed by the e's leaving i or entering f , 
we conclude that N 
(r) accepts 
L(s) U L(t), which is the same as L(r). That is, Fig. 3.40 is a correct 
construction for the union operator. 
Figure 3.40: NFA for the union of two regular expressions 
b) Suppose r = st. Then construct N(r) as in Fig. 3.41. The start state of 
N (s) 
becomes the start state of N (r), and the accepting state of N(t) is 
the only accepting state of N(r). The accepting state of N (s) and the 
start state of N (t) are merged into a single state, with all the transitions 
in or out of either state. A path from i to f in Fig. 3.41 must go first 
through N(s), and therefore its label will begin with some string in L(s). 
The path then continues through N(t), so the path's label finishes with a 
string in L(t). As we shall soon argue, accepting states never have edges 
out and start states never have edges in, so it is not possible for a path to 
re-enter N(s) after leaving it. Thus, N(r) accepts exactly L(s)L(t), 
and 
is a correct NFA for r = 
st. 
Figure 3.41: NFA for the concatenation of two regular expressions 
3.7. FROM REGULAR EXPRESSIONS TO AUTOMATA 
161 
c) Suppose r = s*. Then for r we construct the NFA N (r) 
shown in Fig. 3.42. 
Here, i and f are new states, the start state and lone accepting state of 
N (r) 
. To get from i to f 
, 
we cail either follow the introduced path labeled 
E ,  which takes care of the one string in ~ ( s ) ' ,  
or we can go to the start 
state of N(s), through that NFA, then from its accepting state back to 
its start state zero or more times. These options allow N (r) to accept all 
the strings in L(s)' , 
L ( s ) ~ ,  
and so on, so the entire set of strings accepted 
by N(r) is L(s*). 
Figure 3.42: NFA for the closure of a regular expression 
d) Finally, suppose r = (s). Then L(r) = L(s), and we can use the NFA 
N(s) as N(r). 
The method description in Algorithm 3.23 contains hints as to why the 
inductive construction works as it should. We shall not give a formal correctness 
proof, but we shall list several properties of the constructed NFA's, in addition 
to the all-important fact that N (r) accepts language L(r). These properties 
are interesting in their own right, and helpful in making a formal proof. 
1. N(r) has at most twice as many states as there are operators and operands 
in r. This bound follows from the fact that each step of the algorithm 
creates at most two new states. 
2. N(r) has one start state and one accepting state. The accepting state has 
no outgoing transitions, and the start state has no incoming transitions. 
3. Each state of N (r) other than the accepting state has either one outgoing 
transition on a symbol in C or two outgoing transitions, both on E .  
Example 3.24: Let us use Algorithm 3.23 to construct an NFA for r = 
(a(b)*abb. 
Figure 3.43 shows a parse tree for r that is analogous to the parse 
trees constructed for arithmetic expressions in Section 2.2.3. For subexpression 
rl, the first a, we construct the NFA: 
CHAPTER 3. LEXICAL ANALYSIS 
Figure 3.43: Parse tree for (alb)*abb 
State numbers have been chosen for consistency with what follows. For r2 we 
We can now combine N ( r l )  
and N(rz), 
using the construction of Fig. 3.40 to 
obtain the NFA for r3 = 
r l J r 2 ;  
this NFA is shown in Fig. 3.44. 
Figure 3.44: NFA for r3 
The NFA for r4 = (r3) 
is the same as that for r3. The NFA for r~ 
= (r3)* 
is 
then as shown in ~
i
~
.
 
3.45. We have used the construction in ~
i
~
.
 
3.42 to build 
this NFA from the NFA in Fig. 3.44. 
Now, consider subexpression ra, which is another a. We use the basis con- 
struction for a again, but we must use new states. It is not permissible to reuse 
3.7. FROM REGULAR EXPRESSIONS TO AUTOMATA 
Figure 3.45: NFA for r5 
the NFA we constructed for rl , 
even though rl and 7-6 are the same expression. 
The NFA for r
6
 is: 
To obtain the NFA for r y  = rgr6, we apply the construction of Fig. 3.41. We 
merge states 7 and 7', yielding the NFA of Fig. 3.46. Continuing in this fashion 
with new NFA's for the two subexpressions b called rs and rlo, we eventually 
construct the NFA for (alb)*abb 
that we first met in Fig. 3.34. 
E 
Figure 3.46: NFA for r~ 
3.7.5 
Efficiency of String-Processing Algorithms 
We observed that Algorithm 3.18 processes a string x in time O(lxl), while in 
Section 3.7.3 we concluded that we could simulate an NFA in time proportional 
to the product of 1
x
1
 and the size of the NFA's transition graph. Obviously, it 
164 
CHAPTER 3. LEXICAL ANALYSIS 
is faster to have a DFA to simulate than an NFA, so we might wonder whether 
it ever makes sense to simulate an NFA. 
One issue that may favor an NFA is that the subset construction can, in the 
worst case, exponentiate the number of states. While in principle, the number 
of DFA states does not influence the running time of Algorithm 3.18, should 
the number of states become so large that the transition table does not fit in 
main memory, then the true running time would have to include disk 1/0 
and 
therefore rise noticeably. 
Example 3.25 : 
Consider the family of languages described by regular expres- 
sions of the form L, = (a/ 
b)*a(a/ 
b)"-', that is, each language L, consists of 
strings of a's and b's such that the nth character to the left of the right end 
holds a. An n + I-state NFA is easy to construct. It stays in its initial state 
under any input, but also has the option, on input a, of going to state 1. From 
state 1, 
it goes to state 2 on any input, and so on, until in state n it accepts. 
Figure 3.47 suggests this NFA. 
Figure 3.47: An NFA that has many fewer states than the smallest equivalent 
DFA 
However, any DFA for the language L, must have at least 2n states. We 
shall not prove this fact, but the idea is that if two strings of length n can 
get the DFA to the same state, then we can exploit the last position where 
the strings differ (and therefore one must have a, the other b) to continue the 
strings identically, until they are the same in the last n - 
1 
positions. The DFA 
will then be in a state where it must both accept and not accept. Fortunately, 
as we mentioned, it is rare for lexical analysis to involve patterns of this type, 
and we do not expect to encounter DFA's with outlandish numbers of states in 
practice. 
However, lexical-analyzer generators and other string-processing systems 
often start with a regular expression. We are faced with a choice of converting 
the regular expression to an NFA or DFA. The additional cost of going to a DFA 
is thus the cost of executing Algorithm 3.23 on the NFA (one could go directly 
from a regular expression to a DFA, but the work is essentially the same). I
f
 
the string-processor is one that will be executed many times, as is the case for 
lexical analysis, then any cost of converting to a DFA is worthwhile. However, 
in other string-processing applications, such as grep, where the user specifies 
one regular expression and one or several files to be searched for the pattern 
3.7. FROM REGULAR EXPRESSIONS TO AUTOMATA 
165 
of that expression, it may be more efficient to skip the step of constructing a 
DFA, and simulate the NFA directly. 
Let us consider the cost of converting a regular expression r to an NFA by 
Algorithm 3.23. A key step is constructing the parse tree for r. In Chapter 4 
we shall see several methods that are capable of constructing this parse tree in 
linear time, that is, in time O(lrl), where JrJ 
stands for the size of r - 
the sum 
of the number of operators and operands in r. It is also easy to check that each 
of the basis and inductive constructions of Algorithm 3.23 takes constant time, 
so the entire time spent by the conversion to an NFA is O(lr(). 
Moreover, as we observed in Section 3.7.4, the NFA we construct has at 
most Irl states and at most 21r( transitions. That is, in terms of the analysis 
in Section 3.7.3, we have n 5 Irl and rn 2 
21rJ. Thus, simulating this NFA on 
an input string x takes time O((r( 
x 1x1). This time dominates the time taken 
by the NFA construction, which is O(lrl), and therefore, we conclude that it is 
possible to take a regular expression r and string x, and tell whether x is in 
L(r) in time O(lrl x 1x1). 
The time taken by the subset construction is highly dependent on the num- 
ber of states the resulting DFA has. To begin, notice that in the subset con- 
struction of Fig. 3.32, the key step, the construction of a set of states U from 
a set of states T and an input symbol a, is very much like the construction of 
a new set of states from the old set of states in the NFA simulation of Algo- 
rithm 3.22. We already concluded that, properly implemented, this step takes 
time at most proportional to the number of states and transitions of the NFA. 
Suppose we start with a regular expression r and convert it to an NFA. This 
NFA has at most lrl states and at most 2
1
7
-
1
 transitions. Moreover, there are 
at most lr( input symbols. Thus, for every DFA state constructed, we must 
construct at most lr 1 new states, and each one takes at most O(lrl + 
2(r 
1
)
 time. 
The time to construct a DFA of s states is thus O(lrI2s). 
In the common case where s is about lr(, 
the subset construction takes time 
O(lrI3). However, in the worst case, as in Example 3.25, this time is 0((rl22Irl). 
Figure 3.48 summarizes the options when one is given a regular expression r 
and wants to produce a recognizer that will tell whether one or more strings x 
are in L(r). 
DFA typical case I 
O(JrI3) I 
O(lx() 
AUTOMATON 
NFA 
DFA worst case ( 0(lr1~21'l) I 
O(lx1) 
Figure 3.48: Initial cost and per-string-cost of various methods of recognizing 
the language of a regular expression 
INITIAL 
O(H) 
If the per-string cost dominates, as it does when we build a lexical analyzer, 
PER STRING 
O(b-1 1x1) 
166 
CHAPTER 3. LEXICAL ANALYSIS 
we clearly prefer the DFA. However, in commands like grep, where we run the 
automaton on only one string, we generally prefer the NFA. It is not until 1
x
1
 
approaches JrJ3 
that we would even think about converting to a DFA. 
There is, however, a mixed strategy that is about as good as the better of 
the NFA and the DFA strategy for each expression r and string x. Start off 
simulating the NFA, but remember the sets of NFA states (i.e., the DFA states) 
and their transitions, as we compute them. Before processing the current set of 
NFA states and the current input symbol, check to see whether we have already 
computed this transition, and use the information if so. 
3.7.6 Exercises for Section 3.7 
Exercise 3.7.1 : 
Convert to DFA7s 
the NFA's of: 
a) Fig. 3.26. 
b) Fig. 3.29. 
c) Fig. 3.30. 
Exercise 3.7.2 : 
use Algorithm 3.22 to simulate the NFA7s: 
a) Fig. 3.29. 
b) Fig. 3.30. 
on input aabb. 
Exercise 3.7.3 : Convert the following regular expressions to deterministic 
finite automata, using algorithms 3.23 and 3.20: 
3.8 Design of a Lexical- 
Analyzer Generator 
In this section we shall apply the techniques presented in Section 3.7 to see 
how a lexical-analyzer generator such as Lex is architected. We discuss two 
approaches, based on NFA's and DFA7s; 
the latter is essentially the implemen- 
tation of Lex. 
3.8. DESIGN OF A LEXICAL-ANALYZER GENERATOR 
3.8.1 The Structure of the Generated Analyzer 
Figure 3.49 overviews the architecture of a lexical analyzer generated by Lex. 
The program that serves as the lexical analyzer includes a fixed program that 
simulates an automaton; at this point we leave open whether that automaton 
is deterministic or nondeterministic. The rest of the lexical analyzer consists of 
components that are created from the Lex program by Lex itself. 
Input buffer 
lexemeBegin 
forward 
Automaton 
simulator 
Figure 3.49: A Lex program is turned into a transition table and actions, which 
are used by a finite-automaton simulator 
I 
1 
A 
These components are: 
Lex 
program 
1. A transition table for the automaton. 
2. Those functions that are passed directly through Lex to the output (see 
Section 3.5.2). 
Actions 
Lex 
compiler 
3. The actions from the input program, which appear as fragments of code 
to be invoked at the appropriate time by the automaton simulator. 
To construct the automaton, we begin by taking each regular-expression 
pattern in the Lex program and converting it, using Algorithm 3.23, to an NFA. 
We need a single automaton that will recognize lexemes matching any of the 
patterns in the program, so we combine all the NFA's into one by introducing 
a new start state with €-transitions to each of the start states of the NFA's Ni 
for pattern pi. This construction is shown in Fig. 3.50. 
w 
Example 3.26 : 
We shall illustrate the ideas of this section with the following 
simple, abstract example: 
Transition 
table 
CHAPTER 3. LEXICAL ANALYSIS 
Figure 3.50: An NFA constructed from a Lex program 
a 
{ action Al for pattern pl ) 
abb 
{ action A2 for pattern p
2
 } 
a*b+ { action A3 for pattern p
g
 } 
Note that these three patterns present some conflicts of the type discussed 
in Section 3.5.3. In particular, string abb matches both the second and third 
patterns, but we shall consider it a lexeme for pattern pa, since that pattern 
is listed first in the above Lex program. Then, input strings such as aabbb . 
. . 
have many prefixes that match the third pattern. The Lex rule is to take the 
longest, so we continue reading b's, until another a is met, whereupon we report 
the lexeme to be the initial a's followed by as many b's as there are. 
Figure 3.51 shows three NFA's that recognize the three patterns. The third 
is a simplification of what would come out of Algorithm 3.23. Then, Fig. 3.52 
shows these three NFA's combined into a single NFA by the addition of start 
state 0 and three €-transitions. 
3
.
8
.
2
 Pattern Matching Based on NFA's 
If the lexical analyzer simulates an NFA such as that of Fig. 3.52, then it must 
read input beginning at the point on its input which we have referred to as 
ZexerneBegin. As it moves the pointer called forward ahead in the input, it 
calculates the set of states it is in at each point, following Algorithm 3.22. 
Eventually, the NFA simulation reaches a point on the input where there 
are no next states. At that point, there is no hope that any longer prefix of the 
input would ever get the NFA to an accepting state; rather, the set of states 
will always be empty. Thus, we are ready to decide on the longest prefix that 
is a lexeme matching some pattern. 
3.8. DESIGN OF A LEXICAL-ANALYZER GENERATOR 
Figure 3.51: NFA's for a, abb, and a*b+ 
Figure 3.52: Combined NFA 
none 
Figure 3.53: Sequence of sets of states entered when processing input aaba 
170 
CHAPTER 3. LEXICAL ANALYSIS 
We look backwards in the sequence of sets of states, until we find a set that 
includes one or more accepting states. If there are several accepting states in 
that set, pick the one associated with the earliest pattern pi in the list from 
the Lex program. Move the forward pointer back to the end of the lexeme, and 
perform the action Ai associated with pattern pi. 
Example 3.27 
: 
Suppose we have the patterns of Example 3.26 and the input 
begins aaba. Figure 3.53 shows the sets of states of the NFA of Fig. 3.52 that 
we enter, starting with 6-closure of the initial state 0, which is ( O , l ,  3,7}, and 
proceeding ftom there. After reading the fourth input symbol, we are in an 
empty set of states, since in Fig. 3.52, there are no transitions out of state 8 on 
input a. 
Thus, we need to back up, looking for a set of states that includes an ac- 
cepting state. Notice that, as indicated in Fig. 3.53, after reading a we are 
in a set that includes state 2 and therefore indicates that the pattern a has 
been matched. However, after reading aab, we are in state 8, which indicates 
that a*b+ 
has been matched; prefix aab is the longest prefix that gets us to an 
accepting state. We therefore select aab as the lexeme, and execute action A3, 
which should include a return to the parser indicating that the token whose 
pattern is ps = 
a*b+ 
has been found. 
3.8.3 DFA's for Lexical Analyzers 
Another architecture, resembling the output of Lex, is to convert the NFA 
for all the patterns into an equivalent DFA, using the subset construction of 
Algorithm 3.20. Within each DFA state, if there are one or more accepting 
NFA states, determine the first pattern whose accepting state is represented, 
and make that pattern the output of the DFA state. 
Example 3.28: Figure 3.54 shows a transition diagram based on the DFA 
that is constructed by the subset construction from the NFA in Fig. 3.52. The 
accepting states are labeled by the pattern that is identified by that state. For 
instance, the state {6,8} has two accepting states, corresponding to patterns 
abb and a*b+. 
Since the former is listed first, that is the pattern associated 
with state {6,8). 
We use the DFA in a lexical analyzer much as we did the NFA. We simulate 
the DFA until at some point there is no next state (or strictly speaking, the 
next state is 0, the dead state corresponding to the empty set of NFA states). 
At that point, we back up through the sequence of states we entered and, as 
soon as we meet an accepting DFA state, we perform the action associated with 
the pattern for that state. 
Example 3.29: Suppose the DFA of Fig. 3.54 is given input abba. The se- 
quence of states entered is 0137,247,58,68, and at the final a there is no tran- 
sition out of state 68. Thus, we consider the sequence from the end, and in this 
case, 68 itself is an accepting state that reports pattern pa = 
abb. 
3.8. DESIGN OF A LEXICAL-ANALYZER GENERATOR 
a 
start 
a 
b 
a*b+ 
abb 
a*b+ 
Figure 3.54: Transition graph for DFA handling the patterns a, abb, and a*b+ 
3.8.4 Implementing the Lookahead Operator 
Recall from Section 3.5.4 that the Lex lookahead operator / in a Lex pattern 
rl/r2 
is sometimes necessary, because the pattern rl for a particular token may 
need to describe some trailing context r 2  in order to correctly identify the actual 
lexeme. When converting the pattern rl /rz to an NFA, we treat the / as if it 
were e, so we do not actually look for a / on the input. However, if the NFA 
recognizes a prefix xy of the input buffer as matching this regular expression, 
the end of the lexeme is not where the NFA entered its accepting state. Rather 
the end occurs when the NFA enters a state s such that 
1. s has an €-transition on the (imaginary) /, 
2. There is a path from the start state of the NFA to state s that spells out 
x. 
3. There is a path from state s to the accepting state that spells out y. 
4. x is as long as possible for any xy satisfying conditions 1-3. 
If there is only one c-transition state on the imaginary / in the NFA, then 
the end of the lexeme occurs when this state is entered for the last time as the 
following example illustrates. If the NFA has more than one c-transition state 
on the imaginary /, then the general problem of finding the correct state s is 
much more difficult. 
Example 3.30 : 
An NFA for the pattern for the Fortran I F  with lookahead, 
from Example 3.13, is shown in Fig. 3.55. Notice that the c-transition from 
state 2 to state 3 represents the lookahead operator. State 6 indicates the pres- 
ence of the keyword IF. However, we find the lexeme IF 
by scanning backwards 
to the last occurrence of state 2, whenever state 6 is entered. 
172 
CHAPTER 3. LEXICAL ANALYSIS 
Dead States in DFA's 
Technically, the automaton in Fig. 3.54 is'not quite a DFA. The reason 
is that a DFA has a transition from every state on every input symbol in 
its input alphabet. Here, we have omitted transitions to the dead state 
0, and we have therefore omitted the transitions from the dead state to 
itself on every input. Previous NFA-to-DFA examples did not have a way 
to get from the start state to 0, but the NFA of Fig. 3.52 does. 
However, when we construct a DFA for use in a lexical analyzer, it 
is important that we treat the dead state differently, since we must know 
when there is no longer any possibility of recognizing a longer lexeme. 
Thus, we suggest always omitting transitions to the dead state and elimi- 
nating the dead state itself. In fact, the problem is harder than it appears, 
since an NFA-to-DFA construction may yield several states that cannot 
reach any accepting state, and we must know when any of these states 
have been reached. Section 3.9.6 discusses how to combine all these states 
into one dead state, so their identification becomes easy. It is also inter- 
esting to note that if we construct a DFA from a regular expression using 
Algorithms 3.20 and 3.23, then the DFA will not have any states besides 
0 that cannot lead to an accepting state. 
Figure 3.55: NFA recognizing the keyword IF 
3.8.5 Exercises for Section 3.8 
Exercise 3
.
8
.
1
 
: 
Suppose we have two tokens: (1) the keyword i f ,  and (2) id- 
entifiers, which are strings of letters other than i f .  Show: 
a) The NFA for these tokens, and 
b) The DFA for these tokens. 
Exercise 3
.
8
.
2
 
: 
Repeat Exercise 3.8.1 for tokens consisting of (1) 
the keyword 
while, (2) the keyword when, 
and (3) identifiers consisting of strings of letters 
and digits, beginning with a letter. 
! 
Exercise 3
.
8
.
3
:
 Suppose we were to revise the definition of a DFA to allow 
zero or one transition out of each state on each input symbol (rather than 
exactly one such transition, as in the standard DFA definition). Some regular 
3.9. OPTIMIZATION OF DFA-BASED PATTERN 
MATCHERS 
173 
expressions would then have smaller "DFA's" than they do under the standard 
definition of a DFA. Give an example of one such regular expression. 
!! Exercise 3.8.4 : 
Design an algorithm to recognize Lex-lookahead patterns of 
the form r l  
/r2, 
where r1 and r2 are regular expressions. Show how your algo- 
rithm works on the following inputs: 
3.9 Optimization of DFA-Based Pattern 
Matchers 
In this section we present three algorithms that have been used to implement 
and optimize pattern matchers constructed from regular expressions. 
1. The first algorithm is useful in a Lex compiler, because it constructs a 
DFA directly from a regular expression, without constructing an interme- 
diate NFA. The resulting DFA also may have fewer states than the DFA 
constructed via an NFA. 
2. The second algorithm minimizes the number of states of any DFA, by 
combining states that have the same future behavior. The algorithm 
itself is quite efficient, running in time O(n 
log n), where n is the number 
of states of the DFA. 
3. The third algorithm produces more compact representations of transition 
tables than the standard, two-dimensional table. 
3.9.1 Important States of an NFA 
To begin our discussion of how to go directly from a regular expression to a 
DFA, we must first dissect the NFA construction of Algorithm 3.23 and consider 
the roles played by various states. We call a state of an NFA important if it has a 
non-e out-transition. Notice that the subset construction (Algorithm 3.20) uses 
only the important states in a set T when it computes 6-closure(move(~, 
a)), 
the 
set of states reachable from T on input a. That is, the set of states move(s, a) 
is nonempty only if state s is important. During the subset construction, two 
sets of NFA states can be identified (treated as if they were the same set) if 
they: 
1. Have the same important states, and 
2. Either both have accepting states or neither does. 
CHAPTER 3. LEXICAL ANALYSIS 
When the NFA is constructed from a regular expression by Algorithm 3.23, 
we can say more about the important states. The only important states are 
those introduced as initial states in the basis part for a particular symbol posi- 
tion in the regular expression. That is, each important state corresponds to a 
particular operand in the regular expression. 
The constructed NFA has only one accepting state, but this state, having 
no out-transitions, is not an important state. By concatenating a unique right 
endmarker # to a regular expression r, we give the accepting state for r a 
transition on #, 
making it an important state of the NFA for (r)#. In other 
words, by using the augmented regular expression (r)#, we can forget about 
accepting states as the subset construction proceeds; when the construction is 
complete, any state with a transition on # must be an accepting state. 
The important states of the NFA correspond directly to the positions in 
the regular expression that hold symbols of the alphabet. It is useful, as we 
shall see, to present the regular expression by its syntax tree, where the leaves 
correspond to operands and the interior nodes correspond to operators. An 
interior node is called a cat-node, or-node, or star-node if it is labeled by the 
concatenation operator (dot), 
union operator 1, or star operator *, respectively. 
We can construct a syntax tree for a regular expression just as we did for 
arithmetic expressions in Section 2.5.1. 
Example 3.31 : 
Figure 3.56 shows the syntax tree for the regular expression 
of our running example. Cat-nodes are represented by circles. 
Figure 3.56: Syntax tree for (aJb)*abb# 
Leaves in a syntax tree are labeled by e or by an alphabet symbol. To each 
leaf not labeled e, we attach a unique integer. We refer to this integer as the 
OPTIMIZATION OF DFA-BASED PATTERN MATCHERS 
175 
position of the leaf and also as a position of its symbol. Note that a symbol 
can have several positions; for instance, a has positions 1 and 3 in Fig. 3.56. 
The positions in the syntax tree correspond to the important states of the 
constructed NFA. 
Example 3.32 : 
Figure 3.57 shows the NFA for the same regular expression as 
Fig. 3.56, with the important states numbered and other states represented by 
letters. The numbered states in the NFA and the positions in the syntax tree 
correspond in a way we shall soon see. 
Figure 3.57: NFA constructed by Algorithm 3.23 for (a(b)*abb# 
3.9.2 Functions Computed From the Syntax Tree 
To construct a DFA directly from a regular expression, we construct its syntax 
tree and then compute four functions: nullable, firstpos, lastpos, and followpas, 
defined as follows. Each definition refers to the syntax tree for a particular 
augmented regular expression (r) 
# 
. 
1. nullable(n) 
is true for a syntax-tree node n if and only if the subexpression 
represented by n has E in its language. That is, the subexpression can be 
"made null" or the empty string, even though there may be other strings 
it can represent as well. 
2. firstpos(n) is the set of positions in the subtree rooted at n that corre- 
spond to the first symbol of at least one string in the language of the 
subexpression rooted at n. 
3. lastpos(n) is the set of positions in the subtree rooted at n that corre- 
spond to the last symbol of at least one string in the language of the 
subexpression rooted at n. 
176 
CHAPTER 3. LEXICAL ANALYSIS 
4. followpos(p), 
for a position p, is the set of positions q in the entire syntax 
tree such that there is some string x = alaz - . 
a, in L ((r)#) such that 
for some i, there is a way to explain the membership of x in ~ ( ( r ) # )  
by 
matching ai to position p of the syntax tree and ai+l to position q. 
Example 3.33 : 
Consider the cat-node n in Fig. 3.56 that corresponds to the 
expression (alb)*a. We claim nullable(n) is false, since this node generates all 
strings of a's and b's ending in an a; 
it does not generate E .  On the other hand, 
the star-node below it is nullable; it generates c along with all other strings of 
a's and b's. 
firstpos(n) = 
{1,2,3). In a typical generated string like aa, the first position 
of the string corresponds to position 1 
of the tree, and in a string like ba, the 
first position of the string comes from position 2 of the tree. However, when 
the string generated by the expression of node n is just a, then this a comes 
from position 3. 
lastpos(n) = (3). That is, no matter what string is generated from the 
expression of node n, the last position is the a from position 3 of the tree. 
followpos is trickier to compute, but we shall see the rules for doing so 
shortly. Here is an example of the reasoning: followpos(1) = {1,2,3). Consider 
a string . 
. ac . . 
. , 
where the c is either a or b, and the a comes from position 1. 
That is, this a is one of those generated by the a in expression (a1 
b) 
* . This 
a could be followed by another a or b coming from the same subexpression, in 
which case c comes from position 1 or 2. It is also possible that this a is the 
last in the string generated by (alb)*, 
in which case the symbol c must be the 
a that comes from position 3. Thus, 1, 2, and 3 are exactly the positions that 
can follow position 1. 
3.9.3 Computing nullable, firstpos, and lastpos 
We can compute nullable, firstpos, and lastpos by a straightforward recursion 
on the height of the tree. The basis and inductive rules for nullable and firstpos 
are summarized in Fig. 3.58. The rules for lastpos are essentially the same as 
for firstpos, but the roles of children cl and cz must be swapped in the rule for 
a cat-node. 
Example 3.34 : Of all the nodes in Fig. 3.56 only the star-node is nullable. 
We note from the table of Fig. 3.58 that none of the leaves are nullable, because 
they each correspond to non-E operands. The or-node is not nullable, because 
neither of its children is. The star-node is nullable, because every star-node is 
nullable. Finally, each of the cat-nodes, having at least one nonnullable child, 
is not nullable. 
The computation of firstpos and lastpos for each of the nodes is shown in 
Fig. 3.59, with firstpos(n) to the left of node n, and lastpos(n) to its right. Each 
of the leaves has only itself for firstpos and lastpos, as required by the rule for 
non-c leaves in Fig. 3.58. For the or-node, we take the union of firstpos at the 
OPTIMIZATION OF DFA-BASED PATTERN MATCHERS 
177 
Figure 3.58: Rules for computing nullable and firstpos 
children and do the same for lastpos. The rule for the star-node says that we 
take the value of firstpos or lastpos at the one child of that node. 
Yaw, consider the lowest cat-node, which we shall call n. To compute 
firstpos(n), we first consider whether the left operand is nullable, which it is 
in this case. Therefore, firstpos for n is the union of firstpos for each of its 
children, that is {I, 
2) U (3) = {1,2,3). The rule for lastpos does not ap- 
pear explicitly in Fig. 3.58, but as we mentioned, the rules are the same as 
for firstpos, with the children interchanged. That is, to compute lastpos(n) we 
must ask whether its right child (the leaf with position 3) is nullable, which it 
is not. Therefore, lastpos(n) is the same as lastpos of the right child, or {3). 
1
7
 
firstpos(n) 
8 
{i> 
firstpos(cl) U firstpos(c2) 
if ( nullable(cl) ) 
firstpos(cl) U firstpos(c2) 
else firstpos(cl) 
firstpos(c1) 
NODE n 
A leaf labeled 6 
A leaf with position i 
An or-node n = cl /c2 
A cat-node n = 
~
1
~
2
 
A star-node n = 
cl* 
3.9.4 Computing followpos 
nullable(n) 
true 
false 
nullable(cl) or 
nu1 
1 
able 
(c2) 
nullable(cl) and 
nu1 
1 
able 
(c2) 
true 
Finally, we need to see how to compute followpos. There are only two ways 
that a position of a regular expression can be made to follow another. 
1. If n is a cat-node with left child cl and right child c2, then for every 
position i in lastpos(cl), all positions in firstpos(c2) are in followpos(i). 
2. If n is a star-node, and i is a position in lastpos(n), then all positions in 
firstpos(n) are in followpos(i) 
. 
Example 3.35 : 
Let us continue with our running example; recall that firstpos 
and lastpos were computed in Fig. 3.59. Rule 1 
for followpos requires that we 
look at each cat-node, and put each position in firstpos of its right child in 
followpos for each position in lastpos of its left child. For the lowest cat-node in 
Fig. 3.59, that rule says position 3 is in followpos(l) and followpos(2). The next 
cat-node above says that 4 is in followpos(3), and the remaining two cat-nodes 
give us 5 in followpos(4) and 6 in followpos(5). 
CHAPTER 3. LEXICAL ANALYSIS 
Figure 3.59: firstpos and lastpos for nodes in the syntax tree for (alb)*abb# 
We must also apply rule 2 to the star-node. That rule tells us positions 1 
and 
2 are in both followpos(1) and followpos(2), since both firstpas and lastpos for 
this node are {1,2). The complete sets followpos are summarized in Fig. 3.60. 
Figure 3.60: The function followpos 
We can represent the function followpos by creating a directed graph with 
a node for each position and an arc from position i to position j if and only if 
j is in followpos(i). Figure 3.61 shows this graph for the function of Fig. 3.60. 
It should come as no surprise that the graph for followpos is almost an NFA 
without €-transitions for the underlying regular expression, and would become 
one if we: 
1. Make all positions in firstpos of the root be initial states, 
2. Label each arc from i to j by the symbol at position i, and 
OPTIMIZATION OF DFA-BASED PATTERN MATCHERS 
Figure 3.61: Directed graph for the function followpos 
3. Make the position associated with endmarker # be the only accepting 
state. 
3.9.5 
Converting a Regular Expression Directly to a DFA 
Algorithm 3.36 : 
Construction of a DFA from a regular expression r. 
INPUT: A regular expression r. 
OUTPUT: A DFA D that recognizes L(r). 
METHOD: 
1. Construct a syntax tree T from the augmented regular expression (r)#. 
2. Compute nullable, firstpos, lastpos, and followpos for T ,  
using the methods 
of Sections 3.9.3 and 3.9.4. 
3. Construct Dstates, the set of states of DFA D, and Dtran, the transition 
function for D, by the procedure of Fig. 3.62. The states of D are sets of 
positions in T. Initially, each state is "unmarked," and a state becomes 
"marked" just before we consider its out-transitions. The start state of 
D is firstpos(no), where node no is the root of T. The accepting states 
are those containing the position for the endmarker symbol #. 
Example 3.37: We can now put together the steps of our running example 
to construct a DFA for the regular expression r = (aJb)*abb. 
The syntax tree 
for (r)# appeared in Fig. 3.56. We observed that for this tree, nullable is true 
only for the star-node, and we exhibited firstpos and lastpos in Fig. 3.59. The 
values of followpos appear in Fig. 3.60. 
The value of firstpos for the root of the tree is (1,2,3), so this set is the 
start state of D. Call this set of states A. We must compute Dtran[A, 
a] 
and Dtran[A, 
b]. Among the positions of A, 1 and 3 correspond to a, while 2 
corresponds to b. Thus, Dtran[A, 
a] = 
followpos(l) U followpos(3) = {I, 
2,3,4), 
CHAPTER 3. LEXICAL ANALYSIS 
initialize Dstates to contain only the unmarked state firstpos(no), 
where no is the root of syntax tree T for (r)#; 
while ( there is an unmarked state S in Dstates ) { 
mark S; 
for ( each input symbol a ) { 
let U be the union of followpos(p) for all p 
in S 
that correspond to a; 
if ( U is not in Dstates ) 
add U as an unmarked state to Dstates; 
Figure 3.62: Construction of a DFA directly from a regular expression 
and Dtran[A, 
b] = 
followpos(2) = {1,2,3). The latter is state A, and so does 
not have to be added to Dstates, but the former, B = 
{1,2,3,4}, is new, so we 
add it to Dstates and proceed to compute its transitions. The complete DFA is 
shown in Fig. 3.63. 
start 
Figure 3.63: DFA constructed from Fig. 3.57 
3.9.6 Minimizing the Number of States of a DFA 
There can be many DFA's that recognize the same language. For instance, note 
that the DFA's of Figs. 3.36 and 3.63 both recognize language ~((alb)*abb). 
Not only do these automata have states with different names, but they don't 
even have the same number of states. If we implement a lexical analyzer as 
a DFA, we would generally prefer a DFA with as few states as possible, since 
each state requires entries in the table that describes the lexical analyzer. 
The matter of the names of states is minor. We shall say that two automata 
are the same up to state names if one can be transformed into the other by doing 
nothing more than changing the names of states. Figures 3.36 and 3.63 are not 
the same up to state names. However, there is a close relationship between the 
OPTIMIZATION OF DFA-BASED PATTERN MATCHERS 
181 
states of each. States A and C of Fig. 3.36 are actually equivalent, in the sense 
that neither is an accepting state, and on any input they transfer to the same 
state - 
to B on input a and to C on input b. Moreover, both states A and C 
behave like state 123 of Fig. 3.63. Likewise, state B of Fig. 3.36 behaves like 
state 1234 of Fig. 3.63, state D behaves like state 1235, and state E behaves 
like state 1236. 
It turns out that there is always a unique (up to state names) minimum 
state DFA for any regular language. Moreover, this minimum-state DFA can be 
constructed from any DFA for the same language by grouping sets of equivalent 
states. In the case of L ((a1 
b)*abb) 
, 
Fig. 3.63 is the minimum-state DFA, and it 
can be constructed by partitioning the states of Fig. 3.36 as {A, C){B){D){E). 
In order to understand the algorithm for creating the partition of states 
that converts any DFA into its minimum-state equivalent DFA, we need to 
see how input strings distinguish states from one another. We say that string 
x distinguishes state s from state t if exactly one of the states reached from 
s and t by following the path with label x is an accepting state. State s is 
distinguishable from state t if there is some string that distinguishes them. 
Example 3.38 : 
The empty string distinguishes any accepting state from any 
nonaccepting state. In Fig. 3.36, the string bb distinguishes state A from state 
B, since bb takes A to a nonaccepting state C, but takes B to accepting state 
E. 
The state-minimization algorithm works by partitioning the states of a DFA 
into groups of states that cannot be distinguished. Each group of states is then 
merged into a single state of the minimum-state DFA. The algorithm works 
by maintaining a partition, whose groups are sets of states that have not yet 
been distinguished, while any two states from different groups are known to be 
distinguishable. When the partition cannot be refined further by breaking any 
group into smaller groups, we have the minimum-state DFA. 
Initially, the partition consists of two groups: the accepting states and the 
nonaccepting states. The fundamental step is to take some group of the current 
partition, say A = 
{sl 
, 
s2, . 
. . , 
sk), and some input symbol a, and see whether 
a can be used to distinguish between any states in group A. We examine the 
transitions from each of sl 
, 
s2, . 
. . , 
sk on input a, and if the states reached fall 
into two or more groups of the current partition, we split A into a collection of 
groups, so that si and sj are in the same group if and only if they go to the 
same group on input a. We repeat this process of splitting groups, until for 
no group, and for no input symbol, can the group be split further. The idea is 
formalized in the next algorithm. 
Algorithm 3.39 : 
Minimizing the number of states of a DFA. 
INPUT: A DFA D with set of states S, 
input alphabet C, state state so, and 
set of accepting states F. 
OUTPUT: A DFA D' accepting the same language as D and having as few 
states as possible. 
CHAPTER 3. LEXICAL ANALYSIS 
Why the State-Minimization Algorithm Works 
We need to prove two things: that states remaining in the same group in 
IIfinal are indistinguishable by any string, and that states winding up in 
different groups are distinguishable. The first is an induction on i that 
if after the ith iteration of step (2) of Algorithm 3.39, s and t are in the 
same group, then there is no string of length i or less that distinguishes 
them. We shall leave the details of the induction to you. 
The second is an induction on i that if states s and t are placed in 
different groups at the ith iteration of step (2), then there is a string that 
distinguishes them. The basis, when s and t are placed in different groups 
of the initial partition, is easy: one must be accepting and the other not, 
so c distinguishes them. For the induction, there must be an input a and 
states p and q such that s and t go to states p and q, respectively, on input 
a. Moreover, p and q must already have been placed in different groups. 
Then by the inductive hypothesis, there is some string x that distinguishes 
p from q. Therefore, ax distinguishes s from t. 
METHOD: 
1. Start with an initial partition II with two groups, F and S - 
F, the 
accepting and nonaccepting states of D. 
2. Apply the procedure of Fig. 3.64 to construct a new partition anew. 
initially, let IInew 
= 
II; 
for ( each group G of II ) { 
partition G into subgroups such that two states s and t 
are in the same subgroup if and only if for all 
input symbols a, states s and t have transitions on a 
to states in the same group of 11; 
/* 
at worst, a state will be in a subgroup by itself */ 
replace G in IInew 
by the set of all subgroups formed; 
1 
Figure 3.64: Construction of IInew 
3. If IIne, = II, let IIfinal 
= 
l
l
 and continue with step (4). Otherwise, repeat 
step (2) with IInew 
in place of II. 
4. Choose one state in each group of IIfinal 
as the representative for that 
group. The representatives will be the states of the minimum-state DFA 
D'. The other components of D
'
 
are constructed as follows: 
OPTIMIZATION OF DFA-BASED PATTERN MATCHERS 
183 
Eliminating the Dead State 
The minimization algorithm sometimes produces a DFA with one dead 
state - 
one that is not accepting and transfers to itself on each input 
symbol. This state is technically needed, because a DFA must have a 
transition from every state on every symbol. However, as discussed in 
Section 3.8.3, we often want to know when there is no longer any possibility 
of acceptance, so we can establish that the proper lexeme has already been 
seen. Thus, we may wish to eliminate the dead state and use an automaton 
that is missing some transitions. This automaton has one fewer state than 
the minimum-state DFA, but is strictly speaking not a DFA, because of 
the missing transitions to the dead state. 
(a) The state state of Dl is the representative of the group containing 
the start state of D. 
(b) The accepting states of D' are the representatives of those groups 
that contain an accepting state of D. Note that each group contains 
either only accepting states, or only nonaccepting states, because we 
started by separating those two classes of states, and the procedure 
of Fig. 3.64 always forms new groups that are subgroups of previously 
constructed groups. 
(c) Let s be the representative of some group G of IIfinal, 
and let the 
transition of D from s on input a be to state t. Let r be the rep- 
resentative of t's group H. Then in Dl, there is a transition from s 
to r on input a. Note that in D, every state in group G must go to 
some state of group H on input a, or else, group G would have been 
split according to Fig. 3.64. 
Example 3.40 : 
Let us reconsider the DFA of Fig. 3.36. The initial partition 
consists of the two groups {A, B, 
C, 
D}{E}, which are respectively the nonac- 
cepting states and the accepting states. To construct II,,,, 
the procedure of 
Fig. 3.64 considers both groups and inputs a and b. The group {E} 
cannot be 
split, because it has only one state, so (E} will remain intact in I
T
,
,
,
.
 
The other group {A, 
B, 
C, 
D} can be split, so we must consider the effect of 
each input symbol. On input a, each of these states goes to state B, so there 
is no way to distinguish these states using strings that begin with a. On input 
b, states A, B, and C go to members of group {A, B, 
C, 
D}, while state D goes 
to E, a member of another group. Thus, in IInew, 
group {A, B, 
C, 
D} is split 
into {A, B, 
C}{D}, and IInew 
for this round is {A, B, 
C){D){E}. 
184 
CHAPTER 3. LEXICAL ANALYSIS 
In the next round, we can split {A, B, 
C} into {A, C}{B}, since A and 
C each go to a member of {A, B, 
C) on input b, while B goes to a member of 
another group, {D}. Thus, after the second round, I
t
,
,
,
 
= 
{A, C} 
{B} 
{D} 
{E). 
For the third round, we cannot split the one remaining group with more than 
one state, since A and C each go to the same state (and therefore to the same 
group) on each input. We conclude that ITfinal = {A, C}{B){D){E). 
Now, we shall construct the minimum-state DFA. It has four states, corre- 
sponding to the four groups of ITfinal, and let us pick A, B, D, and E as the 
representatives of these groups. The initial state is A, and the only accepting 
state is 2
3
.
 
Figure 3.65 shows the transition function for the DFA. For instance, 
the transition from state E on input b is to A, since in the original DFA, E goes 
to C on input b, and A is the representative of C7s 
group. For the same reason, 
the transition on b from state A is to A itself, while all other transitions are as 
in Fig. 3.36. 
Figure 3.65: Transition table of minimum-state DFA 
3.9.7 State Minimization in Lexical Analyzers 
To apply the state minimization procedure to the DFA7s generated in Sec- 
tion 3.8.3, we must begin Algorithm 3.39 with the partition that groups to- 
gether all states that recognize a particular token, and also places in one group 
all those states that do not indicate any token. An example should make the 
extension clear. 
Example 3.41 : 
For the DFA of Fig. 3.54, the initial partition is 
That is, states 0137 and 7 belong together because neither announces any token. 
States 8 and 58 belong together because they both announce token a*b+. 
Note 
that we have added a dead state 8, which we suppose has transitions to itself 
on inputs a and b. The dead state is also the target of missing transitions on a 
from states 8, 58, and 68. 
We must split 0137 from 7, because they go to different groups on input a. 
We also split 8 from 58, because they go to different groups on b. Thus, all 
states are in groups by themselves, and Fig. 3.54 is the minimum-state DFA 
OPTIMIZATION OF DFA-BASED PATTERN MATCHERS 
185 
recognizing its three tokens. Recall that a DFA serving as a lexical analyzer 
will normally drop the dead state, while we treat missing transitions as a signal 
to end token recognition. 
3.9.8 Trading Time for Space in DFA Simulation 
The simplest and fastest way to represent the transition function of a DFA is 
a two-dimensional table indexed by states and characters. Given a state and 
next input character, we access the array to find the next state and any special 
action we must take, e.g., returning a token to the parser. Since a typical lexical 
analyzer has several hundred states in its DFA and involves the ASCII alphabet 
of 128 input characters, the array consumes less than a megabyte. 
However, compilers are also appearing in very small devices, where even 
a megabyte of storage may be too much. For such situations, there are many 
methods that can be used to compact the transition table. For instance, we can 
represent each state by a list of transitions - 
that is, character-state pairs - 
ended by a default state that is to be chosen for any input character not on the 
list. If we choose as the default the most frequently occurring next state, we 
can often reduce the amount of storage needed by a large factor. 
There is a more subtle data structure that allows us to combine the speed 
of array access with the compression of lists with defaults. We may think of 
this structure as four arrays, as suggested in Fig. 3.66.5 The base array is used 
to determine the base location of the entries for state s, which are located in 
the next and check arrays. The default array is used to determine an alternative 
base location if the check array tells us the one given by base[s] 
is invalid. 
default 
base 
next 
check 
Figure 3.66: Data structure for representing transition tables 
To compute nextState(s, 
a), the transition for state s on input a, we examine 
the next and check entries in location 1 = base[s] 
+a, where character a is treated 
as an integer, presumably in the range 0 to 127. If check[l] 
= s, then this entry 
- 
5 ~ n  
practice, there would be another array indexed by states to give the action associated 
with that state, if any. 
186 
CHAPTER 3. LEXICAL ANALYSIS 
is valid, and the next state for state s on input a is next[l]. If check[l] # s, then 
we determine another state t = default[s] and repeat the process as if t were 
the current state. More formally, the function nextstate is defined as follows: 
int nextState(s, a) { 
if ( check[base[s] + 
a] = 
s ) return next[base[s] + 
a]; 
else return nextState(default[s], a); 
1 
The intended use of the structure of Fig. 3.66 is to make the next-check 
arrays short by taking advantage of the similarities among states. For instance, 
state t, the default for state s, might be the state that says "we are working on 
an identifier," like state 10 in Fig. 3.14. Perhaps state s is entered after seeing 
the letters th, which are a prefix of keyword then as well as potentially being 
the prefix of some lexeme for an identifier. On input character e, we must go 
from state s to a special state that remembers we have seen the, but otherwise, 
state s behaves as t does. Thus, we set check[base[s] + 
el to s (to confirm that 
this entry is valid for s) and we set next[base[s] 
+ 
el to the state that remembers 
the. Also, default[s] is set to t. 
While we may not be able to choose base values so that no next-check entries 
remain unused, experience has shown that the simple strategy of assigning base 
values to states in turn, and assigning each base[s] value the lowest integer so 
that the special entries for state s are not previously occupied utilizes little 
more space than the minimum possible. 
3.9.9 Exercises for Section 3.9 
Exercise 3.9.1 : 
Extend the table of Fig. 3.58 to include the operators (a) ? 
and (b) +. 
Exercise 3.9.2 : 
Use Algorithm 3.36 to convert the regular expressions of Ex- 
ercise 3.7.3 directly to deterministic finite automata. 
! 
Exercise 3.9.3 : 
We can prove that two regular expressions are equivalent by 
showing that their minimum-state DFA's are the same up to renaming of states. 
Show in this way that the following regular expressions: (a[ 
b)*, (a* 
/b*)*, 
and 
((cla)b*)* are all equivalent. Note: You may have constructed the DFA7s 
for 
these expressions in response to Exercise 3.7.3. 
! Exercise 3.9.4 : 
Construct the minimum-state DFA7s 
for the following regular 
expressions: 
3.20. SUMMARY OF CHAPTER 3 
Do you see a pattern? 
!! Exercise 3.9.5 : To make formal the informal claim of Example 3.25, show 
that any deterministic finite automaton for the regular expression 
where (alb) 
appears n - 
1 
times at the end, must have at least 2" states. Hint: 
Observe the pattern in Exercise 3.9.4. What condition regarding the history of 
inputs does each state represent? 
3.10 Summary of Chapter 3 
+ Tokens. The lexical analyzer scans the source program and produces as 
output a sequence of tokens, which are normally passed, one at a time to 
the parser. Some tokens may consist only of a token name while others 
may also have an associated lexical value that gives information about 
the particular instance of the token that has been found on the input. 
+ Lexernes. Each time the lexical analyzer returns a token to the parser, 
it has an associated lexeme - 
the sequence of input characters that the 
token represents. 
+ Buffering. Because it is often necessary to scan ahead on the input in 
order to see where the next lexeme ends, it is usually necessary for the 
lexical analyzer to buffer its input. Using a pair of buffers cyclicly and 
ending each buffer's contents with a sentinel that warns of its end are two 
techniques that accelerate the process of scanning the input. 
+ Patterns. Each token has a pattern that describes which sequences of 
characters can form the lexemes corresponding to that token. The set 
of words, or strings of characters, that match a given pattern is called a 
language. 
+ Regular Expressions. These expressions are commonly used to describe 
patterns. Regular expressions are built from single characters, using 
union, concatenation, and the Kleene closure, or any-number-of, oper- 
ator. 
+ Regular Definitions. Complex collections of languages, such as the pat- 
terns that describe the tokens of a programming language, are often de- 
fined by a regular definition, which is a sequence of statements that each 
define one variable to stand for some regular expression. The regular ex- 
pression for one variable can use previously defined variables in its regular 
expression. 
188 
CHAPTER 3. LEXICAL ANALYSIS 
+ Extended Regular-Expression Notation. A number of additional opera- 
tors may appear as shorthands in regular expressions, to make it easier 
to express patterns. Examples include the + operator (one-or-more-of), 
? (zero-or-one-of), and character classes (the union of the strings each 
consisting of one of the characters). 
+ Transition Diagrams. The behavior of a lexical analyzer can often be 
described by a transition diagram. These diagrams have states, each 
of which represents something about the history of the characters seen 
during the current search for a lexeme that matches one of the possible 
patterns. There are arrows, or transitions, from one state to another, 
each of which indicates the possible next input characters that cause the 
lexical analyzer to make that change of state. 
+ Finite Automata. These are a formalization of transition diagrams that 
include a designation of a start state and one or more accepting states, 
as well as the set of states, input characters, and transitions among 
states. Accepting states indicate that the lexeme for some token has been 
found. Unlike transition diagrams, finite automata can make transitions 
on empty input as well as on input characters. 
+ Deterministic Finite Automata. A DFA is a special kind of finite au- 
tomaton that has exactly one transition out of each state for each input 
symbol. Also, transitions on empty input are disallowed. The DFA is 
easily simulated and makes a good implementation of a lexical analyzer, 
similar to a transition diagram. 
+ Nondeterministic Finite Automata. Automata that are not DFA7s are 
called nondeterministic. NFA's often are easier to design than are DFA's. 
Another possible architecture for a lexical analyzer is to tabulate all the 
states that NFA7s 
for each of the possible patterns can be in, as we scan 
the input characters. 
+ Conversion Among Pattern Representations. It is possible to convert any 
regular expression into an NFA of about the same size, recognizing the 
same language as the regular expression defines. Further, any NFA can 
be converted to a DFA for the same pattern, although in the worst case 
(never encountered in common programming languages) the size of the 
automaton can grow exponentially. It is also possible to convert any non- 
deterministic or deterministic finite automaton into a regular expression 
that defines the same language recognized by the finite automaton. 
+ Lex. There is a family of software systems, including Lex and Flex, 
that are lexical-analyzer generators. The user specifies the patterns for 
tokens using an extended regular-expression notation. Lex converts these 
expressions into a lexical analyzer that is essentially a deterministic finite 
automaton that recognizes any of the patterns. 
3.11. REFERENCES FOR CHAPTER 3 
189 
+ Mnimixation of Finite Automata. For every DFA there is a minimum- 
st 
ate D M  accepting the same language. Moreover, the minimum-state 
DFA for a given language is unique except for the names given to the 
various states. 
3.11 References for Chapter 3 
Regular expressions were first developed by Kleene in the 1950's [9]. Kleene was 
interested in describing the events that could be represented by McCullough and 
Pitts' [I 
2
1
 finite-automaton model of neural activity. Since that time regular 
expressions and finite automata have become widely used in computer science. 
Regular expressions in various forms were used from the outset in many 
popular Unix utilities such as awk, ed, egrep, grep, lex, sed, sh, and vi. The 
IEEE 1003 and ISO/IEC 9945 standards documents for the Portable Operating 
System Interface (POSIX) define the POSIX extended regular expressions which 
are similar to the original Unix regular expressions with a few exceptions such 
as mnemonic representations for character classes. Many scripting languages 
such as Perl, Python, and Tcl have adopted regular expressions but often with 
incompatible extensions. 
The familiar finite-automaton model and the minimization of finite au- 
tomata, as in Algorithm 3.39, come from Huffman [6] and Moore [14]. Non- 
deterministic finite automata were first proposed by Rabin and Scott [15]; 
the 
subset construction of Algorithm 3.20, showing the equivalence of deterministic 
and nondeterministic finite automata, is from there. 
McNaughton and Yamada [13] first gave an algorithm to convert regular 
expressions directly to deterministic finite automat 
a. Algorithm 3.36 described 
in Section 3.9 was first used by Aho in creating the Unix regular-expression 
matching tool egrep. This algorithm was also used in the regular-expression 
pattern matching routines in awk [3]. The approach of using nondeterministic 
automata as an intermediary is due Thompson [17]. The latter paper also con- 
tains the algorithm for the direct simulation of nondeterministic finite automata 
(Algorithm 3.22), which was used by Thompson in the text editor QED. 
Lesk developed the first version of Lex and then Lesk and Schmidt created 
a second version using Algorithm 3.36 [lo]. Many variants of Lex have been 
subsequently implemented. The GNU version, Flex, can be downloaded, along 
with documentation at [4]. Popular Java versions of Lex include JFlex (71 and 
JLex [8]. 
The KMP algorithm, discussed in the exercises to Section 3.4 just prior to 
Exercise 3.4.3, is from [ll]. Its generalization to many keywords appears in [2] 
and was used by Aho in the first implementation of the Unix utility f grep. 
The theory of finite automata and regular expressions is covered in [5]. A 
survey of string-matching techniques is in [I]. 
1. Aho, A. V., "Algorithms for finding patterns in strings," in Handbook of 
Theoretical Computer Science (J. van Leeuwen, ed.), Vol. A, Ch. 5, MIT 
CHAPTER 3. LEXICAL ANALYSIS 
Press, Cambridge, 1990. 
2. Aho, A. V. and M. J. Corasick, "Efficient string matching: an aid to 
bibliographic search," Comm. AC1M18:6 (1975), pp. 333-340. 
3. Aho, A. V., B. W. Kernighan, and P. J. Weinberger, The AWK Program- 
ming Language, Addison-Wesley, Boston, MA, 1988. 
4. Flex home page h t t p  
: 
//www .gnu. org/sof tware/f lex/, Free Software 
Foundation. 
5. Hopcroft, J. E., R. Motwani, and J. D. Ullman, Introduction to Automata 
Theory, Languages, and Computation, Addison-Wesley, 
Boston MA, 2006. 
6. Huffman, D. A., "The synthesis of sequential machines," J. Franklin Inst. 
257 (1954), pp. 3-4, 161, 190, 275-303. 
7. JFlex home page h t t p  
: 
// 
j f lex. 
de/ . 
8. http: 
//www. cs 
.princeton. edu/"appel/modern/java/J~ex 
. 
9. Kleene, S. C., "Representation of events in nerve nets," in [16], 
pp. 3-40. 
10. Lesk, M. E., "Lex - 
a lexical analyzer generator," Computing Science 
Tech. Report 39, Bell Laboratories, Murray Hill, NJ, 1975. A similar 
document with the same title but with E. Schmidt as a coauthor, appears 
in Vol. 2 of the Unix Programmer's Manual, Bell laboratories, Murray Hill 
NJ,1975; see http://dinosaur.compilertools.net/lex/index.html. 
11. Knuth, D. E., J. H. Morris, and V. R. Pratt, "Fast pattern matching in 
strings," SIAM J. Computing 6:2 (1977), pp. 323-350. 
12. McCullough, W. S. and W. Pitts, "A logical calculus of the ideas imma- 
nent in nervous activity," Bull. Math. Biophysics 5 (1943), pp. 115-133. 
13. McNaughton, R. and H. Yamada, "Regular expressions and state graphs 
for automata," IRE Trans. on Electronic Computers EC-9:l (1960), pp. 
38-47. 
14. Moore, E. F., "Gedanken experiments on sequential machines," in [16], 
pp. 129-153. 
15. Rabin, M. 0. and D. Scott, "Finite automata and their decision prob- 
lems," IBM J. Res. and Devel. 3:2 (1959), pp. 114-125. 
16. Shannon, C. and J. McCarthy (eds.), Automata Studies, Princeton Univ. 
Press, 1956. 
17. Thompson, K., "Regular expression search algorithm," Comm. A 
CM 11:6 
(1968), pp. 419-422. 
Chapter 4 
Syntax Analysis 
This chapter is devoted to parsing methods that are typically used in compilers. 
We first present the basic concepts, then techniques suitable for hand implemen- 
tation, and finally algorithms that have been used in automated tools. Since 
programs may contain syntactic errors, we discuss extensions of the parsing 
methods for recovery from common errors. 
By design, every programming language has precise rules that prescribe the 
syntactic structure of well-formed programs. In C, for example, a program is 
made up of functions, a function out of declarations and statements, a statement 
out of expressions, and so on. The syntax of programming language constructs 
can be specified by context-free grammars or BNF (Backus-Naur Form) nota- 
tion, introduced in Section 2.2. Grammars offer significant benefits for both 
language designers and compiler writers. 
A grammar gives a precise, yet easy-to-understand, syntactic specification 
of a programming language. 
From certain classes of grammars, we can construct automatically an effi- 
cient parser that determines the syntactic structure of a source program. 
As a side benefit, the parser-construction process can reveal syntactic 
ambiguities and trouble spots that might have slipped through the initial 
design phase of a language. 
The structure imparted to a language by a properly designed grammar 
is useful for translating source programs into correct object code and for 
detecting errors. 
A grammar allows a language to be evolved or developed iteratively, by 
adding new constructs to perform new tasks. These new constructs can 
be integrated more easily into an implementation that follows the gram- 
matical structure of the language. 
CHAPTER 4. SYNTAX ANALYSIS 
4.1 Introduction 
In this section, we examine the way the parser fits into a typical compiler. We 
then look at typical grammars for arithmetic expressions. Grammars for ex- 
pressions suffice for illustrating the essence of parsing, since parsing techniques 
for expressions carry over to most programming constructs. This section ends 
with a discussion of error handling, since the parser must respond gracefully to 
finding that its input cannot be generated by its grammar. 
4.1.1 The Role of the Parser 
In our compiler model, the parser obtains a string of tokens from the lexical 
analyzer, as shown in Fig. 4.1, and verifies that the string of token names 
can be generated by the grammar for the source language. We expect the 
parser to report any syntax errors in an intelligible fashion and to recover from 
commonly occurring errors to continue processing the remainder of the program. 
Conceptually, for well-formed programs, the parser constructs a parse tree and 
passes it to the rest of the compiler for further processing. In fact, the parse 
tree need not be constructed explicitly, since checking and translation actions 
can be interspersed with parsing, as we shall see. Thus, the parser and the rest 
of the front end could well be implemented by a single module. 
Symbol 
Table 
Figure 4.1: Position of parser in compiler model 
intermediate 
- 
representatio6 
SOurce 
progra$ 
There are three general types of parsers for grammars: universal, top-down, 
and bottom-up. Universal parsing methods such as the Cocke-Younger-Kasami 
algorithm and Earley's algorithm can parse any grammar (see the bibliographic 
notes). These general methods are, however, too inefficient to use in production 
compilers. 
The methods commonly used in compilers can be classified as being either 
top-down or bottom-up. As implied by their names, top-down methods build 
parse trees from the top (root) to the bottom (leaves), while bottom-up methods 
start from the leaves and work their way up to the root. In either case, the 
input to the parser is scanned from left to right, one symbol at a time. 
token 
Lexical 
/ 
parse 
~~~t of 
-1 
Analyzer 
I Front End 
4.1. INTRODUCTION 
193 
The most efficient top-down and bottom-up methods work only for sub- 
classes of grammars, but several of these classes, particularly, LL and LR gram- 
mars, are expressive enough to describe most of the syntactic constructs in 
modern programming languages. Parsers implemented by hand often use LL 
grammars; for example, the predictive-parsing approach of Section 2.4.2 works 
for LL grammars. Parsers for the larger class of LR grammars are usually 
constructed using automated tools. 
In this chapter, we assume that the output of the parser is some represent- 
ation of the parse tree for the stream of tokens that comes from the lexical 
analyzer. In practice, there are a number of tasks that might be conducted 
during parsing, such as collecting information about various tokens into the 
symbol table, performing type checking and other kinds of semantic analysis, 
and generating intermediate code. We have lumped all of these activities into 
the "rest of the front end" box in Fig. 4.1. 
These activities will be covered in 
detail in subsequent chapters. 
4.1.2 
Representative Grammars 
Some of the grammars that will be examined in this chapter are presented here 
for ease of reference. Constructs that begin with keywords like while or int, are 
relatively easy to parse, because the keyword guides the choice of the grammar 
production that must be applied to match the input. We therefore concentrate 
on expressions, which present more of challenge, because of the associativity 
and precedence of operators. 
Associativity and precedence are captured in the following grammar, which 
is similar to ones used in Chapter 2 for describing expressions, terms, and 
factors. E represents expressions consisting of terms separated by + signs, T 
represents terms consisting of factors separated by * signs, and F represents 
factors that can be either parenthesized expressions or identifiers: 
E  + E + T I T  
T  + T * F I F  
F  + ( E )  
1 i
d
 
Expression grammar (4.1) 
belongs to the class of LR grammars that are suitable 
for bottom-up parsing. This grammar can be adapted to handle additional 
operators and additional levels of precedence. However, it cannot be used for 
top-down parsing because it is left recursive. 
The following non-left-recursive variant of the expression grammar (4.1) will 
be used for top-down parsing: 
E  + TE' 
E
'
 + +TE'I e 
T  + FT' 
T' + * F T '  I e 
F  + ( E )  
I i
d
 
194 
CHAPTER 4. SYNTAX ANALYSIS 
The following grammar treats + and * alike, so it is useful for illustrating 
techniques for handling ambiguities during parsing: 
Here, E represents expressions of all types. Grammar (4.3) permits more than 
one parse tree for expressions like a + 
b * c. 
4.1.3 
Syntax Error Handling 
The remainder of this section considers the nature of syntactic errors and gen- 
eral strategies for error recovery. Two of these strategies, called panic-mode and 
phrase-level recovery, are discussed in more detail in connection with specific 
parsing methods. 
If a compiler had to process only correct programs, its design and implemen- 
tation would be simplified greatly. However, a compiler is expected to assist 
the programmer in locating and tracking down errors that inevitably creep into 
programs, despite the programmer's best efforts. Strikingly, few languages have 
been designed with error handling in mind, even though errors are so common- 
place. Our civilization would be radically different if spoken languages had 
the same requirements for syntactic accuracy as computer languages. Most 
programming language specifications do not describe how a compiler should 
respond to errors; error handling is left to the compiler designer. Planning the 
error handling right from the start can both simplify the structure of a compiler 
and improve its handling of errors. 
Common programming errors can occur at many different levels. 
Lexical errors include misspellings of identifiers, keywords, or operators - 
e.g., the use of an identifier elipsesize instead of ellipsesize - 
and 
missing quotes around text intended as a string. 
Syntactic errors include misplaced semicolons or extra or missing braces; 
that is, '((" or ")." As another example, in C or Java, the appearance 
of a case statement without an enclosing switch is a syntactic error 
(however, this situation is usually allowed by the parser and caught later 
in the processing, as the compiler attempts to generate code). 
Semantic errors include type mismatches between operators and operands. 
An example is a return statement in a Java method with result type void. 
Logical errors can be anything from incorrect reasoning on the part of 
the programmer to the use in a C program of the assignment operator = 
instead of the comparison operator ==. The program containing = may 
be well formed; however, it may not reflect the programmer's intent. 
The precision of parsing methods allows syntactic errors to be detected very 
efficiently. Several parsing methods, such as the LL and LR methods, detect 
4.1. INTRODUCTION 
an error as soon as possible; that is, when the stream of tokens from the lexical 
analyzer cannot be parsed further according to the grammar for the language. 
More precisely, they have the viable-prefix property, meaning that they detect 
that an error has occurred as soon as they see a prefix of the input that cannot 
be completed to form a string in the language. 
Another reason for emphasizing error recovery during parsing is that many 
errors appear syntactic, whatever their cause, and are exposed when parsing 
cannot continue. A few semantic errors, such as type mismatches, can also be 
detected efficiently; however, accurate detection of semantic and logical errors 
at compile time is in general a difficult task. 
The error handler in a parser has goals that are simple to state but chal- 
lenging to realize: 
Report the presence of errors clearly and accurately. 
Recover from each error quickly enough to detect subsequent errors. 
Add minimal overhead to the processing of correct programs. 
Fortunately, common errors are simple ones, and a relatively straightforward 
error-handling mechanism often suffices. 
How should an error handler report the presence of an error? At the very 
least, it must report the place in the source prograr.1 where an error is detected, 
because there is a good chance that the actual error occurred within the previous 
few tokens. A common strategy is to print the offending line with a pointer to 
the position at which an error is detected. 
4.1.4 Error-Recovery Strategies 
Once an error is detected, how should the parser recover? Although no strategy 
has proven itself universally acceptable, a few methods have broad applicabil- 
ity. The simplest approach is for the parser to quit with an informative error 
message when it detects the first error. Additional errors are often uncovered 
if the parser can restore itself to a state where processing of the input can con- 
tinue with reasonable hopes that the further processing will provide meaningful 
diagnostic information. If errors pile up, it is better for the compiler to give 
up after exceeding some error limit than to produce an annoying avalanche of 
"spurious" errors. 
The balance of this section is devoted to the following recovery strategies: 
panic-mode, phrase-level, error-productions, and global-correction. 
Panic-Mode Recovery 
With this method, on discovering an error, the parser discards input symbols 
one at a time until one of a designated set of synchronizing tokens is found. 
The synchronizing tokens are usually delimiters, such as semicolon or 3, whose 
role in the source program is clear and unambiguous. The compiler designer 
CHAPTER 4. SYNTAX ANALYSIS 
must select the synchronizing tokens appropriate for the source language. While 
panic-mode correction often skips a considerable amount of input without check- 
ing it for additional errors, it has the advantage of simplicity, and, unlike some 
methods to be considered later, is guaranteed not to go into an infinite loop. 
Phrase-Level Recovery 
On discovering 
an error, a parser may perform local correction on the remaining 
input; that is, it may replace a prefix of the remaining input by some string that 
allows the parser to continue. A typical local correction is to replace a comma 
by a semicolon, delete an extraneous semicolon, or insert a missing semicolon. 
The choice of the local correction is left to the compiler designer. Of course, 
we must be careful to choose replacements that do not lead to infinite loops, as 
would be the case, for example, if we always inserted something on the input 
ahead of the current input symbol. 
Phrase-level replacement has been used in several error-repairing compilers, 
as it can correct any input string. Its major drawback is the difficulty it has in 
coping with situations in which the actual error has occurred before the point 
of detection. 
Error Product 
ions 
By anticipating common errors that might be encountered, we can augment the 
grammar for the language at hand with productions that generate the erroneous 
constructs. A parser constructed from a grammar augmented by these error 
productions detects the anticipated errors when an error production is used 
during parsing. The parser can then generate appropriate error diagnostics 
about the erroneous construct that has been recognized in the input. 
Global Correction 
Ideally, we would like a compiler to make as few changes as possible in processing 
an incorrect input string. There are algorithms for choosing a minimal sequence 
of changes to obtain a globally least-cost correction. Given an incorrect input 
string x and grammar G, these algorithms will find a parse tree for a related 
string y, such that the number of insertions, deletions, and changes of tokens 
required to transform x into y is as small as possible. Unfortunately, these 
methods are in general too costly to implement in terms of time and space, so 
these techniques are currently only of theoretical interest. 
Do note that a closest correct program may not be what the programmer had 
in mind. Nevertheless, the notion of least-cost correction provides a yardstick 
for evaluating error-recovery techniques, and has been used for finding optimal 
replacement strings for phrase-level recovery. 
4.2. CONTEXT-FREE GRAMMARS 
4.2 Context-Free Grammars 
Grammars were introduced in Section 2.2 to systematically describe the syntax 
of programming language constructs like expressions and statements. Using 
a syntactic variable stmt to denote statements and variable expr to denote 
expressions, the production 
stmt -+ if ( expr ) stmt else stmt 
(4.4) 
specifies the structure of this form of conditional statement. Other productions 
then define precisely what an expr is and what else a stmt can be. 
This section reviews the definition of a context-free grammar and introduces 
terminology for talking about parsing. In particular, the notion of derivations 
is very helpful for discussing the order in which productions are applied during 
parsing. 
4.2.1 The Formal Definition of a Context-Free Grammar 
From Section 2.2, a context-free grammar (grammar for short) consists of ter- 
minals, nonterminals, a start symbol, and productions. 
1. Terminals 
are the basic symbols from which strings are formed. The term 
"token name" is a synonym for '"erminal" and frequently we will use the 
word "token" for terminal when it is clear that we are talking about just 
the token name. We assume that the terminals are the first components 
of the tokens output by the lexical analyzer. In (4.4), the terminals are 
the keywords if and else and the symbols "(" and ") ." 
2. Nonterminals are syntactic variables that denote sets of strings. In (4.4), 
stmt and expr are nonterminals. The sets of strings denoted by nontermi- 
nals help define the language generated by the grammar. Nonterminals 
impose a hierarchical structure on the language that is key to syntax 
analysis and translation. 
3. In a grammar, one nonterminal is distinguished as the start symbol, and 
the set of strings it denotes is the language generated by the grammar. 
Conventionally, the productions for the start symbol are listed first. 
4. The productions of a grammar specify the manner in which the termi- 
nals and nonterminals can be combined to form strings. Each production 
consists of: 
(a) A nonterminal called the head or left side of the production; this 
production defines some of the strings denoted by the head. 
(b) The symbol +. 
Sometimes : 
: 
= has been used in place of the arrow. 
(c) A body or right side consisting of zero or more terminals and non- 
terminals. The components of the body describe one way in which 
strings of the nonterminal at the head can be constructed. 
198 
CHAPTER 4. SYNTAX ANALYSIS 
Example 4.5 : 
The grammar in Fig. 4.2 defines simple arithmetic expressions. 
In this grammar, the terminal symbols are 
The nonterminal symbols are expression, term and factor, and expression is the 
start symbol 
expression 
expression 
expression 
term 
term 
term 
factor 
factor 
expression + term 
expression - term 
term 
term * factor 
term / factor 
factor 
( expression 1 
id 
Figure 4.2: Grammar for simple arithmetic expressions 
4.2.2 
Notational Convent 
ions 
To avoid always having to state that "these are the terminals," "these are the 
nontermiaals ," and so on, the following notational conventions for grammars 
will be used throughout the remainder of this book. 
1. These symbols are terminals: 
(a) Lowercase letters early in the alphabet, such as a, b, e. 
(b) Operator symbols such as +, r, 
and so on. 
(c) Punctuation symbols such as parentheses, comma, and so on. 
(d) The digits 0,1,. . 
. 
,9. 
(e) Boldface strings such as id or if, each of which represents a single 
terminal symbol. 
2. These symbols are nonterminals: 
(a) Uppercase letters early in the alphabet, such as A, B, C. 
(b) The letter S, which, when it appears, is usually the start symbol. 
(c) Lowercase, italic names such as expr or stmt. 
(d) When discussing programming constructs, uppercase letters may be 
used to represent nonterminals for the constructs. For example, non- 
terminals for expressions, terms, and factors are often represented by 
E, T, and F, respectively. 
4.2. CONTEXT-FREE GRAMMARS 
199 
3. Uppercase letters late in the alphabet, such as X, Y, 2, 
represent grammar 
symbols; that is, either nonterminals or terminals. 
4. Lowercase letters late in the alphabet, chiefly u, 
v, . 
. . , 
x ,  represent (pos- 
sibly empty) strings of terminals. 
5. Lowercase Greek letters, a, 
,O, y 
for example, represent (possibly empty) 
strings of grammar symbols. Thus, a generic production can be written 
as A + 
a, 
where A is the head and a the body. 
6. A set of productions A -+ al, 
A + 
a2, 
. . 
. , A -+ a k  with a common head 
A (call them A-productions), may be written A + 
a1 / as I . . 
I ak. Call 
a l ,  
a2,. 
. 
. , 
a k  the alternatives for A. 
7. Unless stated otherwise, the head of the first production is the start sym- 
bol. 
Example 4.6 : 
Using these conventions, the grammar of Example 4.5 can be 
rewritten concisely as 
E + E + T ( E - T I T  
T + T * F I T / F I F  
F -+ 
( E )  1 id 
The notational conventions tell us that E, T, and F are nonterminals, with E 
the start symbol. The remaining symbols are terminals. 
4.2.3 Derivations 
The construction of a parse tree can be made precise by taking a derivational 
view, in which productions are treated as rewriting rules. Beginning with the 
start symbol, each rewriting step replaces a nonterminal by the body of one of its 
productions. This derivational view corresponds to the top-down construction 
of a parse tree, but the precision afforded by derivations will be especially helpful 
when bottom-up parsing is discussed. As we shall see, bottom-up parsing is 
related to a class of derivations known as "rightmost" derivations, in which the 
rightmost nonterminal is rewritten at each step. 
For example, consider the following grammar, with a single nonterminal E, 
which adds a production E -+ - 
E to the grammar (4.3): 
The production E -+ - 
E signifies that if E denotes an expression, then - 
E 
must also denote an expression. The replacement of a single E by - 
E will be 
described by writing 
CHAPTER 4. SYNTAX ANALYSIS 
which is read, "E derives -E." 
The production E --+ ( E ) can be applied 
to replace any instance of E in any string of grammar symbols by (E), e.g., 
E * E + 
(E) * E or E * E + 
E * (E). We can take a single E and repeatedly 
apply productions in any order to get a sequence of replacements. For example, 
We call such a sequence of replacements a derivation of -(id) from E. This 
derivation provides a proof that the string -(id) is one particular instance of 
an expression. 
For a general definition of derivation, consider a nonterminal A in the middle 
of a sequence of grammar symbols, as in aAP, where a and ,O are arbitrary 
strings of grammar symbols. Suppose A -+ y is a production. Then, we write 
aAP =+- 
ayp. The symbol +- 
means, "derives in one step." When a sequence 
of derivation steps a
1
 + 
a
2
 + . + 
a, rewrites a
1
 to a,, we say a
1
 derives 
a,. Often, we wish to say, "derives in zero or more steps." For this purpose, 
we can use the symbol &- . Thus, 
1. a % a, 
for any string a, 
and 
2. If a &  
p and p + y ,  then a %  
y. 
+ 
Likewise, + means, "derives in one or more steps." 
If S % a, 
where S is the start symbol of a grammar G, we say that a is a 
sentential form of G. Note that a sentential form may contain both terminals 
and nonterminals, and may be empty. A sentence of G is a sentential form with 
no nonterminals. The language generated by a grammar is its set of sentences. 
Thus, a string of terminals w is in L(G), the language generated by G, if and 
only if w 
is a sentence of G (or S % w). 
A language that can be generated by 
a grammar is said to be a context-free language. If two grammars generate the 
same language, the grammars are said to be equivalent. 
The string -(id + id) is a sentence of grammar (4.7) because there is a 
derivation 
E S- -E S- -(E) + 
-(E + 
E )  3 -(id + 
E) + 
-(id + 
id) 
(4.8) 
The strings E, - 
E, - 
(E) 
, 
. 
. 
. , 
- 
(id + 
id) are all sentential forms of this gram- 
mar. We write & % - 
(id + 
id) to indicate that - 
(id + id) can be derived 
from E. 
At each step in a derivation, there are two choices to be made. We need 
to choose which nonterminal to replace, and having made this choice, we must 
pick a production with that nonterminal as head. For example, the following 
alternative derivation of -(id + 
id) differs from derivation (4.8) in the last two 
steps: 
4.2. CONTEXT-FREE GRAMMARS 
20 1 
Each nonterminal is replaced by the same body in the two derivations, but the 
order of replacements is different. 
To understand how parsers work, we shall consider derivations in which the 
nonterminal to be replaced at each step is chosen as follows: 
1. In lefimost derivations, the leftmost nonterminal in each sentential is al- 
ways chosen. If a + 
p is a step in which the leftmost nonterminal in a is 
replaced, we write a 
P. 
lm 
2. In rightmost derivations, the rightmost nonterminal is always chosen; we 
write a + p in this case. 
r m  
Derivation (4.8) is leftmost, so it can be rewritten as 
Note that (4.9) is a rightmost derivation. 
Using our notational conventions, every leftmost step can be written as 
wAy + wSy, where w consists of terminals only, A -+ 6 is the production 
lm 
applied, and y is a string of grammar symbols. To emphasize that a derives ,
8
 
by a leftrnost derivation, we write a % p. If S % a, 
then we say that a is a 
lm 
lm 
left-sentential 
form of the grammar at hand. 
Analogous definitions hold for rightmost derivations. Rightmost derivations 
are sometimes called canonical derivations. 
4.2.4 Parse Trees and Derivations 
A parse tree is a graphical representation of a derivation that filters out the 
order in which productions are applied to replace nonterminals. Each interior 
node of a parse tree represents the application of a production. The interior 
node is labeled with the  ont terminal A in the head of the production; the 
children of the node are labeled, from left to right, by the symbols in the body 
of the production by which this A was replaced during the derivation. 
For example, the parse tree for -(id + id) in Fig. 4.3, results from the 
derivation (4.8) as well as derivation (4.9). 
The leaves of a parse tree are labeled by nonterminals or terminals and, read 
from left to right, constitute a sentential form, called the yield or frontier of the 
tree. 
To see the relationship between derivations and parse trees, consider any 
derivation a1 .j 
a 2  + 
. 
- . + 
a,, where a1 is a single nonterminal A. For each 
sentential form a
i
 in the derivation, we can construct a parse tree whose yield 
is a
i
.
 
The process is an induction on i. 
BASIS: The tree for a
1
 = A is a single node labeled A. 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.3: Parse tree for -(id + 
id) 
INDUCTION: Suppose we already have constructed a parse tree with yield 
ai-1 = XI 
X2 . 
. Xk (note that according to our notational conventions, each 
grammar symbol Xi is either a nonterminal or a terminal). Suppose ai is 
derived from ai-1 by replacing Xj, a nonterminal, by ,8 = Y1Y2 . Ym. That 
is, at the ith step of the derivation, production Xj -+ 
,8 is applied to ai-1 to 
derive ai = 
XIXz -. 
-Xj-1,8Xj+l . exIE' 
To model this step of the derivation, find the jth leaf from the left in the 
current parse tree. This leaf is labeled Xj. Give this leaf m children, labeled 
Yl, Y2,. 
. 
. , 
Ym, from the left. As a special case, if m = 0, then ,8 = e, and we 
give the jth leaf one child labeled E .  
Example 4.10 : The sequence of parse trees constructed from the derivation 
(4.8) is shown in Fig. 4.4. In the first step of the derivation, E + -E. To 
model this step, add two children, labeled - 
and E, 
to the root E of the initial 
tree. The result is the second tree. 
In the second step of the derivation, - 
E + 
- 
(E). Consequently, add three 
children, labeled (, E ,  and ), to the leaf labeled E of the second tree, to 
obtain the third tree with yield -(E). Continuing in this fashion we obtain the 
complete parse tree as the sixth tree. 
Since a parse tree ignores variations in the order in which symbols in senten- 
tial forms are replaced, there is a many-to-one relationship between derivations 
and parse trees. For example, both derivations (4.8) and (4.9), are associated 
with the same final parse tree of Fig. 4.4. 
In what follows, we shall frequently parse by producing a leftmost or a 
rightmost derivation, since there is a one-to-one relationship between parse 
trees and either leftmost or rightmost derivations. Both leftmost and rightmost 
derivations pick a particular order for replacing symbols in sentential forms, so 
they too filter out variations in the order. It is not hard to show that every parse 
tree has associated with it a unique leftmost and a unique rightmost derivation. 
4.2. CONTEXT-FREE GRAMMARS 
Figure 4.4: Sequence of parse trees for derivation (4.8) 
4.2.5 
Ambiguity 
From Section 2.2.4, a grammar that produces more than one parse tree for some 
sentence is said to be ambiguous. Put another way, an ambiguous grammar is 
one that produces more than one leftmost derivation or more than one rightmost 
derivation for the same sentence. 
Example 4.11 : 
The arithmetic expression grammar (4.3) permits two distinct 
leftmost derivations for the sentence id + 
id * id: 
The corresponding parse trees appear in Fig. 4.5. 
Note that the parse tree of Fig. 4.5(a) reflects the commonly assumed prece- 
dence of + and *, while the tree of Fig. 4.5(b) does not. That is, it is customary 
to treat operator * as having higher precedence than +, corresponding to the 
fact that we would normally evaluate an expression like a + 
b * c as a + 
(b * c), 
rather than as (a 
+ 
b) * c. 
For most parsers, it is desirable that the grammar be made unambiguous, 
for if it is not, we cannot uniquely determine which parse tree to select for a 
sentence. In other cases, it is convenient to use carefully chosen ambiguous 
grammars, together with disambiguating rules that "throw away" undesirable 
parse trees, leaving only one tree for each sentence. 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.5: Two parse trees for id+id*id 
4.2.6 
Verifying the Language Generated by a Grammar 
Although compiler designers rarely do so for a complete programming-language 
grammar, it is useful to be able to reason that a given set of productions gener- 
ates a particular language. Troublesome constructs can be studied by writing 
a concise, abstract grammar and studying the language that it generates. We 
shall construct such a grammar for conditional statements below. 
A proof that a grammar G generates a language L has two parts: show that 
every string generated by G is in L, and conversely that every string in L can 
indeed be generated by G. 
Example 4.12 : 
Consider the following grammar: 
It may not be initially apparent, but this simple grammar generates all 
strings of balanced parentheses, and only such strings. To see why, we shall 
show first that every sentence derivable from S is balanced, and then that every 
balanced string is derivable from S. To show that every sentence derivable from 
S 
is balanced, we use an inductive proof on the number of steps n in a derivation. 
BASIS: The basis is n = 1. The only string of terminals derivable from S in 
one step is the empty string, which surely is balanced. 
INDUCTION: Now assume that all derivations of fewer than n steps produce 
balanced sentences, and consider a leftmost derivation of exactly n steps. Such 
a derivation must be of the form 
The derivations of x and y from S take fewer than n steps, so by the inductive 
hypothesis x and y 
are balanced. Therefore, the string (x)y 
must be balanced. 
That is, it has an equal number of left and right parentheses, and every prefix 
has at least as many left parentheses as right. 
4.2. CONTEXT-FREE GRAMMARS 
2
0
5
 
Having thus shown that any string derivable from S is balanced, we must 
next show that every balanced string is derivable from S. To do so, use induction 
on the length of a string. 
BASIS: If the string is of length 0, it must be E, which is balanced. 
INDUCTION: First, observe that every balanced string has even length. As- 
sume that every balanced string of length less than 2n is derivable from S, 
and consider a balanced string w of length 2n, n 2 
1. Surely w begins with a 
left parenthesis. Let (x) 
be the shortest nonempty prefix of w having an equal 
number of left and right parentheses. Then w can be written as w = (x) 
y where 
both x and y are balanced. Since x and y are of length less than 2n, they are 
derivable from S by the inductive hypothesis. Thus, we can find a derivation 
of the form 
proving that w = (x)y 
is also derivable from S. 
4.2.7 Context-Free Grammars Versus Regular 
Expressions 
Before leaving this section on grammars and their properties, we establish that 
grammars are a more powerful notation than regular expressions. Every con- 
struct that can be described by a regular expression can be described by a gram- 
mar, but not vice-versa. Alternatively, every regular language is a context-free 
language, but not vice-versa. 
For example, the regular expression (alb)*abb 
and the grammar 
describe the same language, the set of strings of a's and b's ending in abb. 
We can construct mechanically a grammar to recognize the same language 
as a nondeterministic finite automaton (NFA). The grammar above was con- 
structed from the NFA in Fig. 3.24 using the following construction: 
1. For each state i of the NFA, create a nonterminal Ai. 
2. If state i has a transition to state j on input a, add the production Ai -+ 
aAj. If state i goes to state j on input E ,  add the production Ai --+ A,. 
3. If i is an accepting state, add Ai 
-+ e. 
4. If i is the start state, make Ai be the start symbol of the grammar. 
206 
CHAPTER 4. SYNTAX ANALYSIS 
On the other hand, the language L = {anbn I n > 1) 
with an equal number 
of a's and b's is a prototypical example of a language that can be described 
by a grammar but not by a regular expression. To see why, suppose L were 
the language defined by some regular expression. We could construct a DFA D 
with a finite number of states, say k, 
to accept L. Since D has only k states, for 
an input beginning with more than k a's, D must enter some state twice, say 
si, as in Fig. 4.6. Suppose that the path from si back to itself is labeled with 
a sequence ajdi. Since aib<s in the language, there must be a path labeled bi 
from si to an accepting state f. But, then there is also a path from the initial 
state so through si to f labeled ajbi, as shown in Fig. 4.6. Thus, D also accepts 
ajbi, which is not in the language, contradicting the assumption that L is the 
language accepted by D. 
path labeled aj-i 
... 
a 
~
t
h
 
labeled a' 
path labeled b" 
... 
... 
Figure 4.6: DFA D accepting both ai 
bi and a j  
bi. 
Colloquially, we say that "finite automata cannot count 
," meaning that 
a finite automaton cannot accept a language like {anbn I n > 1) that would 
require it to keep count of the number of a's before it sees the b's. Likewise, "a 
grammar can count two items but not three," as we shall see when we consider 
non-context-free language constructs in Section 4.3.5. 
4.2.8 Exercises for Section 4.2 
Exercise 4.2.1 : 
Consider the context-free grammar: 
and the string aa + 
a*. 
a) Give a leftmost derivation for the string. 
b) Give a rightmost derivation for the string. 
c) Give a parse tree for the string. 
! 
d) Is the grammar ambiguous or unambiguous? Justify your answer. 
! 
e) Describe the language generated by this grammar. 
Exercise 4.2.2 : 
Repeat Exercise 4.2.1 for each of the following grammas and 
strings: 
4.2. CONTEXT-FREE GRAMMARS 
b) S -+ + S S ( * S S I a with string + * aaa. 
! 
C) S -+ S ( S ) S ( E with string (
0
0
)
.
 
! 
e) S -+ ( L )  
I a and L - +  
L ,  S I S with string ((a,a),a,(a)). 
!! 
f) S -+ a S b S I b S a S I E with string aabbab. 
! 
g) The following grammar for boolean expressions: 
bexpr 
-+ 
bexpr or bterm 1 bterm 
bterm 
-+ 
bterm and bfactor 1 bfactor 
bfactor 
--+ 
not bfactor 1 ( bexpr ) 1 true 1 false 
Exercise 4.2.3 : 
Design grammars for the following languages: 
a) The set of all strings of 0s and 1s such that every 0 is immediately followed 
by at least one 1. 
! 
b) The set of all strings of 0s and 1s that are palindromes; that is, the string 
reads the same backward as forward. 
! 
c) The set of all strings of 0s and 1s with an equal number of 0s and 1s. 
!! 
d) The set of all strings of 0s and 1s with an unequal number of 0s and 1s. 
! 
e) The set of all strings of 0s and 1s in which 011 does not appear as a 
substring. 
!! 
f) The set of all strings of 0s and 1s of the form xy, where x # y and x and 
y are of the same length. 
! 
Exercise 4.2.4 : There is an extended grammar notation in common use. In 
this notation, square and curly braces in production bodies are metasymbols 
(like -+ or 1
)
 with the following meanings: 
i) Square braces around a grammar symbol or symbols denotes that these 
constructs are optional. Thus, production A -+ X [Y] Z has the same 
effect as the two productions A -+ X Y Z and A -+ X 2. 
ii) Curly braces around a grammar symbol or symbols says that these sym- 
bols may be repeated any number of times, including zero times. Thus, 
A -+ X {Y Z) has the same effect as the infinite sequence of productions 
A - + X , A - + X Y  Z , A - + X Y  
Z Y  
Z,andsoon. 
208 
CHAPTER 4. SYNTAX ANALYSIS 
Show that these two extensions do not add power to grammars; that is, any 
language that can be generated by a grammar with these extensions can be 
generated by a grammar without the extensioms. 
Exercise 4.2.5 : Use the braces described in Exercise 4.2.4 to simplify the 
following grammar for statement blocks and conditional statements: 
stmt 
-
i
 if expr then stmt else stmt 
I 
if stmt then stmt 
I 
begin stmtList end 
stmtList -
i
 
stmt ; 
stmtLdst ( stmt 
! 
Exercise 4.2.6 : 
Extend the idea of Exercise 4.2.4 to allow any regular expres- 
sion of grammar symbols in the body of a production. Show that this extension 
does not allow grammars to define any new languages. 
! 
Exercise 4.2.7 : 
A grammar symbol X (terminal or nonterminal) is useless if 
there is no derivation of the form S $- wXy % wzy. That is, X can never 
appear in the derivation of any sentence. 
a) Give an algorithm to eliminate from a grammar all productions containing 
useless symbols. 
b) Apply your algorithm to the grammar: 
Exercise 4.2.8: The grammar in Fig. 4.7 generates declarations for a sin- 
gle numerical identifier; these declarations involve four different, independent 
properties of numbers. 
stmt 
-+ 
declare id optionList 
optionList -+ 
optionList option I E 
option 
-+ 
mode I scale 1 precision I base 
mode 
-+ 
real 1 complex 
scale 
+ fixed I floating 
precision 
+ single I double 
base 
+ binary ( decimal 
Figure 4.7: A grammar for multi-attribute declarations 
a) Generalize the grammar of Fig. 4.7 by allowing n options Ai, for some 
fixed n and for i = 1 , 2  
. . 
. , 
n, where Ai can be either ai or bi. Your 
grammar should use only O(n) grammar symbols and have a total length 
of productions that is O(n) 
. 
4.3. WRITING A GRAMMAR 
209 
! 
b) The grammar of Fig. 4.7 and its generalization in part (a) allow declara- 
tions that are contradictory and/or redundant, such as: 
declare foo r e a l  fixed r e a l  f l o a t i n g  
We could insist that the syntax of the language forbid such declarations; 
that is, every declaration generated by the grammar has exactly one value 
for each of the n options. If we do, then for any fixed n there is only a finite 
number of legal declarations. The language of legal declarations thus has 
a grammar (and also a regular expression), as any finite language does. 
The obvious grammar, in which the start symbol has a production for 
every legal declaration has n! productions and a total production length 
of O(n x n!). You must do better: a total production length that is 
0 
(nzn) 
. 
!! 
c) Show that any grammar for part (b) must have a total production length 
of at least 2". 
d) What does part (c) say about the feasibility of enforcing nonredundancy 
and noncontradiction among options in declarations via the syntax of the 
programming language? 
4.3 
Writing a Grammar 
Grammars are capable of describing most, but not all, of the syntax of pro- 
gramming languages. For instance, the requirement that identifiers be declared 
before they are used, cannot be described by a context-free grammar. Therefore, 
the sequences of tokens accepted by a parser form a superset of the program- 
ming language; subsequent phases of the compiler must analyze the output of 
the parser to ensure compliance with rules that are not checked by the parser. 
This section begins with a discussion of how to divide work between a lexical 
analyzer and a parser. We then consider several transformations that could be 
applied to get a grammar more suitable for parsing. One technique can elim- 
inate ambiguity in the grammar, and other techniques - 
left-recursion elimi- 
nation and left factoring - 
are useful for rewriting grammars so they become 
suitable for top-down parsing. We conclude this section by considering some 
programming language constructs that cannot be described by any grammar. 
4.3.1 
Lexical Versus Syntactic Analysis 
As we observed in Section 4.2.7, everything that can be described by a regular 
expression can also be described by a grammar. We may therefore reasonably 
ask: "Why use regular expressions to define the lexical syntax of a language?" 
There are several reasons. 
210 
CHAPTER 4. SYNTAX ANALYSIS 
1. Separating the syntactic structure of a language into lexical and non- 
lexical parts provides a convenient way of modularizing the front end of 
a compiler into two manageable-sized components. 
2. The lexical rules of a language are frequently quite simple, and to describe 
them we do not need a notation as powerful as grammars. 
3. Regular expressions generally provide a more concise and easier-to-under- 
stand notation for tokens than grammars. 
4. More efficient lexical analyzers can be constructed automatically from 
regular expressions than from arbitrary grammars. 
There are no firm guidelines as to what to put into the lexical rules, as op- 
posed to the syntactic rules. Regular expressions are most useful for describing 
the structure of constructs such as identifiers, constants, keywords, and white 
space. Grammars, on the other hand, are most useful for describing nested 
structures such as balanced parentheses, matching begin-end's, corresponding 
if-then-else's, and so on. These nested structures cannot be described by regular 
expressions. 
4.3.2 Eliminating Ambiguity 
Sometimes an ambiguous grammar can be rewritten to eliminate the ambiguity. 
As an example, we shall eliminate the ambiguity from the following "dangling- 
else" grammar: 
stmt + if expr then stmt 
( 
if expr then stmt else stmt 
(4.14) 
I 
other 
Here "other" 
stands for any other statement. According to this grammar, the 
compound conditional statement 
if El then S1 else if E2 then S2 else S3 
/
T
i
\
\
 
El 
,L.Ll 
e
p
T
(
\
\
\
 
2
%
 then 
stmt 
S
1
 
if n 
then 
stmt 
else 
stmt 
L
L
L
 
E2 
5'2 
S
3
 
Figure 4.8: Parse tree for a conditional statement 
4.3. WRITING A GRAMMAR 
211 
has the parse tree shown in Fig. 4.8.' Grammar (4.14) is ambiguous since the 
string 
if El then if E2 
then S1 else S2 
(4.15) 
has the two parse trees shown in Fig. 4.9. 
if 
,
e
x
p
r
\
 
t
~
i
t
m
j
~
l
l
 
El 
if A 
then ,
s
t
m
t
\
 
else /stmt\ 
E
2
 
S
1
 
s
2
 
if 
,
e
x
p
r
,
 
then ,
s
t
m
t
,
 
Figure 4.9: Two parse trees for an ambiguous sentence 
In all programming languages with conditional statements of this form, the 
first parse tree is preferred. The general rule is, "Match each else with the 
closest unmatched then." 
This disambiguating rule can theoretically be in- 
corporated directly into a grammar, but in practice it is rarely built into the 
productions. 
Example 4.16 : We can rewrite the dangling-else grammar (4.14) as the fol- 
lowing unambiguous grammar. The idea is that a statement appearing between 
a then and an else must be "matched" ; 
that is, the interior statement must 
not end with an unmatched or open then. A matched statement is either an 
if-then-else statement containing no open statements or it is any other kind 
of unconditional statement. Thus, we may use the grammar in Fig. 4.10. This 
grammar generates the same strings as the dangling-else grammar (4.14), but 
it allows only one parsing for string (4.15); namely, the one that associates each 
else with the closest previous unmatched then. 
[7 
 he subscripts on E and S are just to distinguish different occurrences of the same 
nonterminal, and do not imply distinct nonterminals. 
2
~
e
 
should note that C and its derivatives are included in this class. Even though the C 
family of languages do not use the keyword then, its role is played by the closing parenthesis 
for the condition that follows if. 
CHAPTER 4. SYNTAX ANALYSIS 
stmt + matched-stmt 
( 
open-stmt 
matched-stmt + if expr then matched-stmt else matched-stmt 
1 
other 
open-stmt + if expr then stmt 
1 
if expr then matched-stmt else open-stmt 
Figure 4.10: Unambiguous grammar for if-then-else statements 
4.3.3 Elimination of Left Recursion 
A grammar is left recursive if it has a nonterminal A such that there is a 
+ 
derivation A * Aa for some string a .  Top-down parsing methods cannot 
handle left-recursive grammars, so a transformation is needed to eliminate left 
recursion. In Section 2.4.5, we discussed immediate left recursion, where there 
is a production of the form A --+ Aa. Here, we study the general case. In 
Section 2.4.5, we showed how the left-recursive pair of productions A -+ Aa 1 ,fl 
could be replaced by the non-left-recursive productions: 
without changing the strings derivable from A. This rule by itself suffices for 
many grammars. 
Example 4.17 : The non-left-recursive expression grammar (4.2), repeated 
here, 
is obtained by eliminating immediate left recursion from the expression gram- 
mar (4.1). The left-recursive pair of productions E -+ E + T I T are replaced 
by E -+ T E' and E' -+ + T E' I c. The new productions for T and T' are 
obtained similarly by eliminating immediate left recursion. 
Immediate left recursion can be eliminated by the following technique, which 
works for any number of A-productions. First, group the productions as 
where no pi 
begins with an A. Then, replace the A-productions by 
4.3. WRITING A GRAMMAR 
The nonterminal A generates the same strings as before but is no longer left 
recursive. This procedure eliminates all left recursion from the A and A' pro- 
ductions (provided no ai is E), 
but it does not eliminate left recursion involving 
derivations of two or more steps. For example, consider the grammar 
The nonterminal S is left recursive because S 
Aa + Sda, but it is not 
immediately left recursive. 
Algorithm 4.19, below, systematically eliminates left recursion from a gram- 
mar. It is guaranteed to work if the grammar has no cycles (derivations of the 
+ 
form A + A) or 6-productions (productions of the form A -+ E). Cycles can be 
eliminated systematically from a grammar, as can E-productions 
(see Exercises 
4.4.6 and 4.4.7). 
Algorithm 4.19 : 
Eliminating left recursion. 
INPUT: Grammar G with no cycles or e-productions. 
OUTPUT: An equivalent grammar with no left recursion. 
METHOD: Apply the algorithm in Fig. 4.11 to G. Note that the resulting 
non-left-recursive grammar may have E-productions. 
1) arrange the nonterminals in some order A1, A2, . . 
. , 
A,. 
2) 
for ( each i from 1 
to n ) { 
3) 
for ( each j from 1 
to i - 
1 
) { 
4) 
replace each production of the form Ai -+ Aj7 by the 
productions Ai -+ 617 I 627 1 - .  I dk7, 
where 
Aj -+ dl 1 d2 1 . . 
. 1 dk are all current Aj-productions 
5 
> 
} 
6) 
eliminate the immediate left recursion among the Ai-productions 
7) 1 
Figure 4.11: Algorithm to eliminate left recursion from a grammar 
The procedure in Fig. 4.11 works as follows. In the first iteration for i = 
1, the outer for-loop of lines (2) through (7) eliminates any immediate left 
recursion among A1-productions. Any remaining A1 productions of the form 
Al -+ Ala must therefore have 1 > 1. After the i - 
1st iteration of the outer for- 
loop, all nonterminals Ale, 
where k < i, are "cleaned"; that is, any production 
Ak -+ 
Ala, must have 1 > k. As a result, on the ith iteration, the inner loop 
214 
CHAPTER 4. SYNTAX ANALYSIS 
of lines (3) through (5) progressively raises the lower limit in any production 
Ai -+ A,a, 
until we have m _> i. Then, eliminating immediate left recursion 
for the Ai productions at line (6) forces m to be greater than i. 
Example 4.20 : 
Let us apply Algorithm 4.19 to the grammar (4.18). Techni- 
cally, the algorithm is not guaranteed to work, because of the €-production, but 
in this case, the production A -+ c turns out to be harmless. 
We order the nonterminals S, A. There is no immediate left recursion 
among the S-productions, so nothing happens during the outer loop for i = 1. 
For i = 
2, we substitute for S 
in A -+ S 
d to obtain the following A-productions. 
A - + A c  I A a d  1 b d  1 
E 
Eliminating the immediate left recursion among these A-productions yields the 
following grammar. 
4.3.4 
Left Factoring 
a, 
Left factoring is a grammar transformation that is useful for producing a gram- 
mar suitable for predictive, or top-down, parsing. When the choice between 
two alternative A-productions is not clear, we may be able to rewrite the pro- 
ductions to defer the decision until enough of the input has been seen that we 
can make the right choice. 
For example, if we have the two productions 
stmt 
-+ 
if expr then stmt else strnt 
I 
if expr then stmt 
on seeing the input if, we cannot immediately tell which production to choose 
to expand stmt. In general, if A + 
apl I aP2 are two A-productions, and the 
input begins with a nonempty string derived from a, 
we do not know whether 
to expand A to aPl or a h .  However, we may defer the decision by expanding 
A to aA'. Then, after seeing the input derived from a, 
we expand A' to PI or 
to P2. That is, left-factored, the original productions become 
Algorithm 4.2 
1 : 
Left factoring a grammar. 
INPUT: Grammar G. 
OUTPUT: An equivalent left-factored grammar. 
4.3. WRITING A GRAMMAR 
215 
METHOD: For each nonterminal A, find the longest prefix a! common to two 
or more of its alternatives. If a! # E - 
i.e., there is a nontrivial common 
prefix - 
replace all of the A-productions A + 
up1 1 cupz 1 - - . / a!/?, I y, 
where 
y 
represents all alternatives that do not begin with a, 
by 
Here A' is a new nonterminal. Repeatedly apply this transformation until no 
two alternatives for a nonterminal have a common prefix. 
Example 4.22 : The following grammar abstracts the "dangling-else" prob- 
lem: 
Here, i, t, and e stand for if, then, and else; E and S stand for "conditional 
expression" and "statement ." Left-factored, this grammar becomes: 
Thus, we may expand S to iEtSS1 on input i, and wait until iEtS has been 
seen to decide whether to expand St 
to eS or to e. Of course, these grammars 
are both ambiguous, and on input e, it will not be clear which alternative for 
St 
should be chosen. Example 4.33 discusses a way out of this dilemma. 
4.3.5 Non-Context-Free Language Constructs 
A few syntactic constructs found in typical programming languages cannot be 
specified using grammars alone. Here, we consider two of these constructs, 
using simple abstract languages to illustrate the difficulties. 
Example 4.25 : 
The language in this example abstracts the problem of check- 
ing that identifiers are declared before they are used in a program. The language 
consists of strings of the form wcw, where the first w represents the declaration 
of an identifier w, c represents an intervening program fragment, and the second 
w represents the use of the identifier. 
The abstract language is L1 = {wcw I w is in (alb)*). L1 consists of 
all words composed of a repeated string of a's and b's separated by c, such 
as aabcaab. While it is beyond the scope of this book to prove it, the non- 
context-freedom of L1 directly implies the non-context-freedom of programming 
languages like C and Java, which require declaration of identifiers before their 
use and which allow identifiers of arbitrary length. 
For this reason, a grammar for C or Java does not distinguish among identi- 
fiers that are different character strings. Instead, all identifiers are represented 
216 
CHAPTER 4. SYNTAX ANALYSIS 
by a token such as id in the grammar. In a compiler for such a language, 
the semantic-analysis phase checks that identifiers are declared before they are 
used. 
Example 4.26 : The non-context-free language in this example abstracts the 
problem of checking that the number of formal parameters in the declaration of a 
function agrees with the number of actual parameters in a use of the function. 
The language consists of strings of the form anbmcndm. (Recall an means a 
written n times.) Here an and bm could represent the formal-parameter lists of 
two functions declared to have n and rn arguments, respectively, while cn and 
dm represent the actual-parameter lists in calls to these two functions. 
The abstract language is Lz = {anbmcndm 
I n > 1 
and m > I). That is, La 
consists of strings in the language generated by the regular expression a*b*c*d" 
such that the number of a's and c's are equal and the number of b's and d's are 
equal. This language is not context free. 
Again, the typical syntax of function declarations and uses does not concern 
itself with counting the number of parameters. For example, a function call in 
C-like language might be specified by 
stmt + id ( expr-list ) 
expr-list + expr-list , 
expr 
I 
expr 
with suitable productions for expr. Checking that the number of parameters in 
a call is correct is usually done during the semantic-analysis phase. 
4.3.6 Exercises for Section 4.3 
Exercise 4.3.1 : 
The following is a grammar for regular expressions over sym- 
bols a and b only, using + in place of 1 for union, to avoid conflict with the use 
of vertical bar as a metasymbol in grammars: 
rexpr 
-+ 
rexpr + rterm ( rterm 
rterm 
-+ 
rterm rfactor I rfactor 
rfactor 
+ rfactor * 1 rprirnary 
rprimary + a 1 b 
a) Left factor this grammar. 
b) Does left factoring make the grammar suitable for top-down parsing? 
c) In addition to left factoring, eliminate left recursion from the original 
grammar. 
d) Is the resulting grammar suitable for top-down parsing? 
Exercise 4.3.2 : 
Repeat Exercise 4.3.1 on the following grammars: 
4.4. TOP-DO 
WN PARSING 
a) The grammar of Exercise 4.2.1. 
b) The grammar of Exercise 4.2.2(a). 
c) The grammar of Exercise 4.2.2(c). 
d) The grammar of Exercise 4.2.2(e). 
e) The grammar of Exercise 4.2.2(g). 
! 
Exercise 4.3.3 : 
The following grammar is proposed to remove the "dangling- 
else ambiguity" discussed in Section 4.3.2: 
stmt 
+ if expr then stmt 
I 
matchedstmt 
matchedstmt + if expr then matchedstmt else stmt 
1 
other 
Show that this grammar is still ambiguous. 
4
.
4
 Top-Down Parsing 
Top-down parsing can be viewed as the problem of constructing a parse tree for 
the input string, starting from the root and creating the nodes of the parse tree 
in preorder (depth-first, as discussed in Section 2.3.4). Equivalently, top-down 
parsing can be viewed as finding a leftmost derivation for an input string. 
Example 4.27 : 
The sequence of parse trees in Fig. 4.12 for the input id+id*id 
is a top-down parse according to grammar (4.2), repeated here: 
E 
+ T E '  
E' 
-+ 
+ T E 1 (  
€ 
T  + F T '  
T' 
-+ 
* F T I I €  
F  + ( E )  
( id 
This sequence of trees corresponds to a leftmost derivation of the input. 
At each step of a top-down parse, the key problem is that of determining 
the production to be applied for a nonterminal, say A. Once an A-production 
is chosen, the rest of the parsing process consists of "matching7' the terminal 
symbols in the production body with the input string. 
The section begins with a general form of top-down parsing, called recursive- 
descent parsing, which may require backtracking to find the correct A-produc- 
tion to be applied. Section 2.4.2 introduced predictive parsing, a special case of 
recursive-descent parsing, where no backtracking is required. Predictive parsing 
chooses the correct A-production by looking ahead at the input a fixed number 
of symbols, typically we may look only at one (that is, the next input symbol). 
218 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.12: Top-down parse for id + 
id * id 
For example, consider the top-down parse in Fig. 4.12, which constructs 
a tree with two nodes labeled El. At the first E' node (in preorder), the 
production E' -+ 
+TE' is chosen; at the second E' node, the production E' -+ t 
is chosen. A predictive parser can choose between El-productions by looking 
at the next input symbol. 
The class of grammars for which we can construct predictive parsers looking 
k symbols ahead in the input is sometimes called the LL(k) class. We discuss the 
LL(1) class in Section 4.4.3, but introduce certain computations, called FIRST 
and FOLLOW, in a preliminary Section 4.4.2. From the FIRST and FOLLOW 
sets for a grammar, we shall construct "predictive parsing tables," which make 
explicit the choice of production during top-down parsing. These sets are also 
useful during bottom-up parsing, 
In Section 4.4.4 we give a nonrecursive parsing algorithm that maintains 
a stack explicitly, rather than implicitly via recursive calls. Finally, in Sec- 
tion 4.4.5 we discuss error recovery during top-down parsing. 
4.4. TOP-DOWN PARSING 
4.4.1 Recursive-Descent Parsing 
void A() { 
1) 
Choose an A-production, A + 
XI 
X2 . . 
. 
Xk; 
2) 
for ( i = l t o k )  { 
3 
if ( Xi is a nonterminal ) 
4) 
call procedure Xi 
() 
; 
5 
else if ( Xi equals the current input symbol a ) 
6) 
advance the input to the next symbol; 
7 )  
else /* an error has occurred */; 
1 
I 
Figure 4.13: A typical procedure for a nonterminal in a top-down parser 
A recursive-descent parsing program consists of a set of procedures, one for each 
nonterminal. Execution begins with the procedure for the start symbol, which 
halts and announces success if its procedure body scans the entire input string. 
Pseudocode for a typical nonterminal appears in Fig. 4.13. Note that this 
pseudocode is nondeterministic, since it begins by choosing the A-production 
to apply in a manner that is not specified. 
General recursive-descent may require backtracking; that is, it may require 
repeated scans over the input. However, backtracking is rarely needed to parse 
programming language constructs, so backtracking parsers are not seen fre- 
quently. Even for situations like natural language parsing, backtracking is not 
very efficient, and tabular methods such as the dynamic programming algo- 
rithm of Exercise 4.4.9 or the method of Earley (see the bibliographic notes) 
are preferred. 
To allow backtracking, the code of Fig. 4.13 needs to be modified. First, we 
cannot choose a unique A-production at line (I), 
so we must try each of several 
productions in some order. Then, failure at line (7) is not ultimate failure, but 
suggests only that we need to return to line (1) 
and try another A-production. 
Only if there are no more A-productions to try do we declare that an input 
error has been found. In order to try another A-production, we need to be able 
to reset the input pointer to where it was when we first reached line (1). Thus, 
a local variable is needed to store this input pointer for future use. 
Example 4.29 : 
Consider the grammar 
To construct a parse tree top-down for the input string w = 
cad, begin with a 
tree consisting of a single node labeled S, 
and the input pointer pointing to c, 
the first symbol of w. S has only one production, so we use it to expand S and 
220 
CHAPTER 4. SYNTAX ANALYSIS 
obtain the tree of Fig. 4.14(a). The leftmost leaf, labeled c, matches the first 
symbol of input w, so we advance the input pointer to a, the second symbol of 
w, and consider the next leaf, labeled A. 
Figure 4.14: Steps in a top-down parse 
Now, we expand A using the first alternative A -+ 
a b to obtain the tree of 
Fig. 4.14(b). We have a match for the second input symbol, a, so we advance 
the input pointer to d, the third input symbol, and compare d against the next 
leaf, labeled b. Since b does not match d, we report failure and go back to A to 
see whether there is another alternative for A that has not been tried, but that 
might produce a match. 
In going back to A, we must reset the input pointer to position 2, the 
position it had when we first came to A, which means that the procedure for A 
must store the input pointer in a local variable. 
The second alternative for A produces the tree of Fig. 4.14(c). The leaf 
a matches the second symbol of w and the leaf d matches the third symbol. 
Since we have produced a parse tree for w, we halt and announce successful 
completion of parsing. 
El 
A left-recursive grammar can cause a recursive-descent parser, even one 
with backtracking, to go into an infinite loop. That is, when we try to expand 
a nonterminal A, we may eventually find ourselves again trying to expand A 
without having consumed any input. 
4.4.2 FIRST and FOLLOW 
The construction of both top-down and bottom-up parsers is aided by two 
functions, FIRST and FOLLOW, associated with a grammar G. During top- 
down parsing, FIRST and FOLLOW allow us to choose which production to 
apply, based on the next input symbol. During panic-mode error recovery, sets 
of tokens produced by FOLLOW can be used as synchronizing tokens. 
Define FIRST(&), 
where a is any string of grammar symbols, to be the set 
of terminals that begin strings derivedPom a. If a % 6 ,  then E is also in 
FIRST@). For example, in Fig. 4.15, A + cy, so c is in FIRST(A). 
For a preview of how FIRST can be used during predictive parsing, consider 
two A-productions A + 
a / P, where FIRST(&) 
and FIRST@) are disjoint sets. 
We can then choose between these A-productions by looking at the next input 
4.4. TOP-DO 
WN PARSING 
Figure 4.15: Terminal c is in FIRST(A) 
and a is in FOLLOW(A) 
symbol a, since a can be in at most one of FIRST(~U) 
and  FIRST(^), not both. 
For instance, if a is in FIRST@) 
choose the production A -+ P. This idea will 
be explored when LL(1) grammars are defined in Section 4.4.3. 
Define FOLLOW(A), 
for nonterminal A, to be the set of terminals a that can 
appear immediately to the right of A in some sentential form; t$t 
is, the set 
of terminals a such that there exists a derivation of the form S + aAap, for 
some a! and p, as in Fig. 4.15. Note that there may have been symbols between 
A and a, at some time during the derivation, but if so, they derived r and 
disappeared. In addition, if A can be the rightmost symbol in some sentential 
form, then $ is in FOLLOW(A); 
recall that $ is a special "endmarker" symbol 
that is assumed not to be a symbol of any grammar. 
To compute FIRST(X) 
for all grammar symbols X, apply the following rules 
until no more terminals or E: can be added to any FIRST set. 
1. If X is a terminal, then FIRST(X) 
= {XI. 
2. If X is a nonterminal and X + 
YlY2 . 
. 
- Yk is a production for some k 2 1, 
then place a in FIRST(X) 
if for some i, a is in FIRST(Y,), 
and r is in all of 
FIRST(Y~), 
. 
. 
. , 
FIRST(Y,-I); 
that is, Y
l
 . 
-. 
x-1 &- r. If E is in FIRST(Y,) 
for all j = 1,2, 
. 
. . , 
k ,  then add E: to FIRST(X). 
For example, everything 
in FIRST(YI) 
is surely in FIRST(X). 
If 
does not derive 6, then we add 
nothing more to FIRST(X), 
but if Yl &- r, then we add F1RST(Y2), 
and 
SO on. 
3. If X -+ r is a production, then add r to FIRST(X). 
Now, we can compute FIRST for any string XlX2 , 
. 
. 
Xn as follows. Add to 
FIRST(X~ 
X2 
. . 
. 
Xn) all non-r symbols of FIRST(X~). 
Also add the non-r sym- 
bols of  FIRST(^^), if 6 is in FIRST(X~); 
the non-E 
symbols of FIRST(&), if r is 
in FIRST(XI) 
and  FIRST(^^); and so on. Finally, add r to F1RST(X1X2 
. 
. Xn) 
if, for all i, 
E is in FIRST(X~). 
To compute FOLLOW(A) 
for all nonterminals A, apply the following rules 
until nothing can be added to any FOLLOW set. 
1. Place $ in FOLLOW(S), 
where S is the start symbol, and $ is the input 
right endmarker. 
222 
CHAPTER 4. SYNTAX ANALYSIS 
2. If there is a production A -+ aBP, then everything in FIRST@) except E 
is in FOLLOW(B). 
3. If there is a production A -+ aB, or a production A -+ aBP, where 
FIRST(@) 
contains E, then everything in FOLLOW (A) is in FOLLOW (B) 
. 
Example 4.30 : 
Consider again the non-left-recursive grammar (4.28). Then: 
1. FIRST(F) = FIRST(T) 
= 
FIRST(E) = {(, 
id). To see why, note that the 
two productions for F have bodies that start with these two terminal 
symbols, id and the left parenthesis. T has only one production, and its 
body starts with F. Since F does not derive E, FIRST(T) 
must be the 
same as FIRST(F). 
The same argument covers FIRST(E). 
2. FIRST(E') = {+, 
E). The reason is that one of the two productions for E' 
has a body that begins with terminal +, and the other's body is E. When- 
ever a nonterminal derives E, we place E in FIRST for that nonterminal. 
3. FIRST(T') = {*, 
6). The reasoning is analogous to that for FIRST(E'). 
4. FOLLOW@) = FOLLOW(E') 
= {), $1. Since E is the start symbol, 
FOLLOW(E) 
must contain $. The production body ( E ) explains why the 
right parenthesis is in FOLLOW(E). 
For El, note that this nonterminal 
appears only at the ends of bodies of E-productions. Thus, FOLLOW(E') 
must be the same as FOLLOW(E). 
5. FOLLOW(T) 
= 
FOLLOW(T') 
= {+, 
), $1. Notice that T appears in bodies 
only followed by E'. Thus, everything except E that is in FIRST(E') must 
be in FOLLOW (T) 
; 
that explains the symbol +. However, since FIRST(E') 
contains E (i.e., E' & E), 
and E' is the entire string following T in the 
bodies of the E-productions, everything in FOLLOW(E) 
must also be in 
FOLLOW(T). 
That explains the symbols $ and the right parenthesis. As 
for T', since it appears only at the ends of the T-productions, it must be 
that FOLLOW(T') 
= 
FOLLOW(T). 
6. FOLLOW(F) 
= {+, 
*, ), $1. The reasoning is analogous to that for T in 
point (5). 
4.4.3 
LL(1) Grammars 
Predictive parsers, that is, recursive-descent parsers needing no backtracking, 
can be constructed for a class of grammars called LL(1). The first "L" in LL(1) 
stands for scanning the input from left to right, the second "L" for producing 
a leftmost derivation, and the "1" for using one input symbol of lookahead at 
each step to make parsing action decisions. 
4.4. TOP-DOWN PARSING 
223 
Transition Diagrams for Predictive Parsers 
Transition diagrams are useful for visualizing predictive parsers. For exam- 
ple, the transition diagrams for nonterminals E and E' of grammar (4.28) 
appear in Fig. 4.16(a). To construct the transition diagram from a gram- 
mar, first eliminate left recursion and then left factor the grammar. Then, 
for each nonterminal A, 
1. Create an initial and final (return) state. 
2. For each production A + 
XIXz 
- . Xk, 
create a path from the initial 
to the final state, with edges labeled XI, 
X 2 , .  
. 
. , 
Xk. If A -+ 
t, the 
path is an edge labeled t. 
Transition diagrams for predictive parsers differ from those for lexical 
analyzers. Parsers have one diagram for each nouterminal. The labels of 
edges can be tokens or nonterminals. A transition on a token (terminal) 
means that we take that transition if that token is the next input symbol. 
A transition on a nonterminal A is a call of the procedure for A. 
With an LL(1) grammar, the ambiguity of whether or not to take an 
€-edge can be resolved by making €-transitions 
the default choice. 
Transition diagrams can be simplified, provided the sequence of gram- 
mar symbols along paths is preserved. We may also substitute the dia- 
gram for a nonterminal A in place of an edge labeled A. The diagrams in 
Fig. 4.16(a) and (b) are equivalent: if we trace paths from E to an accept- 
ing state and substitute for E', then, in both sets of diagrams, the grammar 
symbols along the paths make up strings of the form T + 
T + 
. 
. . 
+ 
T. The 
diagram in (b) can be obtained from (a) by transformations akin to those 
in Section 2.5.4, where we used tail-recursion removal and substitution of 
procedure bodies to optimize the procedure for a nonterminal. 
The class of LL(1) grammars is rich enough to cover most programming 
constructs, although care is needed in writing a suitable grammar for the source 
language. For example, no left-recursive or ambiguous grammar can be LL(1). 
A grammar G is LL(1) if and only if whenever A --+ cu I ,
D
 are two distinct 
productions of G, the following conditions hold: 
1. For no terminal a do both a and ,
O
 derive strings beginning with a. 
2. At most one of cu and ,
D
 can derive the empty string. 
3. If ,
O
 3 t, then cu does not derive any string beginning with a terminal 
in FOLLOW(A). 
Likewise, if 
& t, then P does not derive any string 
beginning with a terminal in FOLLOW(A). 
4.4. TOP-DOWNPARSING 
225 
If, after performing the above, there is no production at all in M[A, 
a], then 
set M[A, 
a] to error (which we normally represent by an empty entry in the 
table). 
Example 4.32 
: 
For the expression grammar (4.28), Algorithm 4.31 produces 
the parsing table in Fig. 4.17. Blanks are error entries; nonblanks indicate a 
production with which to expand a nonterminal. 
Figure 4.17: Parsing table M for Example 4.32 
N
O
N
 - 
TERMINAL 
E 
E' 
T 
T' 
F 
Consider production E -+ TE'. Since 
this production is added to M[E, 
(
1
 and M[E, 
id]. Production El -+ +TE1 
is 
added to M[E', +] since FIRST(+T 
El) = {+}. Since FOLLOW (El) = {), $1, 
production E' + 
E is added to MIE1, 
)] and MIE1, 
$1. 
INPUT SYMBOL 
Algorithm 4.31 can be applied to any grammar G to produce a parsing table 
M. 
For every LL(1) grammar, each parsing-table entry uniquely identifies a 
production or signals an error. For some grammars, however, M may have 
some entries that are multiply defined. For example, if G is left-recursive or 
ambiguous, then A
d
 will have at least one multiply defined entry. Although left- 
recursion elimination and left factoring are easy to do, there are some grammars 
for which no amount of alteration will produce an LL(1) grammar. 
The language in the following example has no LL(1) grammar at all. 
Example 4.33 
: The following grammar, which abstracts the dangling-else 
problem, is repeated here from Example 4.22: 
The parsing table for this grammar appears in Fig. 4.18. The entry for MIS1, 
el 
contains both S' --+ eS and S' -+ 6 .  
The grammar is ambiguous and the ambiguity is manifested by a choice in 
what production to use when an e (else) is seen. We can resolve this ambiguity 
) 
El+€ 
TI+& 
id 
E +TE' 
T + 
FTI 
F -+ id 
$ 
E1+e 
TI-+€ 
* 
T1-+*FT' 
+ 
El -+ +TE1 
T 
( 
E -+ TE' 
T -+ FT' 
F -+ ( E )  
226 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.18: Parsing table M for Example 4.33 
NON 
- 
TERMINAL 
S 
S1 
E 
by choosing S' -+ eS. This choice corresponds to associating an else with the 
closest previous then. Note that the choice S' -+ 
c would prevent e from ever 
being put on the stack or removed from the input, and is surely wrong. 0 
4.4.4 Nonrecursive Predictive Parsing 
INPUT 
SYMBOL 
A nonrecursive predictive parser can be built by maintaining a stack explicitly, 
rather than implicitly via recursive calls. The parser mimics a leftmost deriva- 
tion. If w is the input that has been matched so far, then the stack holds a 
sequence of grammar symbols a such that 
a 
S + a  
The table-driven parser in Fig. 4.19 has an input buffer, a stack containing a 
sequence of grammar symbols, a parsing table constructed by Algorithm 4.31, 
and an output stream. The input buffer contains the string to be parsed, 
followed by the endmarker $. We reuse the symbol $ to mark the bottom of the 
stack, which initially contains the start symbol of the grammar on top of $. 
The parser is controlled by a program that considers X ,  the symbol on top 
of the stack, and a, 
the current input symbol. If X is a nonterminal, the parser 
chooses an X-production by consulting entry M[X, 
a] 
of the parsing table IM. 
(Additional code could be executed here, for example, code to construct a node 
in a parse tree.) Otherwise, it checks for a match between the terminal X and 
current input symbol a. 
The behavior of the parser can be described in terms of its configurations, 
which give the stack contents and the remaining input. The next algorithm 
describes how configurations are manipulated. 
Algorithm 4.34 : 
Table-driven predictive parsing. 
b 
E+b 
INPUT: A string w and a parsing table M for grammar G. 
OUTPUT: If w is in L(G), a leftmost derivation of w; otherwise, an error 
indication. 
e 
S' + 
€ 
S1 
-+ eS 
i 
S 
--+ iEtSS1 
t 
$ 
S' -+ 
€ 
4.4. TOP-DOWN PARSING 
Stack 
Input 
l a l + l b l $ l  
Output 
- 
X +  
- 
Y 
- 
Figure 4.19: Model of a table-driven predictive parser 
METHOD: Initially, the parser is in a configuration with w$ in the input buffer 
and the start symbol S of G on top of the stack, above $. The program in 
Fig. 4.20 uses the predictive parsing table M  to produce a predictive parse for 
the input. 
Predictive 
Parsing 
Program 
set zp to point to the first symbol of w; 
set X to the top stack symbol; 
while ( X # $ ) { /* stack is not empty */ 
if ( X is a ) pop the stack and advance zp; 
else if ( X is a terminal ) error(); 
else if ( M [ X ,  
a] is an error entry ) error(); 
else if ( M[X,a] 
= 
X  -+ Y1Y2 
Y
k
 ) { 
output the production X 
-+ YlY2 - .  
Yk; 
pop the stack; 
push Yk, 
Yk-1,. . . , 
Y
l
 onto the stack, with Yl on top; 
1 
set X to the top stack symbol; 
1 
t 
z 
- 
$ 
Figure 4.20: Predictive parsing algorithm 
- 
Example 4.35 : 
Consider grammar (4.28); we have already seen its the parsing 
table in Fig. 4.17. On input id + id * id, the nonrecursive predictive parser 
of Algorithm 4.34 makes the sequence of moves in Fig. 4.21. These moves 
correspond to a leftmost derivation (see Fig. 4.12 for the full derivation): 
Parsing 
Table 
M 
228 
CHAPTER 4. SYNTAX ANALYSIS 
output E -+ TE' 
output T -+ FT' 
output F -+ id 
match id 
output T' -+ E. 
output E' + 
+ TE' 
match + 
output T -+ FT' 
output F -+ id 
match id 
output T' -+ * FT' 
match * 
output F + 
id 
match id 
output T' -+ E 
output E' -+ 
E. 
Figure 4.21: Moves made by a predictive parser on input id + 
id * id 
Note that the sentential forms in this derivation correspond to the input that 
has already been matched (in column MATCHED) 
followed by the stack contents. 
The matched input is shown only to highlight the correspondence. For the same 
reason, the top of the stack is to the left; when we consider bottom-up parsing, 
it will be more natural to show the top of the stack to the right. The input 
pointer points to the leftmost symbol of the string in the INPUT 
column. 
4.4.5 
Error Recovery in Predictive Parsing 
This discussion of error recovery refers to the stack of a table-driven predictive 
parser, since it makes explicit the terminals and nonterminals that the parser 
hopes to match with the remainder of the input; the techniques can also be 
used with recursive-descent parsing. 
An error is detected during predictive parsing when the terminal on top of 
the stack does not match the next input symbol or when nonterminal A is on 
top of the stack, a is the next input symbol, and M[A, 
a] is error (i.e., the 
parsing-table entry is empty). 
Panic Mode 
Panic-mode error recovery is based on the idea of skipping symbols on the 
the input until a token in a selected set of synchronizing tokens appears. Its 
4.4. TOP-DO 
W N  
PARSING 
229 
effectiveness depends on the choice of synchronizing set. The sets should be 
chosen so that the parser recovers quickly from errors that are likely to occur 
in practice. Some heuristics are as follows: 
1. As a starting point, place all symbols in FOLLOW(A) 
into the synchro- 
nizing set for nonterminal A. If we skip tokens until an element of 
FOLLOW(A) is seen and pop A from the stack, it is likely that parsing 
can continue. 
It is not enough to use FOLLOW(A) 
as the synchronizing set for A. For 
example, if semicolons terminate statements, as in C, then keywords that 
begin statements may not appear in the FOLLOW set of the nontermi- 
nal representing expressions. A missing semicolon after an assignment 
may therefore result in the keyword beginning the next statement be- 
ing skipped. Often, there is a hierarchical structure on constructs in a 
language; for example, expressions appear within statements, which ap- 
pear within blocks, and so on. We can add to the synchronizing set of a 
lower-level construct the symbols that begin higher-level constructs. For 
example, we might add keywords that begin statements to the synchro- 
nizing sets for the nonterminals generating expressions. 
3. If we add symbols in FIRST(A) to the synchronizing set for nonterminal 
A, then it may be possible to resume parsing according to A if a symbol 
in FIRST(A) appears in the input. 
4. If a nonterminal can generate the empty string, then the production de- 
riving E can be used as a default. Doing so may postpone some error 
detection, but cannot cause an error to be missed. This approach reduces 
the number of nonterminals that have to be considered during error re- 
covery. 
5. If a terminal on top of the stack cannot be matched, a simple idea is to 
pop the terminal, issue a message saying that the terminal was inserted, 
and continue parsing. In effect, this approach takes the synchronizing set 
of a token to consist of all other tokens. 
Example 4.36 : Using FIRST and FOLLOW symbols as synchronizing tokens 
works reasonably well when expressions are parsed according to the usual gram- 
mar (4.28). The parsing table for this grammar in Fig. 4.17 is repeated in 
Fig. 4.22, with "synch" indicating synchronizing tokens obtained from the 
FOLLOW set of the nonterminal in question. The FOLLOW sets for the non- 
terminals are obtained from Example 4.30. 
The table in Fig. 4.22 is to be used as follows. If the parser looks up entry 
&![A, 
a] and finds that it is blank, then the input symbol a is skipped. If the 
entry is "synch," then the nonterminal on top of the stack is popped in an 
attempt to resume parsing. If a token on top of the stack does not match the 
input symbol, then we pop the token from the stack, as mentioned above. 
230 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.22: Synchronizing tokens added to the parsing table of Fig. 4.17 
N
O
N
 - 
TERMINAL 
E 
E' 
T 
T' 
F 
On the erroneous input ) id * +id, the parser and error recovery mechanism 
of Fig. 4.22 behave as in Fig. 4.23. 
STACK 
INPUT 
REMARK 
E $ 
) id * + 
id $ 
error, skip ) 
E $ 
id * + 
id $ 
id is in FIRST(E) 
TE'$ 
id * +id $ 
FT'E' $ 
id * + 
id $ 
id TIE'$ 
id * + 
id $ 
TIE' $ 
* + i d $  
* FT'E' $ 
* + i d $  
FT'E' $ 
+ 
id $ 
error, M 
[F, 
+] = synch 
TIE' $ 
+ 
id $ 
F has been popped 
E' $ 
+ 
id $ 
+ 
TE' $ 
+ 
id $ 
TE' $ 
id $ 
FT'E' $ 
id $ 
id TIE' $ 
id $ 
TIE' $ 
$ 
E' $ 
$ 
$ 
$ 
INPUT SYMBOL 
Figure 4.23: Parsing and error recovery moves made by a predictive parser 
id 
E -+ TE' 
T + 
FT' 
F + 
id 
The above discussion of panic-mode recovery does not address the important 
issue of error messages. The compiler designer must supply informative error 
messages that not only describe the error, they must draw attention to where 
the error was discovered. 
+ 
E + 
+TE1 
synch 
T' + 
E 
synch 
* 
T
'
 
--+ *FT' 
synch 
( 
E + 
TE' 
T + 
FT' 
F + 
( E )  
) 
synch 
E-+c 
synch 
T'+e 
synch 
$ 
synch 
E+c 
synch 
TI-+€ 
synch 
4.4. TOP-DO 
WN PARSING 
Phrase-level Recovery 
Phrase-level error recovery is implemented by filling in the blank entries in 
the predictive parsing table with pointers to error routines. These routines 
may change, insert, or delete symbols on the input and issue appropriate error 
messages. They may also pop from the stack. Alteration of stack symbols or the 
pushing of new symbols onto the stack is questionable for several reasons. First, 
the steps carried out by the parser might then not correspond to the derivation 
of any word in the language at all. Second, we must ensure that there is no 
possibility of an infinite loop. Checking that any recovery action eventually 
results in an input symbol being consumed (or the stack being shortened if the 
end of the input has been reached) is a good way to protect against such loops. 
4.4.6 Exercises for Section 4.4 
Exercise 4.4.1 : 
For each of the following grammars, devise predictive parsers 
and show the parsing tables. You may left-factor and/or eliminate left-recursion 
from your grammars first. 
a) The grammar of Exercise 4.2.2(a). 
b) The grammar of Exercise 4.2.2(b). 
c) The grammar of Exercise 4.2.2(c). 
d) The grammar of Exercise 4.2.2 
(d) 
. 
e) The grammar of Exercise 4.2.2(e). 
f) The grammar of Exercise 4.2.2(g). 
! 
! Exercise 4.4.2 : 
Is it possible, by modifying the grammar in any way, to con- 
struct a predictive parser for the language of Exercise 4.2.1 (postfix expressions 
with operand a)? 
Exercise 4.4.3 : Compute FIRST and FOLLOW for the grammar of Exercise 
4.2.1. 
Exercise 4.4.4: Compute FIRST and FOLLOW for each of the grammars of 
Exercise 4.2.2. 
Exercise 4.4.5 : The grammar S -+ a S a I a a generates all even-length 
strings of a's. We can devise a recursive-descent parser with backtrack for this 
grammar. If we choose to expand by production S -+ a a first, then we shall 
only recognize the string aa. Thus, any reasonable recursive-descent parser will 
try S -+ a S a first. 
a) Show that this recursive-descent parser recognizes inputs aa, aaaa, and 
aaaaaaaa, but not aaaaaa. 
232 
CHAPTER 4. SYNTAX ANALYSIS 
!! 
b) What language does this recursive-descent parser recognize? 
The following exercises are useful steps in the construction of a "Chomsky 
Normal Form" grammar from arbitrary grammars, as defined in Exercise 4.4.8. 
! 
Exercise 4.4.6: A grammar is €-free if no production body is E (called an 
E-production). 
a) Give an algorithm to convert any grammar into an €-free grammar that 
generates the same language (with the possible exception of the empty 
string - 
no E-free 
grammar can generate c ) .  
b) Apply your algorithm to the grammar S + 
aSbS I bSaS I E. Hint: First 
find all the nonterminals that are nullable, meaning that they generate E, 
perhaps by a long derivation. 
! Exercise 4.4.7: A single production is a production whose body is a single 
nonterminal, i.e., a production of the form A -+ A. 
a) Give an algorithm to convert any grammar into an €-free grammar, with 
no single productions, that generates the same language (with the possible 
exception of the empty string) Hint: First eliminate E-productions, and 
then find for which pairs of nonterminals A and B does A % B by a 
sequence of single productions. 
b) Apply your algorithm to the grammar (4.1) in Section 4.1.2. 
c) Show that, as a consequence of part (a), we can convert a grammar into 
an equivalent grammar that has no cycles (derivations of one or more 
steps in which A % A for some nonterminal A). 
!! Exercise 4.4.8 : 
A grammar is said to be in Chomsky Normal Form (CNF) if 
every production is either of the form A -+ 
BC or of the form A -+ a, where 
A, B, and C are nonterminals, and a is a terminal. Show how to convert 
any grammar into a CNF grammar for the same language (with the possible 
exception of the empty string - 
no CNF grammar can generate E). 
! 
Exercise 4.4.9 : 
Every language that has a context-free grammar can be rec- 
ognized in at most O(n3) time for strings of length n. A simple way to do so, 
called the Cocke- Younger-Kasami (or CYK) algorithm is based on dynamic pro- 
gramming. That is, given a string ala2 . 
- . 
a,, we construct an n-by-n table T 
i+l "'aj. 
such that Tij is the set of nonterminals that generate the substring a -a 
If the underlying grammar is in CNF (see Exercise 4.4.8), then one table entry 
can be filled in in O(n) time, provided we fill the entries in the proper order: 
lowest value of j 
- 
i first. Write an algorithm that correctly fills in the entries 
of the table, and show that your algorithm takes O(n3) 
time. Having filled in 
the table, how do you determine whether ala2 . 
. . 
a, is in the language? 
4.5. BOTTOM-UP PARSING 
233 
! Exercise 4.4.10: Show how, having filled in the table as in Exercise 4.4.9, 
we can in O(n) time recover a parse tree for alaz - - - a,. 
Hint: modify the 
table so it records, for each nonterminal A in each table entry Tij, some pair of 
nonterminals in other table entries that justified putting A in Tij. 
! 
Exercise 4.4.11 : 
Modify your algorithm of Exercise 4.4.9 so that it will find, 
for any string, the smallest number of insert, delete, and mutate errors (each 
error a single character) needed to turn the string into a string in the language 
of the underlying grammar. 
stmt 
+ 
I 
I 
stmt Tail 
--+ 
I 
list 
+ 
list 
Tail 
+ 
--+ 
if e then stmt stmt Tail 
while e do stmt 
begin list end 
S 
else stmt 
€ 
stmt list Tail 
; list 
€ 
Figure 4.24: A grammar for certain kinds of statements 
! 
Exercise 4.4.12 : 
In Fig. 4.24 is a grammar for certain statements. You may 
take e and s to be terminals standing for conditional expressions and "other 
statements," respectively. If we resolve the conflict regarding expansion of 
the optional "else" (nonterminal stmtTail) by preferring to consume an else 
from the input whenever we see one, we can build a predictive parser for this 
grammar. Using the idea of synchronizing symbols described in Section 4.4.5: 
a) Build an error-correcting predictive parsing table for the grammar. 
b) Show the behavior of your parser on the following inputs: 
(i) 
if e then s ; if e then s end 
(ii) while e do begin s ; if e then s ; end 
4.5 
Bottom-Up Parsing 
A bottom-up parse corresponds to the construction of a parse tree for an input 
string beginning at the leaves (the bottom) and working up towards the root 
(the top). It is convenient to describe parsing as the process of building parse 
trees, although a front end may in fact carry out a translation directly without 
building an explicit tree. The sequence of tree snapshots in Fig. 4.25 illustrates 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.25: A bottom-up parse for id * id 
a bottom-up parse of the token stream id * id, with respect to the expression 
grammar (4.1). 
This section introduces a general style of bottom-up parsing known as shift- 
reduce parsing. The largest class of grammars for which shift-reduce parsers can 
be built, the LR grammars, will be discussed in Sections 4.6 and 4.7. Although 
it is too much work to build an LR parser by hand, tools called automatic parser 
generators make it easy to construct efficient LR parsers from suitable gram- 
mars. The concepts in this section are helpful for writing suitable grammars 
to make effective use of an LR parser generator. Algorithms for implementing 
parser generators appear in Section 4.7. 
4
.
5
.
1
 Reductions 
We can think of bottom-up parsing as the process of "reducing" a string w to 
the start symbol of the grammar. At each reduction step, a specific substring 
matching the body of a production is replaced by the nonterminal at the head 
of that production. 
The key decisions during bottom-up parsing are about when to reduce and 
about what production to apply, as the parse proceeds. 
Example 4.37 : 
The snapshots in Fig. 4.25 illustrate a sequence of reductions; 
the grammar is the expression grammar (4.1). The reductions will be discussed 
in terms of the sequence of strings 
id * id, F * id, T * id, T * F, T, E 
The strings in this sequence are formed from the roots of all the subtrees in the 
snapshots. The sequence starts with the input string id*id. The first reduction 
produces F 
* 
id by reducing the leftmost id to F ,  
using the production F -+ id. 
The second reduction produces T * id by reducing F to T. 
Now, we have a choice between reducing the string T, which is the body 
of E -+ T, and the string consisting of the second id, which is the body of 
F -+ id. Rather than reduce T to E, the second id is reduced to T ,  resulting 
in the string T * F .  This string then reduces to T. The parse completes with 
the reduction of T to the start symbol E. 
4.5. BOTTOM- UP PARSING 
235 
By definition, a reduction is the reverse of a step in a derivation (recall that 
in a derivation, a nonterminal in a sentential form is replaced by the body of 
one of its productions). The goal of bottom-up parsing is therefore to construct 
a derivation in reverse. The following derivation corresponds to the parse in 
Fig. 4.25: 
This derivation is in fact a rightmost derivation. 
4
.
5
.
2
 Handle Pruning 
Bottom-up parsing during a left-to-right scan of the input constructs a right- 
most derivation in reverse. Informally, a "handle" is a substring that matches 
the body of a production, and whose reduction represents one step along the 
reverse of a rightmost derivation. 
For example, adding subscripts to the tokens id for clarity, the handles 
during the parse of idl * id2 according to the expression grammar (4.1) are as 
in Fig. 4.26. Although T is the body of the production E --+ T, 
the symbol T is 
not a handle in the sentential form T * id2. If T were indeed replaced by E, we 
would get the string E * id2, 
which cannot be derived from the start symbol E. 
Thus, the leftmost substring that matches the body of some production need 
not be a handle. 
Figure 4.26: Handles during a parse of idl * id2 
Formally, if S %- aAw 
* apw, 
as in Fig. 4.27, then production A --+ ,6 
r m  
rm 
in the position following a 
is a handle of apw. 
Alternatively, a handle of a 
right-sentential form y 
is a production A -+ 
,6 and a position of y 
where the 
string p 
may be found, such that replacing ,
6
 at that position by A produces 
the previous right-sentential form in a rightmost derivation of y. 
Notice that the string w 
to the right of the handle must contain only terminal 
symbols. For convenience, 
we refer to 
the body ,6 rather than A --+ ,6 as a handle. 
Note we say "a handle" rather than "the handle," because the grammar could 
be ambiguous, with more than one rightmost derivation of apw. 
If a grammar 
is unambiguous, then every right-sentential form of the grammar has exactly 
one handle. 
A rightmost derivation in reverse can be obtained by "handle pruning." 
That is, we start with a string of terminals w 
to be parsed. If w 
is a sentence 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.27: A handle A -+ P in the parse tree for apw 
of the grammar at hand, then let w = 
yn, where y
,
 
is the nth right-sentential 
form of some as yet unknown rightmost derivation 
To reconstruct this derivation in reverse order, we locate the handle Pn in 
yn and replace Pn by the head of the relevant production A, -+ ,On to obtain 
the previous right-sentential form ?,-I. 
Note that we do not yet know how 
handles are to be found, but we shall see methods of doing so shortly. 
We then repeat this process. That is, we locate the handle Pn-l in 7,-1 and 
reduce this handle to obtain the right-sentential form ~ ~ - 2 .  
If by continuing this 
process we produce a right-sentential form consisting only of the start symbol 
S, 
then we halt and announce successful completion of parsing. The reverse of 
the sequence of productions used in the reductions is a rightmost derivation for 
the input string. 
4.5.3 Shift-Reduce Parsing 
Shift-reduce parsing is a form of bottom-up parsing in which a stack holds 
grammar symbols and an input buffer holds the rest of the string to be parsed. 
As we shall see, the handle always appears at the top of the stack just before 
it is identified as the handle. 
We use $ to mark the bottom of the stack and also the right end of the 
input. Conventionally, when discussing bottom-up parsing, we show the top of 
the stack on the right, rather than on the left as we did for top-down parsing. 
Initially, the stack is empty, and the string w is on the input, as follows: 
During a left-to-right scan of the input string, the parser shifts zero or more 
input symbols onto the stack, until it is ready to reduce a string P of grammar 
symbols on top of the stack. It then reduces ,
O
 to the head of the appropriate 
production. The parser repeats this cycle until it has detected an error or until 
the stack contains the start symbol and the input is empty: 
4,5. BOTTOM-UP PARSING 
Upon entering this configuration, the parser halts and announces successful 
completion of parsing. Figure 4.28 steps through the actions a shift-reduce 
parser might take in parsing the input string idl 
*id2 
according to 
the expression 
grammar (4.1). 
shift 
reduce by F -+ id 
reduce by T -+ F 
shift 
shift 
reduce by F -+ id 
reduce by T -+ T * F 
reduce by E -+ 
T 
accept 
Figure 4.28: Configurations of a shift-reduce parser on input idl*id2 
While the primary operations are shift and reduce, there are actually four 
possible actions a shift-reduce parser can make: (1) shift, (2) reduce, (3) accept, 
and (4) error. 
1. S
h
i
f
t
.
 
Shift the next input symbol onto the top of the stack. 
2. Reduce. The right end of the string to be reduced must be at the top of 
the stack. Locate the left end of the string within the stack and decide 
with what nonterminal to replace the string. 
3. Accept. Announce successful completion of parsing. 
4. Error. Discover a syntax error and call an error recovery routine. 
The use of a stack in shift-reduce parsing is justified by an important fact: 
the handle will always eventually appear on top of the stack, never inside. This 
fact can be shown by considering the possible forms of two successive steps 
in any rightmost derivation. Figure 4.29 illustrates the two possible cases. In 
case (I), A is replaced by PBg, and then the rightmost nonterminal B in the 
body PBy is replaced by y. In case (2), A is again expanded first, but this time 
the body is a string y of terminals only. The next rightmost nonterminal B will 
be somewhere to the left of y. 
In other words: 
(1) S % aAz + aPByz =+ aPyyz 
r
*
m
 
r m  
r m  
(2) 
S + aBxAz + aBxyz +- ayxyz 
r m  
r m  
r m  
CHAPTER 4. SYNTAX ANALYSIS 
a
P
Y
Y
z
 
Case (1) 
a
y
x
y
z
 
Case (2) 
Figure 4.29: Cases for two successive steps of a rightmost derivation 
Consider case (1) in reverse, where a shift-reduce parser has just reached the 
configuration 
The parser reduces the handle y 
to B to reach the configuration 
The parser can now shift the string y onto the stack by a sequence of zero or 
more shift moves to reach the configuration 
with the handle P
B
y
 
on top of the stack, and it gets reduced to A. 
Now consider case (2). In configuration 
the handle y 
is on top of the stack. After reducing the handle y
 
to B, the parser 
can shift the string xy 
to get the next handle y on top of the stack, ready to be 
reduced to A: 
In both cases, after making a reduction the parser had to shift zero or more 
symbols to get the next handle onto the stack. It never had to go into the stack 
to find the handle. 
4.5.4 Conflicts During Shift-Reduce Parsing 
There are context-free grammars for which shift-reduce parsing cannot be used. 
Every shift-reduce parser for such a grammar can reach a configuration in which 
the parser, knowing the entire stack contents and the next input symbol, cannot 
decide whether to shift or to reduce (a shift/reduce conflict), or cannot decide 
4.5. BOTTOM-UP PARSING 
239 
which of several reductions to make (a reduce/reduce conflict). We now give 
some examples of syntactic constructs that give rise to such grammars. Techni- 
cally, these grammars are not in the LR(k) class of grammars defined in Section 
4.7; we refer to them as non-LR grammars. The k in LR(k) refers to the number 
of symbols of lookahead on the input. Grammars used in compiling usually fall 
in the LR(1) class, with one symbol of lookahead at most. 
Example 4.38 : 
An ambiguous grammar can never be LR. For example, con- 
sider the dangling-else grammar (4.14) of Section 4.3: 
stmt + if expr then stmt 
I 
if expr then stmt else stmt 
I 
other 
If we have a shift-reduce parser in configuration 
STACK 
. 
. if expr then stmt 
INPUT 
else . . . 
$ 
we cannot tell whether if expr then stmt is the handle, no matter what appears 
below it on the stack. Here there is a shiftlreduce conflict. Depending on what 
follows the else on the input, it might be correct to reduce if expr then stint 
to stmt, or it might be correct to shift else and then to look for another stmt 
to complete the alternative if expr then stmt else stmt. 
Note that shift-reduce parsing can be adapted to parse certain ambigu- 
ous grammars, such as the if-then-else grammar above. If we resolve the 
shiftlreduce conflict on else in favor of shifting, the parser will behave as we 
expect, associating each else with the previous unmatched then. We discuss 
parsers for such ambiguous grammars in Section 4.8. 
Another common setting for conflicts occurs when we know we have a han- 
dle, but the stack contents and the next input symbol are insufficient to de- 
termine which production should be used in a reduction. The next example 
illustrates this situation. 
Example 4.39: Suppose we have a lexical analyzer that returns the token 
name id for all names, regardless of their type. Suppose also that our lan- 
guage invokes procedures by giving their names, with parameters surrounded 
by parentheses, and that arrays are referenced by the same syntax. Since the 
translation of indices in array references and parameters in procedure calls 
are different, we want to use different productions to generate lists of actual 
parameters and indices. Our grammar might therefore have (among others) 
productions such as those in Fig. 4.30. 
A statement beginning with p ( i ,  
j) would appear as the token stream 
id(id, id) to the parser. After shifting the first three tokens onto the stack, 
a shift-reduce parser would be in configuration 
CHAPTER 4. SYNTAX ANALYSIS 
stmt 
stmt 
parameter-list 
parameter-list 
parameter 
expr 
expr 
expr-list 
expr-list 
id ( parameter-list ) 
expr := expr 
parameter-list , 
parameter 
parameter 
id 
id ( expr-list ) 
id 
expr-list , 
expr 
expr 
Figure 4.30: Productions involving procedure calls and array references 
It is evident that the id on top of the stack must be reduced, but by which 
production? The correct choice is production (5) if p  is a procedure, but pro- 
duction (7) if p is an array. The stack does not tell which; information in the 
symbol table obtained from the declaration of p  must be used. 
One solution is to change the token id in production (1) to procid and to 
use a more sophisticated lexical analyzer that returns the token name procid 
when it recognizes a lexeme that is the name of a procedure. Doing so would 
require the lexical analyzer to consult the symbol table before returning a token. 
If we made this modification, then on processing p ( i ,  
j) the parser would 
be either in the configuration 
STACK 
. 
. procid ( id 
or in the configuration above. In the former case, we choose reduction by 
production (5); 
in the latter case by production (7). Notice how the symbol 
third from the top of the stack determines the reduction to be made, even 
though it is not involved in the reduction. Shift-reduce parsing can utilize 
information far down in the stack to guide the parse. 
4.5.5 Exercises for Section 4.5 
Exercise 4.5.1: For the grammar S -+ 0 S 1 I 0 1 of Exercise 4.2.2(a), 
indicate the handle in each of the folhwing right-sentential forms: 
Exercise 4.5.2 : 
Repeat Exercise 4.5.1 for the grammar S -+ S S + I S S * I a 
of Exercise 4.2.1 and the following right-sentential forms: 
4.6. INTRODUCTION TO LR PARSING: SIMPLE LR 
Exercise 4.5.3 : Give bottom-up parses for the following input strings and 
grammars: 
a) The input 000111 according to the grammar of Exercise 4.5.1. 
b) The input aaa * a + + according to the grammar of Exercise 4.5.2. 
4.6 
Introduction to LR Parsing: Simple LR 
The most prevalent type of bottom-up parser today is based on a concept called 
LR(k) parsing; the "L" is for left-to-right scanning of the input, the "R" for 
constructing a rightmost derivation in reverse, and the k for the number of 
input symbols of lookahead that are used in making parsing decisions. The 
cases k = 0 or k = 1 are of practical interest, and we shall only consider LR 
parsers with k 5 1 
here. When (k) is omitted, k is assumed to be 1. 
This section introduces the basic concepts of LR parsing and the easiest 
method for constructing shift-reduce parsers, called "simple LR" (or SLR, for 
short). Some familiarity with the basic concepts is helpful even if the LR 
parser itself is constructed using an automatic parser generator. We begin with 
"items" and "parser states;" the diagnostic output from an LR parser generator 
typically includes parser states, which can be used to isolate the sources of 
parsing conflicts. 
Section 4.7 introduces two, more complex methods - 
canonical-LR and 
LALR - 
that are used in the majority of LR parsers. 
4.6.1 Why LR Parsers? 
LR parsers are table-driven, much like the nonrecursive LL parsers of Sec- 
tion 4.4.4. A grammar for which we can construct a parsing table using one of 
the methods in this section and the next is said to be an LR grammar. Intu- 
itively, for a grammar to be LR it is sufficient that a left-to-right shift-reduce 
parser be able to recognize handles of right-sentential forms when they appear 
on top of the stack. 
LR parsing is attractive for a variety of reasons: 
LR parsers can be constructed to recognize virtually all programming- 
language constructs for which context-free grammars can be written. Non- 
LR context-free grammars exist, but these can generally be avoided for 
typical programming-language constructs. 
242 
CHAPTER 4. SYNTAX ANALYSIS 
The LR-parsing method is the most general nonbacktracking shift-reduce 
parsing method known, yet it can be implemented as efficiently as other, 
more primitive shift-reduce methods (see the bibliographic notes). 
An LR parser can detect a syntactic error as soon as it is possible to do 
so on a left-to-right scan of the input. 
The class of grammars that can be parsed using LR methods is a proper 
superset of the class of grammars that can be parsed with predictive or 
LL methods. For a grammar to be LR(k), we must be able to recognize 
the occurrence of the right side of a production in a right-sentential form, 
with k input symbols of lookahead. This requirement is far less stringent 
than that for LL(k) grammars where we must be able to recognize the 
use of a production seeing only the first k symbols of what its right side 
derives. Thus, it should not be surprising that LR grammars can describe 
more languages than LL grammars. 
The principal drawback of the LR method is that it is too much work to 
construct an LR parser by hand for a typical programming-language grammar. 
A specialized tool, an LR parser generator, is needed. Fortunately, many such 
generators are available, and we shall discuss one of the most commonly used 
ones, Yacc, in Section 4.9. Such a generator takes a context-free grammar and 
automatically produces a parser for that grammar. If the grammar contains 
ambiguities or other constructs that are difficult to parse in a left-to-right scan 
of the input, then the parser generator locates these constructs and provides 
detailed diagnostic messages. 
4.6.2 
Items and the LR(0) Automaton 
How does a shift-reduce parser know when to shift and when to reduce? For 
example, with stack contents $ T and next input symbol * in Fig. 4.28, how 
does the parser know that T on the top of the stack is not a handle, so the 
appropriate action is to shift and not to reduce T to E? 
An LR parser makes shift-reduce decisions by maintaining states to keep 
track of where we are in a parse. States represent sets of "items." An LR(0) 
item (item for short) of a grammar G is a production of G with a dot at some 
position of the body. Thus, production A -+ 
XYZ yields the four items 
The production A -+ 
E. generates only one item, A -+ - . 
Intuitively, an item indicates how much of a production we have seen at a 
given point in the parsing process. For example, the item A -+ 
.XYZ indicates 
that we hope to see a string derivable from XYZ next on the input. Item 
4.6. INTRODUCTION T O  LR PARSING: SIMPLE LR 
243 
Representing Item Sets 
A parser generator that produces a bottom-up parser may need to rep- 
resent items and sets of items conveniently. Note that an item can be 
represented by a pair of integers, the first of which is the number of one 
of the productions of the underlying grammar, and the second of which is 
the position of the dot. Sets of items can be represented by a list of these 
pairs. However, as we shall see, the necessary sets of items often include 
"closure" items, where the dot is at the beginning of the body. These can 
always be reconstructed from the other items in the set, and we do not 
have to include them in the list. 
A -+ X-YZ 
indicates that we have just seen on the input a string derivable from 
X and that we hope next to see a string derivable from Y 
2. 
Item A -+ XY 
Z. 
indicates that we have seen the body XYZ and that it may be time to reduce 
XYZ to A. 
One collection of sets of LR(0) items, called the canonical LR(0) collection, 
provides the basis for constructing a deterministic finite automaton that is used ' 
to make parsing decisions. Such an automaton is called an LR(0) aut~maton.~ 
In particular, each state of the LR(0) automaton represents a set of items in 
the canonical LR(0) collection. The automaton for the expression grammar 
(4.1), shown in Fig. 4.31, will serve as the running example for discussing the 
canonical LR(0) collection for a grammar. 
To construct the canonical LR(0) collection for a grammar, we define an 
augmented grammar and two functions, CLOSURE and GOTO. If G is a grammar 
with start symbol S, 
then G', the augmented grammar for G, is G with a new 
start symbol St and production S' -+ S. The purpose of this new starting 
production is to indicate to the parser when it should stop parsing and announce 
acceptance of the input. That is, acceptance occurs when and only when the 
parser is about to reduce by St 
-+ S. 
Closure of Item Sets 
If I is a set of items for a grammar G, then CLOSURE(I) 
is the set of items 
constructed from I by the two rules: 
1. Initially, add every item in I to CLOSURE(I). 
2. If A -+ a-BP 
is in CLOSURE(I) 
and B -+ y 
is a production, then add the 
item B -+ .y 
to CLOSURE(I), 
if it is not already there. Apply this rule 
until no more new items can be added to CLOSURE(I). 
3~echnically, 
the automaton misses being deterministic according to the definition of Sec- 
tion 3.6.4, because we do not have a dead state, corresponding to the empty set of items. As 
a result, there are some state-input pairs for which no next state exists. 
244 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.31: LR(0) automaton for the expression grammar (4.1) 
Intuitively, A + 
cr-BP in CLOSURE(I) 
indicates that, at some point in the 
parsing process, we think we might next see a substring derivable from BP 
as input. The substring derivable from BP will have a prefix derivable from 
B by applying one of the B-productions. We therefore add items for all the 
B-productions; that is, if B + 
y 
is a production, we also include B -+ .
y
 
in 
CLOSURE(I). 
Example 4.40 : 
Consider the augmented expression grammar: 
E' + E 
E 
-+ 
E + T  ( T  
T + T * F  1 F 
E 
-+ 
(E) I id 
If I is the set of one item {[E' -+ .El}, then CLOSURE(I) 
contains the set 
of items I. in Fig. 4.31. 
4.6. INTRODUCTION T O  LR PARSING: SIMPLE LR 
245 
To see how the closure is computed, E' -+ -E is put in CLOSURE(I) 
by 
rule (1). Since there is an E immediately to the right of a dot, we add the 
E-productions with dots at the left ends: E -+ .E + 
T and E -+ ST. Now there 
is a T immediately to the right of a dot in the latter item, so we add T -+ ST 
* 
F 
and T -+ .F. Next, the F to the right of a dot forces us to add F + 
.(E) 
and 
F -+ -id, 
but no other items need to be added. 
0 
The closure can be computed as in Fig. 4.32. A convenient way to imple- 
ment the function closure is to keep a boolean array added, indexed by the 
nonterminals of G, such that added[B] is set to true if and when we add the 
item B -+ .
y
 
for each B-production B -+ 
y. 
SetOfItems CLOSURE(I) 
{ 
J = 
I ;  
repeat 
for ( each item A -+ a.BP in J ) 
for ( each production B -+ y 
of G ) 
if ( B -+ .
y
 
is not in J ) 
add B -+ .
y
 
to J; 
until no more items are added to J on one round; 
return J; 
Figure 4.32: Computation of CLOSURE 
Note that if one B-production is added to the closure of I with the dot at the 
left end, then all B-productions will be similarly added to the closure. Hence, 
it is not necessary in some circumstances actually to list the items B -+ - y 
added to I by CLOSURE. A list of the nonterminals B whose productions were 
so added will suffice. We divide all the sets of items of interest into two classes: 
1. Kernel items: the initial item, S' -+ .S, and all items whose dots are not 
at the left end. 
2. Nonkernel items: all items with their dots at the left end, except for 
S' -+ .S. 
Moreover, each set of items of interest is formed by taking the closure of a set 
of kernel items; the items added in the closure can never be kernel items, of 
course. Thus, we can represent the sets of items we are really interested in 
with very little storage if we throw away all nonkernel items, knowing that they 
could be regenerated by the closure process. In Fig. 4.31, nonkernel items are 
in the shaded part of the box for a state. 
246 
CHAPTER 4. SYNTAX ANALYSIS 
The Function GOT0 
The second useful function is GOTO(I, 
X) where I is a set of items and X is a 
grammar symbol. GOTO(I, 
X) is defined to be the closure of the set of all items 
[A -+ ax.,8] 
such that [A --+ a . xP] 
is in I. Intuitively, the GOT0 function 
is used to define the transitions in the LR(0) automaton for a grammar. The 
states of the automaton correspond to sets of items, and GOTO(& 
X )  specifies 
the transition from the state for I under input X. 
Example 4.41 : 
If I is the set of two items {[El + 
E.], [E 
-+ E. + 
TI), then 
GOTO(I, 
+) contains the items 
We computed GOTO(I, 
+) by examining I 
for items with + immediately to 
the right of the dot. El -+ E- is not such an item, but E -+ E- 
+ 
T is. We 
moved the dot over the + to get E -+ E + 
ST 
and then took the closure of this 
singleton set. 
We are now ready for the algorithm to construct C, the canonical collection 
of sets of LR(0) items for an augmented grammar GI - 
the algorithm is shown 
in Fig. 4.33. 
void iterns(G1) { 
C = 
CLOSURE({[S' 
-+ .S])); 
repeat 
for ( each set of items I in C ) 
for ( each grammar symbol X ) 
if ( GOTO(& 
X) is not empty and not in C ) 
add GOTO(I, 
X) to C; 
until no new sets of items are added to C on a round; 
1 
Figure 4.33: Computation of the canonical collection of sets of LR(0) items 
Example 4.42 : 
The canonical collection of sets of LR(0) items for grammar 
(4.1) and the GOTO function are shown in Fig. 4.31. GOTO is encoded by the 
transitions in the figure. 
4.6. INTRODUCTION T O  L R  PARSING: SIMPLE L R  
247 
Use of the LR(0) Automaton 
The central idea behind "Simple LR," or SLR, parsing is the construction from 
the grammar of the LR(0) automaton. The states of this automaton are the 
sets of items from the canonical LR(0) collection, and the transitions are given 
by the GOTO function. The LR(0) automaton for the expression grammar (4.1) 
appeared earlier in Fig. 4.31. 
The start state of the LR(0) automaton is CLOSURE({[S' 
-+ .S]}), where S' 
is the start symbol of the augmented grammar. All states are accepting states. 
We say "state j" to refer to the state corresponding to the set of items Ij. 
How can LR(0) automata help with shift-reduce decisions? Shift-reduce 
decisions can be made as follows. Suppose that the string y 
of grammar symbols 
takes the LR(0) automaton from the start state 0 to some state j. Then, shift 
on next input symbol a if state j has a transition on a. Otherwise, we choose 
to reduce; the items in state j will tell us which production to use. 
The LR-parsing algorithm to be introduced in Section 4.6.3 uses its stack to 
keep track of states as well as grammar symbols; in fact, the grammar symbol 
can be recovered from the state, so the stack holds states. The next example 
gives a preview of how an LR(0) automaton and a stack of states can be used 
to make shift-reduce parsing decisions. 
Example 4.43 : 
Figure 4.34 illustrates the actions of a shift-reduce parser on 
input id *id, using the LR(0) automaton in Fig. 4.31. We use a stack to hold 
states; for clarity, the grammar symbols corresponding to the states on the 
stack appear in column SYMBOLS. 
At line (I), 
the stack holds the start state 0 
of the automaton; the corresponding symbol is the bottom-of-stack marker $. 
Figure 4.34: The parse of id * id 
LINE 
(1) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
(8) 
(9) 
The next input symbol is id and state 0 has a transition on id to state 5. 
We therefore shift. At line (2), state 5 (symbol id) has been pushed onto the 
stack. There is no transition from state 5 on input *, so we reduce. From item 
[F 
--+ id-] 
in state 5, the reduction is by production F -+ 
id. 
STACK 
0 
0 5 
0 3 
0 2 
0 2 7  
0 2 7 5  
02710 
0 2 
0 1 
SYMBOLS 
$ 
$ id 
$ F 
$ T 
$ T *  
$ T * i d  
$ T * F  
$ T  
$ E 
INPUT 
i d * i d $  
* id $ 
* id $ 
* i d $  
id $ 
$ 
$ 
$ 
$ 
ACTION 
shift to 5 
reduce by F -+ id 
reduce by T -+ 
F 
shift to 7 
shift to 5 
reduce by F --+ id 
reduce by T --+ T * F 
reduce by E -+ 
T 
accept 
CHAP?IER 4. SYNTAX ANALYSIS 
With symbols, a reduction is implemented by popping the body of the pro- 
duction from the stack (on line (2), the body is id) and pushing the head of 
the production (in this case, F). With states, we pop state 5 for symbol id, 
which brings state 0 to the top and look for a transition on F, 
the head of the 
production. In Fig. 4.31, state 0 has a transition on F to state 3, so we push 
state 3, with corresponding symbol F; 
see line (3). 
As another example, consider line (5), with state 7 (symbol *) on top of the 
stack. This state has a transition to state 5 on input id, so we push state 5 
(symbol id). State 5 has no transitions, so we reduce by F -+ 
id. When we 
pop state 5 for the body id, state 7 comes to the top of the stack. Since state 7 
has a transition on F to state 10, we push state 10 (symbol F). 
4.6.3 
The LR-Parsing Algorithm 
A schematic of an LR parser is shown in Fig. 4.35. It consists of an input, 
an output, a stack, a driver program, and a parsing table that has two pasts 
(ACTION 
and GOTO). 
The driver program is the same for all LR parsers; only 
the parsing table changes from one parser to another. The parsing program 
reads characters from an input buffer one at a time. Where a shift-reduce parser 
would shift a symbol, an LR parser shifts a state. Each state summarizes the 
information contained in the stack below it. 
Input 
t
l
 
Stack 
Output 
sm 
S m -  1 
Figure 4.35: Model of an LR parser 
The stack holds a sequence of states, sosl . 
. . 
s
,
,
 
where s
,
 
is on top. In the 
SLR method, the stack holds states from the LR(0) automaton; the canonical- 
LR and LALR methods are similar. By construction, each state has a corre- 
sponding grammar symbol. Recall that states correspond to sets of items, and 
that there is a transition from state i to state j if GOTO(I~, 
X) = Ij. All tran- 
sitions to state j must be for the same grammar symbol X. Thus, each state, 
except the start state 0, has a unique grammar symbol associated with it.4 
LR 
Parsing 
Program 
4 ~ h e  
converse need not hold; that is, more than one state may have the same grammar 
+ 
... 
$ 
ACTION 
GOT0 
4.6. INTRODUCTION TO LR PARSING: SIMPLE LR 
Structure of the LR Parsing Table 
The parsing table consists of two parts: a parsing-action function ACTION and 
a goto function GOTO. 
1. The ACTION function takes as arguments a state i and a terminal a (or 
$, the input endmarker). The value of  ACTION[^, a] 
can have one of four 
forms: 
(a) Shift j, where j is a state. The action taken by the parser effectively 
shifts input a to the stack, but uses state j to represent a. 
(b) Reduce A -+ 
P. The action of the parser effectively reduces P on the 
top of the stack to head A. 
(c) Accept. The parser accepts the input and finishes parsing. 
(d) Error. The parser discovers an error in its input and takes some 
corrective action. We shall have more to say about how such error- 
recovery routines work in Sections 4.8.3 and 4.9.4. 
2. We extend the GOTO function, defined on sets of items, to states: if 
GOTO[I~, 
A] = Ij, 
then GOT0 also maps a state i and a nonterminal A to 
state j. 
LR-Parser Configurations 
To describe the behavior of an LR parser, it helps to have a notation repre- 
senting the complete state of the parser: its stack and the remaining input. A 
configuration of an LR parser is a pair: 
where the first component is the stack contents (top on the right), and the 
second component is the remaining input. This configuration represents the 
right-sentential form 
in essentially the same way as a shift-reduce parser would; the only difference is 
that instead of grammar symbols, the stack holds states from which grammar 
symbols can be recovered. That is, Xi is the grammar symbol represented 
by state si. Note that so, the start state of the parser, does not represent a 
grammar symbol, and serves as a bottom-of-stack marker, as well as playing an 
important role in the parse. 
symbol. See for example states 1 and 8 in the LR(0) automaton in Fig. 4.31, which are both 
entered by transitions on E, or states 2 and 9, which are both entered by transitions on T. 
CHAPTER 4. SYNTAX ANALYSIS 
Behavior of the LR Parser 
The next move of the parser from the configuration above is determined by 
reading ai, the current input symbol, and s,, 
the state on top of the stack, 
and then consulting the entry ACTION[S, , 
ail in the parsing action table. The 
configurations resulting after each of the four types of move are as follows 
1. If ACTION[S,, ail = 
shift s, the parser executes a shift move; it shifts the 
next state s onto the stack, entering the configuration 
The symbol ai need not be held on the stack, since it can be recovered 
from s, if needed (which in practice it never is). The current input symbol 
is now Ui+l. 
2. If ACTION[S,, ail = reduce A -+ P, then the parser executes a reduce 
move, entering the configuration 
where r is the length of P, and s = GOTO[S,-,, A]. Here the parser 
first popped r state symbols off the stack, exposing state s,-,. 
The 
parser then pushed s, the entry for GOTO[S,-,, 
A], onto the stack. The 
current input symbol is not changed in a reduce move. For the LR parsers 
we shall construct, Xm-T+l . 
. 
X,, 
the sequence of grammar symbols 
corresponding to the states popped off the stack, will always match P, 
the right side of the reducing production. 
The output of an LR parser is generated after a reduce move by executing 
the semantic action associated with the reducing production. For the time 
being, we shall assume the output consists of just printing the reducing 
production. 
3. If ACTION[S,, ail = accept, parsing is completed. 
4. I
f
 ACTION[S,, 
ail = 
error, the parser has discovered an error and calls an 
error recovery routine. 
The LR-parsing algorithm is summarized below. All LR parsers behave 
in this fashion; the only difference between one LR parser and another is the 
information in the ACTION and GOT0 fields of the parsing table. 
Algorithm 4.44 : 
LR-parsing algorithm. 
INPUT: An input string w and an LR-parsing table with functions ACTION and 
GOT0 for a grammar G. 
4.6. INTRODUCTION TO LR PARSING: SIMPLE LR 
251 
OUTPUT: If w is in L(G), 
the reduction steps of a bottom-up parse for w; 
otherwise, an error indication. 
METHOD: Initially, the parser has so on its stack, where so is the initial state, 
and w$ in the input buffer. The parser then executes the program in Fig. 4.36. 
let a be the first symbol of w$; 
while(1) { /* repeat forever */ 
let s be the state on top of the stack; 
if ( ACTION[S, a] = shift t ) { 
push t onto the stack; 
let a be the next input symbol; 
} else if ( ACTION[S, a] = 
reduce A -+ ,
O
 ) { 
pop I,OI symbols off the stack; 
let state t now be on top of the stack; 
push GO TO[^, A] onto the stack; 
output the production A -+ p; 
) else if ( ACTION[S, a] = accept ) break; /* 
parsing is done */ 
else call error-recovery routine; 
} 
Figure 4.36: LR-parsing program 
Example 4.45: Figure 4.37 shows the ACTION and GOT0 functions of an 
LR-parsing table for the expression grammar (4.1), repeated here with the 
productions numbered: 
The codes for the actions are: 
1. si means shift and stack state i, 
2. r j  
means reduce by the production numbered j, 
3. acc means accept, 
4. blank means error. 
Note that the value of GOTO[S, a] for terminal a is found in the ACTION 
field connected with the shift action on input a for state s. The GOTO field 
gives GOTO[S, 
A] for nonterminals A. Although we have not yet explained how 
the entries for Fig. 4.37 were selected, we shall deal with this issue shortly. 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.37: Parsing table for expression grammar 
STATE 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
On input id * id + 
id, the sequence of stack and input contents is shown 
in Fig. 4.38. Also shown for clarity, are the sequences of grammar symbols 
corresponding to the states held on the stack. For example, at line (1) 
the LR 
parser Is in state 0, the initial state with no grammar symbol, and with id the 
first input symbol. The action in row 0 and column id of the action field of 
Fig. 4.37 is s5, meaning shift by pushing state 5. That is what has happened at 
line (2): the state symbol 5 has been pushed onto the stack, and id has been 
removed from the input. 
Then, * becomes the current input symbol, and the action of state 5 on input 
* is to reduce by F -+ id. One state symbol is popped off the stack. State 0 
is then exposed. Since the goto of state 0 on F is 3, state 3 is pushed onto the 
stack. We now have the configuration in line (3). Each of the remaining moves 
is determined similarly. 
4.6.4 Constructing SLR-Parsing Tables 
ACTION 
i
d
+
*
 (
)
 $ 
s5 
s4 
s6 
acc 
r2 
s7 
r2 
r2 
r4 
r4 
r4 
r4 
s5 
s4 
r6 
r6 
r6 
r6 
s5 
s4 
s5 
s4 
s 
6 
s l l  
r l  s7 
r l  
r l  
r3 r3 
r3 
r3 
r5 
r5 
r5 
r5 
The SLR method for constructing parsing tables is a good starting point for 
studying LR parsing. We shall refer to the parsing table constructed by this 
method as an SLR table, and to an LR parser using an SLR-parsing table as an 
SLR parser. The other two methods augment the SLR method with lookahead 
information. 
The SLR method begins with LR(0) items and LR(0) automata, introduced 
in Section 4.5. That is, given a grammar, G, we augment G to produce GI, 
with a new start symbol St. From G', we construct C, the canonical collection 
of sets of items for Gt together with the GOT0 function. 
GOT0 
E
T
F
 
1
2
 3 
8
2
 3 
9 
3 
10 
4.6. INTRODUCTION TO LR PARSING: SIMPLE LR 
253 
-
-
 
shift 
reduce by F -+ id 
reduce by T -+ F 
shift 
shift 
reduce by F -+ id 
reduce by T -+ T * F 
reduce by E --+ T 
shift 
shift 
reduce by F -+ id 
reduce by T -+ F 
reduce by E -+ E + 
T 
accept 
Figure 4.38: Moves of an LR parser on id * id + 
id 
The ACTION and GOT0 entries in the parsing table are then constructed 
using the following algorithm. It requires us to know FOLLOW(A) 
for each 
nonterminal A of a grammar (see Section 4.4). 
Algorithm 4.46 : 
Constructing an SLR-parsing table. 
INPUT: An augmented grammar GI. 
OUTPUT: The SLR-parsing table functions ACTION and GOT0 for GI. 
METHOD: 
1. Construct C = {Io, 
11, 
. . . ,I,), 
the collection of sets of LR(0) items for 
GI. 
2. State i is constructed from I, 
. The parsing actions for state i are deter- 
mined as follows: 
(a) If [A -+ a.a/3] is in I, and GOTO(&, 
a) = 
Ij, 
then set ACTION[& a] to 
"shift j." Here a must be a terminal. 
(b) If [A 
-+ a*] 
is in I,, then set  ACTION[^, a] to "reduce A --+ a" for all 
a in FOLLOW(A); 
here A may not be Sf. 
(c) If [Sf -+ S.] is in I,, then set  ACTION[^, $1 to "accept ." 
If any conflicting actions result from the above rules, we say the grammar 
is not SLR(1). The algorithm fails to produce a parser in this case. 
254 
CHAPTER 4. SYNTAX ANALYSIS 
3. The goto transitions for state i are constructed for all nonterminals A 
using the rule: If GOTO(I,, 
A) = I,, then GO TO[^, A] = 
j. 
4. All entries not defined by rules (2) and (3) are made "error." 
5. The initial state of the parser is the one constructed from the set of items 
containing [St 
-+ 
as]. 
The parsing table consisting of the ACTION and GOTO functions determined 
by Algorithm 4.46 is called the SLR(1) table for G. An LR parser using the 
SLR(1) table for G is called the SLR(1) parser for G, and a grammar having an 
SLR(1) parsing table is said to be SLR(1). We usually omit the "(I)" after the 
"SLR," since we shall not deal here with parsers having more than one symbol 
of lookahead. 
Example 4.47 : 
Let us construct the SLR table for the augmented expression 
grammar. The canonical collection of sets of LR(0) items for the grammar was 
shown in Fig. 4.31. First consider the set of items Io: 
The item F -+ .(E) gives rise to the entry ACTION[O, 
(1 = shift 4, and the 
item F -+ -id 
to the entry ACTION[O, 
id] = shift 5. Other items in I
.
 yield no 
actions. Now consider Il : 
The first item yields  ACTION[^, $1 = 
accept, and the second yields  ACTION[^, +] 
= shift 6. Next consider 12: 
Since FOLLOW(E) 
= {$, +, 
) 3, 
the first item makes 
 ACTION[^, $1 = 
 ACTION[^, +] = 
 ACTION[^, )] = 
reduce E -t T 
The second item makes  ACTION[^, *] = shift 7. Continuing in this fashion we 
obtain the ACTION and GOTO tables that were shown in Fig. 4.31. In that 
figure, the numbers of productions in reduce actions are the same as the order 
in which they appear in the original grammar (4.1). That is, E +- 
E + 
T is 
number 1, E -+ 
T is 2, and so on. 
4.6. INTRODUCTION TO LR PARSING: SIMPLE LR 
255 
Example 4.48 : 
Every SLR(1) grammar is unambiguous, but there are many 
unambiguous grammars that are not SLR(1). Consider the grammar with pro- 
ductions 
Think of L and R as standing for 1-value and r-value, respectively, and * as an 
operator indicating "contents of." 
The canonical collection of sets of LR(0) 
items for grammar (4.49) is shown in Fig. 4.39. 
Figure 4.39: Canonical LR(0) collection for grammar (4.49) 
Consider the set of items Iz. The first item in this set makes ACTION[& =] 
be "shift 6." Since FOLLOW(R) 
contains = (to see why, consider the derivation 
S + 
L = 
R =+ *R 
= 
R), 
the second item sets  ACTION[^, =] 
to ?educe R -+ L." 
Since there is both a shift and a reduce entry in  ACTION[^, =], 
state 2 has a 
shiftlreduce conflict on input symbol =. 
Grammar (4.49) is not ambiguous. This shiftlreduce conflict arises from 
the fact that the SLR parser construction method is not powerful enough to 
remember enough left context to decide what action the parser should take on 
input =, 
having seen a string reducible to L. The canonical and LALR methods, 
to be discussed next, will succeed on a larger collection of grammars, including 
5As in Section 2.8.3, an kvalue designates a location and an r-value is a value that can be 
stored in a location. 
256 
CHAPTER 4. SYNTAX ANALYSIS 
grammar (4.49). Note, however, that there are unambiguous grammars for 
which every LR parser construction method will produce a parsing action table 
with parsing action conflicts. Fortunately, such grammars can generally be 
avoided in programming language applications. 
4.6.5 
Viable Prefixes 
Why can LR(0) automata be used to make shift-reduce decisions? The LR(0) 
automaton for a grammar characterizes the strings of grammar symbols that 
can appear on the stack of a shift-reduce parser for the grammar. The stack 
contents must be a prefix of a right-sentential form. If the stack holds a and 
the rest of the input is f, then a sequence of reductions will take a x  to S. In 
terms of derivations, S + ax. 
rm 
Not all prefixes of right-sentential forms can appear on the stack, however, 
since the parser must not shift past the handle. For example, suppose 
Then, at various times during the parse, the stack will hold (, (E, 
and (E), but 
it must not hold (E)*, 
since (E) is a handle, which the parser must reduce to 
F before shifting *. 
The prefixes of right sentential forms that can appear on the stack of a shift- 
reduce parser are called viable prefixes. They are defined as follows: a viable 
prefix is a prefix of a right-sentential form that does not continue past the right 
end of the rightmost handle of that sentential form. By this definition, it is 
always possible to add terminal symbols to the end of a viable prefix to obtain 
a right-sentential form. 
SLR parsing is based on the fact that LR(0) automata recognize viable 
prefixes. We s ~ y  
item A -t 
is valid for a viable prefix aPl if there is a 
derivation St + aAw + aP1P2w. In general, an item will be valid for many 
rm 
rm 
viable prefixes. 
The fact that A -+ P1.P2 is valid for aP1 tells us a lot about whether to 
shift or reduce when we find apl on the parsing stack. In particular, if ,& # E ,  
then it suggests that we have not yet shifted the handle onto the stack, so shift 
is our move. If ,& = 
e, then it looks as if A -+ P1 is the handle, and we should 
reduce by this production. Of course, two valid items may tell us to do different 
things for the same viable prefix. Some of these conflicts can be resolved by 
looking at the next input symbol, and others can be resolved by the methods 
of Section 4.8, but we should not suppose that all parsing action conflicts can 
be resolved if the LR method is applied to an arbitrary grammar. 
We can easily compute the set of valid items for each viable prefix that 
can appear on the stack of an LR parser. In fact, it is a central theorem of 
LR-parsing theory that the set of valid items for a viable prefix y is exactly 
the set of items reached from the initial state along the path labeled y in the 
LR(0) automaton for the grammar. In essence, the set of valid items embodies 
4.6. INTRODUCTION T O  LR PARSING: SIMPLE LR 
257 
Items as States of an NFA 
A nondeterministic finite automaton N for recognizing viable prefixes can 
be constructed by treating the items themselves as states. There is a 
transition from A -+ a.XP to A -+ aX.P labeled X ,  and there is a 
transition from A -+ a.BP to B -+ .
y
 
labeled c. Then  CLOSURE(^) for 
set of items (states of N )  
I is exactly the E-closure 
of a set of NFA states 
defined in Section 3.7.1. Thus, GOTO(& X) gives the transition from I 
on symbol X in the DFA constructed from N by the subset construction. 
Viewed in this way, the procedure items(G1) 
in Fig. 4.33 is just the subset 
construction itself applied to the NFA N  with items as states. 
all the useful information that can be gleaned from the stack. While we shall 
not prove this theorem here, we shall give an example. 
Example 4.50 : Let us consider the augmented expression grammar again, 
whose sets of items and GOTO function are exhibited in Fig. 4.31. Clearly, the 
string E + 
T* is a viable prefix of the grammar. The automaton of Fig. 4.31 
will be in state 7 after having read E + 
T*. State 7 contains the items 
which are precisely the items valid for E+T*. To see why, consider the following 
three rightmost derivations 
The first derivation shows the validity of T -+ T * -F, 
the second the validity 
of F --+ .(E), 
and the third the validity of F -+ 
.id. It can be shown that there 
are no other valid items for E + 
T*, although we shall not prove that fact here. 
4.6.6 Exercises for Section 4.6 
Exercise 4.6.1 : 
Describe all the viable prefixes for the following grammars: 
a) The grammar S + 0 S 1 I 0 1 
of Exercise 4.2.2(a). 
258 
CHAPTER 4. SYNTAX ANALYSIS 
! 
b) The grammar S + S S + ( S S * I a of Exercise 4.2.1. 
! 
c) The grammar S -+ S ( S ) ( 6 of Exercise 4.2.2(c). 
Exercise 4.6.2 : 
Construct the SLR sets of items for the (augmented) grammar 
of Exercise 4.2.1. Compute the GOT0 function for these sets of items. Show 
the parsing table for this grammar. Is the grammar SLR? 
Exercise 4.6.3 : 
Show the actions of your parsing table from Exercise 4.6.2 on 
the input aa * a+. 
Exercise 4.6.4 : 
For each of the (augmented) grammars of Exercise 4.2.2(a)- 
(g) 
: 
a) Construct the SLR sets of items and their GOTO function. 
b) Indicate any action conflicts in your sets of items. 
c) Construct the SLR-parsing table, if one exists. 
Exercise 4.6.5 : 
Show that the following grammar: 
is LL(1) but not SLR(1). 
Exercise 4.6.6 : 
Show that the following grammar: 
is SLR(1) but not LL(1). 
! 
! 
Exercise 4.6.7 : 
Consider the family of grammars G, defined by: 
S 
+= 
Ai 
bi 
f o r 1 L i F n  
Ai -+ 
aj Ai 
I aj 
for 1 
< i , j  
< n  
and i # j 
Show that: 
a) G, has 2n2 - 
n productions. 
b) G, has 2, + 
n2 + 
n sets of LR(0) items. 
What does this analysis say about how large LR parsers can get? 
4.7. MORE POWERFUL LR PARSERS 
259 
! 
Exercise 4.6.8 
: We suggested that individual items could be regarded as 
states of a nondeterministic finite automaton, while sets of valid items are the 
states of a deterministic finite automaton (see the box on "Items as States of 
an NFA" in Section 4.6.5). For the grammar S + S S + I S S * I a of 
Exercise 4.2.1: 
a) Draw the transition diagram (NFA) for the valid items of this grammar 
according to the rule given in the box cited above. 
b) Apply the subset construction (Algorithm 3.20) to your NFA from part 
(a). How does the resulting DFA compare to the set of LR(0) items for 
the grammar? 
!! 
c) Show that in all cases, the subset construction applied to the NFA that 
comes from the valid items for a grammar produces the LR(0) sets of 
it 
ems. 
! Exercise 4.6.9 : 
The following is an ambiguous grammar: 
Construct for this grammar its collection of sets of LR(0) items. If we try to 
build an LR-parsing table for the grammar, there are certain conflicting actions. 
What are they? Suppose we tried to use the parsing table by nondeterminis- 
tically choosing a possible action whenever there is a conflict. Show all the 
possible sequences of actions on input abab. 
4.7 More Powerful LR Parsers 
In this section, we shall extend the previous LR parsing techniques to use one 
symbol of lookahead on the input. There are two different methods: 
1. The "canonical-LR" or just "LR" method, which makes full use of the 
lookahead symbol(s). This method uses a large set of items, called the 
LR(1) items. 
2. The "lookahead-LR" or "LALR" method, which is based on the LR(0) 
sets of items, and has many fewer states than typical parsers based on the 
LR(1) items. By carefully introducing lookaheads into the LR(0) items, 
we can handle many more grammars with the LALR method than with 
the SLR method, and build parsing tables that are no bigger than the 
SLR tables. LALR is the method of choice in most situations. 
After introducing both these methods, we conclude with a discussion of how to 
compact LR parsing tables for environments with limited memory. 
260 
CHAPTER 4. SYNTAX ANALYSIS 
4.7.1 
Canonical LR(1) Items 
We shall now present the most general technique for constructing an LR parsing 
table from a grammar. Recall that in the SLR method, state 
i calls for reduction 
by A -+ a if the set of items Ii 
contains item [A 
--+ as] and a is in FOLLOW(A). 
In some situations, however, when state i appears on top of the stack, the 
viable prefix pa on the stack is such that PA cannot be followed by a in any 
right-sentential form. Thus, the reduction by A -+ a should be invalid on input 
a. 
Example 4.51 : 
Let us reconsider Example 4.48, where in state 2 we had item 
R -+ L., which could correspond to A -+ a above, and a could be the = sign, 
which is in FOLLOW(R). 
Thus, the SLR parser calls for reduction by R -+ L 
in state 2 with = as the next input (the shift action is also called for, because 
of item S -+ 
L.=R in state 2). However, there is no right-sentential form of the 
grammar in Example 4.48 that begins R = . 
. 
. . Thus state 2, which is the 
state corresponding to viable prefix L only, should not really call for reduction 
of that L to R. 
It is possible to carry more information in the state that will allow us to 
rule out some of these invalid reductions by A -+ a. By splitting states when 
necessary, we can arrange to have each state of an LR parser indicate exactly 
which input symbols can follow a handle a for which there is a possible reduction 
to A. 
The extra information is incorporated into the state by redefining items to 
include a terminal symbol as a second component. The general form of an item 
becomes [A -+ a p, a], where A -+ a/? is a production and a is a terminal or 
the right endmarker $. We call such an object an LR(1) item. The 1 refers 
to the length of the second component, called the lookahead of the item.6 The 
lookahead has no effect in an item of the form [A -+ a$, a], where ,8 is not c, 
but an item of the form [A -+ a*, 
a] calls for a reduction by A -+ a only if the 
next input symbol is a. Thus, we are compelled to reduce by A -+ a only on 
those input symbols a for which [A -+ as, 
a] is an LR(1) item in the state on 
top of the stack. The set of such a's will always be a subset of FOLLOW(A), 
but it could be a proper subset, as in Example 4.51. 
Formally, we say LR(1) item [A -+ 
an@, 
a] is valid for a viable prefix y 
if 
there is a derivation S 3 
SAW 
=+ Gapw, where 
r m  
r m  
1. y 
= 
Sa, and 
2. Either a is the first symbol of w, or w is E and a is $. 
Example 4.52 : 
Let us consider the grammar 
'Lookaheads that are strings of length greater than one are possible, of course, but we 
shall not consider such lookaheads here. 
4.7. MORE POWERFUL LR PARSERS 
There is a rightmost derivation S 2 
aaBab + aaaBab. We see that item [B 
-+ 
r m  
r m  
a.B, a] is valid for a viable prefix y = aaa by letting S 
= aa, A = B ,  w = ab, 
a = 
a, and p = 
B in the above definition. There is also a rightmost derivation 
S 3 
B a B  j 
BaaB. From this derivation we see that item [ B  
-+ a-B, 
$
1
 is 
r m  
r m  
valid for viable prefix Baa. 
4.7.2 Constructing LR(1) Sets of Items 
The method for building the collection of sets of valid LR(1) items is essentially 
the same as the one for building the canonical collection of sets of LR(0) items. 
We need only to modify the two procedures CLOSURE and GOTO. 
SetOfftems CLOSURE(I) 
{ 
repeat 
for ( each item [A -+ a.BP, 
a] in I ) 
for ( each production B -+ y 
in G' ) 
for ( each terminal b in FIRST(,&) ) 
add [B 
-+ .y, 
b] to set I ;  
until no more items are added to I; 
return I; 
1 
SetOfftems GOTO(& X) { 
initialize J to be the empty set; 
for ( each item [A -t a.X,O, 
a] in I ) 
add item [A -+ ax./?, 
a] to set J; 
return CLOSURE(J); 
} 
void items(Gt) { 
initialize C to CLOSURE({[S' 
-+ .S, 
$11); 
repeat 
for ( each set of items I in C ) 
for ( each grammar symbol X ) 
if ( GOTO(I, 
X) is not empty and not in C ) 
add GOTO(& X )  
to C; 
until no new sets of items are added to C; 
1 
Figure 4.40: Sets-of-LR( 
1)-items construction for grammar G' 
262 
CHAPTER 4. SYNTAX ANALYSIS 
To appreciate the new definition of the CLOSURE operation, in particular, 
why b must be in  FIRST(^^), consider an item of the form [A -+ a-BP, 
a] 
in the 
set of items valid for some viable prefix y. Then there is a rightmost derivation 
S % bAax + GcrBpax, where y = da. Suppose pax derives terminal string 
r m  
r m  
by. Then for each production of the form B -+ 1
1
 for some v, 
we have derivation 
S $ yBby + yqby. Thus, [B 
-+ .q, b] is valid for y. Note that b can be the 
rm 
rm 
first terminal derived from P, or it is possible that P derives c in the derivation 
pax %- by, and b can therefore be a. To summarize both possibilities we say 
rrn 
that b can be any terminal in F I R S T ( ~ ~ X ) ,  
where FIRST is the function from 
Section 4.4. Note that x cannot contain the first terminal of by, so FIRST(PUX) 
= FIRST(/?U). 
We now give the LR(1) sets of items construction. 
Figure 4.41: The GOT0 graph for grammar (4.55) 
Algorithm 4.53 : 
Construction of the sets of LR(1) items. 
INPUT: An augmented grammar G'. 
OUTPUT: The sets of LR(1) items that are the set of items valid for one or 
more viable prefixes of G'. 
4.7. MORE POWERFUL LR PARSERS 
263 
METHOD: The procedures CLOSURE and GOT0 and the main routine items 
for constructing the sets of items were shown in Fig. 4.40. 
Example 4.54 : 
Consider the following augmented grammar. 
We begin by computing the closure of {[St 
-+ -S, 
$1). To close, we match 
the item [St 
-+ -S, 
$
1
 with the item [A -+ a-BP, 
a] in the procedure CLOSURE. 
That is, A = St, 
a = e, B = S, P = e, and a = $. Function CLOSURE tells us 
to add [B -+ .y, 
b] for each production B -+ 
y and terminal b in FIRST(P~). 
In 
terms of the present grammar, B -+ y must be S -+ CC, and since ,
8
 is c and 
a is $, b may only be $. Thus we add [S 
-+ .CC, 
$1. 
We continue to compute the closure by adding all items [C -+ .y, 
b] for b 
in FIRST(C$). That is, matching [S 
-+ .CC, 
$
1
 against [A 
-+ 
a.B,O, 
a], we have 
A = S, a = 
6 ,  B = C, p = C, and a = 
$. Since C does not derive the empty 
string, FIRST(C$) = FIRST(C). Since FIRST@) contains terminals c and d, we 
add items [C -+ -cC, 
c], [C -+ .cC, 
dl, [C -t -d, 
c] and [C -+ 
-d, 
dl. None of the 
new items has a nonterminal immediately to the right of the dot, so we have 
completed our first set of LR(1) items. The initial set of items is 
I,: S + . S , $  
S -+ .CC, $ 
C -+ .cC, c/d 
C -+ .d, c/d 
The brackets have been omitted for notational convenience, and we use the 
notation [C -+ .cC, c/d as a shorthand for the two items [C -+ .cC, c] and 
[C 
-+ .cC, 4 .  
Now we compute GOTO(I,, 
X )  for the various values of X. For X = S we 
must close the item [St 
-+ S., $1. No additional closure is possible, since the 
dot is at the right end. Thus we have the next set of items 
For X = C we close [S -+ C.C, $1. We add the C-productions with second 
component $ and then can add no more, yielding 
Next, let X = 
c. We must close {[C -+ c.C, c/d}. We add the C-productions 
with second component cld, yielding 
CHAPTER 4. SYNTAX ANALYSIS 
Finally, let X = 
d, and we wind up with the set of items 
We have finished considering GOTO on Io. We get no new sets from 11, 
but I2 
has goto's on C, c, and d. For GOTO(I~, 
C) we get 
15 : S - i  CC',$ 
no closure being needed. To compute GO TO(^^, c) we take the closure of 
{[C + 
c-C, $
1
1
,
 
to obtain 
I, : c-i 
c-C, $ 
c 
-+ .cC, $ 
C + 
.d, $ 
Note that I6 
differs from I3 
only in second components. We shall see that it 
is common for several sets of LR(1) items for a grammar to have the same 
first components and differ in their second components. When we construct 
the collection of sets of LR(0) items for the same grammar, each set of LR(0) 
items will coincide with the set of first components of one or more sets of LR(1) 
items. We shall have more to say about this phenomenon when we discuss 
LALR parsing. 
Continuing with the GOT0 function for 12, 
GO TO(^^, d) is seen to be 
Turning now to 13, 
the GOTO'S of I3 
on c and d are I3 
and 14, 
respectively, and 
GOTO (I3, 
C) is 
I4 
and I5 
have no GOTO'S, since all items have their dots at the right end. The 
GOTO'S 
of I6 
on c and d are I6 
and IT, 
respectively, and G O T O ( ~ ,  
C )  
is 
The remaining sets of items yield no GOTO'S, 
so we are done. Figure 4.41 
shows the ten sets of items with their goto's. 
4.7, MORE POWERFUL LR PARSERS 
265 
4.7.3 Canonical LR(1) Parsing Tables 
We now give the rules for constructing the LR(1) ACTION and GOT0 functions 
from the sets of LR(1) items. These functions are represented by a table, as 
before. The only difference is in the values of the entries. 
Algorithm 4.56 : 
Construction of canonical-LR parsing tables. 
INPUT: An augmented grammar GI. 
OUTPUT: The canonical-LR parsing table functions ACTION and GOT0 for G'. 
METHOD: 
1. Construct C' = {Io, 
Il 
, . . , 
I,), the collection of sets of LR(1) items for 
G'. 
2. State i of the parser is constructed from Ti. The parsing action for state 
i is determined as follows. 
(a) If [A -+ a-a@, 
b] is in I, and GOTO(I,,U) 
= I,, 
then set  ACTION[^, a] 
to "shift j 
." Here a must be a terminal. 
(b) If [A -+ a*, 
a] is in Ii, 
A # S', then set  ACTION[^, a] to "reduce 
A -+ a." 
(c) If [St 
-+ S-, 
$1 is in I,, then set  ACTION[^, $1 to "accept." 
If any conflicting actions result from the above rules, we say the grammar 
is not LR(1). The algorithm fails to produce a parser in this case. 
3. The goto transitions for state i are constructed for all nonterminals A 
using the rule: If GOTO(&, 
A) = Ij, 
then GO TO[^, A] = 
j. 
4. All entries not defined by rules (2) and (3) are made "error." 
5. The initial state of the parser is the one constructed from the set of items 
containing [S' -+ .S, $1. 
The table formed from the parsing action and goto functions produced by 
Algorithm 4.44 is called the canonical LR(1) parsing table. An LR parser using 
this table is called a canonical-LR(1) parser. If the parsing action function 
has no multiply defined entries, then the given grammar is called an LR(1) 
grammar. As before, we omit the "(1)" if it is understood. 
Example 4.57 : The canonical parsing table for grammar (4.55) is shown in 
Fig. 4.42. Productions 1, 2, and 3 are S + CC, C -+ cC, and C -+ d, 
respectively. 
Every SLR(1) grammar is an LR(1) grammar, but for an SLR(1) grammar 
the canonical LR parser may have more states than the SLR parser for the 
same grammar. The grammar of the previous examples is SLR and has an SLR 
parser with seven states, compared with the ten of Fig. 4.42. 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.42: Canonical parsing table for grammar (4.55) 
STATE 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
4.7.4 
Constructing LALR Parsing Tables 
We now introduce our last parser construction method, the LALR (Eoolcahead- 
LR) technique. This method is often used in practice, because the tables ob- 
tained by it are considerably smaller than the canonical LR tables, yet most 
common syntactic constructs of programming languages can be expressed con- 
veniently by an LALR grammar. The same is almost true for SLR grammars, 
but there are a few constructs that cannot be conveniently handled by SLR 
techniques (see Example 4.48, for example). 
For a comparison of parser size, the SLR and LALR tables for a grammar 
always have the same number of states, and this number is typically several 
hundred states for a language like C. The canonical LR table would typically 
have several thousand states for the same-size language. Thus, it is much easier 
and more economical to construct SLR and LALR tables than the canonical 
LR tables. 
By way of introduction, let us again consider grammar (4.55), whose sets of 
LR(1) items were shown in Fig. 4.41. Take a pair of similar looking states, such 
as I4 
and 17. Each of these states has only items with first component C -+ 
d.. 
In 14, 
the lookaheads are c or d; in 17, 
$ is the only lookahead. 
To see the difference between the roles of I4 
and I7 
in the parser, note that 
the grammar generates the regular language c*dc*d. 
When reading an input 
cc . 
. cdcc . cd, the parser shifts the first group of c's and their following d 
onto the stack, entering state 4 after reading the d. The parser then calls for a 
reduction by C -+ 
d, provided the next input symbol is c or d. The requirement 
that c or d follow makes sense, since these are the symbols that could begin 
strings in c*d. If $ follows the first d, we have an input like ccd, which is not 
in the language, and state 4 correctly declares an error if $ is the next input. 
The parser enters state 7 after reading the second d. Then, the parser must 
ACTION 
c 
d 
$ 
s3 s4 
acc 
s6 
s7 
s3 s4 
r3 r3 
r l  
s6 
s7 
r3 
r2 
r2 
r2 
GOT0 
S
C
 
1
2
 
5 
8 
9 
4.7. MORE POWERFUL LR PARSERS 
267 
see $ on the input, or it started with a string not of the form c*dc*d. 
It thus 
makes sense that state 7 should reduce by C -+ d on input $ and declare error 
on inputs c or d. 
Let us now replace I4 
and I7 
by 147, 
the union of I4 
and 17, 
consisting of 
the set of three items represented by [C -+ d., c/d/$]. The goto's on d to I4 
or 
I7 
from lo, 
12, 
13, 
and I6 
now enter 147. The action of state 47 is to reduce on 
any input. The revised parser behaves essentially like the original, although it 
might reduce d to C in circumstances where the original would declare error, 
for example, on input like ccd or cdcdc. The error will eventually be caught; in 
fact, it will be caught before any more input symbols are shifted. 
More generally, we can look for sets of LR(1) items having the same core, 
that is, set of first components, and we may merge these sets with common 
cores into one set of items. For example, in Fig. 4.41, I4 
and I7 
form such a 
pair, with core {C -+ d.). Similarly, I3 
and I6 
form another pair, with core 
{C -+ c.C, C -+ .cC, C -+ .d). There is one more pair, Is 
and 19, 
with common 
core {C -+ cC-). Note that, in general, a core is a set of LR(0) items for the 
grammar at hand, and that an LR(1) grammar may produce more than two 
sets of items with the same core. 
Since the core of GOTO(& 
X) depends only on the core of I, 
the goto's of 
merged sets can themselves be merged. Thus, there is no problem revising the 
goto function as we merge sets of items. The action functions are modified to 
reflect the non-error actions of all sets of items in the merger. 
Suppose we have an LR(1) grammar, that is, one whose sets of LR(1) items 
produce no parsing-action conflicts. If we replace all states having the same core 
with their union, it is possible that the resulting union will have a conflict, but 
it is unlikely for the following reason: Suppose in the union there is a conflict 
on lookahead a because there is an item [A -+ a-, 
a] calling for a reduction by 
A -+ a, 
and there is another item [B 
-+ P.ay, b] calling for a shift. Then some 
set of items from which the union was formed has item [A -+ a*, 
a], and since 
the cores of all these states are the same, it must have an item [B -+ @say, 
c] 
for some c. But then this state has the same shiftjreduce conflict on a, and 
the grammar was not LR(1) as we assumed. Thus, the merging of states with 
common cores can never produce a shiftjreduce conflict that was not present 
in one of the original states, because shift actions depend only on the core, not 
the lookahead. 
It is possible, however, that a merger will produce a reducejreduce conflict, 
as the following example shows. 
Example 4.58 : 
Consider the grammar 
which generates the four strings acd, ace, bed, and bee. The reader can check 
that the grammar is LR(1) by constructing the sets of items. Upon doing so, 
268 
CHAPTER 4. SYNTAX ANALYSIS 
we find the set of items {[A 
-+ c., 4, 
[B 
-+ c., el} valid for viable prefix ac and 
{ [ A  
-+ c-, 
el, [B 
-+ cq, 4) 
valid for bc. Neither of these sets has a conflict, and 
their cores are the same. However, their union, which is 
generates a reducelreduce conflict, since reductions by both A -+ c and B -+ 
c 
are called for on inputs d and e. 
We are now prepared to give the first of two LALR table-construction al- 
gorithms. The general idea is to construct the sets of LR(1) items, and if no 
conflicts arise, merge sets with common cores. We then construct the parsing 
table from the collection of merged sets of items. The method we are about to 
describe serves primarily as a definition of LALR(1) grammars. Constructing 
the entire collection of LR(1) sets of items requires too much space and time to 
be useful in practice. 
Algorithm 4.59 : 
An easy, but space-consuming LALR table construction. 
INPUT: An augmented grammar G'. 
OUTPUT: The LALR parsing-table functions ACTION and GOT0 for G'. 
METHOD: 
1. Construct C = (Io, 
11, 
. 
. 
. , 
I,), the collection of sets of LR(1) items. 
2. For each core present among the set of LR(1) items, find all sets having 
that core, and replace these sets by their union. 
3. Let C' = {Jo, 
J1,. . 
. , 
J,) be the resulting sets of LR(1) items. The 
parsing actions for state i are constructed from Ji in the same manner as 
in Algorithm 4.56. I
f
 there is a parsing action conflict, the algorithm fails 
to produce a parser, and the grammar is said not to be LALR(1). 
4. The GOTO table is constructed as follows. If J is the union of one or 
more sets of LR(1) items, that is, J = Il n I2 
n 
n Ik, then the 
cores of GOTO(I~, 
X) 
, 
GOTO(I~, 
X) 
, 
. 
. . , 
GOTO(&, 
X) 
are the same, since 
11, 
12, 
. 
. 
. , 
Ik 
all have the same core. Let K be the union of all sets of 
items having the same core as GOTO(I~, 
X). Then GOTO(J, 
X) = 
K. 
The table produced by Algorithm 4.59 is called the LALR parsing table for 
G. If there are no parsing action conflicts, then the given grammar is said to 
be an LALR(1) grammar. The collection of sets of items constructed in step 
(3) is called the LALR(1) collection. 
4.7. MORE POWERFUL LR PARSERS 
269 
Example 4.60 : 
Again consider grammar (4.55) whose GOTO graph was shown 
in Fig. 4.41. As we mentioned, there are three pairs of sets of items that can 
be merged. I3 
and I6 
are replaced by their union: 
I4 
and I
7
 are replaced by their union: 
and I8 
and I9 
are replaced by their union: 
The LALR action and goto functions for the condensed sets of items are shown 
in Fig. 4.43. 
STATE 
Figure 4.43: LALR parsing table for the grammar of Example 4.54 
0 
1 
2 
36 
47 
5 
89 
To see how the GOTO'S are computed, consider G O T O ( I ~ ~ ,  
C). In the original 
set of LR(1) items, G0T0(13, C) = 18, 
and I8 
is now part of Isg, 
so we make 
G O T O ( I ~ ~ ,  
C) be 189. 
We could have arrived at the same conclusion if we 
considered Is, 
the other part of 13,. 
That is, G0T0(16, 
C) = Ig 
, 
and I9 is 
now part of 189. For another example, consider GOTO(I~, 
c), an entry that is 
exercised after the shift action of I2 
on input c. In the original sets of LR(1) 
items, G O T O ( ~ ~ ,  
c) = 
16. Since I6 
is now part of 136, 
G0T0(12, 
C) 
becomes 13s. 
Thus, the entry in Fig. 4.43 for state 2 and input c is made s36, meaning shift 
and push state 36 onto the stack. 
ACTION 
c 
d 
When presented with a string from the language c* 
dc* 
d, both the LR parser 
of Fig. 4.42 and the LALR parser of Fig. 4.43 make exactly the same sequence 
of shifts and reductions, although the names of the states on the stack may 
differ. For instance, if the LR parser puts I3 
or I6 
on the stack, the LALR 
GOT0 
$
S
C
 
s36 
s47 
acc 
s36 
s47 
s36 
s47 
r3 
r3 
r3 
r l  
r2 
r2 
r2 
1
2
 
5 
89 
270 
CHAPTER 4. SYNTAX ANALYSIS 
parser will put IS6 
on the stack. This relationship holds in general for an LALR 
grammar. The LR and LALR parsers will mimic one another on correct inputs. 
When presented with erroneous input, the LALR parser may proceed to do 
some reductions after the LR parser has declared an error. However, the LALR 
parser will never shift another symbol after the LR parser declares an error. 
Far example, on input ccd followed by $, the LR parser of Fig. 4.42 will put 
on the stack, and in state 4 will discover an error, because $ is the next input 
symbol and state 4 has action error on $. In contrast, the LALR parser of Fig. 
4.43 will make the corresponding moves, putting 
on the stack. But state 47 on input $ has action reduce C -+ d. The LALR 
parser will thus change its stack to 
Now the action of state 89 on input $ is reduce C -+ cC. The stack becomes 
whereupon a similar reduction is called for, obtaining stack 
Finally, state 2 has action error on input $, so the error is now discovered. 
4
.
7
.
5
 Efficient Construction of LALR Parsing Tables 
There are several modifications we can make to Algorithm 4.59 to avoid con- 
structing the full collection of sets of LR(1) items in the process of creating an 
LALR(1) parsing table. 
First, we can represent any set of LR(0) or LR(1) items I by its kernel, 
that is, by those items that are' either the initial item - 
[Sf -+ .S] or 
[St 
-+ -S, 
$1 - 
or that have the dot somewhere other than at the beginning 
of the production body. 
We can construct the LALR(1)-item kernels from the LR(0)-item kernels 
by a process of propagation and spontaneous generation of lookaheads, 
that we shall describe shortly. 
If we have the LALR(1) kernels, we can generate the LALR(1) parsing 
table by closing each kernel, using the function CLOSURE of Fig. 4.40, and 
then computing table entries by Algorithm 4.56, as if the LALR(1) sets 
of items were canonical LR(1) sets of items. 
4.7. MORE POWERFUL LR PARSERS 
271 
Example 4.61 : 
We shall use as an example of the efficient LALR(1) table- 
construction method the non-SLR grammar from Example 4.48, which we re- 
produce below in its augmented form: 
The complete sets of LR(0) items for this grammar were shown in Fig. 4.39. 
The kernels of these items are shown in Fig. 4.44. 
Figure 4.44: Kernels of the sets of LR(0) items for grammar (4.49) 
Now we must attach the proper lookaheads ta the LR(0) items in the kernels, 
to create the kernels of the sets of LALR(1) items. There are two ways a 
lookahead b can get attached to an LR(0) item B -+ 7.6 in some set of LALR(1) 
items J: 
1. There is a set of items I, with a kernel item A -+ a.P,a, and J = 
GOTO(& 
X), 
and the construction of 
GOTO (CLOSURE({[A 
-+ asp, 
a])), X) 
as given in Fig. 4.40, contains [B 
-+ 74, 
b], regardless of a. Such a looka- 
head b is said to be generated spontaneously for B -+ 7.6. 
2. As a special case, lookahead $ is generated spontaneously for the item 
S
'
 
-+ .S 
in the initial set of items. 
3. All is as in (I), but a = b, and GOTO (CLOSURE({[A 
-+ asp, b])), X) 
, 
as 
given in Fig. 4.40, contains [B -+ 7.6, b] only because A -+ Q.P has b as 
one of its associated lookaheads. In such a case, we say that lookaheads 
propagate from A -+ a.P in the kernel of I to B -+ 
7.6 in the kernel of 
J. Note that propagation does not depend on the particular lookahead 
symbol; either all lookaheads propagate from one item to another, or none 
do. 
272 
CHAPTER 4. SYNTAX ANALYSIS 
We need to determine the spontaneously generated lookaheads for each set 
of LR(0) items, and also to determine which items propagate lookaheads from 
which. The test is actually quite simple. Let # be a symbol not in the grammar 
at hand. Let A -+ a
m
p
 be a kernel LR(0) item in set I. Compute, for each X ,  
J = GOTO (CLOSURE({[A 
-+ a*@, 
#
I
}
)
,
 
X) 
. For each kernel item in J, we 
examine its set of lookaheads. If # is a lookahead, then lookaheads propagate 
to that item from A -+ amp. Any other lookahead is spontaneously generated. 
These ideas are made precise in the following algorithm, which also makes use 
of the fact that the only kernel items in J must have X immediately to the left 
of the dot; that is, they must be of the form B -+ yX.6. 
Algorithm 4.62 : 
Determining lookaheads. 
INPUT: The kernel K of a set of LR(0) items I and a grammar symbol X. 
OUTPUT: The lookaheads spontaneously generated by items in I for kernel 
items in GOTO(& X )  and the items in I from which lookaheads are propagated 
to kernel items in GOTO(I, 
X). 
METHOD: The algorithm is given in Fig. 4.45. 
for ( each item A -+ 
a-p 
in K ) { 
J := CLOSURE({[A 
--+ asp,#]) ); 
if ( [B 
-+ ySX6, a] is in J, 
and a is not # ) 
conclude that lookahead a is generated spontaneously for item 
B --+ yX.6 in GOTO(I, 
X); 
if ( [B -+ 
ymX6, #] 
is in J ) 
conclude that lookaheads propagate from A -+ a
m
p
 in I to 
B -+ yX.6 in GOTO(& 
X); 
1 
Figure 4.45: Discovering propagated and spontaneous lookaheads 
We are now ready to attach lookaheads to the kernels of the sets of LR(0) 
items to form the sets of LALR(1) items. First, we know that $ is a looka- 
head for S' --+ .S in the initial set of LR(0) items. Algorithm 4.62 gives us all 
the lookaheads generated spontaneously. After listing all those lookaheads, we 
must allow them to propagate until no further propagation is possible. There 
are many different approaches, all of which in some sense keep track of "new" 
lookaheads that have propagated into an item but which have not yet propa- 
gated out. The next algorithm describes one technique to propagate lookaheads 
to all items. 
Algorithm 4.63 : 
Efficient computation of the kernels of the LALR(1) collec- 
tion of sets of items. 
INPUT: An augmented grammar G'. 
4.7. MORE POWERFUL LR PARSERS 
OUTPUT: The kernels of the LALR(1) collection of sets of items for GI. 
METHOD: 
1. Construct the kernels of the sets of LR(0) items for G. If space is not at 
a premium, the simplest way is to construct the LR(0) sets of items, as in 
Section 4.6.2, and then remove the nonkernel items. If space is severely 
constrained, we may wish instead to store only the kernel items for each 
set, and compute GOT0 for a set of items I 
by first computing the closure 
of I. 
2. Apply Algorithm 4.62 to the kernel of each set of LR(0) items and gram- 
mar symbol X to determine which lookaheads are spontaneously gener- 
ated for kernel items in GOTO(& 
X), 
and from which items in I 
lookaheads 
are propagated to kernel items in GOTO(I, 
X). 
3. Initialize a table that gives, for each kernel item in each set of items, the 
associated lookaheads. Initially, each item has associated with it only 
those lookaheads that we determined in step (2) were generated sponta- 
neously. 
4. Make repeated passes over the kernel items in all sets. When we visit an 
item i, 
we look up the kernel items to which i propagates its lookaheads, 
using information tabulated in step (2). The current set of lookaheads 
for i is added to those already associated with each of the items to which 
i propagates its lookaheads. We continue making passes over the kernel 
items until no more new lookaheads are propagated. 
Example 4.64: Let us construct the kernels of the LALR(1) items for the 
grammar of Example 4.61. The kernels of the LR(0) items were shown in 
Fig. 4.44. When we apply Algorithm 4.62 to the kernel of set of items Io, 
we 
first compute CLOSURE({[S' 
-+ .S, 
#I)), 
which is 
Among the items in the closure, we see two where the lookahead = has been 
generated spontaneously. The first of these is L + 
. * R. This item, with * to 
the right of the dot, gives rise to [L --+ *.R, 
=]. That is, = is a spontaneously 
generated lookahead for L -+ *.R, which is in set of items Iq. Similarly, [L -+ 
-id, 
=] tells us that = is a spontaneously generated lookahead for L -+ id. in 
15. 
As # is a lookahead for all six items in the closure, we determine that the 
item St 
-+ .S in I
.
 propagates lookaheads to the following six items: 
CHAPTER 4. SYNTAX ANALYSIS 
St -+ S. in Il 
L  -+ *-R 
in I4 
S  -+ L. = 
R  in I2 
L  -+ id- 
in I5 
S  -+ R. in I3 
R  -
i
 
L. in I2 
L  -+ id. 
R  -+ L. 
19: S - + L = R .  
FROM 
Io: S'-+'S 
12: S - + L . = R  
Figure 4.46: Propagation of lookaheads 
T o  
I :  S t + S .  
12: S + L - = R  
12: 
R + L .  
13: S + R .  
14: L  -+ *-R 
15: L  -+ id. 
16: S - + L = . R  
In Fig. 4.47, we show steps (3) and (4) of Algorithm 4.63. The column 
labeled INIT 
shows the spontaneously generated lookaheads for each kernel item. 
These are only the two occurrences of = 
discussed earlier, and the spontaneous 
lookahead $ for the initial item S' -+ .S. 
On the first pass, the lookahead $ propagates from St -+ S  in I. to the 
six items listed in Fig. 4.46. The lookahead = propagates from L  -+ *.R in I4 
to items L -+ * R. in I7 and R -+ L. in Is. It also propagates to itself and to 
L  -+ id in 15, 
but these lookaheads are already present. In the second and third 
passes, the only new lookahead propagated is $, discovered for the successors of 
I2 and I4 on pass 2 and for the successor of I6 on pass 3. No new lookaheads are 
propagated on pass 4, so the final set of lookaheads is shown in the rightmost 
column of Fig. 4.47. 
Note that the shiftlreduce conflict found in Example 4.48 using the SLR 
method has disappeared with the LALR technique. The reason is that only 
lookahead $ is associated with R -+ L. in 12, so there is no conflict with the 
parsing action of shift on = generated by item S  -+ L.=R in 12. 
4.7. MORE POWERFUL LR PARSERS 
Figure 4.47: Computation of lookaheads 
SET 
ITEM 
Io: S'+.S 
I :  S ' + S .  
I,: 
S + L . = R  
R +  L. 
13: S + R .  
14: L + 
*.R 
15: L + 
id. 
I :  S + L = . R  
17: L  + 
*R. 
I*: R + 
L. 
19: S +  L =  R
e
 
4.7.6 Compaction of LR Parsing Tables 
A typical programming language grammar with 50 to 100 terminals and 100 
productions may have an LALR parsing table with several hundred states. The 
action function may easily have 20,000 entries, each requiring at least 8 bits 
to encode. On small devices, a more efficient encoding than a two-dimensional 
array may be important. We shall mention briefly a few techniques that have 
been used to compress the ACTION and GOT0 fields of an LR parsing table. 
One useful technique for compacting the action field is to recognize that 
usually many rows of the action table are identical. For example, in Fig. 4.42, 
states 0 and 3 have identical action entries, and so do 2 and 6. We can therefore 
save considerable space, at little cost in time, if we create a pointer for each 
state into a one-dimensional array. Pointers for states with the same actions 
point to the same location. To access information from this array, we assign 
each terminal a number from zero to one less than the number of terminals, 
and we use this integer as an offset from the pointer value for each state. In 
a given state, the parsing action for the ith terminal will be found i locations 
past the pointer value for that state. 
Further space efficiency can be achieved at the expense of a somewhat slower 
parser by creating a list for the actions of each state. The list consists of 
(terminal-symbol, action) pairs. The most frequent action for a state can be 
LOOKAHEADS 
PASS 3 
$ 
$ 
$ 
$ 
$ 
=/$ 
=/$ 
$ 
=/$ 
=/$ 
$ 
PASS 2 
$ 
$ 
$ 
$ 
$ 
=/$ 
=/$ 
$ 
=/$ 
=/$ 
INIT 
$ 
- 
- 
- 
- 
PASS 1 
$ 
$ 
$ 
$ 
$ 
=/$ 
=/$ 
- 
- 
- 
- 
276 
CHAPTER 4. SYNTAX ANALYSIS 
placed at the end of the list, and in place of a terminal we may use the notation 
"any," meaning that if the current input symbol has not been found so far on 
the list, we should do that action no matter what the input is. Moreover, error 
entries can safely be replaced by reduce actions, for further uniformity along a 
row. The errors will be detected later, before a shift move. 
Example 4.65 : 
Consider the parsing table of Fig. 4.37. First, note that the 
actions for states 0, 4, 6, and 7 agree. We can represent them all by the list 
SYMBOL 
ACTION 
id 
s5 
( 
s4 
any 
error 
State 1 
has a similar list: 
+ 
s6 
$ 
acc 
any 
error 
In state 2, we can replace the error entries by r2, so reduction by production 2 
will occur on any input but *. Thus the list for state 2 is 
State 3 has only error and r4 entries. We can replace the former by the 
latter, so the list for state 3 consists of only the pair (any, r4). States 5, 10, 
and 1
1
 can be treated similarly. The list for state 8 is 
+ 
s6 
1 
s l l  
any 
error 
and for state 9 
* 
s7 
) 
s l l  
any 
r l  
We can also encode the GOTO table by a list, but here it appears more 
efficient to make a list of pairs for each nonterminal A. Each pair on the list 
for A is of the form (currentstate, 
nextstate), indicating 
4.7. MORE POWERFUL LR PARSERS 
277 
This technique is useful because there tend to be rather few states in any one 
column of the GOTO table. The reason is that the GOT0 on nonterminal A 
can only be a state derivable from a set of items in which some items have A 
immediately to the left of a dot. No set has items with X and Y immediately 
to the left of a dot if X # Y .  Thus, each state appears in at most one GOT0 
column. 
For more space reduction, we note that the error entries in the goto table are 
never consulted. We can therefore replace each error entry by the most common 
non-error entry in its column. This entry becomes the default; it is represented 
in the list for each column by one pair with any in place of currentstate. 
Example 4.66 : Consider Fig. 4.37 again. The column for F has entry 10 for 
state 7, and all other entries are either 3 or error. We may replace error by 3 
and create for column F the list 
Similarly, a suitable list for column T is 
For column E we may choose either 1 or 8 to be the default; two entries are 
necessary in either case. For example, we might create for column E the list 
This space savings in these small examples may be misleading, because the 
total number of entries in the lists created in this example and the previous one 
together with the pointers from states to action lists and from nonterminals 
to next-state lists, result in unimpressive space savings over the matrix imple- 
mentation of Fig. 4.37. For practical grammars, the space needed for the list 
representation is typically less than ten percent of that needed for the matrix 
representation. The table-compression methods for finite automata that were 
discussed in Section 3.9.8 can also be used to represent LR parsing tables. 
4.7.7 Exercises for Section 4.7 
Exercise 4.7.1 : 
Construct the 
a) canonical LR, and 
b) LALR 
278 
CHAPTER 4. SYNTAX ANALYSIS 
sets of items for the grammar S -+ S S + I S S * I a of Exercise 4.2.1. 
Exercise 4.7.2 : 
Repeat Exercise 4.7.1 for each of the (augmented) grammars 
of Exercise 4.2.2(a)-(g). 
! 
Exercise 4.7.3 : For the grammar of Exercise 4.7.1, use Algorithm 4.63 to 
compute the collection of LALR sets of items from the kernels of the LR(0) sets 
of items. 
! Exercise 4.7.4 : 
Show that the following grammar 
is LALR(1) but not SLR(1). 
! 
Exercise 4.7.5 : 
Show that the following grammar 
is LR(1) but not LALR(1). 
4.8 Using Ambiguous Grammars 
It is a fact that every ambiguous grammar fails to be LR and thus is not in 
any of the classes of grammars discussed in the previous two sections. How- 
ever, certain types of ambiguous grammars are quite useful in the specification 
and implementation of languages. For language constructs like expressions, an 
ambiguous grammar provides a shorter, more natural specification than any 
equivalent unambiguous grammar. Another use of ambiguous grammars is in 
isolating commonly occurring syntactic constructs for special-case optimiza- 
tion. With an ambiguous grammar, we can specify the special-case constructs 
by carefully adding new productions to the grammar. 
Although the grammars we use are ambiguous, in all cases we specify dis- 
ambiguating rules that allow only one parse tree for each sentence. In this way, 
the overall language specification becomes unambiguous, and sometimes it be- 
comes possible to design an LR parser that follows the same ambiguity-resolving 
choices. We stress that ambiguous constructs should be used sparingly and in 
a strictly controlled fashion; otherwise, there can be no guarantee as to what 
language is recognized by a parser. 
4.8. USING AMBIGUOUS GRAMMARS 
4.8.1 Precedence and Associativity to Resolve Conflicts 
Consider the ambiguous grammar (4.3) for expressions with operators + and 
*, repeated here for convenience: 
E - + E + E I  E * E I  (E) 
l i d  
This grammar is ambiguous because it does not specify the associativity or 
precedence of the operators + and *. The unambiguous grammar (4.1), which 
includes productions E  -+ E  + 
T and T -+ T * F, 
generates the same language, 
but gives + 
lower precedence than *, and makes both operators left associative. 
There are two reasons why we might prefer to use the ambiguous grammar. 
First, as we shall see, we can easily change the associativity and precedence 
of the operators + and * without disturbing the productions of (4.3) or the 
number of states in the resulting parser. Second, the parser for the unam- 
biguous grammar will spend a substantial fraction of its time reducing by the 
productions E  -+ T and T -+ F, 
whose sole function is to enforce associativity 
and precedence. The parser for the ambiguous grammar (4.3) will not waste 
time reducing by these single productions (productions whose body consists of 
a single nonterminal) . 
The sets of LR(0) items for the ambiguous expression grammar (4.3) aug- 
mented by E' -+ E  are shown in Fig. 4.48. Since grammar (4.3) is ambiguous, 
there will be parsing-action conflicts when we try to produce an LR parsing 
table from the sets of items. The states corresponding to sets of items I7 
and 
I8 
generate these conflicts. Suppose we use the SLR approach to constructing 
the parsing action table. The conflict generated by I7 between reduction by 
E  -+ E  + 
E  and shift on + or * cannot be resolved, because + and * are each 
in FOLLOW(E). 
Thus both actions would be called for on inputs + and *. A 
similar conflict is generated by Is, 
between reduction by E  -+ E  * E  and shift 
on inputs + and *. In fact, each of our LR parsing table-construction methods 
will generate these conflicts. 
However, these problems can be resolved using the precedence and associa- 
tivity information for + and *. Consider the input id + 
id * id, which causes a 
parser based on Fig. 4.48 to enter state 7 after processing id + 
id; in particular 
the parser reaches a configuration 
For convenience, the symbols corresponding to the states 1, 4, and 7 are also 
shown under PREFIX. 
If * takes precedence over +, we know the parser should shift * onto the 
stack, preparing to reduce the * and its surrounding id symbols to an expression. 
This choice was made by the SLR parser of Fig. 4.37, based on an unambiguous 
grammar for the same language. On the other hand, if + 
takes precedence over 
*, we know the parser should reduce E  + 
E to E. Thus the relative precedence 
CHAPTER 4. SYNTAX ANALYSIS 
I,,: 
E' -+ .E 
E-+.E+E 
E + - E * E  
E + 
.(E) 
E -+ .id 
13: E -+ id. 
I,: 
E -+ (E.) 
E + E . + E  
E + E . * E  
Figure 4.48: Sets of LR(0) 
items for an augmented expression grammar 
of + 
followed by * uniquely determines how the parsing action conflict between 
reducing E -+ E + 
E and shifting on * in state 7 should be resolved. 
If the input had been id + 
id + 
id instead, the parser would still reach a 
configuration in which it had stack 0 1 
4 7 after processing input id + 
id. On 
input + there is again a shift/reduce conflict in state 7. Now, however, the 
associativity of the + 
operator determines how this conflict should be resolved. 
If + is left associative, the correct action is to reduce by E -+ E + 
E. That is, 
the id symbols sbrrounding the first + 
must be grouped first. Again this choice 
coincides with what the SLR parser for the unambiguous grammar would do. 
In summary, assuming + is left associative, the action of state 7 on input 
+ should be to reduce by E -+ E + 
E, and assuming that * takes precedence 
over +, 
the action of state 7 on input * should be to shift. Similarly, assuming 
that * is left associative and takes precedence over +, we can argue that state 
8, which can appear on top of the stack only when E * E are the top three 
grammar symbols, should have the action reduce E + 
E * E on both + and * 
inputs. In the case of input +, the reason is that * takes precedence over +, 
while in the case of input *, the rationale is that * is left associative. 
4.8. USING AMBIGUOUS GRAMMARS 
28 
1 
Proceeding in this way, we obtain the LR parsing table shown in Fig. 4.49. 
Productions 1 through 4 are E -+ E + E, E --+ E * E, -+ (E), and E -+ 
id, respectively. It is interesting that a similar parsing action table would be 
produced by eliminating the reductions by the single productions E -+ T and 
T -+ F from the SLR table for the unambiguous expression grammar (4.1) 
shown in Fig. 4.37. Ambiguous grammars like the one for expressions can be 
handled in a similar way in the context of LALR and canonical LR parsing. 
Figure 4.49: Parsing table for grammar (4.3) 
4.8.2 
The "Dangling-Else" Ambiguity 
GOT0 
E 
1 
6 
7 
8 
STATE 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
Consider again the following grammar for conditional statements: 
ACTION 
- 
. 
i
d
+
*
(
)
$
 
s3 
s2 
s4 
s5 
acc 
s3 
s2 
r4 
r4 
r4 
r4 
s3 
s 
2 
s3 
s 
2 
s4 
s5 
s9 
r l  s5 
r l  r l  
r2 
r2 
r2 
r2 
r3 
r3 
r3 r3 
stmt -+ if expr then stmt else stmt 
I if expr then strnt 
I 
other 
As we noted in Section 4.3.2, this grammar is ambiguous because it does not 
resolve the dangling-else ambiguity. To simplify the discussion, let us consider 
an abstraction of this grammar, where i stands for if expr then, e stands for 
else, and a stands for "all other productions.'' We can then write the grammar, 
with augmenting production S' -+ 
S, as 
The sets of LR(0) items for grammar (4.67) are shown in Fig. 4.50. The ambi- 
guity in (4.67) gives rise to a shiftjreduce conflict in la. 
There, S 
-+ iS.eS calls 
for a shift of e and, since FOLLOW(S) 
= {e, $1, 
item S 
--+ i s .  calls for reduction 
by S 
-+ is on input e. 
Translating back to the if-then-else terminology, given 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.50: LR(0) states for augmented grammar (4.67) 
if expr then stmt 
on the stack and else as the first input symbol, should we shift else onto the 
stack (i.e., shift e) or reduce if expr then stmt (i.e, reduce by S --+ is)? The 
answer is that we should shift else, because it is "associated" with the previous 
then. In the terminology of grammar (4.67), the e on the input, standing for 
else, can only form part of the body beginning with the i S  now on the top of 
the stack. If what follows e on the input cannot be parsed as an S, 
completing 
body iSeS, then it can be shown that there is no other parse possible. 
We conclude that the shiftlreduce conflict in I4 should be resolved in favor 
of shift on input e. The SLR parsing table constructed from the sets of items 
of Fig. 4.48, using this resolution of the parsing-action conflict in I4 on input 
e, is shown in Fig. 4.51. Productions 1 
through 3 are S -+ iSeS, S -+ 
i s ,  and 
S -+ a, respectively. 
Figure 4.51: LR parsing table for the "dangling-else" grammar 
GOT0 
S 
1 
4 
6 
STATE 
0 
1 
2 
3 
4 
5 
6 
ACTION 
i
e
a
$
 
s 
2 
s3 
acc 
s 
2 
s3 
r3 
r3 
s 
5 
r2 
s2 
s3 
r l  
r l  
4.8. USING AMBIGUOUS GRAMMARS 
283 
For example, on input iiaea, the parser makes the moves shown in Fig. 4.52, 
corresponding to the correct resolution of the "dangling-else." At line (5), state 
4 selects the shift action on input e, whereas at line (9), state 4 calls for reduction 
by S -+ iS on input $. 
2 
i  
i  
i i a  
i i S  
i i S e  
i i S e a  
i i S e S  
i  
S 
S 
ACTION 
shift 
shift 
shift 
shift 
reduce by S -+ a  
shift 
reduce by S -+ a  
reduce by S -+ iSeS 
reduce by S -+ iS 
accept 
Figure 4.52: Parsing actions on input iiaea 
By way of comparison, if we are unable to use an ambiguous grammar to 
specify conditional statements, then we would have to use a bulkier grammar 
along the lines of Example 4.16. 
4
.
8
.
3
 Error Recovery in LR Parsing 
An LR parser will detect an error when it consults the parsing action table and 
finds an error entry. Errors are never detected by consulting the goto table. An 
LR parser will announce an error as soon as there is no valid continuation for 
the portion of the input thus far scanned. A canonical LR parser will not make 
even a single reduction before announcing an error. SLR and LALR parsers 
may make several reductions before announcing an error, but they will never 
shift an erroneous input symbol onto the stack. 
In LR parsing, we can implement panic-mode error recovery as follows. We 
scan down the stack until a state s with a goto on a particular nonterminal 
A is found. Zero or more input symbols are then discarded until a symbol 
a  is found that can legitimately follow A. The parser then stacks the state 
GOTO(S, 
A) and resumes normal parsing. There might be more than one choice 
for the nonterminal A. Normally these would be nonterminals representing 
major program pieces, such as an expression, statement, or block. For example, 
if A is the nonterminal stmt, a  might be semicolon or ), 
which marks the end 
of a statement sequence. 
This method of recovery attempts to eliminate the phrase containing the 
syntactic error. The parser determines that a string derivable from A contains 
an error. Part of that string has already been processed, and the result of this 
284 
CHAPTER 4. SYNTAX ANALYSIS 
processing is a sequence of states on top of the stack. The remainder of the 
string is still in the input, and the parser attempts to skip over the remainder 
of this string by looking for a symbol on the input that can legitimately follow 
A. By removing states from the stack, skipping over the input, and pushing 
GOTO(S, A) on the stack, the parser pretends that it has found an instance of 
A and resumes normal parsing. 
Phrase-level recovery is implemented by examining each error entry in the 
LR parsing table and deciding on the basis of language usage the most likely 
programmer error that would give rise to that error. An appropriate recovery 
procedure can then be constructed; presumably the top of the stack and/or first 
input symbols would be modified in a way deemed appropriate for each error 
entry. 
In designing specific error-handling routines for an LR parser, we can fill in 
each blank entry in the action field with a pointer to an error routine that will 
take the appropriate action selected by the compiler designer. The actions may 
include insertion or deletion of symbols from the stack or the input or both, 
or alteration and transposition of input symbols. We must make our choices 
so that the LR parser will not get into an infinite loop. A safe strategy will 
assure that at least one input symbol will be removed or shifted eventually, or 
that the stack will eventually shrink if the end of the input has been reached. 
Popping a stack state that covers a nonterminal should be avoided, because 
this modification eliminates from the stack a construct that has already been 
successfully parsed. 
Example 4.68 : 
Consider again the expression grammar 
Figure 4.53 shows the LR parsing table from Fig. 4.49 for this grammar, 
modified for error detection and recovery. We have changed each state that 
calls for a particular reduction on some input symbols by replacing error entries 
in that state by the reduction. This change has the effect of postponing the 
error detection until one or more reductions are made, but the error will still 
be caught before any shift move takes place. The remaining blank entries from 
Fig. 4.49 have been replaced by calls to error routines. 
The error routines are as follows. 
el: This routine is called from states 0, 2, 4 and 5, all of which expect the 
beginning of an operand, either an id or a left parenthesis. Instead, +, *, 
or the end of the input was found. 
push state 3 (the goto of states 0, 2, 4 and 5 on id); 
issue diagnostic "missing operand." 
e2: Called from states 0, 1, 
2, 4 and 5 on finding a right parenthesis. 
remove the right parenthesis from the input; 
issue diagnostic "unbalanced right parenthesis." 
4.8. USING AMBIGUOUS GRAMMARS 
Figure 4.53: LR parsing table with error routines 
e3: Called from states 1 
or 6 when expecting an operator, and an id or right 
parenthesis is found. 
GOT0 
E 
1 
6 
7 
8 
STATE 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
push state 4 (corresponding to symbol +) onto the stack; 
issue diagnostic "missing operator." 
ACTION 
i
d
+
*
 
$ 
s3 
el el s2 
e2 
el 
e3 s4 
s5 
e3 
e2 
acc 
s3 el e l  s2 
e2 
el 
r4 
r4 
r4 
r4 
r4 
r4 
s3 
el el s2 
e2 
el 
s3 el el s2 
e2 
el 
e3 
s4 
s5 
e3 
s9 
e4 
r l  
r l  
s5 
r l  
r l  
r l  
r2 
r2 
r2 
r2 
r2 
r2 
r3 
r3 
r3 
r3 
r3 
r3 
e4: Called from state 6 when the end of the input is found. 
push state 9 (for a right parenthesis) onto the stack; 
issue diagnostic "missing right parenthesis." 
On the erroneous input id + 
), the sequence of configurations entered by the 
parser is shown in Fig. 4.54. 
4.8.4 Exercises for Section 4.8 
! 
Exercise 4.8.1 : 
The following is an ambiguous grammar for expressions with 
n binary, infix operators, at n different levels of precedence: 
a) As a function of n, what are the SLR sets of items? 
b) How would you resolve the conflicts in the SLR items so that all oper- 
ators are left associative, and el takes precedence over 62, which takes 
precedence over 03, and so on? 
c) Show the SLR parsing table that results from your decisions in part (b). 
286 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.54: Parsing and error recovery moves made by an LR parser 
STACK 
0 
0 3 
0 1 
0 1 4  
0 1 4  
0 1 4 3  
0 1 4 7  
0 1 
d) Repeat parts (a) and (c) for the unambiguous grammar, which defines 
the same set of expressions, shown in Fig. 4.55. 
e) How do the counts of the number of sets of items and the sizes of the tables 
for the two (ambiguous and unambiguous) grammars compare? What 
does that comparison tell you about the use of ambiguous expression 
grammars? 
SYMBOLS 
id 
E 
E +  
E +  
E + i d  
E +  
E +  
Figure 4.55: Unambiguous grammar for n operators 
! 
Exercise 4.8.2 : 
In Fig. 4.56 is a grammar for certain statements, similar to 
that discussed in Exercise 4.4.12. Again, e and s are terminals standing for 
conditional expressions and "other statements," respectively. 
INPUT 
i d + ) $  
+ I $  
+I$ 
) $ 
$ 
$ 
$ 
$ 
a) Build an LR parsing table for this grammar, resolving conflicts in the 
usual way for the dangling-else problem. 
ACTION 
"unbalanced right parenthesis" 
e2 removes right parenthesis 
"missing operand" 
el pushes state 3 onto stack 
b) Implement error correction by filling in the blank entries in the parsing 
table with extra reduce-actions or suitable error-recovery routines. 
c) Show the behavior of your parser on the following inputs: 
(i) 
if e then s ; if e then s end 
(ii) 
while e do begin s ; if e then s ; end 
4.9. PARSER GENERATORS 
stmt + if e then stmt 
I 
if e then stmt else stmt 
( 
while e do stmt 
( 
begin list end 
I 
list 
+ list ; stmt 
I 
stmt 
Figure 4.56: A grammar for certain kinds of statements 
4.9 Parser Generators 
This section shows how a parser generator can be used to facilitate the construc- 
tion of the front end of a compiler. We shall use the LALR parser generator 
Yacc as the basis of our discussion, since it implements many of the concepts 
discussed in the previous two sections and it is widely available. Yacc stands for 
"yet another compiler-compiler," reflecting the popularity of parser generators 
in the early 1970s when the first version of Yacc was created by S. C. Johnson. 
Yacc is available as a command on the UNIX system, and has been used to help 
implement many production compilers. 
4.9.1 The Parser Generator Yacc 
A translator can be constructed using Yacc in the manner illustrated in Fig. 
4.57. First, a file, say translate. 
y, containing a Yacc specification of the 
translator is prepared. The UNIX system command 
yacc translate. 
y 
transforms the file translate. 
y into a C program called y . 
tab. c using the 
LALR method outlined in Algorithm 4.63. The program y.tab. c is a repre- 
sentation of an LALR parser written in C, along with other C routines that the 
user may have prepared. The LALR parsing table is compacted as described 
in Section 4.7. By compiling y 
. 
tab. c along with the ly library that contains 
the LR parsing program using the command 
we obtain the desired object program a. 
out that performs the translation spec- 
ified by the original Yacc program.7 If other procedures are needed, they can 
be compiled or loaded with y 
. 
tab . 
c, just as with any C program. 
A Yacc source program has three parts: 
7 ~ h e  
name ly is system dependent. 
CHAPTER 4. SYNTAX ANALYSIS 
n - ~ Y a c c ~ t  
y.tab.c 
specificatio 
compiler 
translate. 
y 
input 
a. 
out 
output 
y.tab.c 
Figure 4.57: Creating an input/output translator with Yacc 
declarations 
%% 
u 
compiler 
translation rules 
a. 
out 
supporting C routines 
Example 4.69 : To: illustrate how to prepare a Yacc source program, let us 
construct a simple desk calculator that reads an arithmetic expression, evaluates 
it, and then prints its numeric value. We shall build the desk calculator starting 
with the with the following grammar for arithmetic expressions: 
E + E + T I T  
T  -
,
 T * F I F  
F  + ( E 
) 
I digit 
The token digit is a single digit between 0 and 9. A Yacc desk calculator 
program derived from this grammar is shown in Fig. 4.58. 
The Declarations Part 
There are two sections in the declarations part of a Yacc program; both are 
optional. In the first section, we put ordinary C declarations, delimited by %C 
and %). 
Here we piace declarations of any temporaries used by the translation 
rules or procedures of the second and third sections. In Fig. 4.58, this section 
contains only the include-st 
at 
ement 
that causes the C preprocessor to include the standard header file cctype 
. 
h> 
that contains the predicate isdigit. 
Also in the declarations part are declarations of grammar tokens. In Fig. 
4.58, the statement 
%token DIGIT 
4.9. PARSER GENERATORS 
%token 
DIGIT 
%% 
l i n e  
: expr ) \ n )  
( printf ( 
"%d\ntt 
, 
$1) ; ) 
9 
expr 
: expr ) + )  term 
( $$ = $1 + $3; 3 
1 term 
9 
term 
: term ) * )  
f a c t o r  { $$ = $1 * $3; ) 
I f a c t o r  
9 
f a c t o r  : ' 0  expr ) ) )  
( $$ = $2; 3 
I DIGIT 
%% 
y y l e x 0  ( 
i n t  c; 
c = get 
char () ; 
i f  ( i s d i g i t  
(c) 
) ( 
yylval = c-'0); 
return DIGIT; 
J 
return c ;  
3 
Figure 4.58: Yacc specification of a simple desk calculator 
declares DIGIT to be a token. Tokens declared in this section can then be 
used in the second and third parts of the Yacc specification. If Lex is used 
to create the lexical analyzer that passes token to the Yacc parser, then these 
token declarations are also made available to the analyzer generated by Lex, as 
discussed in Section 3.5.2. 
The Translation Rules Part 
In the part of the Yacc specification after the first %% pair, we put the translation 
rules. Each rule consists of a grammar production and the associated semantic 
action. A set of productions that we have been writing: 
(head) -+ (body), I (body)z I . - .  
I (body), 
would be written in Yacc as 
CHAPTER 4. SYNTAX ANALYSIS 
(head) 
: 
  body)^ 
C (semantic a c t i ~ n ) ~  
) 
I 
(body)z 
C (semantic a ~ t i o n ) ~  
) 
I 
(body), 
C (semanticaction), 3 
In a Yacc production, unquoted strings of letters and digits hot declared to 
be tokens are taken to be nonterminals. A quoted single character, e.g. 'c', 
is taken to be the terminal symbol c, as wkll as the integer code for the token 
represented by that character (i.e., Lex 
would return the character code for ) c ' 
to the parser, as an integer). Alternative bodies can be separated by a vertical 
bar, and a semicolon follows each head with its alternatives and their semantic 
actions. The first head is taken to be the start symbol. 
A Yacc semantic action is a sequence of C statements. In a semantic action, 
the symbol $$ refers to the attribute value associated with the nonterminal of 
the head, while $i refers to the value associated with the ith grammar symbol 
(terminal or nonterminal) of the body. The semantic action is performed when- 
ever we reduce by the associated production, so normally the semantic action 
computes a value for $$ in terms of the $i's. In the Yacc specification, we have 
written the two E-productions 
and their associated semantic actions as: 
expr : expr '+) term 
I $$ = $1 + $3; 3 
1 term 
s 
Note that the nonterminal term in the first production is the third grammar 
symbol of the body, while + 
is the second. The semantic action associated with 
the first production adds the value of the expr 
and the term 
of the body and 
assigns the result as the value for the nonterminal expr 
of the head. We have 
omitted the semantic action for the second production altogether, since copying 
the value is the default action for productions with a single grammar symbol 
in the body. In general, ( 
$$ = $1; ) 
is the default semantic action. 
Notice that we have added a new starting production 
line : expr '\n' 
( printf 
("%d\nfl, 
$1) 
; 3 
to the Yacc specification. This production says that an input to the desk 
calculator is to be an expression followed by a newline character. The semantic 
action associated with this production prints the decimal value of the expression 
followed by a newline character. 
4.9. PARSER GENERATORS 
The Supporting C-Routines Part 
The third part of a Yacc specification consists of supporting C-routines. A 
lexical analyzer by the name yylex () must be provided. Using Lex to produce 
yylex() is a common choice; see Section 4.9.3. Other procedures such as error 
recovery routines may be added as necessary. 
The lexical analyzer yylex() produces tokens consisting of a token name 
and its associated attribute value. If a token name such as DIGIT is returned, 
the token name must be declared in the first section of the Yacc specification. 
The attribute value associated with a token is communicated to the parser 
through a Y acc-defined variable yylval. 
The lexical analyzer in Fig. 4.58 is very crude. It reads input characters 
one at a time using the C-function get 
char 
() 
. If the character is a digit, the 
value of the digit is stored in the variable yylval, and the token name DIGIT 
is returned. Otherwise, the character itself is returned as the token name. 
4.9.2 Using Yacc with Ambiguous Grammars 
Let us now modify the Yacc specification so that the resulting desk calculator 
becomes more useful. First, we shall allow the desk calculator to evaluate a 
sequence of expressions, one to a line. We shall also allow blank lines between 
expressions. We do so by changing the first rule to 
lines : lines expr ) \n) 
( printf (I1%g\n", 
$2) ; 3 
I lines )\n7 
I /* empty */ 
9 
In Yacc, an empty alternative, as the third line is, denotes e. 
Second, we shall enlarge the class of expressions to include numbers instead 
of single digits and to include the arithmetic operators +, -, (both binary and 
unary), *, and /. The easiest way to specify this class of expressions is to use 
the ambiguous grammar 
E + E + E  
I E - E  I E * E  I E / E  1 - E 1 number 
The resulting Yacc specification is shown in Fig. 4.59. 
Since the grammar in the Yacc specification in Fig. 4.59 is ambiguous, the 
LALR algorithm will generate parsing-action conflicts. Yacc reports the num- 
ber of parsing-action conflicts that are generated. A description of the sets of 
items and the parsing-action conflicts can be obtained by invoking Yacc with a 
-v option. This option generates an additional file y . 
output that contains the 
kernels of the sets of items found for the grammar, a description of the parsing 
action conflicts generated by the LALR algorithm, and a readable represen- 
tation of the LR parsing table showing how the parsing action conflicts were 
resolved. Whenever Yacc reports that it has found parsing-action conflicts, it 
CHAPTER 4. SYNTAX ANALYSIS 
%< 
#include <ctype.h> 
#include <stdio.h> 
#define Y
Y
S
T
Y
P
E
 double /* double type f o r  Yacc stack */ 
%3 
%token 
N
U
M
B
E
R
 
% l e f t  )+' '-' 
% l e f t  '*' '/) 
%right 
U
M
I
N
U
S
 
%% 
lines : lines expr ' 
\n) 
< printf ("%g\n8' 
, 
$2) 
; 3 
I lines '\n' 
I /* empty */ 
9 
expr 
: expr '+' expr 
< $$ = $1 + $3; 1 
1 e x p r ' - ' e x p r  
< $ $ = $ I - $ 3 ; )  
I e x p r ' * ) e x p r  
< $ $ = $ 1 * $ 3 ; >  
I e x p r ' / ) e x p r  
< $ $ = $ 1 / $ 3 ; )  
1 ) ( )  expr '1) 
< $$ = $2; 3 
I 
' - 9  
expr %prec 
U
M
I
N
U
S
 < $$ = - $2; 3 
I N
U
M
B
E
R
 
9 
%% 
yylex0 < 
i n t  c ;  
while ( ( c = getchar0 
== ' ' 1; 
if ( (C == ) .  
P )  
( I (isdigit 
(c)) ) < 
ungetc(c, stdin) ; 
scanf ("%lfN, 
&yylval) 
; 
return N
U
M
B
E
R
;
 
3 
return c; 
Figure 4.59: Yacc specification for a more advanced desk calculator. 
4.9. PARSER GENERAT 
293 
is wise to create and consult the file y . 
output to see why the parsing-action 
conflicts were generated and to see whether they were resolved correctly. 
Unless otherwise instructed Y acc will resolve all parsing action conflicts 
using the following two rules: 
1. A reduce/reduce conflict is resolved by choosing the conflicting production 
listed first in the Yacc specification. 
2. A shift/reduce conflict is resolved in favor of shift. This rule resolves the 
shift/reduce conflict arising from the dangling-else ambiguity correctly. 
Since these default rules may not always be what the compiler writer wants, 
Yacc provides a general mechanism for resolving shiftlreduce conflicts. In the 
declarations portion, we can assign precedences and associativities to terminals. 
The declaration 
makes + and - 
be of the same precedence and be left associative. We can declare 
an operator to be right associative by writing 
and we can force an operator to be a nonassociative binary operator (i.e., two 
occurrences of the operator cannot be combined at all) by writing 
The tokens are given precedences in the order in which they appear in the 
declarations part, lowest first. Tokens in the same declaration have the same 
precedence. Thus, the declaration 
%right UMINUS 
in Fig. 4.59 gives the token UMINUS a precedence level higher than that of the 
five preceding terminals. 
Yacc resolves shiftlreduce conflicts by attaching a precedence and associa- 
tivity to each production involved in a conflict, as well as to each terminal 
involved in a conflict. If it must choose between shifting input symbol a and re- 
ducing by production A -+ a, 
Yacc reduces if the precedence of the production 
is greater than that of a, or if the precedences are the same and the associativity 
of the production is l e f t  
. Otherwise, shift is the chosen action. 
Normally, the precedence of a production is taken to be the same as that of 
its rightmost terminal. This is the sensible decision in most cases. For example, 
given productions 
294 
CHAPTER 4. SYNTAX ANALYSIS 
we would prefer to reduce by E -+ 
E+E with lookahead +, because the + in 
the body has the same precedence as the lookahead, but is left associative. 
With lookahead *, we would prefer to shift, because the lookahead has higher 
precedence than the + in the production. 
In those situations where the rightmost terminal does not supply the proper 
precedence to a production, we can force a precedence by appending to a pro- 
duct 
ion the tag 
Xprec (terminal) 
The precedence and associativity of the production will then be the same as that 
of the terminal, which presumably is defined in the declaration section. Yacc 
does not report shiftlreduce conflicts that are resolved using this precedence 
and associativity mechanism. 
This "terminal" can be a placeholder, like UMINUS 
in Fig. 4.59; this termi- 
nal is not returned by the lexical analyzer, but is declared solely to define a 
precedence for a production. In Fig. 4.59, the declaration 
%right UMINUS 
assigns to the token UMINUS 
a precedence that is higher than that of * and /. 
In the translation rules part, the tag: 
Xprec UMINUS 
at the end of the production 
expr : '-' expr 
makes the unary-minus operator in this production have a higher precedence 
than any other operator. 
4
.
9
.
3
 Creating Yacc Lexical Analyzers with Lex 
Lex 
was designed to produce lexical analyzers that could be used with Yacc. The 
Lex library 11 
will provide a driver program named yylex 
0, 
the name required 
by Yacc 
for its lexical analyzer. If Lex 
is used to produce the lexical analyzer, 
we replace the routine yylex() in the third part of the Yacc specification by 
the statement 
and we have each Lex action return a terminal known to Yacc. By using 
the #include "1ex.yy. 
ctl 
statement, the program yylex 
has access to Yacc's 
names for tokens, since the Lex output file is compiled as part of the Yacc 
output file y 
. 
tab 
. 
c. 
Under the UNIX system, if the Lex specification is in the file first 
.l 
and 
the Yacc 
specification in second. 
y, 
we can say 
4.9. PARSER GENERATORS 
lex first.1 
yacc sec0nd.y 
cc y.tab.c -1y -11 
to obtain the desired translator. 
The Lex 
specification in Fig. 4.60 can be used in place of the lexical analyzer 
in Fig. 4.59. The last pattern, meaning "any character," must be written \n 
l . 
since the dot in Lex matches any character except newline. 
number 
[0-91 
+\e. 
? 1 [o-91 
*\e. 
[o-91 
+ 
%% 
[ 
1 
( /* skip blanks */ ) 
(number) ( sscanf 
(yytext 
, "%lfl', 
&yylval) ; 
return NUMBER; ) 
\n I . 
{ return yytext 
C01 ; ) 
Figure 4.60: Lex specification for yylex() in Fig. 4.59 
4.9.4 Error Recovery in Yacc 
In Yacc, 
error recovery uses a form of error productions. First, the user de- 
cides what "major" nonterminals will have error recovery associated with them. 
Typical choices are some subset of the nonterminals generating expressions, 
statements, blocks, and functions. The user then adds to the grammar error 
productions of the form A --+ error a, where A is a major nonterminal and 
a is a string of grammar symbols, perhaps the empty string; error is a Yacc 
reserved word. Yacc will generate a parser from such a specification, treating 
the error productions as ordinary productions. 
However, wherl the parser generated by Yacc 
encounters an error, it treats 
the states whose sets of items contain error productions in a special way. On 
encountering an error, Yacc 
pops symbols from its stack until it finds the top- 
most state on its stack whose underlying set of items includes an item of the 
form A --+ . 
error a. The parser then "shifts" a fictitious token error onto the 
stack, as though it saw the token error on its input. 
When a is e, a reduction to A occurs immediately and the semantic action 
associated with the production A -+ . 
error (which might be a user-specified 
error-recovery routine) is invoked. The parser then discards input symbols until 
it finds an input symbol on which normal parsing can proceed. 
If a is not empty, Yacc skips ahead on the input looking for a substring 
that can be reduced to a. If a consists entirely of terminals, then it looks for 
this string of terminals on the input, and "reduces" them by shifting them onto 
the stack. At this point, the parser will have error a on top of its stack. The 
parser will then reduce error cu to A, and resume normal parsing. 
For example, an error production of the form 
CHAPTER 4. SYNTAX ANALYSIS 
%C 
#include <ctype.h> 
#include <stdio.h> 
#define YYSTYPE double /* double type f o r  Yacc stack */ 
%3 
%token 
N
U
M
B
E
R
 
% l e f t  ) + )  ) - )  
% l e f t  ) * )  '/) 
%right 
U
M
I
N
U
S
 
%% 
l i n e s  : l i n e s  expr )\n) 
C printf("%g\ntt, 
$2); 1 
I l i n e s  )\n) 
I /* empty */ 
1 error '\n) { yyerror ("reenter previous line: It) ; 
yyerrok; 3 
9 
expr 
: e x p r ) + ) e x p r  
C $ $ = $ 1 + $ 3 ; )  
I expr '-' expr 
C $$ = $1 - $3; 3 
I expr ) * )  expr 
I $$ = $1 * $3; I 
I e x p r ) / ) e x p r  
C $ $ = $ 1 / $ 3 ; )  
1 ) ( )  expr 
C $$ = $2; 3 
1 
9 - )  expr %prec 
U
M
I
N
U
S
 C $$ = - $2; 
I N
U
M
B
E
R
 
Figure 4.61: Desk calculator with error recovery 
stmt --+ error ; 
would specify to the parser that it should skip just beyond the next semicolon 
on seeing an error, and assume that a statement had been found. The semantic 
routine for this error production would not need to manipulate the input, but 
could generate a diagnostic message and set a flag to inhibit generation of object 
code, for example. 
Example 4.70 : 
Figure 4.61 shows the Yacc desk calculator of Fig. 4.59 with 
the error production 
l i n e s  : error '\n) 
This error production causes the desk calculator to suspend normal parsing 
when a syntax error is found on an input line. On encountering the error, 
4.10. SUMMARY OF CHAPTER 4 
297 
the parser in the desk calculator starts popping symbols from its stack until it 
encounters a state that has a shift action on the token error. State 0 is such a 
state (in this example, it's the only such state), since its items include 
lines += 
- error ' 
\nJ 
Also, state 0 is always on the bottom of the stack. The parser shifts the token 
error onto the stack, and then proceeds to skip ahead in the input until it has 
found a newline character. At this point the parser shifts the newline onto the 
stack, reduces error '\nJ to lines, and emits the diagnostic message "reenter 
previous line:". The special Yacc 
routine yyerrok 
resets the parser to its normal 
mode of operation. 
4.9.5 Exercises for Section 4.9 
! 
Exercise 4.9.1 : 
Write a Yacc 
program that takes boolean expressions as input 
[as given by the grammar of Exercise 4.2.2(g)] and produces the truth value of 
the expressions. 
! Exercise 4.9.2 
: Write a Yacc program that takes lists (as defined by the 
grammar of Exercise 4.2.2(e), but with any single character as an element, not 
just a) and produces as output a linear representation of the same list; i.e., a 
single list of the elements, in the same order that they appear in the input. 
! 
Exercise 4.9.3 
: 
Write a Yacc 
program that tells whether its input is a palin- 
drome (sequence of characters that read the same forward and backward). 
!! Exercise 4.9.4 
: 
Write a Yacc 
program that takes regular expressions (as de- 
fined by the grammar of Exercise 4.2.2(d), but with any single character as an 
argument, not just a) and produces as output a transition table for a nonde- 
terministic finite automaton recognizing the same language. 
4.10 Summary of Chapter 4 
+ Parsers. A parser takes as input tokens from the lexical analyzer and 
treats the token names as terminal symbols of a context-free grammar. 
The parser then constructs a parse tree for its input sequence of tokens; 
the parse tree may be constructed figuratively (by going through the cor- 
responding derivation steps) or literally. 
+ Context-Free Grammars. A grammar specifies a set of terminal symbols 
(inputs), another set of nonterminals (symbols representing syntactic con- 
structs), and a set of productions, each of which gives a way in which 
strings represented by one nonterminal can be constructed from terminal 
symbols and strings represented by certain other nonterminals. A pro- 
duction consists of a head (the nonterminal to be replaced) and a body 
(the replacing string of grammar symbols). 
CHAPTER 4. SYNTAX ANALYSIS 
+ Derivations. The process of starting with the start-nonterminal of a gram- 
mar and successively replacing it by the body of one of its productions is 
called a derivation. If the leftmost (or rightmost) nonterminal is always 
replaced, then the derivation is called leftmost (respectively, rightmost). 
+ Parse Trees. A parse tree is a picture of a derivation, in which there is 
a node for each nonterminal that appears in the derivation. The children 
of a node are the symbols by which that nonterminal is replaced in the 
derivation. There is a one-to-one correspondence between parse trees, left- 
most derivations, and rightmost derivations of the same terminal string. 
+ Ambiguity. A grammar for which some terminal string has two or more 
different parse trees, or equivalently two or more leftmost derivations or 
two or more rightmost derivations, is said to be ambiguous. In most cases 
of practical interest, it is possible to redesign an ambiguous grammar so 
it becomes an unambiguous grammar for the same language. However, 
ambiguous grammars with certain tricks applied sometimes lead to more 
efficient parsers. 
+ Top-Down and Bottom- Up Parsing. Parsers are generally distinguished 
by whether they work top-down (start with the grammar's start symbol 
and construct the parse tree from the top) or bottom-up (start with the 
terminal symbols that form the leaves of the parse tree and build the 
tree from the bottom). Top-down parsers include recursive-descent and 
LL parsers, while the most common forms of bottom-up parsers are LR 
parsers. 
+ Design of Grammars. Grammars suitable for top-down parsing often are 
harder to design than those used by bottom-up parsers. It is necessary 
to eliminate left-recursion, a situation where one nonterminal derives a 
string that begins with the same nonterminal. We also must left-factor - 
group productions for the same nonterminal that have a common prefix 
in the body. 
+ Recursive-Descent Parsers. These parsers use a procedure for each non- 
terminal. The procedure looks at its input and decides which production 
to apply for its nonterminal. Terminals in the body of the production are 
matched to the input at the appropriate time, while nonterminals in the 
body result in calls to their procedure. Backtracking, in the case when 
the wrong production was chosen, is a possibility. 
+ LL(1) Parsers. A grammar such that it is possible to choose the correct 
production with which to expand a given nonterminal, looking only at 
the next input symbol, is called LL(1). These grammars allow us to 
construct a predictive parsing table that gives, for each nonterminal and 
each lookahead symbol, the correct choice of production. Error correction 
can be facilitated by placing error routines in some or all of the table 
entries that have no legitimate production. 
4.20. SUMMARY OF CHAPTER 4 
299 
+ Shift-Reduce Parsing. Bottom-up parsers generally operate by choosing, 
on the basis of the next input symbol (lookahead symbol) and the contents 
of the stack, whether to shift the next input onto the stack, or to reduce 
some symbols at the top of the stack. A reduce step takes a production 
body at the top of the stack and replaces it by the head of the production. 
+ Viable Prefixes. In shift-reduce parsing, the stack contents are always a 
viable prefix - 
that is, a prefix of some right-sentential form that ends 
no further right than the end of the handle of that right-sentential form. 
The handle is the substring that was introduced in the last step of the 
right 
most derivation of that sentential form. 
+ Valid Items. An item is a production with a dot somewhere in the body. 
An item is valid for a viable prefix if the production of that item is used 
to generate the handle, and the viable prefix includes all those symbols 
to the left of the dot, but not those below. 
+ LR Parsers. Each of the several kinds of LR parsers operate by first 
constructing the sets of valid items (called LR states) for all possible 
viable prefixes, and keeping track of the state for each prefix on the stack. 
The set of valid items guide the shift-reduce parsing decision. We prefer 
to reduce if there is a valid item with the dot at the right end of the body, 
and we prefer to shift the lookahead symbol onto the stack if that symbol 
appears immediately to the right of the dot in some valid item. 
+ Simple LR Parsers. In an SLR parser, we perform a reduction implied by 
a valid item with a dot at the right end, provided the lookahead symbol 
can follow the head of that production in some sentential form. The 
grammar is SLR, and this method can be applied, if there are no parsing- 
action conflicts; that is, for no set of items, and for no lookahead symbol, 
are there two productions to reduce by, nor is there the option to reduce 
or to shift. 
+ Canonical-LR Parsers. This more complex form of LR parser uses items 
that are augmented by the set of lookahead symbols that can follow the use 
of the underlying production. Reductions are only chosen when there is a 
valid item with the dot at the right end, and the current lookahead symbol 
is one of those allowed for this item. A canonical-LR parser can avoid some 
of the parsing-action conflicts that are present in SLR parsers, but often 
has many more states than the SLR parser for the same grammar. 
+ Lookahead-LR Parsers. LALR parsers offer many of the advantages of 
SLR and Canonical-LR parsers, by combining the states that have the 
same kernels (sets of items, ignoring the associated lookahead sets). Thus, 
the number of states is the same as that of the SLR parser, but some 
parsing-action conflicts present in the SLR parser may be removed in 
the LALR parser. LALR parsers have become the method of choice in 
practice. 
300 
CHAPTER 4. SYNTAX ANALYSIS 
+ Bottom- Up Parsing o
f
 Ambiguous Grammars. In many important situa- 
tions, such as parsing arithmetic expressions, we can use an ambiguous 
grammar, and exploit side information such as the precedence of operators 
to resolve conflicts between shifting and reducing, or between reduction by 
two different productions. Thus, LR parsing techniques extend to many 
ambiguous grammars. 
+ Y acc. The parser-generator Y 
acc takes a (possibly) ambiguous grammar 
and conflict-resolution information and constructs the LALR states. It 
then produces a function that uses these states to perform a bottom-up 
parse and call an associated function each time a reduction is performed. 
4.11 References for Chapter 4 
The context-free grammar formalism originated with Chomsky [5], as part of 
a study on natural language. The idea also was used in the syntax description 
of two early languages: Fortran by Backus [2] and Algol 60 by Naur [26]. The 
scholar Panini devised an equivalent syntactic notation to specify the rules of 
Sanskrit grammar between 400 B.C. and 200 B.C. [19]. 
The phenomenon of ambiguity was observed first by Cantor [4] and Floyd 
[13]. Chomsky Normal Form (Exercise 4.4.8) is from [6]. The theory of context- 
free grammars is summarized in [17]. 
Recursive-descent parsing was the method of choice for early compilers, 
such as [16], 
and compiler-writing systems, such as META [28] and TMG [25]. 
LL grammars were introduced by Lewis and Stearns [24]. Exercise 4.4.5, the 
linear-time simulation of recursive-descent , 
is from [3]. 
One of the earliest parsing techniques, due to Floyd [14], 
involved the prece- 
dence of operators. The idea was generalized to parts of the language that do 
not involve operators by Wirth and Weber [29]. These techniques are rarely 
used today, but can be seen as leading in a chain of improvements to LR parsing. 
LR parsers were introduced by Knuth [22], and the canonical-LR parsing 
tables originated there. This approach was not considered practical, because the 
parsing tables were larger than the main memories of typical computers of the 
day, until Korenjak [23] gave a method for producing reasonably sized parsing 
tables for typical programming languages. DeRemer developed the LALR [8] 
and SLR [9] methods that are in use today. The construction of LR parsing 
tables for ambiguous grammars came from [I] 
and [12]. 
Johnson's Yacc very quickly demonstrated the practicality of generating 
parsers with an LALR parser generator for production compilers. The manual 
for the Yacc parser generator is found in [20]. The open-source version, Bison, 
is described in [lo]. A similar LALR-based parser generator called CUP [18] 
supports actions written in Java. Top-down parser generators incude Antlr 
[27], 
a recursive-descent parser generator that accepts actions in C++, Java, or 
C#, and LLGen 
[15], which is an LL(1)-based generator. 
Dain [7] gives a bibliography on syntax-error handling. 
4.11. REFERENCES FOR CHAPTER 4 
301 
The general-purpose dynamic-programming parsing algorithm described in 
Exercise 4.4.9 was invented independently by J. Cocke (unpublished) by Young- 
er [30] and Kasami [21]; 
hence the "CYK algorithm." There is a more complex, 
general-purpose algorithm due to Earley [I 
I] that tabulates LR-items for each 
substring of the given input; this algorithm, while also O(n3) in general, is only 
O(n2) 
on unambiguous grammars. 
1. Aho, A. V., S. C. Johnson, and J. D. Ullman, "Deterministic parsing of 
ambiguous grammars," Comm. A CM 18:8 (Aug., 1975), pp. 441-452. 
2. Backus, J.W, "The syntax and semantics of the proposed international 
algebraic language of the Zurich-ACM-GAMM Conference," Proc. Intl. 
Conf. Information Processing, UNESCO, Paris, (1959) pp. 125-132. 
3. Birman, A. and J. D. Ullman, "Parsing algorithms with backtrack," In- 
formation and Control 23:l (1973), pp. 1-34. 
4. Cantor, D. C., "On the ambiguity problem of Backus systems," J. ACM 
9:4 (1962), pp. 477-479. 
5. Chomsky, N., "Three models for the description of language," IRE Trans. 
on Information Theory IT-2:3 (1956), pp. 113-124. 
6. Chomsky, N., "On certain formal properties of grammars," Information 
and Control 2:2 (1959), pp. 137-167. 
7. Dain, J., "Bibliography on Syntax Error Handling in Language Transla- 
tion Systems," 1991. Available from the comp 
. 
compilers 
newsgroup; see 
http://compilers.iecc.com/comparch/article/91-O4-O5O. 
8. DeRemer, F., "Practical Translators for LR(k) Languages," Ph.D. thesis, 
MIT, Cambridge, MA, 1969. 
9. DeRemer, F., "Simple LR(k) grammars," Cornrn. ACM 14:7 (July, 1971), 
pp. 453-460. 
10. Donnelly, C. and R. Stallman, "Bison: The YACC-compatible Parser 
Generator," http: 
//www 
. 
gnu. 
org/software/bison/manual/ . 
11. Earley, J., "An efficient context-free parsing algorithm," Comm. A CM 
13:2 (Feb., 1970), pp. 94-102. 
12. Earley, J., "Ambiguity and precedence in syntax description," Acta In- 
formatica 4:2 (1975), pp. 183-192. 
13. Floyd, R. W., "On ambiguity in phrase-structure languages,'' Comm. 
ACM 5:10 (Oct., 1962), pp. 526-534. 
14. Floyd, R. W., "Syntactic analysis and operator precedence," J. ACM 10:3 
(1963), pp. 316-333. 
302 
CHAPTER 4. SYNTAX ANALYSIS 
15. Grune, D and C. J. H. Jacobs, "A programmer-friendly LL(1) parser 
generator," Software Practice and Experience 18:l (Jan., 1988), pp. 29- 
38. See also http 
: 
//www 
. 
cs 
. 
vu. 
nl/"ceriel/LLgen. 
html 
. 
16. Hoare, C. A. R., "Report on the Elliott Algol translator," Computer J. 
5:2 (1962), pp. 127-129. 
17. Hopcroft, J. E., R. Motwani, and J. D. Ullman, Introduction to Automata 
Theory, Languages, and Computation, 
Addison-Wesley, Boston MA, 2001. 
18. Hudson, S. E. et al., "CUP LALR Parser Generator in Java," Available 
athttp://www2.cs.tum.edu/projects/cup/. 
19. Ingerman, P. Z., "Panini-Backus form suggested," Comm. ACM 10:3 
(March 1967), p. 137. 
20. Johnson, S. C., "Yacc - 
Yet Another Compiler Compiler," Computing 
Science Technical Report 32, Bell Laboratories, Murray Hill, NJ, 1975. 
Available at http 
: 
//dinosaur. 
compilertools. 
net/yacc/ 
. 
21. Kasami, T., "An efficient recognition and syntax analysis algorithm for 
context-free languages," AFCRL-65-758, Air Force Cambridge Research 
Laboratory, Bedford, MA, 1965. 
22. Knuth, D. E., "On the translation of languages from left to right," Infor- 
mation and Control 8:6 (1965), pp. 607-639. 
23. Korenjak, A. J., "A practical method for constructing LR(k) processors," 
Comm. ACM 12:lI (Nov., 1969), pp. 613-623. 
24. Lewis, P. M. I1 and R. E. Stearns, "syntax-directed transduction," J. 
ACM 15:3 (1968), pp. 465-488. 
25. McClure, R. M., "TMG - 
a syntax-directed compiler," proc. 20th ACM 
Natl. Conf. (1965), pp. 262-274. 
26. Naur, P. et al., "Report on the algorithmic language ALGOL 60," Comm. 
ACM 3:5 (May, 1960), pp. 299-314. See also Comm. ACM 6:l (Jan., 
1963), pp. 1-17. 
27. Parr, T., "ANTLR," http: 
//www 
. 
antlr 
. 
org/ 
. 
28. Schorre, D. V., "Meta-11: a syntax-oriented compiler writing language," 
Proc. 19th ACM Natl. Conf. (1964) pp. D1.3-1-D1.3-11. 
29. Wirth, N. and H. Weber, "Euler: a generalization of Algol and its formal 
definition: Part I," Comm. ACM 9:l (Jan., 1966), pp. 13-23. 
30. Younger, D 
.H., "Recognition and parsing of context-free languages in time 
n3," Information and Control 10:2 (1967), pp. 189-208. 
Chapter 5 
Syntax-Directed 
Translation 
This chapter develops the theme of Section 2.3: the translation of languages 
guided by context-free grammars. The translation techniques in this chapter 
will be applied in Chapter 6 to type checking and intermediate-code generation. 
The techniques are also useful for implementing little languages for specialized 
tasks; this chapter includes an example from typesetting. 
We associate information with a language construct by attaching attributes 
to the grammar symbol(s) representing the construct, as discussed in Sec- 
tion 2.3.2. A syntax-directed definition specifies the values of attributes by 
associating semantic rules with the grammar productions. For example, an 
infix-to-postfix translator might have a production and rule 
This production has two nonterminals, E and T; the subscript in El distin- 
guishes the occurrence of E in the production body from the occurrence of E 
as the head. Both E and T have a string-valued attribute code. The semantic 
rule specifies that the string E. 
code is formed by concatenating El. 
code, T. 
code, 
and the character ' + I .  While the rule makes it explicit that the translation of 
E is built up from the translations of El, T, and I+', it may be inefficient to 
implement the translation directly by manipulating strings. 
From Section 2.3.5, a syntax-directed translation scheme embeds program 
fragments called semantic actions within production bodies, as in 
E -+ El +T { print ' + I  } 
By convention, semantic actions are enclosed within curly braces. (If curly 
braces occur as grammar symbols, we enclose them within single quotes, as in 
304 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
I{' and I } ' . )  The position of a semantic action in a production body determines 
the order in which the action is executed. In production (5.2), the action 
occurs at the end, after all the grammar symbols; in general, semantic actions 
may occur at any position in a production body. 
Between the two notations, syntax-directed definitions can be more readable, 
and hence more useful for specifications. However, translation schemes can be 
more efficient, and hence more useful for implementations. 
The most general approach to syntax-directed translation is to construct a 
parse tree or a syntax tree, and then to compute the values of attributes at the 
nodes of the tree by visiting the nodes of the tree. In many cases, translation 
can be done during parsing, without building an explicit tree. We shall therefore 
study a class of syntax-directed translations called "L-attributed translations" 
(L for left-to-right), which encompass virtually all translations that can be 
performed during parsing. We also study a smaller class, called "S-attributed 
translations" (S for synthesized), which can be performed easily in connection 
with a bottom-up parse. 
5.1 Syntax-Directed Definitions 
A s 
yntax-directed definition (SDD) is a context-free grammar together with, 
attributes and rules. Attributes are associated with grammar symbols and rules 
are associated with productions. If X is a symbol and a is one of its attributes, 
then we write X.a to denote the value of a at a particular parse-tree node 
labeled X. I
f
 we implement the nodes of the parse tree by records or objects, 
then the attributes of X can be implemented by data fields in the records that 
represent the nodes for X. Attributes may be of any kind: numbers, types, table 
references, or strings, for instance. The strings may even be long sequences of 
code, say code in the intermediate language used by a compiler. 
5.1.1 Inherited and Synthesized Attributes 
We shall deal with two kinds of attributes for nonterminals: 
1. A synthesized attribute for a nonterminal A at a parse-tree node N is 
defined by a semantic rule associated with the production at N. Note 
that the production must have A as its head. A synthesized attribute at 
node N is defined only in terms of attribute values at the children of N 
and at N itself. 
2. An inherited attribute for a nonterminal B at a parse-tree node N is 
defined by a semantic rule associated with the production at the parent 
of N. Note that the production must have B as a symbol in its body. An 
inherited attribute at node N is defined only in terms of attribute values 
at N's parent, N itself, and N's siblings. 
5.1. SYNTAX-DIRECTED DEFINITIONS 
305 
An Alternative Definition of Inherited Attributes 
No additional translations are enabled if we allow an inherited attribute 
B.c at a node N to be defined in terms of attribute values at the children 
of N, as well as at N itself, at its parent, and at its siblings. Such rules can 
be "simulated" by creating additional attributes of B, say B.cl , 
B.c2, 
. . 
. . 
These are synthesized attributes that copy the needed attributes of the 
children of the node labeled B. We then compute B.c as an inherited 
attribute, using the attributes B.cl, B.cz,. 
. . in place of attributes at the 
children. Such attributes are rarely needed in practice. 
While we do not allow an inherited attribute at node N to be defined in terms of 
attribute values at the children of node N, we do allow a synthesized attribute 
at node N to be defined in terms of inherited attribute values at node N itself. 
Terminals can have synthesized attributes, but not inherited attributes. At- 
tributes for terminals have lexical values that are supplied by the lexical ana- 
lyzer; there are no semantic rules in the SDD itself for computing the value of 
an attribute for a terminal. 
Example 5.1 : The SDD in Fig. 5.1 is based on our familiar grammar for 
arithmetic expressions with operators + and *. It evaluates expressions termi- 
nated by an endmarker n. In the SDD, each of the nonterminals has a single 
synthesized attribute, called val. We also suppose that the terminal digit has 
a synthesized attribute lexval, which is an integer value returned by the lexical 
analyzer. 
Figure 5.1: Syntax-directed definition of a simple desk calculator 
PRODUCTION 
1) L + E n  
2) 
E + E l  + T 
3) 
E + T  
4) 
T + T l  * F 
5) 
T + F  
6) 
F + ( E )  
7) F + 
digit 
The rule for production 1, 
L -+ E n, sets L.val to E.va1, which we shall see 
is the numerical value of the entire expression. 
Production 2, E -+ El + T, also has one rule, which computes the val 
attribute for the head E as the sum of the values at El and T. At any parse- 
SEMANTIC 
RULES 
L.val = 
E.val 
E.val=E1.val+T.val 
E.val = 
T.val 
T.val=Tl.vaExF.val 
T.val = 
F.val 
F.val = 
E.val 
F. 
val = 
digit 
.lexval 
306 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
tree node N labeled E, the value of val for E is the sum of the values of val at 
the children of node N labeled E and T. 
Production 3, E + 
T, has a single rule that defines the value of val for E 
to be the same as the value of val at the child for T. Production 4 is similar to 
the second production; its rule multiplies the values at the children instead of 
adding them. The rules for productions 5 and 6 copy values at a child, like that 
for the third production. Production 7 gives F.val the value of a digit, that is, 
the numerical value of the token digit that the lexical analyzer returned. 
An SDD that involves only synthesized attributes is called S-attributed; the 
SDD in Fig. 5.1 has this property. In an S-attributed SDD, each rule computes 
an attribute for the nonterminal at the head of a production from attributes 
taken from the body of the production. 
For simplicity, the examples in this section have semantic rules without 
side effects. In practice, it is convenient to allow SDD's to have limited side 
effects, such as printing the result computed by a desk calculator or interacting 
with a symbol table. Once the order of evaluation of attributes is discussed 
in Section 5.2, we shall allow semantic rules to compute arbitrary functions, 
possibly involving side effects. 
An S-attributed SDD can be implemented naturally in conjunction with an 
LR parser. In fact, the SDD in Fig. 5.1 mirrors the Yacc program of Fig. 4.58, 
which illustrates translation during LR parsing. The difference is that, in the 
rule for production 1, 
the Yacc program prints the value E.val as a side effect, 
instead of defining the attribute L.va1. 
An SDD without side effects is sometimes called an attribute grammar. The 
rules in an attribute grammar define the value of an attribute purely in terms 
of the values of other attributes and constants. 
5
.
1
.
2
 Evaluating an SDD at the Nodes of a Parse Tree 
To visualize the translation specified by an SDD, it helps to work with parse 
trees, even though a translator need not actually build a parse tree. Imagine 
therefore that the rules of an SDD are applied by first constructing a parse tree 
and then using the rules to evaluate all of the attributes at each of the nodes 
of the parse tree. A parse tree, showing the value(s) of its attribute(s) is called 
an annotated parse tree. 
How do we construct an annotated parse tree? In what order do we evaluate 
attributes? Before we can evaluate an attribute at a node of a parse tree, we 
must evaluate all the attributes upon which its value depends. For example, 
if all attributes are synthesized, as in Example 5.1, then we must evaluate the 
ual attributes at all of the children of a node before we can evaluate the val 
attribute at the node itself. 
With synthesized attributes, we can evaluate attributes in any bottom-up 
order, such as that of a postorder traversal of the parse tree; the evaluation of 
S-attributed definitions is discussed in Section 5.2.3. 
5.1. SYNTAX-DIRECTED DEFINITIONS 
307 
For SDD's with both inherited and synthesized attributes, there is no guar- 
antee that there is even one order in which to evaluate attributes at nodes. 
For instance, consider nonterminals A and B, with synthesized and inherited 
attributes A.s and B.i, respectively, along with the production and rules 
These rules are circular; it is impossible to evaluate either A.s at a node N or B.i 
at the child of N without first evaluating the other. The circular dependency 
of A.s and B.i at some pair of nodes in a parse tree is suggested by Fig. 5.2. 
Figure 5.2: The circular dependency of A.s and B.i on one another 
It is computationally difficult to determine whether or not there exist any 
circularities in any of the parse trees that a given SDD could have to translate.' 
Fortunately, there are useful subclasses of SDD's that are sufficient to guarantee 
that an order of evaluation exists, as we shall see in Section 5.2. 
Example 5.2 : Figure 5.3 shows an annotated parse tree for the input string 
3 * 5 + 
4 n, constructed using the grammar and rules of Fig. 5.1. The values 
of lexval are presumed supplied by the lexical analyzer. Each of the nodes for 
the nonterminals has attribute val computed in a bottom-up order, and we see 
the resulting values associated with each node. For instance, at the node with 
a child labeled *, after computing T.val= 3 and F.val = 5 at its first and third 
children, we apply the rule that says T.val is the product of these two values, 
or 15. 
Inherited attributes are useful when the structure of a parse tree does not 
"match" the abstract syntax of the source code. The next example shows how 
inherited attributes can be used to overcome such a mismatch due to a grammar 
designed for parsing rat 
her than translation. 
'without going into details, while the problem is decidable, it cannot be solved by a 
polynomial-time algorithm, even if F = 
N'P, since it has exponential time complexity. 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
I 
I 
2
a
j
=
\
 
F.val = 
4 
I 
T.va1 = 
3 
F.val= 5 digit.lexval= 4 
I 
I 
F.val = 
3 
digit. 
lexval = 
5 
I 
digit. 
lexval = 
3 
Figure 5.3: Annotated parse tree for 3 * 5 + 
4 n 
Example 5.3 : 
The SDD in Fig. 5.4 computes terms like 3 * 5 and 3 * 5 * 7. 
The top-down parse of input 3 * 5 begins with the production T + 
F T'. Here, 
F generates the digit 3, but the operator * is generated by TI. Thus, the left 
operand 3 appears in a different subtree of the parse tree from *. An inherited 
attribute will therefore be used to pass the operand to the operator. 
The grammar in this example is an excerpt from a non-left-recursive version 
of the familiar expression grammar; we used such a grammar as a running 
example to illustrate top-down parsing in Section 4.4. 
1) T + F T 1  
TI. 
inh = 
F.val 
T.val = 
T1.syn 
4) 
F -+ digit 
I F.val = digit 
.lexval 
Figure 5.4: An SDD based on a grammar suitable for top-down parsing 
Each of the nonterminals T and F has a synthesized attribute val; the 
terminal digit has a synthesized attribute lexval. The nonterminal T' has two 
attributes: an inherited attribute inh and a synthesized attribute syn. 
5.1. SYNTAX-DIRECTED 
DEFINITIONS 
309 
The semantic rules are based on the idea that the left operand of the operator 
* is inherited. More precisely, the head T' of the production TI -+ * F Ti 
inherits the left operand of * in the production body. Given a term x * y * z, 
the root of the subtree for * y * z inherits x. Then, the root of the subtree for 
* 
x inherits the value of x * y, and so on, if there are more factors in the term. 
Once all the factors have been accumulated, the result is passed back up the 
tree using synthesized attributes. 
To see how the semantic rules are used, consider the annotated parse tree 
for 3 * 5 in Fig. 5.5. The leftmost leaf in the parse tree, labeled digit, has 
attribute value lexval = 3, where the 3 is supplied by the lexical analyzer. Its 
parent is for production 4, F -+ digit. The only semantic rule associated with 
this production defines F. 
val = digit. 
lexval, which equals 3. 
digit. 
lexval = 
3 
F.val = 
5 
Ti.syn 
= 
15 
digit. 
lexval = 
5 
E 
Figure 5.5: Annotated parse tree for 3 * 5 
At the second child of the root, the inherited attribute T1.inh is defined by 
the semantic rule T1.inh = F.val associated with production 1. Thus, the left 
operand, 3, for the * operator is passed from left to right across the children of 
the root. 
The production at the node for TI is TI -+ * FT;. (We retain the subscript 
1 
in the annotated parse tree to distinguish between the two nodes for TI.) The 
inherited attribute Ti. 
inh is defined by the semantic rule Ti. 
inh = 
TI. 
inh x F. 
val 
associated with production 2. 
With T1.inh = 3 and F.val = 5, we get T;.inh = 15. At the lower node 
for Ti, the production is TI -+ 
E .  The semantic rule T1.syn = T1.inh defines 
Ti 
.syn = 15. The syn attributes at the nodes for T' pass the value 15 up the 
tree to the node for T, where T.val = 
15. 
5.1.3 Exercises for Section 5.1 
Exercise 5.1.1 : 
For the SDD of Fig. 5.1, give annotated parse trees for the 
following expressions: 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
Exercise 5.1.2: Extend the SDD of Fig. 5.4 to handle expressions as in 
Fig. 5.1. 
Exercise 5.1.3 : 
Repeat Exercise 5.1.1, using your SDD from Exercise 5.1.2. 
5.2 Evaluation Orders for SDD's 
"Dependency graphs" are a useful tool for determining an evaluation order for 
the attribute instances in a given parse tree. While an annotated parse tree 
shows the values of attributes, a dependency graph helps us determine how 
those values can be computed. 
In this section, in addition to dependency graphs, we define two impor- 
tant classes of SDD's: the "S-attributed" and the more general "L-attributed" 
SDD's. The translations specified by these two classes fit well with the parsing 
methods we have studied, and most translations encountered in practice can be 
written to conform to the requirements of at least one of these classes. 
5.2.1 Dependency Graphs 
A dependency graph depicts the flow of information among the attribute in- 
stances in a particular parse tree; an edge from one attribute instance to an- 
other means that the value of the first is needed to compute the second. Edges 
express constraints implied by the semantic rules. In more detail: 
For each parse-tree node, say a node labeled by grammar symbol X, the 
dependency graph has a node for each attribute associated with X .  
Suppose that a semantic rule associated with a production p defines the 
value of synthesized attribute A.b in terms of the value of X.c (the rule 
may define A.b in terms of other attributes in addition to X.c). Then, 
the dependency graph has an edge from X.c to A.b. More precisely, at 
every node N labeled A where production p is applied, create an edge to 
attribute b at N, from the attribute c at the child of N corresponding to 
this instance of the symbol X in the body of the production.2 
Suppose that a semantic rule associated with a production p defines the 
value of inherited attribute B.c in terms of the value of X.a. Then, the 
dependency graph has an edge from X.a to B.c. For each node N labeled 
B that corresponds to an occurrence of this B in the body of production 
p, create an edge to attribute c at N from the attribute a at the node A
d
 
2~ince 
a node N can have several children labeled X, we again assume that subscripts 
distinguish among uses of the same symbol at different places in the production. 
5.2. EVALUATION ORDERS FOR SDD'S 
311 
that corresponds to this occurrence of X. Note that M could be either 
the parent or a sibling of N. 
Example 5.4 : 
Consider the following production and rule: 
At every node N 
labeled E, with children corresponding to the body of this 
production, the synthesized attribute ual at N is computed using the values of 
ual at the two children, labeled E and T. Thus, a portion of the dependency 
graph for every parse tree in which this production is used looks like Fig. 5.6. 
As a convention, we shall show the parse tree edges as dotted lines, while the 
edges of the dependency graph are solid. 
E 
val 
Figure 5.6: E. 
val is synthesized from El. 
val and E2. 
val 
Example 5.5 : An example of a complete dependency graph appears in Fig. 
5.7. The nodes of the dependency graph, represented by the numbers 1 
through 
9, correspond to the attributes in the annotated parse tree in Fig. 5.5. 
T 9 val 
, 
, 
. 
, 
. 
. 
digit 1 lexval 
* 
digit 2 lexval 
(
5
 
Figure 5.7: Dependency graph for the annotated parse tree of Fig. 5.5 
Nodes 1 
and 2 represent the attribute lexval associated with the two leaves 
labeled digit. Nodes 3 and 4 represent the attribute ual associated with the 
two nodes labeled F. The edges to node 3 from 1 
and to node 4 from 2 result 
312 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
from the semantic rule that defines F.ual in terms of digit.lexua1. In fact, F.ual 
equals digit.lexual, but the edge represents dependence, not equality. 
Nodes 5 and 6 represent the inherited attribute T1.inh associated with each 
of the occurrences of nonterminal TI. The edge to 5 from 3 is due to the rule 
T1.inh = F.ual, which defines T1.inh at the right child of the root from F.ua1 
at the left child. We see edges to 6 from node 5 for T1.inh and from node 4 
for F.val, because these values are multiplied to evaluate the attribute inh at 
node 6. 
Nodes 7 and 8 represent the synthesized attribute syn associated with the 
occurrences of TI. The edge to node 7 from 6 is due to the semantic rule 
T1.syn = T1.inh associated with production 3 in Fig. 5.4. The edge to node 8 
from 7 is due to a semantic rule associated with production 2. 
Finally, node 9 represents the attribute T.ual. The edge to 9 from 8 is due 
to the semantic rule, T. 
ual = 
T1.syn, 
associated with production 1. 
5.2.2 
Ordering the Evaluation of Attributes 
The dependency graph characterizes the possible orders in which we can evalu- 
ate the attributes at the various nodes of a parse tree. If the dependency graph 
has an edge from node M to node N, then the attribute corresponding to M 
must be evaluated before the attribute of N. Thus, the only allowable orders 
of evaluation are those sequences of nodes Nl, N2,. 
. 
. , 
Nk such that if there is 
an edge of the dependency graph from Ni to Nj; then i < 
j. Such an ordering 
embeds a directed graph into a linear order, and is called a topological sort of 
the graph. 
If there is any cycle in the graph, then there are no topological sorts; that is, 
there is no way to evaluate the SDD on this parse tree. If there are no cycles, 
however, then there is always at least one topological sort. To see why, since 
there are no cycles, we cad surely find a node with no edge entering. For if there 
were no such node, we could proceed from predecessor to predecessor until we 
came back to some node we had already seen, yielding a cycle. Make this node 
the first in the topological order, remove it from the dependency graph, and 
repeat the process on the remaining nodes. 
Example 5.6 : 
The dependency graph of Fig. 5.7 has no cycles. One topologi- 
cal sort is the order in which the nodes have already been numbered: 1,2,. 
. . ,9. 
Notice that every edge of the graph goes from a node to a higher-numbered node, 
so this order is surely a topological sort. There are other topological sorts as 
well, suchas 1,3,5,2,4,6,7,8,9. 
5.2.3 S-Attributed Definitions 
As mentioned earlier, given an SDD, it is very hard to tell whether there exist 
any parse trees whose dependency graphs have cycles. In practice, translations 
can be implemented using classes of SDD's that guarantee an evaluation order, 
5.2. EVALUATION ORDERS FOR SDD'S 
313 
since they do not permit dependency graphs with cycles. Moreover, the two 
classes introduced in this section can be implemented efficiently in connection 
with top-down or bot 
tom-up parsing. 
The first class is defined as follows: 
a An SDD is S-attributed if every attribute is synthesized. 
Example 5.7 : 
The SDD of Fig. 5.1 is an example of an S-attributed definition. 
Each attribute, L.val, E.va1, T.val, and F.val is synthesized. 
C7 
When an SDD is S-attributed, we can evaluate its attributes in ahy bottom- 
up order of the nodes of the parse tree. It is often especially simple to evaluate 
the attributes by performing a postorder traversal of the parse tree and evalu- 
ating the attributes at a node N when the traversal leaves N for the last time. 
That is, we apply the function postorder, defined below, to the root of the parse 
tree (see also the box "Preorder and Postorder Traversals" in Section 2.3.4): 
postorder (N) { 
for ( each child C of N, from the left ) postorder(C); 
evaluate the attributes associated with node N; 
1 
S-attributed definitions can be implemented during bottom-up parsing, since 
a bottom-up parse corresponds to a postorder traversal. Specifically, postorder 
corresponds exactly to the order in which an LR parser reduces a production 
body to its head. This fact will be used in Section 5.4.2 to evaluate synthesized 
attributes and store them on the stack during LR parsing, without creating the 
tree nodes explicitly. 
5.2.4 
L-Attributed Definitions 
The second class of SDD's is called L-attributed definitions. The idea behind 
this class is that, between the attributes associated with a production body, 
dependency-graph edges can go from left to right, but not from right to left 
(hence "L-attributed"). More precisely, each attribute must be either 
1. Synthesized, or 
2. Inherited, but with the rules limited as follows. Suppose that there is 
a production A -+ X1X2 
- - Xn, and that there is an inherited attribute 
Xi.a computed by a rule associated with this production. Then the rule 
may use only: 
(a) Inherited attributes associated with the head A. 
(b) Either inherited or synthesized attributes associated with the occur- 
rences of symbols X1, X2,. 
. . , 
Xipl located to the left of Xi. 
314 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
(c) Inherited or synthesized attributes associated with this occurrence 
of Xi 
itself, but only in such a way that there are no cycles in a 
dependency graph formed by the attributes of this Xi. 
Example 5.8 : 
The SDD in Fig. 5.4 is L-attributed. To see why, consider the 
semantic rules for inherited attributes, which are repeated here for convenience: 
The first of these rules defines the inherited attribute Tf.inh 
using only F.ual, 
and F 
appears to the left of TI in the production body, as required. The second 
rule defines Ti.inh using the inherited attribute T1.inh 
associated with the head, 
and F.va1, where F appears to the left of T,' in the production body. 
In each of these cases, the rules use information "from above or from the 
left 
," as required by the class. The remaining attributes are synthesized. Hence, 
the SDD is L-attributed. 
Example 5.9 : 
Any SDD containing the following production and rules cannot 
be L-attributed: 
The first rule, A.s = B.b, is a legitimate rule in either an S-attributed or L- 
attributed SDD. It defines a synthesized attribute A.s in terms of an attribute 
at a child (that is, a symbol within the production body). 
The second rule defines an inherited attribute B.i, so the entire SDD cannot 
be S-attributed. Further, although the rule is legal, the SDD cannot be L- 
attributed, because the attribute C.c is used to help define B.i, and C is to 
the right of B in the production body. While attributes at siblings in a parse 
tree may be used in L-attributed SDD's, they must be to the left of the symbol 
whose attribute is being defined. 
5.2.5 
Semantic Rules with Controlled Side Effects 
In practice, translations involve side effects: a desk calculator might print a 
result; a code generator might enter the type of an identifier into a symbol table. 
With SDD's, we strike a balance between attribute grammars and translation 
schemes. Attribute grammars have no side effects and allow any evaluation 
order consistent with the dependency graph. Translation schemes impose left- 
to-right evaluation and allow semantic actions to contain any program fragment; 
translation schemes are discussed in Section 5.4. 
We shall control side effects in SDD's in ope of the following ways: 
5.2. EVALUATION ORDERS FOR SDD'S 
315 
Permit incidental side effects that do not constrain attribute evaluation. 
In other words, permit side effects when attribute evaluation based on any 
topological sort of the dependency graph produces a "correct" translation, 
where "correcti7 
depends on the application. 
Constrain the allowable evaluation orders, so that the same translation is 
produced for any allowable order. The constraints can be thought of as 
implicit edges added to the dependency graph. 
As an example of an incidental side effect, let us modify the desk calculator 
of Example 5.1 to print a result. Instead of the rule L.val= E.val, which saves 
the result in the synthesized attribute L. 
val, consider: 
PRODUCTION SEMANTIC 
RULE 
1) 
L + E n  
print(E. 
val) 
Semantic rules that are executed for their side effects, such as print(E.val), will 
be treated as the definitions of dummy synthesized attributes associated with 
the head of the production. The modified SDD produces the same translation 
under any topological sort, since the print statement is executed at the end, 
after the result is computed into E.val. 
Example 5.10 : 
The SDD in Fig. 5.8 takes a simple declaration D consisting 
of a basic type T followed by a list L of identifiers. T can be int or float. For 
each identifier on the list, the type is entered into the symbol-table entry for the 
identifier. We assume that entering the type for one identifier does not affect 
the symbol-table entry for any other identifier. Thus, entries can be updated 
in any order. This SDD does not check whether an identifier is declared more 
than once; it can be modified to do so. 
Figure 5.8: Syntax-directed definition for simple type declarations 
1) D + T L  
2) 
T -+ int 
3) 
T -+ float 
4) 
L + L 1 , i d  
5) 
L + 
id 
Nonterminal D represents a declaration, which, from production 1, 
consists 
of a type T followed by a list L of identifiers. T has one attribute, T.type, which 
is the type in the declaration D. Nonterminal L also has one attribute, which 
we call inh to emphasize that it is an inherited attribute. The purpose of L.inh 
L.inh = 
T.type 
T. 
type = 
integer 
T.type = 
float 
Ll.inh=L.inh 
addType(id. 
entry, L.inh) 
add 
Type(id. 
entry, L. 
inh) 
316 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
is to pass the declared type down the list of identifiers, so that it can be added 
to the appropriate symbol-table entries. 
Productions 2 and 3 each evaluate the synthesized attribute T.type, giving 
it the appropriate value, integer or float. This type is passed to the attribute 
L.inh in the rule for production 1. Production 4 passes L.inh down the parse 
tree. That is, the value Ll . 
inh is computed at a parse-tree node by copying the 
value of L.inh from the parent of that node; the parent corresponds to the head 
of the production. 
Productions 4 and 5 also have a rule in which a function addType is called 
with two arguments: 
1. id.entry, a lexical value that points to a symbol-table object, and 
2. L.inh, the type being assigned to every identifier on the list. 
We suppose that function addType properly installs the type L.inh as the type 
of the represented identifier. 
A dependency graph for the input string float idl, ida, 
id3 appears in 
Fig. 5.9. Numbers 1 
through 10 represent the nodes of the dependency graph. 
Nodes 1, 2, and 3 represent the attribute entry associated with each of the 
leaves labeled id. Nodes 6, 8, and 10 are the dummy attributes that represent 
the application of the function addType to a type and one of these entry values. 
T 
4 '  type 
5 
L 
6 entry 
real 
9 
id3 ' 
3 
entry 
inh 7 ' 
L 
8 entry 
9 
id2 2 
entry 
inh 9 
L 
10 entry 
idl 1 
entry 
Figure 5.9: Dependency graph for a declaration float idl , 
idz , id3 
Node 4 represents the attribute T. 
type, and is actually where attribute eval- 
uation begins. This type is then passed to nodes 5, 7, and 9 representing L.inh 
associated with each of the occurrences of the nonterminal L. 
5.2, EVALUATION ORDERS FOR SDD'S 
317 
5.2.6 Exercises for Section 5.2 
Exercise 5.2.1 : 
What are all the topological sorts for the dependency graph 
of Fig. 5.7? 
Exercise 5.2.2 : 
For the SDD of Fig. 5.8, give annotated parse trees for the 
following expressions: 
a) i n t  a, b, c. 
b) float w ,  x, y, z. 
Exercise 5.2.3 : Suppose that we have a production A -+ BCD. Each of 
the four nonterminals A, B, C ,  
and D have two attributes: s is a synthesized 
attribute, and i is an inherited attribute. For each of the sets of rules below, 
tell whether (i) the rules are consistent with an S-attributed definition (ii) the 
rules are consistent with an L-attributed definition, and (iii) whether the rules 
are consistent with any evaluation order at all? 
b) A.s = 
B.i + 
C.s and D.i = A.i + 
B.s. 
! 
d) A.s = 
D.i, B.i = 
A.s + 
C.s, C.i = 
B.s, and D.i = 
B.i + 
C.i. 
! 
Exercise 5.2.4: This grammar generates binary numbers with a "decimal" 
point: 
Design an L-attributed SDD to compute S.val, the decimal-number value of 
an input string. For example, the translation of string I01 
.lo1 
should be the 
decimal number 5.625. Hint: use an inherited attribute L.side that tells which 
side of the decimal point a bit is on. 
!! Exercise 5.2.5 : 
Design an S-attributed SDD for the grammar and translation 
described in Exercise 5.2.4. 
!! 
Exercise 5.2.6 : 
Implement Algorithm 3.23, which converts a regular expres- 
sion into a nondeterministic finite automaton, by an L-attributed SDD on a 
top-down parsable grammar. Assume that there is a token char representing 
any character, and that char.lexva1 
is the character it represents. You may also 
assume the existence of a function new () that returns a new state, that is, a 
state never before returned by this function. Use any convenient notation to 
specify the transitions of the NFA. 
318 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
5
.
3
 Applications of Synt 
ax-Direct 
ed Translation 
The syntax-directed translation techniques in this chapter will be applied in 
Chapter 6 to type checking and intermediate-code generation. Here, we consider 
selected examples to illustrate some representative SDD's. 
The main application in this section is the construction of syntax trees. Since 
some compilers use syntax trees as an intermediate representation, a common 
form of SDD turns its input string into a tree. To complete the translation to 
intermediate code, the compiler may then walk the syntax tree, using another 
set of rules that are in effect an SDD on the syntax tree rather than the parse 
tree. (Chapter 6 also discusses approaches to intermediate-code generation that 
apply an SDD without ever constructing a tree explicitly.) 
We consider two SDD's for constructing syntax trees for expressions. The 
first, an S-attributed definition, is suitable for use during bottom-up parsing. 
The second, L-attributed, is suitable for use during top-down parsing. 
The final example of this section is an L-attributed definition that deals 
with basic and array types. 
5.3.1 
Construction of Syntax Trees 
As discussed in Section 2.8.2, each node in a syntax tree represents a construct; 
the children of the node represent the meaningful components of the construct. 
A syntax-tree node representing an expression El + Ez has label + and two 
children representing the subexpressions El and E2. 
We shall implement the nodes of a syntax tree by objects with a suitable 
number of fields. Each object will have an op field that is the label of the node. 
The objects will have additional fields as follows: 
If the node is a leaf, an additional field holds the lexical value for the leaf. 
A constructor function Leaf ( 
op, val) creates a leaf object. Alternatively, if 
nodes are viewed as records, then Leaf returns a pointer to a new record 
for a leaf. 
If the node is an interior node, there are as many additional fields as the 
node has children in the syntax tree. A constructor function Node takes 
two or more arguments: Node(op, cl, 
ca, . . 
. , 
ck) creates an object with 
first field op and k additional fields for the k children cl, . 
. 
. , 
ck. 
Example 5.1 
1 : The S-attributed definition in Fig. 5.10 constructs syntax 
trees for a simple expression grammar involving only the binary operators + 
and -. As usual, these operators are at the same precedence level and are 
jointly left associative. All nonterminals have one synthesized attribute node, 
which represents a node of the syntax tree. 
Every time the first production E -+ El + 
T is used, its rule creates a node 
with '+I 
for op and two children, El.node and T.node, for the subexpressions. 
The second production has a similar rule. 
5.3. APPLICATIONS OF SYNTAX-DIRECTED TRANSLATION 
319 
6) 
T --+ nurn 
I T. 
node = new Leaf (num, 
num. 
val) 
PRODUCTION 
1) E --+ El + 
T 
2) 
E -+ El - 
T 
3) 
E + T  
4) 
T - + ( E )  
5) 
T + 
id 
Figure 5.10: Constructing syntax trees for simple expressions 
SEMANTIC 
RULES 
E.node = 
new Node('+', El .node, 
T.node) 
E.node = 
new Node('-', El .node, 
T.node) 
E.node = 
T.node 
T.node = 
E.node 
T.node = new Leaf (id, 
id. 
entry) 
For production 3, E -+ 
T, no node is created, since E.node is the same as 
T.node. Similarly, no node is created for production 4, T --+ ( E ). The value 
of T.node is the same as E.node, since parentheses are used only for grouping; 
they influence the structure of the parse tree and the syntax tree, but once their 
job is done, there is no further need to retain them in the syntax tree. 
The last two T-productions have a single terminal on the right. We use the 
constructor Leaf to create a suitable node, which becomes the value of T.node. 
Figure 5.11 shows the construction of a syntax tree for the input a - 
4 + 
c. 
The nodes of the syntax tree are shown as records, with the op field first. 
Syntax-tree edges are now shown as solid lines. The underlying parse tree, 
which need not actually be constructed, is shown with dotted edges. The third 
type of line, shown dashed, represents the values of E.node and T.node; each 
line points to the appropriate synt 
ax-tree node. 
At the bottom we see leaves for a, 4 and c, constructed by Leaf. We suppose 
that the lexical value id.entry points into the symbol table, and the lexical 
value num.val is the numerical value of a constant. These leaves, or pointers 
to them, become the value of T.node at the three parse-tree nodes labeled T, 
according to rules 5 and 6. Note that by rule 3, the pointer to the leaf for a is 
also the value of E. 
node for the leftmost E in the parse tree. 
Rule 2 causes us to create a node with op equal to the minus sign and 
pointers to the first two leaves. Then, rule 1 
produces the root node of the 
syntax tree by combining the node for - 
with the third leaf. 
If the rules are evaluated during a postorder traversal of the parse tree, or 
with reductions during a bottom-up parse, then the sequence of steps shown in 
Fig. 5.12 ends with ps pointing to the root of the constructed syntax tree. 
With a grammar designed for top-down parsing, the same syntax trees are 
constructed, using the same sequence of steps, even though the structure of the 
parse trees differs significantly from that of syntax trees. 
Example 5.12 : The L-attributed definition in Fig. 5.13 performs the same 
translation as the S-attributed definition in Fig. 5.10. The attributes for the 
grammar symbols E, T, id, and nurn are as discussed in Example 5.11. 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
E. 
node 
.
\
 
' . .  
*
\
 
, 
\ 
E,. nod; 
+ 
\ \  
' 
T. 
node 
... 1
:
 .... 
\ 
' 
\ 
:
'
 
I . 
\ 
: 
\ 
1
,
 
\ 
, 
\ 
E.node 
; 
- 
Tl node 
\ 
l 
id 
\\ 
/' 
: 
I 
1 
\ 
1
.
 
I 
1
.
 
I 
\ 
1
.
 
 node 
num 
1
 
I 
1 
1 
1
'
:
 
I 
I 
1 
I 
I
'
 
. 
I 
I 
id 
I 
1
1
 
I I 
I 
1
1
 
1
1
 
I 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ ' 
id 
j 
I 
numl 4 
I 
t 
to entry for a 
Figure 5.11: Syntax tree for a - 
4 + 
c 
I) pl = 
new Leaf(id, entry-a); 
2) 
pa = 
new Leaf (num, 
4); 
3) p3 = 
new Node('-', pl, 
pz); 
4) 
p4 = 
new Leaf(id, entry-c); 
5) 
pg = 
new Node('+', p3,p4); 
Figure 5.12: Steps in the construction of the syntax tree for a - 
4 + 
c 
The rules for building syntax trees in this example are similar to the rules 
for the desk calculator in Example 5.3. In the desk-calculator example, a term 
x * y was evaluated by passing x as an inherited attribute, since x and * y 
appeared in different portions of the parse tree. Here, the idea is to build a 
syntax tree for x + 
y by passing x as an inherited attribute, since x and + 
y 
appear in different subtrees. Nonterminal E' is the counterpart of nonterminal 
T' in Example 5.3. Compare the dependency graph for a - 
4 + 
c in Fig. 5.14 
with that for 3 a 5 in Fig. 5.7. 
Nonterminal E' has an inherited attribute inh and a synthesized attribute 
s 
yn. Attribute El. 
inh represents the partial syntax tree constructed so far. 
Specifically, it represents the root of the tree for the prefix of the input string 
that is to the left of the subtree for El. At node 5 in the dependency graph in 
Fig. 5.14, E1.inh 
denotes the root of the partial syntax tree for the identifier a; 
that is, the leaf for a. At node 6, E1.inh 
denotes the root for the partial syntax 
5.3. APPLICATIONS OF SYNTAX-DIRECTED TRANSLATION 
32 
1 
2) 
E' -+ + 
T El 
E1.syn 
= 
E1.inh 
T.node = 
E.node 
T.node = 
new Leaf (id, 
id. 
entry) 
El. 
inh = 
new Node('+', 
El. 
inh, 
T. 
node) 
Er.syn 
= 
Ei.syn 
3) 
E' -+ - 
T Ei 
7) 
T -+ num 
I T.node = 
new Leaf(num, 
num.ual) 
El .inh = 
new Node('-', E1.inh, 
T.node) 
E1.syn 
= 
Ei.syn 
Figure 5.13: Constructing syntax trees during top-down parsing 
id 
7 entry 
E 
Figure 5.14: Dependency graph for a - 
4 + 
c, with the SDD of Fig. 5.13 
tree for the input a - 
4. At node 9, E1.inh 
denotes the syntax tree for a - 
4 + 
c. 
Since there is no more input, at node 9, E1.inh 
points to the root of the 
entire syntax tree. The syn attributes pass this value back up the parse tree 
until it becomes the value of E.node. Specifically, the attribute value at node 10 
is defined by the rule E1.syn 
= E'.inh associated with the production El -+ E .  
The attribute value at node 11 is defined by the rule El. 
syn = 
E; . 
s 
yn associated 
with production 2 in Fig. 5.13. Similar rules define the attribu!e values at 
nodes 12 and 13. 
5.3.2 
The Structure of a Type 
Inherited attributes are useful when the structure of the parse tree differs from 
the abstract syntax of the input; attributes can then be used to carry informa- 
322 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
tion from one part of the parse tree to another. The next example shows how 
a mismatch in structure can be due to the design of the language, and not due 
to constraints imposed by the parsing method. 
Example 5.13: In C, the type i
n
t
 [2][3] can be read as, "array of 2 arrays 
of 3 integers." The corresponding type expression array(2, array(3, 
integer)) is 
represented by the tree in Fig. 5.15. The operator array takes two parameters, 
a number and a type. If types are represented by trees, then this operator 
returns a tree node labeled array with two children for a number and a type. 
Figure 5.15: Type expression for i
n
t
 
[2] 
[3] 
With the SDD in Fig. 5.16, nonterminal T generates either a basic type or 
an array type. Nonterminal B generates one of the basic types i
n
t
 and float. 
T 
generates a basic type when T derives B C and C derives E .  Otherwise, C 
generates array components consisting of a sequence of integers, each integer 
surrounded by brackets. 
Figure 5.16: T generates either a basic type or an array type 
PRODUCTION 
T += B C  
B += int 
B += float 
C + [ num ] C1 
C + E  
The nonterminals B and T have a synthesized attribute t representing a 
type. The nonterminal C has two attributes: an inherited attribute b and a 
synthesized attribute t. The inherited b attributes pass a basic type down the 
tree, and the synthesized t attributes accumulate the result. 
An annotated parse tree for the input string i
n
t
 
[2] 
[3] 
is shown in Fig. 5.17. 
The corresponding type expression in Fig. 5.15 is constructed by passing the 
type integer from B, down the chain of C's through the inherited attributes b. 
The array type is synthesized up the chain of C's through the attributes t. 
In more detail, at the root for T -+ B C, nonterminal C inherits the type 
from B, using the inherited attribute C.b. At the rightmost node for C, 
the 
SEMANTIC RULES 
T.t = C.t 
C.b = B.t 
B.t = integer 
B.t = 
float 
C.t = array (num.val, Cl .t) 
C1.b = C.b 
C.t = C.b 
5.3. APPLICATIONS OF SYNTAX-DIRE 
CTED TRANSLATION 
323 
production is C -+ t, so C.t equals C.b. The semantic rules for the production 
C -+ 
[ num ] Cl form C.t by applying the operator array to the operands 
num.va1 
and Cl .t. 
T.t = array(2, array(3, integer)) 
/ 
"" 
C.b = integer 
B.t = integer 
C.t = 
array(2, array(3, integer)) 
i
n
t
 
1
1
2
1
 
"---C.b 
C.t = 
= array(3, 
integer integer) 
\
c
.
b
 
= integer 
[
3
I
 
C.t = integer 
i
n
t
 
C.t = 
array(3, integer) 
/// \ 
Figure 5.17: Syntax-directed translation of array types 
5.3.3 Exercises for Section 5.3 
Exercise 5
.
3
.
1
 
: 
Below is a grammar for expressions involving operator + and 
integer or floating-point operands. Floating-point numbers are distinguished 
by having a decimal point. 
E + E  + T I T  
T -+ num . 
num 1 num 
a) Give an SDD to determine the type of each term T and expression E. 
b) Extend your SDD of (a) to translate expressions into postfix notation. 
Use the unary operator intToFloat to turn an integer into an equivalent 
float. 
! Exercise 5
.
3
.
2
 
: 
Give an SDD to translate infix expressions with + and * into 
equivalent expressions without redundant parentheses. For example, since both 
operators associate from the left, and * takes precedence over +, 
((a*(b+c))*(d)) 
translates into a * (b + 
c) * d. 
! Exercise 5
.
3
.
3
 
: 
Give an SDD to differentiate expressions such as x * (3 * x + 
x * x) involving the operators + and *, the variable x, and constants. Assume 
that no simplification occurs, so that, for example, 3 * x will be translated into 
3 * 1 + 0 * x .  
324 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
5.4 Syntax-Directed Translation Schemes 
Syntax-directed translation schemes are a complementary notation to syntax- 
directed definitions. All of the applications of syntax-directed definitions in 
Section 5.3 can be implemented using syntax-directed translation schemes. 
From Section 2.3.5, a syntax-directed translation scheme (SDT) is a context- 
free grammar with program fragments embedded within production bodies. The 
program fragments are called semantic actions and can appear at any position 
within a production body. By convention, we place curly braces around actions; 
if braces are needed as grammar symbols, then we quote them. 
Any SDT can be implemented by first building a parse tree and then per- 
forming the actions in a left-to-right depth-first order; that is, during a preorder 
traversal. An example appears in Section 5.4.3. 
Typically, SDT's are implemented during parsing, without building a parse 
tree. In this section, we focus on the use of SDT's to implement two important 
classes of SDD7s: 
1. The underlying grammar is LR-parsable, and the SDD is S-attributed. 
2. The underlying grammar is LL-parsable, and the SDD is L-attributed. 
We shall see how, in both these cases, the semantic rules in an SDD can be 
converted into an SDT with actions that are executed at the right time. During 
parsing, an action in a production body is executed as soon as all the grammar 
symbols to the left of the action have been matched. 
SDT's that can be implemented during parsing can be characterized by in- 
troducing distinct marker nonterminals in place of each embedded action; each 
marker M has only one production, A
4
 -+ 
c. If the grammar with marker non- 
terminals can be parsed by a given method, then the SDT can be implemented 
during parsing. 
5.4.1 Postfix Translation Schemes 
By far the simplest SDD implementation occurs when we can parse the grammar 
bottom-up and the SDD is S-attributed. In that case, we can construct an SDT 
in which each action is placed at the end of the production and is executed along 
with the reduction of the body to the head of that production. SDT's with all 
actions at the right ends of the production bodies are called postfix SDT's. 
Example 5.14 : 
The postfix SDT in Fig. 5.18 implements the desk calculator 
SDD of Fig. 5.1, with one change: the action for the first production prints 
a value. The remaining actions are exact counterparts of the semantic rules. 
Since the underlying grammar is LR, and the SDD is S-attributed, these actions 
can be correctly performed along with the reduction steps of the parser. 
5.4. SYNTAX-DIRECTED TRANSLATION SCHEMES 
L -+ E n  
{print(E.val);) 
E -+ 
E 1 + T  
{E.val= El.val+ T.vak} 
E
T
 
{ E.val= T.val; } 
T 
-+ 
TI * F 
{ T.val = 
TI .val x F.val; ) 
T
F
 
{ T.val = 
F.val; } 
F -+ 
( E )  
{F.val=E.val;) 
F + digit 
{ F.val = 
digit.lexva1; ) 
Figure 5.18: Postfix SDT implementing the desk calculator 
5.4.2 
Parser-Stack Implementation of Postfix SDT's 
Postfix SDT's can be implemented during LR parsing by executing the actions 
when reductions occur. The attribute(s) of each grammar symbol can be put 
on the stack in a place where they can be found during the reduction. The 
best plan is to place the attributes along with the grammar symbols (or the LR 
states that represent these symbols) in records on the stack itself. 
In Fig. 5.19, the parser stack contains records with a field for a grammar 
symbol (or parser state) and, below it, a field for an attribute. The three 
grammar symbols X YZ 
are on top of the stack; perhaps they are about to be 
reduced according to a production like A -+ X YZ. Here, we show X.x as the 
one attribute of X, and so on. In general, we can allow for more attributes, 
either by making the records large enough or by putting pointers to records on 
the stack. With small attributes, it may be simpler to make the records large 
enough, even if some fields go unused some of the time. However, if one or more 
attributes are of unbounded size - 
say, they are character strings - 
then it 
would be better to put a pointer to the attribute's value in the stack record 
and store the actual value in some larger, shared storage area that is not part 
of the stack. 
Statelgrammar symbol 
Synthesized attribute(s) 
Figure 5.19: Parser stack with a field for synthesized attributes 
If the attributes are all synthesized, and the actions occur at the ends of the 
productions, then we can compute the attributes for the head when we reduce 
the body to the head. If we reduce by a production such as A -+ X YZ, then 
we have all the attributes of X, Y, and Z available, at known positions on the 
stack, as in Fig. 5.19. After the action, A and its attributes are at the top of 
the stack, in the position of the record for X. 
Example 5.15 : 
Let us rewrite the actions of the desk-calculator SDT of Ex- 
326 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
ample 5.14 so that they manipulate the parser stack explicitly. Such stack 
manipulation is usually done automatically by the parser. 
ACTIONS 
{ print (stack 
[top - 
11. 
val); 
top = top - 
1; ) 
{ stack [top 
- 
21. val = stack [top 
- 
2
1
 .val + 
stack [top] 
.vat 
top = top - 
2; ) 
{ stack [top - 
21. val = stack [top 
- 
21. val x stack [top] 
.val; 
top = top - 
2; } 
{ stack [top 
- 
2].vaE = stack [top 
- 
l].val; 
top = top - 
2; } 
F -+ digit 
Figure 5.20: Implementing the desk calculator on a bottom-up parsing stack 
Suppose that the stack is kept in an array of records called stack, with top 
a cursor to the top of the stack. Thus, stack[top] 
refers to the top record on the 
stack, stack[top 
- 
I] to the record below that, and so on. Also, we assume that 
each record has a field called val, which holds the attribute of whatever grammar 
symbol is represented in that record. Thus, we may refer to the attribute E.va1 
that appears at the third position on the stack as stack[top 
- 
2
1
 .Val. The entire 
SDT is shown in Fig. 5.20. 
For instance, in the second production, E + 
El + 
T ,  we go two positions 
below the top to get the value of El, 
and we find the value of T at the top. The 
resulting sum is placed where the head E will appear after the reduction, that 
is, two positions below the current top. The reason is that after the reduction, 
the three topmost stack symbols are replaced by one. After computing E.val, 
we pop two symbols off the top of the stack, so the record where we placed 
E.val will now be at the top of the stack. 
In the third production, E -+ T, no action is necessary, because the length 
of the stack does not change, and the value of T.va1 at the stack top will simply 
become the value of E.val. The same observation applies to the productions 
T -+ F and F -+ digit. Production F + 
( E ) is slightly different. Although 
the value does not change, two positions are removed from the stack during the 
reduction, so the value has to move to the position after the reduction. 
Note that we have omitted the steps that manipulate the first field of the 
stack records - 
the field that gives the LR state or otherwise represents the 
grammar symbol. If we are performing an LR parse, the parsing table tells us 
what the new state is every time we reduce; see Algorithm 4.44. Thus, we may 
5.4. SYNTAX-DIRECTED TRANSLATION SCHEMES 
simply place that state in the record for the new top of stack. 
5
.
4
.
3
 SDT's With Actions Inside Productions 
An action may be placed at any position within the body of a production. 
It is performed immediately after all symbols to its left are processed. Thus, 
if we have a production B -+ X {a} 
Y, the action a 
is done after we have 
recognized X (if X is a terminal) or all the terminals derived from X (if X is 
a nonterminal) 
. More precisely, 
e If the parse is bottom-up, then we perform action a as soon as this oc- 
currence of X appears on the top of the parsing stack. 
e If the parse is top-down, we perform a 
just before we attempt to expand 
this occurrence of Y (if Y a nonterminal) or check for Y on the input (if 
Y is a terminal). 
SDT's that can be implemented during parsing include postfix SDT's and 
a class of SDT's considered in Section 5.5 that implements L-attributed defini- 
tions. Not all SDT's can be implemented during parsing, as we shall see in the 
next example. 
Example 5.16 
t As an extreme example of a problematic SDT, suppose that 
we turn our desk-calculator running example into an SDT that prints the prefix 
form of an expression, rather than evaluating the expression. The productions 
and actions are shown in Fig. 5.21. 
1) L 
-+ 
E n  
2) 
E 
-+ 
{ print('+'); } El + 
T 
3
)
E
T
 
4) 
T 
-+ 
{ print('*'); } TI * F 
5
)
T
F
 
6) 
F 
-+ 
( E l  
7) 
F + digit { print (digit 
.lexval); } 
Figure 5.2 
1: Problematic SDT for infix-to-prefix translation during parsing 
Unfortunately, it is impossible to implement this SDT during either top- 
down or bottom-up parsing, because the parser would have to perform critical 
actions, like printing instances of * or +, long before it knows whether these 
symbols will appear in its input. 
Using marker nonterminals Mz and M4 for the actions in productions 2 
and 4, respectively, on input 3, a shift-reduce parser (see Section 4.5.3) has 
conflicts between reducing by Mz -+ E ,  reducing by Ma -+ 
t, and shifting the 
digit. 
328 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
Any SDT can be implemented as follows: 
1. Ignoring the actions, parse the input and produce a parse tree as a result. 
2. Then, examine each interior node N, say one for production A -+ a .  Add 
additional children to N for the actions in a, so the children of N from 
left to right have exactly the symbols and actions of a. 
3. Perform a preorder traversal (see Section 2.3.4) of the tree, and as soon 
as a node labeled by an action is visited, perform that action. 
For instance, Fig. 5.22 shows the parse tree for expression 3 * 5 + 
4 with ac- 
tions inserted. If we visit the nodes in preorder, we get the prefix form of the 
expression: + * 3 5 4. 
digit 
digit { piint(3); } 
Figure 5.22: Parse tree with actions embedded 
5
.
4
.
4
 Eliminating Left Recursion From SDT's 
Since no grammar with left recursion can be parsed deterministically top-down, 
we examined left-recursion elimination in Section 4.3.3. When the grammar is 
part of an SDT, we also need to worry about how the actions are handled. 
First, consider the simple case, in which the only thing we care about is 
the order in which the actions in an SDT are performed. For example, if each 
action simply prints a string, we care only about the order in which the strings 
are printed. In this case, the following principle can guide us: 
When transforming the grammar, treat the actions as if they were termi- 
nal symbols. 
5.4. SYNTAX-DIRECTED TRANSLATION SCHEMES 
329 
This principle is based on the idea that the grammar transformation preserves 
the order of the terminals in the generated string. The actions are therefore 
executed in the same order in any left-to-right parse, top-down or bottom-up. 
The "trick" for eliminating left recursion is to take two productions 
that generate strings consisting of a ,d and any number of a's, and replace them 
by productions that generate the same strings using a new nonterminal R (for 
"remainder") of the first production: 
If @ does not begin with A, then A no longer has a left-recursive production. In 
regular-definition terms, with both sets of productions, A is defined by @(a)*. 
See Section 4.3.3 for the handling of situations where A has more recursive or 
nonrecursive productions. 
Example 5.17 : 
Consider the following E-productions from an SDT for trans- 
lating infix expressions into postfix notation: 
E + E 1 + T  { print ('+I) 
; ) 
E
T
 
If we apply the standard transformation to E, 
the remainder of the left-recursive 
production is 
and p, the body of the other production is T. If we introduce R for the remain- 
der of E ,  we get the set of productions: 
When the actions of an SDD compute attributes rather than merely printing 
output, we must be more careful about how we eliminate left recursion from a 
grammar. However, if the SDD is S-attributed, then we can always construct 
an SDT by placing attribute-computing actions at appropriate positions in the 
new productions. 
We shall give a general schema for the case of a single recursive production, 
a single nonrecursive production, and a single attribute of the left-recursive 
nonterminal; the generalization to many productions of each type is not hard, 
but is notationally cumbersome. Suppose that the two productions are 
CHAPTER 5. SYNTAX-DIRE 
CTED TRANSLATION 
Here, A.a is the synthesized attribute of left-recursive nonterminal A, and X 
and Y are single grammar symbols with synthesized attributes X.x and Y.y, 
respectively. These could represent a string of several grammar symbols, each 
with its own attribute(s), 
since the schema has an arbitrary function g comput- 
ing A.a in the recursive production and an arbitrary function f computing A.a 
in the second production. In each case, f and g take as arguments whatever 
attributes they are allowed to access if the SDD is S-attributed. 
We want to turn the underlying grammar into 
Figure 5.23 suggests what the SDT on the new grammar must do. In (a) 
we see the effect of the postfix SDT on the original grammar. We apply f once, 
corresponding to the use of production A -+ X ,  
and then apply g as many times 
as we use the production A -+ AY. Since R generates a "remainder" of Y's, 
its translation depends on the string to its left, a string of the form XYY . . Y. 
Each use of the production R -+ YR results in an application of g. For R, we 
use an inherited at 
tribute R
.
i
 
to accumulate the result of successively applying 
g, starting with the value of A.a. 
Figure 5.23: Eliminating left recursion from a postfix SDT 
In addition, R has a synthesized attribute R.s, not shown in Fig. 5.23. 
This attribute is first computed when R ends its generation of Y symbols, as 
signaled by the use of production R -+ E .  R.s is then copied up the tree, so 
it can become the value of A.a for the entire expression XYY - . 
Y. The case 
where A generates XYY is shown in Fig. 5.23, and we see that the value of A.a 
at the root of (a) has two uses of g. So does R.i at the bottom of tree (b), and 
it is this value of R.s that gets copied up that tree. 
To accomplish this translation, we use the following SDT: 
5.4. SYNTAX-DIRECTED TRAIL'SLATION SCHEMES 
Notice that the inherited attribute R.i is evaluated immediately before a use 
of R in the body, while the synthesized attributes A.a and R.s are evaluated 
at the ends of the productions. Thus, whatever values are needed to compute 
these attributes will be available from what has been computed to the left. 
5.4.5 SDT's for L-Attributed Definitions 
In Section 5.4.1, we converted S-attributed SDD's into postfix SDT's, with 
actions at the right ends of productions. As long as the underlying grammar is 
LR, postfix SDT's can be parsed and translated bottom-up. 
Now, we consider the more general case of an L-attributed SDD. We shall 
assume that the underlying grammar can be parsed top-down, for if not it is 
frequently impossible to perform the translation in connection with either an 
LL or an LR parser. With any grammar, the technique below can be imple- 
mented by attaching actions to a parse tree and executing them during preorder 
traversal of the tree. 
The rules for turning an L-attributed SDD into an SDT are as follows: 
1. Embed the action that computes the inherited attributes for a nonterminal 
A immediately before that occurrence of A in the body of the production. 
If several inherited attributes for A depend on one another in an acyclic 
fashion, order the evaluation of attributes so that those needed first are 
computed first. 
2. Place the actions that compute a synthesized attribute for the head of a 
production at the end of the body of that production. 
We shall illustrate these principles with two extended examples. The first 
involves typesetting. It illustrates how the techniques of compiling can be used 
in language processing for applications other than what we normally think of 
as programming languages. The second example is about the generation of 
intermediate code for a typical programming-language construct: a form of 
while-statement . 
Example 5.18 : 
This example is motivated by languages for typesetting math- 
ematical formulas. Eqn is an early example of such a language; ideas from Eqn 
are still found in the T
J
$
K
 typesetting system, which was used to produce this 
book. 
We shall concentrate on only the capability to define subscripts, subscripts 
of subscripts, and so on, ignoring superscripts, built-up fractions, and all other 
mathematical features. In the Eqn language, one writes a sub i sub j to set 
the expression aij. A simple grammar for boxes (elements of text bounded by 
a rectangle) is 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
B -+ B1 B2 ( B1 sub B2 ( ( B1 ) ( text 
Corresponding to these four productions, a box can be either 
1. Two boxes, juxtaposed, with the first, B1, to the left of the other, B2. 
2. A box and a subscript box. The second box appears in a smaller size, 
lower, and to the right of the first box. 
3. A parenthesized box, for grouping of boxes and subscripts. Eqn and T
)
$
 
both use curly braces for grouping, but we shall use ordinary, round paren- 
theses to avoid confusion with the braces that surround actions in SDT's. 
4. A text string, that is, any string of characters. 
This grammar is ambiguous, but we can still use it to parse bottom-up if we 
make subscripting and juxtaposition right associative, with sub taking prece- 
dence over juxtaposition. 
Expressions will be typeset by constructing larger boxes out of smaller ones. 
In Fig. 5.24, the boxes for El and .height are about to be juxtaposed to form 
the box for El .height. The left box for El is itself constructed from the box 
for E and the subscript 1. The subscript 1 is handled by shrinking its box by 
about 30%, 
lowering it, and placing it after the box for E. Although we shall 
treat .height as a text string, the rectangles within its box show how it can be 
constructed from boxes for the individual letters. 
1
-
 
I 
t height 
height 
de.p. 
th. ...*.,.-.-- 
. 
. 
. 
. . . 
. 
- .: 
aePih 
J 
Figure 5.24: Constructing larger boxes from smaller ones 
In this example, we concentrate on the vertical geometry of boxes only. The 
horizontal geometry - 
the widths of boxes - 
is also interesting, especially when 
different characters have different widths. It may not be readily apparent, but 
each of the distinct characters in Fig. 5.24 has a different width. 
The values associated with the vertical geometry of boxes are as follows: 
a) The point size is used to set text within a box. We shall assume that 
characters not in subscripts are set in 10 point type, the size of type in 
this book. Further, we assume that if a box has point size p, then its 
subscript box has the smaller point size 0.7~. 
Inherited attribute B.ps 
will represent the point size of block B. This attribute must be inherited, 
because the context determines by how much a given box needs to be 
shrunk, due to the number of levels of subscripting. 
5.4. SYNTAX-DIRECTED TRANSLATION SCHEMES 
333 
b) Each box has a baseline, which is a vertical position that corresponds to 
the bottoms of lines of text, not counting any letters, like "g" that extend 
below the normal baseline. In Fig. 5.24, the dotted line represents the 
baseline for the boxes E, .height, and the entire expression. The baseline 
for the box containing the subscript 1 
is adjusted to lower the subscript. 
c) A box has a height, which is the distance from the top of the box to the 
baseline. Synthesized attribute B. 
ht gives the height of box B. 
d) A box has a depth, which is the distance from the baseline to the bottom 
of the box. Synthesized attribute B. 
dp gives the depth of box B. 
The SDD in Fig. 5.25 gives rules for computing point sizes, heights, and 
depths. Production 1 
is used to assign B.ps the initial value 10. 
Bl .ps = 
B.ps 
B2 
.ps = 
B.ps 
B. 
ht = 
max(Bl. 
ht, B2. 
ht) 
B.dp = 
max(Bl.dp, B2.d~) 
3) 
B + 
B1 sub B2 
Figure 5.25: SDD for typesetting boxes 
Bl .ps = 
B.ps 
B2.ps = 0.7 x B.ps 
B.ht = 
max(Bl . 
ht, B2. 
ht - 
0.25 x B.ps) 
B.dp = 
max(B1.dp, B2.dp + 
0.25 x B.ps) 
5) 
B + t e x t  
Production 2 handles juxtaposition. Point sizes are copied down the parse 
tree; that is, two sub-boxes of a box inherit the same point size from the larger 
box. Heights and depths are computed up the tree by taking the maximum. 
That is, the height of the larger box is the maximum of the heights of its two 
components, and similarly for the depth. 
Production 3 handles subscripting and is the most subtle. In this greatly 
simplified example, we assume that the point size of a subscripted box is 70% 
of the point size of its parent. Reality is much more complex, since subscripts 
cannot shrink indefinitely; in practice, after a few levels, the sizes of subscripts 
B.ht = 
getHt (B.ps, text.lexval) 
B. 
dp = getDp (B.ps, text 
.lexval) 
334 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
shrink hardly at all. Further, we assume that the baseline of a subscript box 
drops by 25% of the parent's point size; again, reality is more complex. 
Production 4 copies attributes appropriately when parentheses are used. Fi- 
nally, production 5 handles the leaves that represent text boxes. In this matter 
too, the true situation is complicated, so we merely show two unspecified func- 
tions getHt and getDp that examine tables created with each font to determine 
the maximum height and maximum depth of any characters in the text string. 
The string itself is presumed to be provided as the attribute lexual of terminal 
text. 
Our last task is to turn this SDD into an SDT, following the rules for an L- 
attributed SDD, which Fig. 5.25 is. The appropriate SDT is shown in Fig. 5.26. 
For readability, since production bodies become long, we split them across lines 
and line up the actions. Production bodies therefore consist of the contents of 
all lines up to the head of the next production. 
3) 
B + 
B1 sub 
B
2
 
5) 
B -+ text 
{ B. 
ht = getHt (B 
.ps, text 
. 
lexual) 
; 
B. 
dp = getDp (B.ps, 
text 
.lexual); ) 
Figure 5.26: SDT for typesetting boxes 
Our next example concentrates on a simple while-statement and the gener- 
ation of intermediate code for this type of statement. Intermediate code will 
be treated as a string-valued attribute. Later, we shall explore techniques that 
involve the writing of pieces of a string-valued attribute as we parse, thus avoid- 
ing the copying of long strings to build even longer strings. The technique was 
introduced in Example 5.17, where we generated the postfix form of an infix 
5.4. SYNTAX-DIRECTED TRANSLATION SCHEMES 
335 
expression "on-the-fly," rather than computing it as an attribute. However, in 
our first formulation, we create a string-valued at 
tribute by concatenation. 
Example 5.19 : 
For this example, we only need one production: 
S --+ while ( C ) S
1
 
Here, S is the nonterminal that generates all kinds of statements, presumably 
including if-statements, assignment statements, and others. In this example, C 
stands for a conditional expression - 
a boolean expression that evaluates to 
true or false. 
In this flow-of-control example, the only things we ever generate are labels. 
All the other intermediate-code instructions are assumed to be generated by 
parts of the SDT that are not shown. Specifically, we generate explicit instruc- 
tions of the form label L, where L is an identifier, to indicate that L is the 
label of the instruction that follows. We assume that the intermediate code is 
like that introduced in Section 2.8.4. 
The meaning of our while-statement is that the conditional C is evaluated. 
If it is true, control goes to the beginning of the code for S1. If false, then control 
goes to the code that follows the while-statement 's code. The code for S1 must 
be designed to jump to the beginning of the code for the while-statement when 
finished; the jump to the beginning of the code that evaluates C is not shown 
in Fig. 5.27. 
We use the following attributes to generate the proper intermediate code: 
1. The inherited attribute S.next labels the beginning of the code that must 
be executed after S is finished. 
2. The synthesized attribute S. 
code is the sequehce of intermediate-code 
steps that implements a statement S and ends with a jump to S.next. 
3. The inherited attribute C. 
true labels the beginning of the code that must 
be executed if C is true. 
4. The inherited attribute C.false labels the beginning of the code that must 
be executed if C is false. 
5. The synthesized attribute C.code is the sequence of intermediate-code 
steps that implements the condition C and jumps either to C.true or to 
C.false, depending on whether C is true or false. 
The SDD that computes these attributes for the while-statement is shown 
in Fig. 5.27. A number of points merit explanation: 
The function new generates new labels. 
The variables L1 and L2 hold labels that we need in the code. L1 is the 
beginning of the code for the while-statement, and we need to arrange 
336 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
S 
-+ while ( C ) S1 L1 = 
new(); 
L2 = new(); 
S1.next = 
L1; 
C.false = 
S.next; 
C.true = 
L2; 
S.code = 
label (1 L1 11 C.code (1 label 1) L2 )I Sl.code 
Figure 5.27: SDD for while-statements 
that Sl jumps there after it finishes. That is why we set &.next to L1. 
L2 is the beginning of the code for S1, 
and it becomes the value of C. 
true, 
because we branch there when C is true. 
Notice that C.false is set to S.next, because when the condition is false, 
we execute whatever code must follow the code for S. 
We use )I 
as the symbol for concatenation of intermediate-code fragments. 
The value of S.code thus begins with the label L1, then the code for 
condition C, another label L2, and the code for S1. 
This SDD is L-attributed. When we convert it into an SDT, the only re- 
maining issue is how to handle the labels L1 and L2, which are variables, and 
not attributes. If we treat actions as dummy nonterminals, then such variables 
can be treated as the synthesized attributes of dummy nonterminals. Since L1 
and L2 do not depend on any other attributes, they can be assigned to the 
first action in the production. The resulting SDT with embedded actions that 
implements this L-attributed definition is shown in Fig. 5.28. 
C1 
S -+ while ( 
{ L1 = 
new(); L2 = 
new(); C.false = 
S.next; C.true = 
L2; } 
c >  
{ Sl.next = 
LI; ) 
s
1
 
{ S.code = 
label 1) L1 11 C.code )I label )I L2 11 Sl 
.code; } 
Figure 5.28: SDT for while-statements 
5.4.6 Exercises for Section 5.4 
Exercise 5.4.1 : 
We mentioned in Section 5.4.2 that it is possible to deduce, 
from the LR state on the parsing stack, what grammar symbol is represented 
by the state. How would we discover this information? 
Exercise 5.4.2 : 
Rewrite the following SDT: 
5.5. IMPLEMENTING L-ATTRIB 
UTED SDD'S 
337 
so that the underlying grammar becomes non-left-recursive. Here, a, b, c, and 
d are actions, and 0 and 1 are terminals. 
! 
Exercise 5.4.3 : 
The following SDT computes the value of a string of 0's and 
1's interpreted as a positive, binary integer. 
Rewrite this SDT so the underlying grammar is not left recursive, and yet the 
same value of B.ual is computed for the entire input string. 
! Exercise 5.4.4 : 
Write L-attributed SDD's analogous to that of Example 5.19 
for the following productions, each of which represents a familiar flow-of-control 
construct, as in the programming language C. You may need to generate a three- 
address statement to jump to a particular label L, in which case you should 
generate goto L. 
a) S -+ if ( C ) S1 else Sz 
b) S + 
do S1 while ( C ) 
Note that any statement in the list can have a jump from its middle to 
the next statement, so it is not sufficient simply to generate code for each 
statement in order. 
Exercise 5.4.5 : 
Convert each of your SDD's from Exercise 5.4.4 to an SDT 
in the manner of Example 5.19. 
Exercise 5.4.6 : 
Modify the SDD of Fig. 5.25 to include a synthesized attribute 
B.Ee, the length of a box. The length of the concatenation of two boxes is the 
sum of the lengths of each. Then add your new rules to the proper positions in 
the SDT of Fig. 5.26 
Exercise 5.4.7 : 
Modify the SDD of Fig. 5.25 to include superscripts denoted 
by operator sup between boxes. If box B2 is a superscript of box B1, then 
position the baseline of B2 0.6 times the point size of B1 above the baseline of 
B1. Add the new production and rules to the SDT of Fig. 5.26. 
5.5 Implementing L-Attributed SDD's 
Since many translation applications can be addressed using L-attributed defi- 
nitions, we shall consider their implementation in more detail in this section. 
The following methods do translation by traversing a parse tree: 
338 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
1. Build the parse tree and annotate. This method works for any noncircular 
SDD whatsoever. We introduced annotated parse trees in Section 5.1.2. 
2. Build the parse tree, add actions, and execute the actions in preorder. 
This approach works for any L-attributed definition. We discussed how 
to turn an L-attributed SDD into an SDT in Section 5.4.5; in particular, 
we discussed how to embed actions into productions based on the semantic 
rules of such an SDD. 
In this section, we discuss the following methods for translation during parsing: 
3. Use a recursive-descent parser with one function for each nonterminal. 
The function for nonterminal A receives the inherited attributes of A as 
arguments and returns the synthesized attributes of A. 
4. Generate code on the fly, using a recursive-descent parser. 
5. Implement an SDT in conjunction with an LL-parser. The attributes are 
kept on the parsing stack, and the rules fetch the needed attributes from 
known locations on the stack. 
6. Implement an SDT in conjunction with an LR-parser. This method may 
be surprising, since the SDT for an L-attributed SDD typically has ac- 
tions in the middle of productions, and we cannot be sure during an LR 
parse that we are even in that production until its entire body has been 
constructed. We shall see, however, that if the underlying grammar is LL, 
we can always handle both the parsing and translation bottom-up. 
5.5.1 Translation During Recursive-Descent Parsing 
A recursive-descent parser has a function A for each nonterminal A, as discussed 
in Section 4.4.1. We can extend the parser into a translator as follows: 
a) The arguments of function A are the inherited attributes of nonterminal 
A. 
b) The return-value of function A is the collection of synthesized attributes 
of nonterminal A. 
In the body of function A, we need to both parse and handle attributes: 
1. Decide upon the production used to expand A. 
2. Check that each terminal appears on the input when it is required. We 
shall assume that no backtracking is needed, but the extension to recur- 
sive-descent parsing with backtracking can be done by restoring the input 
position upon failure, as discussed in Section 4.4.1. 
5.5. IMPLEMENTING L-ATTRIB 
UTED SDD 'S 
339 
3. Preserve, in local variables, the values of all attributes needed to compute 
inherited attributes for nonterminals in the body or synthesized attributes 
for the head nonterminal. 
4. Call functions corresponding to nonterminals in the body of the selected 
production, providing them with the proper arguments. Since the un- 
derlying SDD is L-attributed, we have already computed these attributes 
and stored them in local variables. 
Example 5.20 : 
Let us consider the SDD and SDT of Example 5.19 for while- 
statements. A pseudocode rendition of the relevant parts of the function S 
appears in Fig. 5.29. 
string S(labe1 next) { 
string Scode, Ccode; /* 
local variables holding code fragments */ 
label L1, L2; /* 
the local labels */ 
if ( current input == token while ) { 
advance input; 
check I(' is next on the input, and advance; 
L1 = new(); 
L2 = new(); 
Ccode = C(next, L2); 
check ')I is next on the input, and advance; 
Scode = 
S(L1); 
return("labe1" )I L l  (1 Ccode I( "label" (1 L2 I( Scode); 
I 
else /* other statement types */ 
1 
Figure 5.29: Implementing while-statements with a recursive-descent parser 
We show S as storing and returning long strings. In practice, it would be 
far more efficient for functions like S and C to return pointers to records that 
represent these strings. Then, the return-statement in function S would not 
physically concatenate the components shown, but rather would construct a 
record, or perhaps tree of records, expressing the concatenation of the strings 
represented by Scode and Ccode, the labels L l  and L2, and the two occurrences 
of the literal string "label". 
Example 5.21 : Now, let us take up the SDT of Fig. 5.26 for typesetting 
boxes. First, we address parsing, since the underlying grammar in Fig. 5.26 
is ambiguous. The following transformed grammar makes juxtaposition and 
subscripting right associative, with sub taking precedence over juxtaposition: 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
S
B
 
B 
4 T B 1  I T  
T 
-+ F sub TI I F 
F -+ 
( B ) 1 text 
The two new nonterminals, T and F, are motivated by terms and factors in 
expressions. Here, a "factor," generated by F, is either a parenthesized box 
or a text string. A "term," generated by T, is a "factor" with a sequence of 
subscripts, and a box generated by B is a sequence of juxtaposed "terms." 
The attributes of B carry over to T and F, 
since the new nonterminals also 
denote boxes; they were introduced simply to aid parsing. Thus, both T and 
F have an inherited attribute ps and synthesized attributes ht and dp, with 
semantic actions that can be adapted from the SDT in Fig. 5.26. 
The grammar is not yet ready for top-down parsing, since the productions 
for B and T have common prefixes. Consider T, for instance. A top-down 
parser cannot choose between the two productions for T by looking one symbol 
ahead in the input. Fortunately, we can use a form of left-factoring, discussed 
in Section 4.3.4, to make the grammar ready. With SDT's, the notion of com- 
mon prefix applies to actions as well. Both productions for T begin with the 
nonterminal F inheriting attribute ps from T. 
The pseudocode in Fig. 5.30 for T(ps) folds in the code for F(ps). After 
left-factoring is applied to T -+ F 
sub TI 1 F, there is only one call to F; 
the 
pseudocode shows the result of substituting the code for F in place of this call. 
The function T will be called as T(lO.O) by the function for B, which we 
do not show. It returns a pair consisting of the height and depth of the box 
generated by nonterminal T; in practice, it would return a record containing 
the height and depth. 
Function T begins by checking for a left parenthesis, in which case it must 
have the production F -+ ( B ) to work with. It saves whatever the B inside the 
parentheses returns, but if that B is not followed by a right parenthesis, then 
there is a syntax error, which must be handled in a manner not shown. 
Otherwise, if the current input is text, then the function T uses getHt and 
getDp to determine the height and depth of this text. 
T then decides whether the next box is a subscript and adjusts the point 
size, if so. We use the actions associated with the production B -+ B sub B 
in Fig. 5.26 for the height and depth of the larger box. Otherwise, we simply 
return what F would have returned: (hl, 
dl). 
5.5.2 On-The-Fly Code Generation 
The construction of long strings of code that are attribute values, as in Ex- 
ample 5.20, is undesirable for several reasons, including the time it could take 
to copy or move long strings. In common cases such as our running code- 
generation example, we can instead incrementally generate pieces of the code 
into an array or output file by executing actions in an SDT. The elements we 
need to make this technique work are: 
5.5. IMPLEMENTING L-ATTRIB 
UTED SDD 
'S 
(float, float) T(float ps) { 
float hl, h2, dl, d2; /* locals to hold heights and depths */ 
/* 
start code for F(ps) */ 
if ( current input == '(I ) { 
advance input ; 
(hl, 
dl) = 
B(ps); 
if (current input != 
I)' ) syntax error: expected I)'; 
advance input; 
1 
else if ( current input == text ) { 
let lexical value text.lexva1 be t; 
advance input; 
h l  = getHt(ps, t); 
dl = getDp(ps, 
t); 
1 
else syntax error: expected text or '(I; 
/* 
end code for F(ps) */ 
if ( current input == sub ) { 
advance input; 
(h2, 
d2) = 
T(0.7 * ps); 
return (max(h1, 
h2 - 
0.25 * ps), max(d1, d2 + 
0.25 * ps)); 
1 
return (hl, 
dl); 
1 
Figure 5.30: Recursive-descent typesetting of boxes 
1. There is, for one or more nonterminals, a main attribute. For conve- 
nience, we shall assume that the main attributes are all string valued. In 
Example 5.20, the attributes S.code and C.code are main attributes; the 
other attributes are not. 
2. The main attributes are synthesized. 
3. The rules that evaluate the main attribute(s) ensure that 
(a) The main attribute is the concatenation of main attributes of non- 
terminals appearing in the body of the production involved, perhaps 
with other elements that are not main attributes, such as the string 
label or the values of labels L1 and L2. 
(b) The main attributes of nonterminals appear in the rule in the same 
order as the nonterminals themselves appear in the production body. 
As a consequence of the above conditions, the main attribute can be constructed 
by emitting the non-main-attribute elements of the concatenation. We can rely 
342 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
The Type of Main Attributes 
Our simplifying assumption that main attributes are of string type is really 
too restrictive. The true requirement is that the type of all the main 
attributes must have values that can be constructed by concatenation of 
elements. For instance, a list of objects of any type would be appropriate, 
as long as we represent these lists in a way that allows elements to be 
efficiently appended to the end of the list. Thus, if the purpose of the 
main attribute is to represent a sequence of intermediate-code statements, 
we could produce the intermediate code by writing statements to the end 
of an array of objects. Of course the requirements stated in Section 5.5.2 
still apply to lists; for example, main attributes must be assembled from 
other main attributes by concatenation in order. 
on the recursive calls to the functions for the nonterminals in a production body 
to emit the value of their main attribute incrementally. 
Example 5.22 : 
We can modify the function of Fig. 5.29 to emit elements of 
the main translation S.code instead of saving them for concatenation into a 
return value of S.code. The revised function S appears in Fig. 5.31. 
void S(labe1 
next) { 
label Ll, L2; /* 
the local labels */ 
if ( current input == token while ) { 
advance input 
; 
check I(' is next on the input, and advance; 
L1= new(); 
L2 = new(); 
prznt("label", Ll); 
C(next, 
L2); 
check I)' is next on the input, and advance; 
print("labell', 
L2); 
S(L1); 
1 
else /* 
other statement types */ 
1 
Figure 5.3 
1 
: On-t 
he-fly recursive-descent code generation for while-st 
atements 
In Fig. 5.31, S and C now have no return value, since their only synthesized 
attributes are produced by printing. Further, the position of the print state- 
ments is significant. The order in which output is printed is: first label L1, 
then the code for C (which is the same as the value of Ccode in Fig. 5.29), then 
5.5. IMPLEMENTING L-ATTRIB UTED SDD 
'S 
label L2, and finally the code from the recursive call to S (which is the same 
as Scode in Fig. 5.29). Thus, the code printed by this call to S is exactly the 
same as the value of Scode that is returned in Fig. 5.29). 
Incidentally, we can make the same change to the underlying SDT: turn the 
construction of a main attribute into actions that emit the elements of that 
attribute. In Fig. 5.32 we see the SDT of Fig. 5.28 revised to generate code on 
the fly. 
S + while ( 
{ L1 = new(); L2 = new(); C.false = 
S.next; 
C.true = 
L2; print("label", Ll); ) 
c )  
{ &.next = 
L1; print("labelU, 
L2); } 
s
1
 
Figure 5.32: SDT for on-the-fly code generation for while statements 
5.5.3 
L-Attributed SDD's and L
L
 Parsing 
Suppose that an L-attributed SDD is based on an LL-grammar and that we have 
converted it to an SDT with actions embedded in the productions, as described 
in Section 5.4.5. We can then perform the translation during LL parsing by 
extending the parser stack to hold actions and certain data items needed for 
attribute evaluation. Typically, the data items are copies of attributes. 
In addition to records representing terminals and nonterminals, the parser 
stack will hold action-records representing actions to be executed and synth- 
esize-records to hold the synthesized attributes for nonterminals. We use the 
following two principles to manage attributes on the stack: 
The inherited attributes of a nonterminal A are placed in the stack record 
that represents that nonterminal. The code to evaluate these attributes 
will usually be represented by an action-record immediately above the 
stack record for A; in fact, the conversion of L-attributed SDD's to SDT's 
ensures that the action-record will be immediately above A. 
The synthesized attributes for a nonterminal A are placed in a separate 
synthesize-record that is immediately below the record for A on the stack. 
This strategy places records of several types on the parsing stack, trusting that 
these variant record types can be managed properly as subclasses of a "stack- 
record" class. In practice, we might combine several records into one, but the 
ideas are perhaps best explained by separating data used for different purposes 
into different records. 
Action-records contain pointers to code to be executed. Actions may also 
appear in synthesize-records; these actions typically place copies of the synthe- 
sized attribute(s) in other records further down the stack, where the value of 
344 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
that attribute will be needed after the synthesize-record and its attributes are 
popped off the stack. 
Let us take a brief look at LL parsing to see the need to make temporary 
copies of attributes. From Section 4.4.4, a table-driven LL parser mimics a 
leftmost derivation. If w is the input that has been matched*so far, then the 
stack holds a sequence of grammar symbols a such that S + wa, where S 
lm 
is the start symbol. When the parser expands by a production A -+ 
B C, it 
replaces A on top of the stack by B C. 
Suppose nonterminal C has an inherited attribute C.i. With A -+ B C, the 
inherited attribute C.i may depend not only on the inherited attributes of A, but 
on all the attributes of B. Thus, we may need to process B completely before 
C.i can be evaluated. We therefore save temporary copies of all the attributes 
needed to evaluate C.i in the action-record that evaluates C.i. Otherwise, when 
the parser replaces A on top of the stack by B C, the inherited attributes of A 
will have disappeared, along with its stack record. 
Since the underlying SDD is L-attributed, we can be sure that the values 
of the inherited attributes of A are available when A rises to the top of the 
stack. The values will therefore be available in time to be copied into the 
action-record that evaluates the inherited attributes of C. Furthermore, space 
for the synthesized attributes of A is not a problem, since the space is in the 
synthesize-record for A, which remains on the stack, below B and C, when the 
parser expands by A -+ 
B C. 
As B is processed, we can perform actions (through a record just above B on 
the stack) that copy its inherited attributes for use by C, as needed, and after B 
is processed, the synthesize-record for B can copy its synthesized attributes for 
use by C, if needed. Likewise, synthesized attributes of A may need temporaries 
to help compute their value, and these can be copied to the synthesize-record 
for A as B and then C are processed. The principle that makes all this copying 
of attributes work is: 
All copying takes place among the records that are created during one 
expansion of one nonterminal. Thus, each of these records knows how far 
below it on the stack each other record is, and can write values into the 
records below safely. 
The next example illustrates the implement 
ation of inherited attributes dur- 
ing LL parsing by diligently copying attribute values. Shortcuts or optimiza- 
tions are possible, particularly with copy rules, which simply copy the value of 
one attribute into another. Shortcuts are deferred until Example 5.24,. which 
also illustrates synthesize-records. 
Example 5.23 : This example implements the the SDT of Fig. 5.32, which 
generates code on the fly for the while-production. This SDT does not have 
synthesized attributes, except for dummy attributes that represent labels. 
Figure 5.33(a) shows the situation as we are about to use the while-produc- 
tion to expand S, presumably because the lookahead symbol on the input is 
5.5. IMPLEMENTING L-ATTRIBUTED SDD'S 
while. The record at the top of stack is for S, 
and it contains only the inherited 
attribute S.next, which we suppose has the value x. Since we are now parsing 
top-down, we show the stack top at the left, according to our usual convention. 
1
*
r
n
v
,
r
1
 
snext = 
x false = 
? 
L1 = 
new(); 
L2 = new(); 
stack[top - 
l].false = 
snext; 
stack[top - 
l].true = 
L2; 
stack[top 
- 
3].all = 
L1; 
stack[top - 
3].al2 = 
L2; 
prznt("labell', 
L l ) ;  
stack[top 
- 
l].next = 
a l l ;  
prznt("label", a/2); 
1 
Figure 5.33: Expansion of S according to the while-statement production 
Figure 5.33(b) shows the situation immediately after we have expanded S. 
There are action-records in front of the nonterminals C and S1, 
corresponding 
to the actions in the underlying SDT of Fig. 5.32. The record for C has room 
for inherited attributes true and false, while the record for S1 has room for 
attribute next, as all S-records must. We show values for these fields as ?, 
because we do not yet know their values. 
The parser next recognizes while and ( on the input and pops their records 
off the stack. Now, the first action is at the top, and it must be executed. This 
action-record has a field snext, which holds a copy of the inherited attribute 
S.next. When S is popped from the stack, the value of S.next is copied into 
the field snext for use during the evaluation of the inherited attributes for C. 
The code for the first action generates new values for L1 and 22, which we 
shall suppose are y and x, 
respectively. The next step is to make x the value of 
C.true. The assignment staclc[top - 
l].true = L2 is written knowing it is only 
executed when this action-record is at the top of stack, so top - 
1 
refers to the 
record below it - 
the record for C. 
The first action-record then copies L l  into field all in the second action, 
where it will be used to evaluate &.next. It also copies L2 into a field called 
a12 of the second action; this value is needed for that action-record to print its 
output properly. Finally, the first action-record prints label 
y to the output. 
The situation after completing the first action and popping its record off 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
Figure 5.34: After the action above C is performed 
the stack is shown in Fig. 5.34. The values of inherited attributes in the record 
for C have been filled in properly, as have the temporaries all and a12 in the 
second action record. At this point, C is expanded, and we presume that the 
code to implement its test containing jumps to labels x and x, 
as appropriate, 
is generated. When the C-record is popped from the stack, the record for ) 
becomes top and causes the parser to check for ) on its input. 
With the action above S1 at the top of the stack, its code sets Sl 
.next and 
emits label 
x. When that is done, the record for S
1
 becomes the top of stack, 
and as it is expanded, we presume it correctly generates code that implements 
whatever kind of statement it is and then jump to label y. 
Example 5.24: Now, let us consider the same while-statement, but with a 
translation that produces the output S. 
code as a synthesized attribute, rather 
than by on-the-fly generation. In order to follow the explanation, it is useful to 
bear in mind the following invariant or inductive hypothesis, which we assume 
is followed for every nonterminal: 
Every nonterminal that has code associated with it leaves that code, as a 
string, in the synthesize-record just below it on the stack. 
Assuming this statement is true, we shall handle the while-production so it 
maintains this statement as an invariant. 
Figure 5.35(a) shows the situation just before S 
is expanded using the pro- 
duction for while-statements. At the top of the stack we see the record for S; 
it 
has a field for its inherited attribute S. 
next, as in Example 5.23. Immediately 
below that record is the synthesize-record for this occurrence of S. The latter 
has a field for S.code, as all synthesize-records for S must have. We also show 
it with some other fields for local storage and actions, since the SDT for the 
while production in Fig. 5.28 is surely part of a larger SDT. 
Our expansion of S is based on the SDT of Fig. 5.28, and it is shown in 
Fig. 5.35(b). As a shortcut, during the expansion, we assume that the inherited 
attribute S.next is assigned directly to C.false, rather than being placed in the 
first action and then copied into the record for C. 
Let us examine what each record does when it becomes the top of stack. 
First, the while record causes the token while to be matched with the input, 
5.5. In/lPLEMENTING L-ATTRIB UTED SDD'S 
next = 
x 
S. 
code 
I 
data 
I 
actions 
top 
Synthesize 
S. 
code 
code = 
? 
Ccode = 
? 
I 
stack[top - 
31. Ccode = code; 1 
12 = 
? 
actions 
L2 = 
new(); 
stack[top 
- 
1] 
.true = 
L2; 
"label" 11 11 1 )  Ccode 
Figure 5.35: Expansion of S 
with synthesized attribute constructed on the stack 
which it must, or else we would not have expanded S in this way. After while 
and ( are popped off the stack, the code for the action-record is executed. It 
generates values for L1 and L2, and we take the shortcut of copying them 
directly to the inherited attributes that need them: &.next and C.true. The 
last two steps of the action cause L1 and L2 to be copied into the record called 
"Synthesize 5'1. code." 
The synthesize-record for S1 
does double duty: not only will it hold the syn- 
thesized attribute Sl. 
code, but it will also serve as an action-record to complete 
the evaluation of the attributes for the entire production S -+ while ( C ) S1. 
In particular, when it gets to the top, it will compute the synthesized attribute 
S.code and place its value in the synthesize-record for the head S. 
When C becomes the top of the stack, it has both its inherited attributes 
computed. By the inductive hypothesis stated above, we suppose it correctly 
generates code to execute its condition and jump to the proper label. We also 
assume that the actions performed during the expansion of C correctly place 
this code in the record below, as the value of synthesized attribute C. 
code. 
After C is popped, the synthesize-record for C.code becomes the top. Its 
code is needed in the synthesize-record for Sl.code, because that is where we 
concatenate all the code elements to form S.code. The synthesize-record for 
C.code therefore has an action to copy C.code into the synthesize-record for 
Sl. 
code. After doing so, the record for token ) reaches the top of stack, and 
causes a check for ) on the input. Assuming that test succeeds, the record for 
S1 becomes the top of stack. By our inductive hypothesis, this nonterminal is 
348 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
Can We Handle L-Attributed SDDSs 
on LR 
Grammars? 
In Section 5.4.1, we saw that every S-attributed SDD on an LR grammar 
can be implemented during a bottom-up parse. From Section 5.5.3 every 
L-attributed SDD on an LL grammar can be parsed top-down. Since LL 
grammars are a proper subset of the LR grammars, and the S-attributed 
SDD's are a proper subset of the L-attributed SDD's, can we handle every 
LR grammar and L-attributed SDD bottom-up? 
We cannot, as the following intuitive argument shows. Suppose we 
have a production A -+ B C in an LR-grammar, and there is an inherited 
attribute B.i that depends on inherited attributes of A. When we reduce 
to B, we still have not seen the input that C generates, so we cannot 
be sure that we have a body of production A -+ B C. Thus, we cannot 
compute B.i yet, since we are unsure whether to use the rule associated 
with this production. 
Perhaps we could wait until we have reduced to C, and know that we 
must reduce B C to A. However, even then, we do not know the inherited 
attributes of A, because even after reduction, we may not be sure of the 
production body that contains this A. We could reason that this decision, 
too, should be deferred, and therefore further defer the computation of B.i. 
If we keep reasoning this way, we soon realize that we cannot make any 
decisions until the entire input is parsed. Essentially, we have reached the 
strategy of "build the parse tree first and then perform the translation." 
expanded, and the net effect is that its code is correctly constructed and placed 
in the field for code in the synthesize-record for S1. 
Now, all the data fields of the synthesize-record for S1 
have been filled in, so 
when it becomes the top of stack, the action in that record can be executed. The 
action causes the labels and code from C.code and &.code to be concatenated 
in the proper order. The resulting string is placed in the record below; that is, 
in the synthesize-record for S. We have now correctly computed S.code, and 
when the synthesize-record for S becomes the top, that code is available for 
placement in another record further down the stack, where it will eventually 
be assembled into a larger string of code implementing a program element of 
which this S is a part. 
5.5.4 
Bottom-Up Parsing of L-Attributed SDDSs 
We can do bottom-up every translation that we can do top-down. More pre- 
cisely, given an L-attributed SDD on an LL grammar, we can adapt the gram- 
mar to compute the same SDD on the new grammar during an LR parse. The 
"trick" has three parts: 
5.5. IMPLEMENTING L-ATTRIB 
UTED SDD 
'S 
349 
1. Start with the SDT constructed as in Section 5.4.5, which places embed- 
ded actions before each nonterminal to compute its inherited attributes 
and an action at the end of the production to compute synthesized at- 
tributes. 
2. Introduce into the grammar a marker nonterminal in place of each em- 
bedded action. Each such place gets a distinct marker, and there is one 
production for any marker M, namely M -+ E .  
3. Modify the action a if marker nonterminal M replaces it in some produc- 
tion A -+ 
a {a) p, and associate with M + 
t an action a' that 
(a) Copies, as inherited attributes of M, any attributes of A or symbols 
of a that action a needs. 
(b) Computes attributes in the same way as a, but makes those at- 
tributes be synthesized attributes of M. 
This change appears illegal, since typically the action associated with 
production M -+ E will have to access attributes belonging to grammar 
symbols that do not appear in this production. However, we shall imple- 
ment the actions on the LR parsing stack, so the necessary attributes will 
always be available a known number of positions down the stack. 
Example 5.25 : 
Suppose that there is a production A -+ 
B C in an LL gram- 
mar, and the inherited attribute B.i is computed from inherited attribute A.i 
by some formula B.i = 
f (A.i). That is, the fragment of an SDT we care about 
is 
We introduce marker M with inherited attribute M.i and synthesized attribute 
M.s. The former will be a copy of A.i and the latter will be B.i. The SDT will 
be written 
Notice that the rule for M does not have A.i available to it, but in fact we 
shall arrange that every inherited attribute for a nonterminal such as A appears 
on the stack immediately below where the reduction to A will later take place. 
Thus, when we reduce t to M, we shall find A.i immediately below it, from 
where it may be read. Also, the value of M.s, which is left on the stack along 
with M ,  
is really B.i and properly is found right below where the reduction to 
B will later occur. 
Example 5.26 : 
Let us turn the SDT of Fig. 5.28 into an SDT that can operate 
with an LR parse of the revised grammar. We introduce a marker M before C 
and a marker N before S1, so the underlying grammar becomes 
350 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
Why Markers Work 
Markers are nonterminals that derive only c and that appear only once 
among all the bodies of all productions. We shall not give a formal proof 
that, when a grammar is LL, marker nonterminals can be added at any 
position in the body, and the resulting grammar will still be LR. The 
intuition, however, is as follows. If a grammar is LL, then we can determine 
that a string w on the input is derived from nonterminal A, in a derivation 
that starts with production A + 
a, by seeing only the first symbol of w 
(or the following symbol if w = e). Thus, if we parse w bottom-up, then 
the fact that a prefix of w must be reduced to a and then to S is known as 
soon as the beginning of w appears on the input. In particular, if we insert 
markers anywhere in a, 
the LR states will incorporate the fact that this 
marker has to be there, and will reduce E: to the marker at the appropriate 
point on the input. 
S + while ( M C ) N S1 
M
+
€
 
N
+
€
 
Before we discuss the actions that are associated with markers M and N, let 
us outline the "inductive hypothesis" about where attributes are stored. 
1. Below the entire body of the while-production - 
that is, below while 
on the stack - 
will be the inherited attribute S.next. We may not know 
the nonterminal or parser state associated with this stack record, but we 
can be sure that it will have a field, in a fixed position of the record, that 
holds S.next before we begin to recognize what is derived from this S. 
2. Inherited attributes C.true and C.false will be just below the stack record 
for C. Since the grammar is presumed to be LL, the appearance of while 
on the input assures us that the while-production is the only one that can 
be recognized, so we can be sure that M will appear immediately below 
C on the stack, and M's record will hold the inherited attributes of C. 
3. Similarly, the inherited attribute Sl 
.next must appear immediately below 
S1 on the stack, so we may place that attribute in the record for N. 
4. The synthesized attribute C.code will appear in the record for C. As 
always when we have a long string as an attribute value, we expect that 
in practice a pointer to (an object representing) the string will appear in 
the record, while the string itself is outside the stack. 
5. Similarly, the synthesized attribute Sl 
.code will appear in the record for 
s1. 
5.5. IMPLEMENTING L-ATTRIBUTED SDD'S 
351 
Let us follow the parsing process for a while-statement. Suppose that a 
record holding S.next appears on the top of the stack, and the next input is 
the terminal while. We shift this terminal onto the stack. It is then certain 
that the production being recognized is the while-production, so the LR parser 
can shift "(" and determine that its next step must be to reduce E. to M. The 
stack at this time is shown in Fig. 5.36. We also show in that figure the action 
that is associated with the reduction to M. We create values for L1 and L2, 
which live in fields of the M-record. Also in that record are fields for C.true and 
C.faEse. These attributes must be in the second and third fields of the record, 
for consistency with other stack records that might appear below C in other 
contexts and also must provide these attributes for C. The action completes 
by assigning values to C.true and C.false, one from the L2 just generated, and 
the other by reaching down the stack to where we know S. 
next is found. 
top 
4 
S. 
next 
wmm 
Code executed during 
reduction of E to M 
Ll = 
new(); 
L2 = 
new(); 
L2 
C.true = 
L2; 
C.fa1se = stack[top 
- 
3l.next; 
Figure 5.36: LR parsing stack after reduction of E: to M 
We presume that the next inputs are properly reduced to C. The synthesized 
attribute C.code is therefore placed in the record for C. This change to the stack 
is shown in Fig. 5.37, which also incorporates the next several records that are 
later placed above C on the stack. 
C. 
true / I C. 
code I 
l ~ l . n e x t  
I Isl.code I 
C. 
false 
z
j
 
Figure 5.37: Stack just before reduction of the while-production body to S 
Continuing with the recognition of the while-st 
at 
ement , the parser should 
next find ")" on the input, which it pushes onto the stack in a record of its 
own. At that point, the parser, which knows it is working on a while-statement 
because the grammar is LL, will reduce E. to N. The single piece of data asso- 
ciated with N is the inherited attribute Sl 
.next. Note that this attribute needs 
352 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
to be in the record for N because that will be just below the record for S1. The 
code that is executed to compute the value of Sl .next is 
This action reaches three records below N, which is at the top of stack when 
the code is executed, and retrieves the value of L1. 
Next, the parser reduces some prefix of the remaining input to S, which 
we have consistently referred to as S1 to distinguish it from the S at the head 
of the production. The value of Sl.code is computed and appears in the stack 
record for S1. This step takes us to the condition that is illustrated in Fig. 5.37. 
At this point, the parser will reduce everything from while to Sl to S. The 
code that is executed during this reduction is: 
tempcode = 
label 11 stack[top - 
4].L1 11 stack[top - 
3].code (1 
label 1) stack[top 
- 
4].L2 11 stack[top].code; 
top = 
top - 
5; 
stack[top] 
. 
code = tempcode; 
That is, we construct the value of S.code in a variable tempcode. That code is 
the usual, consisting of the two labels L1 and L2, the code for C and the code 
for S1. The stack is popped, so S appears where while was. The value of the 
code for S is placed in the code field of that record, where it can be interpreted 
as the synthesized attribute S. 
code. Note that we do not show, in any of this 
discussion, the manipulation of LR states, which must also appear on the stack 
in the field that we have populated with grammar symbols. 
5.5.5 Exercises for Section 5.5 
Exercise 5.5.1 : 
Implement each of your SDD's of Exercise 5.4.4 as a recursive- 
descent parser in the style of Section 5.5.1. 
Exercise 5.5.2 : 
Implement each of your SDD's of Exercise 5.4.4 as a recursive- 
descent parser in the style of Section 5.5.2. 
Exercise 5.5.3 : Implement each of your SDD's of Exercise 5.4.4 with an LL 
parser in the style of Section 5.5.3, with code generated "on the fly." 
Exercise 5.5.4: Implement each of your SDD's of Exercise 5.4.4 with an LL 
parser in the style of Section 5.5.3, but with code (or pointers to the code) 
stored on the stack. 
Exercise 5.5.5 : 
Implement each of your SDD's of Exercise 5.4.4 with an LR 
parser in the style of Section 5.5.4. 
Exercise 5.5.6 : Implement your SDD of Exercise 5.2.4 in the style of Sec- 
tion 5.5.1. Would an implementation in the style of Section 5.5.2 be any differ- 
ent? 
5.6. SUMMARY OF CHAPTER 5 
5.6 Summary of Chapter 5 
+ Inherited and Synthesized Attributes: Syntax-directed definitions may use 
two kinds of attributes. A synthesized attribute at a parse-tree node is 
computed from attributes at its children. An inherited attribute at a node 
is computed from attributes at its parent and/or siblings. 
+ Dependency Graphs: Given a parse tree and an SDD, we draw edges 
among the attribute instances associated with each parse-tree node to 
denote that the value of the attribute at the head of $he edge is computed 
in terms of the value of the attribute at the tail of the edge. 
+ Cyclic Definitions: In problematic SDD's, we find that there are some 
parse trees for which it is impossible to find an order in which we can 
compute all the attributes at all nodes. These parse trees have cycles in 
their associated dependency graphs. It is intractable to decide whether 
an SDD has such circular dependency graphs. 
+ S-Attributed Definitions: In an S-attributed SDD, all attributes are syn- 
thesized. 
+ L-Attributed Definitions: In an L-attributed SDD, attributes may be in- 
herited or synthesized. However, inherited attributes at a parse-tree node 
may depend only on inherited attributes of its parent and on (any) at- 
tributes of siblings to its left. 
+ Syntax Trees: Each node in a syntax tree represents a construct; the chil- 
dren of the node represent the meaningful components of the construct. 
+ Implementing S-Attributed SDD's: An S-attributed definition can be im- 
plemented by an SDT in which all actions are at the end of the production 
(a "postfix" SDT). The actions compute the synthesized attributes of the 
production head in terms of synthesized attributes of the symbols in the 
body. If the underlying grammar is LR, then this SDT can be imple- 
mented on the LR parser stack. 
+ Eliminating Left Recursion From SDT's: If an SDT has only side-effects 
(no attributes are computed), then the standard left-recursion-elimination 
algorithm for grammars allows us to carry the actions along as if they 
were terminals. When attributes are computed, we can still eliminate left 
recursion if the SDT is a postfix SDT. 
+ Implementing L-attributed SDD's by Recursive-Descent Parsing: If we 
have an L-attributed definition on a top-down parsable grammar, we can 
build a recursive-descent parser with no backtracking to implement the 
translation. Inherited at 
tributes become arguments of the functions for 
their nonterminals, and synthesized attributes are returned by that func- 
tion. 
354 
CHAPTER 5. SYNTAX-DIRECTED TRANSLATION 
+ Implementing L-Attributed SDD's on an LL Grammar: Every L-attribut- 
ed definition with an underlying LL grammar can be implemented along 
with the parse. Records to hold the synthesized attributes for a non- 
terminal are placed below that nonterminal on the stack, while inherited 
attributes for a nonterminal are stored with that nonterminal on the stack. 
Action records are also placed on the stack to compute attributes at the 
appropriate time. 
+ Implementing L-Attributed SDD's on an LL Grammar, Bottom-Up: An 
L-attributed definition with an underlying LL grammar can be converted 
to a translation on an LR grammar and the translation performed in con- 
nection with a bottom-up parse. The grammar transformation introduces 
"marker" nonterminals that appear on the bottom-up parser's stack and 
hold inherited attributes of the nonterminal above it on the stack. Syn- 
thesized attributes are kept with their nonterminal on the stack. 
5.7 References for Chapter 5 
Syntax-directed definitions are a form of inductive definition in which the induc- 
tion is on the syntactic structure. As such they have long been used informally 
in mathematics. Their application to programming languages came with the 
use of a grammar to structure the Algol 60 report. 
The idea of a parser that calls for semantic actions can be found in Samelson 
and Bauer [8] and Brooker and Morris [I]. Irons [2] constructed one of the 
first syntax-directed compilers, using synthesized attributes. The class of L- 
attributed definitions comes from [6]. 
Inherited attributes, dependency graphs, and a test for circularity of SDD's 
(that is, whether or not there is some parse tree with no order in which the at- 
tributes can be computed) are from Knuth [5]. Jazayeri, Ogden, and Rounds [3] 
showed that testing circularity requires exponential time, as a function of the 
size of the SDD. 
Parser generators such as Yacc [4] (see also the bibliographic notes in Chap- 
ter 4) support attribute evaluation during parsing. 
The survey by Paakki [7] is a starting point for accessing the extensive 
literature on syntax-directed definitions and translations. 
I. Brooker, R. A. and D. Morris, "A general translation program for phrase 
structure languages," J. ACM 9:l (1962), pp. 1-10. 
2. Irons, E. T., "A syntax directed compiler for Algol 60," Comm. ACM 4:l 
(1961), pp. 51-55. 
3. Jazayeri, M., W. F. Odgen, and W. C. Rounds, "The intrinsic expo- 
nential complexity of the circularity problem for attribute grammars,'' 
Comm. ACM 18:12 (1975), pp. 697-706. 
5.7. REFERENCES FOR CHAPTER 5 
355 
4. Johnson, S. C., "Yacc - 
Yet Another Compiler Compiler," Computing 
Science Technical Report 32, Bell Laboratories, Murray Hill, NJ, 1975. 
Available at http: 
//dinosaur. 
compilertools. 
net/yacc/ 
. 
5. Knuth, D.E., "Semantics of context-free languages," Mathematical Sys- 
tems Theory 2:2 (1968), pp. 127-145. See also Mathematical Systems 
Theory 5:l (1971), pp. 95-96. 
6. Lewis, P. M. 11, D. J. Rosenkrantz, and R. E. Stearns, "Attributed trans- 
lations," J. Computer and System Sciences 9:3 (1974), pp. 279-307. 
7. Paakki, J., "Attribute grammar paradigms - 
a high-level methodology in 
language implementation," Computing Surveys 27:2 (1995) pp. 196-255. 
8. Samelson, K. and F. L. Bauer, "Sequential formula translation," Comm. 
ACM 3:2 (1960), pp. 76-83. 
Chapter 6 
Intermediate-Code 
Generat 
ion 
In the analysis-synthesis model of a compiler, the front end analyzes a source 
program and creates an intermediate representation, from which the back end 
generates target code. Ideally, details of the source language are confined to the 
front end, and details of the target machine to the back end. With a suitably 
defined intermediate representation, a compiler for language i and machine j 
can then be built by combining the front end for language i with the back 
end for machine j. This approach to creating suite of compilers can save a 
considerable amount of effort: rn x n compilers can be built by writing just rn 
front ends and n back ends. 
This chapter deals with intermediate representations, static type checking, 
and intermediate code generation. For simplicity, we assume that a com- 
piler front end is organized as in Fig. 6.1, where parsing, static checking, and 
intermediate-code generation are done sequentially; sometimes they can be com- 
bined and folded into parsing. We shall use the syntax-directed formalisms of 
Chapters 2 and 5 to specify checking and translation. Many of the translation 
schemes can be implemented during either bottom-up or top-down parsing, us- 
ing the techniques of Chapter 5. All schemes can be implemented by creating 
a syntax tree and then walking the tree. 
front end -
-
+
-
b
a
c
k
 
end --- 
--, 
Figure 6.1: Logical structure of a compiler front end 
Static checking includes type checking, which ensures that operators are ap- 
plied to compatible operands. It also includes any syntactic checks that remain 
Parser 
Static 
Checker 
Intermediate 
Code 
~~~~~~t~~ 
intermediate 
code 
Code 
Generator 
358 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
after parsing. For example, static checking assures that a break-statement in 
C is enclosed within a while-, for-, or switch-statement; an error is reported if 
such an enclosing statement does not exist. 
The approach in this chapter can be used for a wide range of intermediate 
representations, including syntax trees and three-address code, both of which 
were introduced in Section 2.8. The term "three-address code7' comes from 
instructions of the general form x = y op x with three addresses: two for the 
operands y and x and one for the result x. 
In the process of translating a program in a given source language into code 
for a given target machine, a compiler may construct a sequence of intermediate 
representations, as in Fig. 6.2. High-level representations are close to the source 
language and low-level representations are close to the target machine. Syntax 
trees are high level; they depict the natural hierarchical structure of the source 
program and are well suited to tasks like static type checking. 
High Level 
Low Level 
Source 
Tar 
get 
-+ Intermediate -+ . . - 
Intermediate-+ 
Program 
Represent 
at 
ion 
Represent 
ation 
Code 
Figure 6.2: A compiler might use a sequence of intermediate representations 
A low-level representation is suitable for machine-dependent tasks like reg- 
ister allocation and instruction selection. Three-address code can range from 
high- to low-level, depending on the choice of operators. For expressions, the 
differences between syntax trees and three-address code are superficial, as we 
shall see in Section 6.2.3. For looping statements, for example, a syntax tree 
represents the components of a statement, whereas three-address code contains 
labels and jump instructions to represent the flow of control, as in machine 
language. 
The choice or design of an intermediate representation varies from compiler 
to compiler. An intermediate representation may either be an actual language 
or it may consist of internal data structures that are shared by phases of the 
compiler. C is a programming language, yet it is often used as an intermediate 
form because it is flexible, it compiles into efficient machine code, and its com- 
pilers are widely available. The original C++ compiler consisted of a front end 
that generated C, treating a C compiler as a back end. 
6.1 Variants of Syntax Trees 
Nodes in a syntax tree represent constructs in the source program; the children 
of a node represent the meaningful components of a construct. A directed 
acyclic graph (hereafter called a DAG) for an expression identifies the common 
subexpressions (subexpressions that occur more than once) of the expression. 
As we shall see in this section, DAG7s 
can be constructed by using the same 
techniques that construct syntax trees. 
6.1. VARIANTS OF SYNTAX TREES 
6.1.1 Directed Acyclic Graphs for Expressions 
Like the syntax tree for an expression, a DAG has leaves corresponding to 
atomic operands and interior codes corresponding to operators. The difference 
is that a node N in a DAG has more than one parent if N represents a com- 
mon subexpression; in a syntax tree, the tree for the common subexpression 
would be replicated as many times as the subexpression appears in the original 
expression. Thus, a DAG not only represents expressions more succinctly, it 
gives the compiler important clues regarding the generation of efficient code to 
evaluate the expressions. 
Example 6.1 : 
Figure 6.3 shows the DAG for the expression 
The leaf for a has two parents, because a appears twice in the expression. 
More interestingly, the two occurrences of the common subexpression b-c are 
represented by one node, the node labeled -. That node has two parents, 
representing its two uses in the subexpressions a*(b-c) and (b-c)*d. Even 
though b and c appear twice in the complete expression, their nodes each have 
one parent, since both uses are in the common subexpression b-c. 
Figure 6.3: Dag for the expression a + a * (b - 
c) + (b - 
c) * d 
The SDD of Fig. 6.4 can construct either syntax trees or DAG's. It was 
used to construct syntax trees in Example 5.11, where functions Leaf and Node 
created a fresh node each time they were called. It will construct a DAG if, 
before creating a new node, these functions first check whether an identical node 
already exists. If a previously created identical node exists, the existing node 
is returned. For instance, before constructing a new node, Node(op, ZeJt, right) 
we check whether there is already a node with label op, and children left and 
right, in that order. If so, Node returns the existing node; otherwise, it creates 
a new node. 
Example 6.2: The sequence of steps shown in Fig. 6.5 constructs the DAG 
in Fig. 6.3, provided Node and Leaf return an existing node, if possible, as 
360 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
5) 
T + 
id 
I T.node = 
new Leaf (id, 
id. 
entry) 
PRODUCTION 
1) E -
i
 
El + 
T 
2) 
E -
i
 
El - 
T 
3) 
E + T  
4) 
T - i ( E )  
6) 
T -+ 
num 
I T. 
node = 
new Leaf (num, 
num. 
val) 
SEMANTIC 
RULES 
E.node = 
new Node('+', El 
.node, 
T.node) 
E.node = 
new Node('-', El 
.node, 
T.node) 
E.node = 
T.node 
T.node = 
E.node 
Figure 6.4: Syntax-directed definition to produce syntax trees or DAG's 
pl = 
Leaf (id, entry-a) 
p2 = 
Leaf (id, entry-a) = 
pl 
p3 = 
Leaf (id, entry- 
b) 
p4 = 
Leaf(id, entry-c) 
P5 = 
Node('-', p3, p4) 
p6 = 
Node('*', pl p5) 
p7 = 
Node(' f 
' PI, 
~
6
)
 
pg = Leaf (id, entry-b) = 
p3 
pg = 
Leaf (id, entry-c) = 
p4 
Pl0 = 
Node('-', p3, 
p4) = 
p5 
pll = 
Leaf (id, 
entry-d) 
P12 = 
Node('*', ~ 5 1 ~ 1 1 )  
P13 = Node('+',p7,pl2) 
Figure 6.5: Steps for constructing the DAG of Fig. 6.3 
discussed above. We assume that entry-a points to the symbol-table entry for 
a, and similarly for the other identifiers. 
When the call to Leaf (id, entry-a) is repeated at step 2, the node created 
by the previous call is returned, so p2 = 
pl. Similarly, the nodes returned at 
steps 8 and 9 are the same as those returned at steps 3 and 4 (i.e., pg = p3 
and p
g
 = 
p4). Hence the node returned at step 10 must be the same at that 
returned at step 5; i.e., plo = 
pg . 
6.1.2 The Value-Number Method for Constructing DAG's 
Often, the nodes of a syntax tree or DAG are stored in an array of records, as 
suggested by Fig. 6.6. Each row of the array represents one record, and therefore 
one node. In each record, the first field is an operation code, indicating the label 
of the node. In Fig. 6.6(b), leaves have one additional field, which holds the 
lexical value (either a symbol-table pointer or a constant, in this case), and 
6.1. VARIANTS OF SYNTAX TREES 
361 
interior nodes have two additional fields indicating the left and right children. 
to entry 
for i 
(a) DAG 
(b) Array. 
Figure 6.6: Nodes of a DAG for i = 
i + 
10 allocated in an array 
In this array, we refer to nodes by giving the integer index of the record 
for that node within the array. This integer historically has been called the 
value number for the node or for the expression represented by the node. For 
instance, in Fig. 6.6, the node labeled + has value number 3, and its left and 
right children have value numbers 1 
and 2, respectively. In practice, we could 
use pointers to records or references to objects instead of integer indexes, but 
we shall still refer to the reference to a node as its "value number." If stored 
in an appropriate data structure, value numbers help us construct expression 
DAG's efficiently; the next algorithm shows how. 
Suppose that nodes are stored in an array, as in Fig. 6.6, and each node is 
referred to by its value number. Let the signature of an interior node be the 
triple (op, 
1, 
r), 
where op is the label, 1 its left child's value number, and r its 
right child's value number. A unary operator may be assumed to have r = 
0. 
Algorithm 6.3: The value-number method for constructing the nodes of a 
DAG. 
INPUT: Label op, node 1, and node r. 
OUTPUT: The value number of a node in the array with signature (op, 
1, r). 
METHOD: Search the array for a node M with label op, left child I ,  and right 
child r. If there is such a node, return the value number of M .  If not, create in 
the array a new node N with label op, left child 1, and right child r, and return 
its value number. 
While Algorithm 6.3 yields the desired output, searching the entire array 
every time we are asked to locate one node is expensive, especially if the array 
holds expressions from an entire program. A more efficient approach is to use a 
hash table, in which the nodes are put into "buckets," each of which typically 
will have only a few nodes. The hash table is one of several data structures 
that support dictionaries efficiently.' A dictionary is an abstract data type that 
'see Aho, A. V., J. E. Hopcroft, and J. D. Ullman, Data Structures and Algorithms, 
Addison-Wesley, 1983, for a discussion of data structures supporting dictionaries. 
362 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
allows us to insert and delete elements of a set, and to determine whether a 
given element is currently in the set. A good data structure for dictionaries, 
such as a hash table, performs each of these operations in time that is constant 
or close to constant, independent of the size of the set. 
To construct a hash table for the nodes of a DAG, we need a hash function 
h that computes the index of the bucket for a signature (op, 
1, r), 
in a way that 
distributes the signatures across buckets, so that it is unlikely that any one 
bucket will get much more than a fair share of the nodes. The bucket index 
h(op, 1, 
r) is computed deterministically from op, 1, and r, so that we may repeat 
the calculation and always get to the same bucket index for node (op, 1, r). 
The buckets can be implemented as linked lists, as in Fig. 6.7. An array, 
indexed by hash value, holds the bucket headers, each of which points to the 
first cell of a list. Within the linked list for a bucket, each cell holds the value 
number of one of the nodes that hash to that bucket. That is, node (op, 1,r) 
can be found on the list whose header is at index h(op, 
I ,  r )  
of the array. 
List elements 
representing nodes 
by hash value 
I 
1 
Array of bucket 
headers indexed 
Figure 6.7: Data structure for searching buckets 
Thus, given the input node op, 1, and r, we compute the bucket index 
h( 
op, 1, r) and search the list of cells in this bucket for the given input node. 
Typically, there are enough buckets so that no list has more than a few cells. 
We may need to look at all the cells within a bucket, however, and for each 
value number v found in a cell, we must check whether the signature (op, 
1, r) 
of the input node matches the node with value number u 
in the list of cells (as 
in Fig. 6.7). I
f
 we find a match, we return v. If we find no match, we know 
no such node can exist in any other bucket, so we create a new cell, add it to 
the list of cells for bucket index h( 
op, 1, 
r) 
, 
and return the value number in that 
new cell. 
2 
5 
6.1.3 Exercises for Section 6.1 
3 
Exercise 6.1.1 : 
Construct the DAG for the expression 
((x + 
y) - 
((x +y) * (x -Y))) + (("+Y) * (" -y)) 
6.2. THREE-ADDRESS CODE 
363 
Exercise 6.1.2: Construct the DAG and identify the value numbers for the 
subexpressions of the following expressions, assuming + 
associates from the left. 
a) a +  b+ ( a +  b). 
6.2 
Three-Address Code 
In three-address code, there is at most one operator on the right side of an 
instruction; that is, no built-up arithmetic expressions are permitted. Thus a 
source-language expression like x+y*z might be translated into the sequence of 
t 
hree-address instructions 
where tl and tz are compiler-generated temporary names. This unraveling of 
multi-operator arithmetic expressions and of nested flow-of-control statements 
makes three-address code desirable for target-code generation and optimization, 
as discussed in Chapters 8 and 9. The use of names for the intermediate values 
computed by a program allows three-address code to be rearranged easily. 
Example 6.4 : Three-address code is a linearized representation of a syntax 
tree or a DAG in which explicit names correspond to the interior nodes of the 
graph. The DAG in Fig. 6.3 is repeated in Fig. 6.8, together with a correspond- 
ing three-address code sequence. 
(a) DAG 
(b) Three-address code 
Figure 6.8: A DAG and its corresponding three-address code 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
6.2.1 
Addresses and Instructions 
Three-address code is built from two concepts: addresses and instructions. In 
object-oriented terms, these concepts correspond to classes, and the various 
kinds of addresses and instructions correspond to appropriate subclasses. Al- 
ternatively, three-address code can be implemented using records with fields 
for the addresses; records called quadruples and triples are discussed briefly in 
Section 6.2.2. 
An address can be one of the following: 
A name. For convenience, we allow source-program names to appear as 
addresses in three-address code. In an implementation, a source name 
is replaced by a pointer to its symbol-table entry, where all information 
about the name is kept. 
A constant. In practice, a compiler must deal with many different types 
of constants and variables. Type conversions within expressions are con- 
sidered in Section 6.5.2. 
A compiler-generated temporary. It is useful, especially in optimizing com- 
pilers, to create a distinct name each time a temporary is needed. These 
temporaries can be combined, if possible, when registers are allocated to 
variables. 
We now consider the common three-address instructions used in the rest of 
this book. Symbolic labels will be used by instructions that alter the flow of 
control. A symbolic label represents the index of a three-address instruction in 
the sequence of instructions. Actual indexes can be substituted for the labels, 
either by making a separate pass or by "backpatching," discussed in Section 6.7. 
Here is a list of the common three-address instruction forms: 
1. Assignment instructions of the form x = y op z, where op is a binary 
arithmetic or logical operation, and x, y, and z are addresses. 
2. Assignments of the form x = op y, where op is a unary operation. Essen- 
tial unary operations include unary minus, logical negation, shift opera- 
tors, and conversion operators that, for example, convert an integer to a 
floating-point number. 
3. Copy instructions of the form x = y, where x is assigned the value of y. 
4. An unconditional jump goto L. The three-address instruction with label 
L is the next to be executed. 
5. Conditional jumps of the form i f  x goto L and ifFalse 
x goto L. These 
instructions execute the instruction with label L next if x is true and 
false, respectively. Otherwise, the following t 
hree-address instruction in 
sequence is executed next, as usual. 
6.2. THREE-ADDRESS CODE 
365 
6. Conditional jumps such as i f  x relop y goto L, which apply a relational 
operator (<, ==, >=, etc.) to x and y, and execute the instruction with 
label L next if x stands in relation relop to y. If not, the three-address 
instruction following i f  x relop y goto L is executed next, in sequence. 
7. Procedure calls and returns are implemented using the following instruc- 
tions: param x for parameters; c a l l p ,  n and y = c a l l p ,  n for procedure 
and function calls, respectively; and return y, where y, representing a 
returned value, is optional. Their typical use is as the sequence of three- 
address instructions 
param x, 
c a l l  p, n 
generated as part of a call of the procedure p(xl,x2,. 
. 
. ,x,). 
The in- 
teger n, indicating the number of actual parameters in "call p, n," is 
not redundant because calls can be nested. That is, some of the first 
param statements could be parameters of a call that comes after p returns 
its value; that value becomes another parameter of the later call. The 
implementation of procedure calls is outlined in Section 6.9. 
8. Indexed copy instructions of the form x = y Cil and x Cil = y. The instruc- 
tion x = 
y Cil sets x to the value in the location i memory units beyond 
location y 
. The instruction x Cil = y sets the contents of the location i 
units beyond x to the value of y. 
9. Address and pointer assignments of the form x = & y, x = * y, and * x = y. 
The instruction x = & y sets the r-value of x to be the location (I-value) 
of y.2 Presumably y is a name, perhaps a temporary, that denotes an 
expression with an bvalue such as A [il [jl 
, and x is a pointer name or 
temporary. In the instruction x = * y, presumably y is a pointer or a 
temporary whose r-value is a location. The r-value of x is made equal 
to the contents of that location. Finally, * x = y sets the r-value of the 
object pointed to by x to the r-value of y. 
Example 6.5 : 
Consider the statement 
do i = i+l; 
while (aci] < v) ; 
Two possible translations of this statement are shown in Fig. 6.9. The transla- 
tion in Fig. 6.9 uses a symbolic label L, attached to the first instruction. The 
2 ~ r o m  
Section 2.8.3, 1- and r-values are appropriate on the left and right sides of assign- 
ments, respectively. 
366 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
translation in (b) shows position numbers for the instructions, starting arbitrar- 
ily at position 100. In both translations, the last instruction is a conditional 
jump to the first instruction. The multiplication i 
* 8 is appropriate for an 
array of elements that each take 8 units of space. 
(a) Symbolic labels. 
(b) Position numbers. 
Figure 6.9: Two ways of assigning labels to three-address statements 
The choice of allowable operators is an important issue in the design of an 
intermediate form. The operator set clearly must be rich enough to implement 
the operations in the source language. Operators that are close to machine 
instructions make it easier to implement the intermediate form on a target 
machine. However, if the front end must generate long sequences of instructions 
for some source-language operations, then the optimizer and code generator 
may have to work harder to rediscover the structure and generate good code 
for these operations. 
6.2.2 Quadruples 
The description of three-address instructions specifies the components of each 
type of instruction, but it does not specify the representation of these instruc- 
tions in a data structure. In a compiler, these instructions can be implemented 
as objects or as records with fields for the operator and the operands. Three 
such representations are called "quadruples," LLtriples," 
and "indirect triples." 
A quadruple (or just "quad') has four fields, which we call op, arg,, arg2, 
and result. The op field contains an internal code for the operator. For instance, 
the three-address instruction x = y + x  
is represented by placing + in op, y in 
arg,, 2 in argz, and x in result. The following are some exceptions to this rule: 
I. Instructions with unary operators like x = minusy or x = y do not use 
arg,. Note that for a copy statement like x = y, op is =, while for most 
other operations, the assignment operator is implied. 
2. Operators like param use neither arg2 nor result. 
3. Conditional and unconditional jumps put the target label in result. 
Example 6.6 : Three-address code for the assignment a = b * - 
c + 
b 
* - 
c ; 
appears in Fig. 6.10(a). The special operator minus is used to distinguish the 
6.2. THREE-ADDRESS CODE 
367 
unary minus operator, as in - 
c, from the binary minus operator, as in b - 
c. 
Note that the unary-minus "three-address" statement has only two addresses, 
as does the copy statement a = ts. 
The quadruples in Fig. 6.10(b) implement the three-address code in (a). 
tl = minus c 
t2 = b * tl 
t3 = minus c 
tq = b * t3 
t5 = t2 + t4 
a = t5 
arg, argz result 
* 
I 
1 
I t2 
minus 
l c 
I 
1 t3 I 
(a) Three-address code 
(b) Quadruples 
Figure 6.10: Three-address code and its quadruple representation 
For readability, we use actual identifiers like a, b, and c in the fields arg,, 
arg, , 
and result in Fig. 6.10(b), instead of pointers to their symbol-table entries. 
Temporary names can either by entered into the symbol table like programmer- 
defined names, or they can be implemented as objects of a class Temp with its 
own methods. 
6.2.3 Triples 
A triple has only three fields, which we call op, arg,, and arg2. Note that 
the result field in Fig. 6.10(b) is used primarily for temporary names. Using 
triples, we refer to the result of an operation x op y by its position, rather 
than by an explicit temporary name. Thus, instead of the temporary tl 
in 
Fig. 6.10 
(b) 
, a triple representation would refer to position (0). Parenthesized 
numbers represent pointers into the triple structure itself. In Section 6.1.2, 
positions or pointers to positions were called value numbers. 
Triples are equivalent to signatures in Algorithm 6.3. Hence, the DAG and 
triple representations of expressions are equivalent. The equivalence ends with 
expressions, since syntax-tree variants and three-address code represent control 
flow quite differently. 
Example 6.7 : The syntax tree and triples in Fig. 6.11 correspond to the 
three-address code and quadruples in Fig. 6.10. In the triple representation in 
Fig. 6.11(b), the copy statement a = ts is encoded in the triple representation 
by placing a in the arg, field and (4) in the arg, field. 
A ternary operation like x Cil = y requires two entries in the triple structure; 
for example, we can put x and i in one triple and y in the next. Similarly, 
x = y C
i
l
 can implemented by treating it as if it were the two instructions 
368 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
Why Do We Need Copy Instructions? 
A simple algorithm for translating expressions generates copy instructions 
for assignments, as in Fig. 6.10(a), where we copy ts into a rather than 
assigning tz 
+ 
t4 
to a directly. Each subexpression typically gets its own, 
new temporary to hold its result, and only when the assignment operator = 
is processed do we learn where to put the value of the complete expression. 
A code-optimization pass, perhaps using the DAG of Section 6.1.1 as an 
intermediate form, can discover that tg can be replaced by a. 
/ 
1 ' 
\. 
b 
minus b 
minus 
C 
C 
(a) Syntax tree 
I 
. 
. 
. 
I 
(b) Triples 
Figure 6.11: Representations of a + 
a * (b - 
c) + 
(b - 
c) * d 
t = y[il and x = t, where t is a compiler-generated temporary. Note that the 
temporary t does not actually appear in a triple, since temporary values are 
referred to by their position in the triple structure. 
A benefit of quadruples over triples can be seen in an optimizing compiler, 
where instructions are often moved around. With quadruples, if we move an 
instruction that computes a temporary t, then the instructions that use t require 
no change. With triples, the result of an operation is referred to by its position, 
so moving an instruction may require us to change all references to that result. 
This problem does not occur with indirect triples, which we consider next. 
Indirect triples consist of a listing of pointers to triples, rather than a listing 
of triples themselves. For example, let us use an array instruction to list pointers 
to triples in the desired order. Then, the triples in Fig. 6.11(b) might be 
represented as in Fig. 6.12. 
With indirect triples, an optimizing compiler can move an instruction by 
reordering the instruction list, without affecting the triples themselves. When 
implemented in Java, an array of instruction objects is analogous to an indi- 
rect triple representation, since Java treats the array elements as references to 
objects. 
6.2. THREE-ADDRESS CODE 
instruction 
35 
0 
36 
1 
3 
7 
2 
38 
3 
39 
4 
40 
5 
op 
arg1 
arg2 
minus 
1 
c 
I 
Figure 6.12: Indirect triples representation of three-address code 
6.2.4 
Static Single- 
Assignment Form 
Statjc single-assignment form (SSA) is an intermediate representation that fa- 
cilitates certain code optimizations. Two distinctive aspects distinguish SSA 
from three-address code. The first is that all assignments in SSA are to vari- 
ables with distinct names; hence the term static single-assigrnent. Figure 6.13 
shows the same intermediate program in three-address code and in static single- 
assignment form. Note that subscripts distinguish each definition of variables 
p and q in the SSA representation. 
(a) Three-address code. 
(b) Static single-assignment form. 
Figure 6.13: Intermediate program in three-address code and SSA 
The same variable may be defined in two different control-flow paths in a 
program. For example, the source program 
i f  ( f l a g  ) x = -1; else x = 1 ;  
y = x * a ;  
has two control-flow paths in which the variable x gets defined. If we use 
different names for x in the true part and the false part of the conditional 
statement, then which name should we use in the assignment y = x * a? Here 
is where the second distinctive aspect of SSA comes into play. SSA uses a 
notational convention called the 4-function to combine the two definitions of x: 
i f  ( f l a g  ) xl = -1; else xa = 1; 
x3 = 4(x1,x2); 
370 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
Here, $(xl, x2) has the value xl if the control flow passes through the true 
part of the conditional and the value x2 if the control flow passes through the 
false part. That is to say, the $-function returns the value of its argument that 
corresponds to the control-flow path that was taken to get to the assignment- 
statement containing the $-function. 
6.2.5 Exercises for Section 6.2 
Exercise 6.2.1 : 
Translate the arithmetic expression a + 
-(b + 
c) into: 
a) A syntax tree. 
b) Quadruples. 
c) Triples. 
d) Indirect triples. 
Exercise 6.2.2 : Repeat Exercise 6.2.1 for the following assignment state- 
ments: 
i
i
.
 a[i] = b*c - b*d. 
iii. x = f (y+l) + 2. 
iu. x = *p + &y. 
! Exercise 6.2.3: Show how to transform a three-address code sequence into 
one in which each defined variable gets a unique variable name. 
6.3 Types and Declarations 
The applications of types can be grouped under checking and translation: 
Type checking uses logical rules to reason about the behavior of a program 
at run time. Specifically, it ensures that the types of the operands match 
the type expected by an operator. For example, the && operator in Java 
expects its two operands to be booleans; the result is also of type boolean. 
Translation Applications. From the type of a name, a compiler can de- 
termine the storage that will be needed for that name at run time. Type 
information is also needed to calculate the address denoted by an array 
reference, to insert explicit type conversions, and to choose the right ver- 
sion of an arithmetic operator, among other things. 
6.3. TYPES AND DECLARATIONS 
371 
In this section, we examine types and storage layout for names declared 
within a procedure or a class. The actual storage for a procedure call or an 
object is allocated at run time, when the procedure is called or the object is 
created. As we examine local declarations at compile time, we can, however, 
lay out relative addresses, where the relative address of a name or a component 
of a data structure is an offset from the start of a data area. 
6.3.1 Type Expressions 
Types have structure, which we shall represent using type expressions: a type 
expression is either a basic type or is formed by applying an operator called a 
type constructor to a type expression. The sets of basic types and constructors 
depend on the language to be checked. 
Example 6.8 : The array type i n t  
[21 C
3
1
 can be read as "array of 2 arrays 
of 3 integers each" and written as a type expression array(2, array(3, integer)). 
This type is represented by the tree in Fig. 6.14. The operator array takes two 
parameters, a number and a type. 
array 
2 / \ 
array 
/ \ 
3 
integer 
Figure 6.14: Type expression for i n t  
[2] [3] 
We shall use the following definition of type expressions: 
A basic type is a type expression. Typical basic types for a language 
include boolean, char, integer, float, and void; the latter denotes "the 
absence of a value." 
A type name is a type expression. 
A type expression can be formed by applying the array type constructor 
to a number and a type expression. 
A record is a data structure with named fields. A type expression can 
be formed by applying the record type constructor to the field names and 
their types. Record types will be implemented in Section 6.3.6 by applying 
the constructor record to a symbol table containing entries for the fields. 
A type expression can be formed by using the type constructor 3 
for func- 
tion types. We write s 3 
t for "function from type s to type t." Function 
types will be useful when type checking is discussed in Section 6.5. 
372 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
Type Names and Recursive Types 
Once a class is defined, its name can be used as a type name in C++ or 
Java; for example, consider Node in the program fragment 
public class Node ( 
. . 
. ) 
. . 
. 
public Node n; 
Names can be used to define recursive types, which are needed for 
data structures such as linked lists. The pseudocode for a list element 
class Cell ( int info; Cell next; 
1 
defines the recursive type Cell as a class that contains a field info and 
a field next of type Cell. Similar recursive types can be defined in C 
using records and pointers. The techniques in this chapter carry over to 
recursive types. 
If s and t are type expressions, then their Cartesian product s x t is a 
type expression. Products are introduced for completeness; they can be 
used to represent a list or tuple of types (e.g., for function parameters). 
We assume that x associates to the left and that it has higher precedence 
than -+. 
Type expressions may contain variables whose values are type expressions. 
Compiler-generated type variables will be used in Section 6.5.4. 
A convenient way to represent a type expression is to use a graph. The 
value-number method of Section 6.1.2, can be adapted to construct a dag for a 
type expression, with interior nodes for type constructors and leaves for basic 
types, type names, and type variables; for example, see the tree in Fig. 6.14.3 
6.3.2 Type Equivalence 
When are two type expressions equivalent? Many type-checking rules have the 
form, "if 
two type expressions are equal then return a certain type else error." 
Potential ambiguities arise when names are given to type expressions and the 
names are then used in subsequent type expressions. The key issue is whether 
a name in a type expression stands for itself or whether it is an abbreviation 
for another type expression. 
3Since type names denote type expressions, they can set up implicit cycles; see the box 
on "Type Names and Recursive Types." If edges to type names are redirected to the type 
expressions denoted by the names, then the resulting graph can have cycles due to recursive 
types. 
6.3. TYPES AND DECLARATIONS 
373 
When type expressions are represented by graphs, two types are structurally 
equivalent if and only if one of the following conditions is true: 
They are the same basic type. 
They are formed by applying the same constructor to structurally equiv- 
alent types. 
One is a type name that denotes the other 
If type names are treated as standing for themselves, then the first two condi- 
tions in the above definition lead to name equivalence of type expressions. 
Name-equivalent expressions are assigned the same value number, if we use 
Algorithm 6.3. Structural equivalence can be tested using the unification algo- 
rithm in Section 6.5.5. 
6.3.3 
Declarations 
We shall study types and declarations using a simplified grammar that declares 
just one name at a time; declarations with lists of names can be handled as 
discussed in Example 5.10. The grammar is 
D + T i d ; D  I c 
T -+ B C 
1 record '(I 
D '3' 
B + int 
( float 
C 3 E: 
( CnumIC 
The fragment of the above grammar that deals with basic and array types 
was used to illustrate inherited attributes in Section 5.3.2. The difference in 
this section is that we consider storage layout as well as types. 
Nonterminal D generates a sequence of declarations. Nonterminal T gen- 
erates basic, array, or record types. Nonterminal B generates one of the basic 
types int and float. Nonterminal C, for "component," generates strings of 
zero or more integers, each integer surrounded by brackets. An array type con- 
sists of a basic type specified by B, followed by array components specified by 
nonterminal C. A record type (the second production for T) is a sequence of 
declarations for the fields of the record, all surrounded by curly braces. 
6.3.4 Storage Layout for Local Names 
From the type of a name, we can determine the amount of storage that will be 
needed for the name at run time. At compile time, we can use these amounts to 
assign each name a relative address. The type and relative address are saved in 
the symbol-table entry for the name. Data of varying length, such as strings, or 
data whose size cannot be determined until run time, such as dynamic arrays, 
is handled by reserving a known fixed amount of storage for a pointer to the 
data. Run-time storage management is discussed in Chapter 7. 
374 
CHAPTER 6. INTERMEDIATE- 
CODE GENERATION 
Address Alignment 
The storage layout for data objects is strongly influenced by the address- 
ing constraints of the target machine. For example, instructions to add 
integers may expect integers to be aligned, that is, placed at certain posi- 
tions in memory such as an address divisible by 4. Although an array of 
ten characters needs only enough bytes to hold ten characters, a compiler 
may therefore allocate 12 bytes - 
the next multiple of 4 - 
leaving 2 bytes 
unused. Space left unused due to alignment considerations is referred to as 
padding. When space is at a premium, a compiler may pack data so that 
no padding is left; additional instructions may then need to be executed 
at run time to position packed data so that it can be operated on as if it 
were properly aligned. 
Suppose that storage comes in blocks of contiguous bytes, where a byte is 
the smallest unit of addressable memory. Typically, a byte is eight bits, and 
some number of bytes form a machine word. Multibyte objects are stored in 
consecutive bytes and given the address of the first byte. 
The width of a type is the number of storage units needed for objects of that 
type. A basic type, such as a character, integer, or float, requires an integral 
number of bytes. For easy access, storage for aggregates such as arrays and 
classes is allocated in one contiguous block of bytes.4 
The translation scheme (SDT) in Fig. 6.15 computes types and their widths 
for basic and array types; record types will be discussed in Section 6.3.6. The 
SDT uses synthesized attributes type and width for each nonterminal and two 
variables t and w to pass type and width information from a B node in a parse 
tree to the node for the production C -+ 
6. In a syntax-directed definition, t 
and w would be inherited attributes for C. 
The body of the T-production consists of nonterminal B, an action, and 
nonterminal C, which appears on the next line. The action between B and C 
sets t to B.type and w to B. 
width. If B -+ 
int then B. 
type is set to integer and 
B. 
width is set to 4, the width of an integer. Similarly, if B -+ 
float then B. 
type 
is float and B. 
width is 8, the width of a float. 
The productions for C determine whether T generates a basic type or an 
array type. If C -+ 
e, then t becomes C.type and w becomes C.width. 
Otherwise, C specifies an array component. The action for C -+ [ 
num 1 Cl 
forms C.type by applying the type constructor array to the operands num.value 
and Cl .type. For instance, the result of applying array might be a tree structure 
such as Fig. 6.14. 
- - 
4~torage 
allocation for pointers in C and C++ is simpler if all pointers have the same 
width. The reason is that the storage for a pointer may need to be allocated before we learn 
the type of the objects it can point to. 
6.3. TYPES AND DECLARATIONS 
B -+ int 
{ B. 
type = integer; B. 
width = 
4; ) 
B -+ float 
{ B. 
type = 
float; B. 
width = 
8; ) 
C -+ 
[ num 1 C1 
{ array(num.value, Cl .type); 
C. 
width = 
num.value x Cl .width; } 
Figure 6.15: Computing types and their widths 
The width of an array is obtained by multiplying the width of an element by 
the number of elements in the array. If addresses of consecutive integers differ by 
4, then address calculations for an array of integers will include multiplications 
by 4. Such multiplications provide opportunities for optimization, so it is helpful 
for the front end to make them explicit. In this chapter, we ignore other machine 
dependencies such as the alignment of data objects on word boundaries. 
E x a m p l e  6.9 : 
The parse tree for the type i n t  
[21 C
3
1
 is shown by dotted lines 
in Fig. 6.16. The solid lines show how the type and width are passed from B, 
down the chain of C's through variables t and w, and then back up the chain 
as synthesized attributes type and width. The variables t and w are assigned 
the values of B.type and B. 
width, respectively, before the subtree with the C 
nodes is examined. The values of t and w are used at the node for C + 
e to 
start the evaluation of the synthesized attributes up the chain of C nodes. 
type = array(2, array(3, integer)) 
.
-
 
. 
width = 24 
-
=
 
integer' 
\ 
type = array(2, array(3, integer)) 
N ' t y p e  = integer 
= 4 
' 
width = 24 
: width = 4 
int 
[ 2 1'. 
type = array(3, integer) 
width = 12 
[ 3 I" 
type = integer 
width = 4 
€ 
Figure 6.16: Syntax-directed translation of array types 
376 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
6.3.5 Sequences of Declarations 
Languages such as C and Java allow all the declarations in a single procedure 
to be processed as a group. The declarations may be distributed within a Java 
procedure, but they can still be processed when the procedure is analyzed. 
Therefore, we can use a variable, say ofset, to keep track of the next available 
relative address. 
The translation scheme of Fig. 6.17 deals with a sequence of declarations 
of the form T id, where T generates a type as in Fig. 6.15. Before the first 
declaration is considered, o8set is set to 0. As each new name x is seen, x is 
entered into the symbol table with its relative address set to the current value 
of oflset, which is then incremented by the width of the type of x. 
P -+ 
{ oflset = 0; ) 
D 
D -+ T id ; { top.put(id.lexerne, T.type, oflset); 
ofset = oflset + 
T. 
width; 1 
Dl 
D + €  
Figure 6.17: Computing the relative addresses of declared names 
The semantic action within the production D -+ T id ; 
Dl creates a symbol- 
table entry by executing top.put(id. 
lexeme, T. 
type, ofset). Here top denotes 
the current symbol table. The method top.put creates a symbol-table entry for 
id.lexerne, with type T.type and relative address ogset in its data area. 
The initialization of ofset in Fig. 6.17 is more evident if the first production 
appears on one line as: 
Nonterminals generating E ,  called marker nonterminals, can be used to rewrite 
productions so that all actions appear at the ends of right sides; see Sec- 
tion 5.5.4. Using a marker nonterminal M ,  (6.1) can be restated as: 
6.3.6 Fields in Records and Classes 
The translation of declarations in Fig. 6.17 carries over to fields in records and 
classes. Record types can be added to the grammar in Fig. 6.15 by adding the 
following production 
T -+ record '(I 
D '>I 
6.3. TYPES AND DECLARATIONS 
377 
The fields in this record type are specified by the sequence of declarations 
generated by D. The approach of Fig. 6.17 can be used to determine the types 
and relative addresses of fields, provided we are careful about two things: 
The field names within a record must be distinct; that is, a name may 
appear at most once in the declarations generated by D. 
The offset or relative address for a field name is relative to the data area 
for that record. 
Example 6.10: The use of a name x for a field within a record does not 
conflict with other uses of the name outside the record. Thus, the three uses of 
x  in the following declarations are distinct and do not conflict with each other: 
f l o a t  x; 
record ( f l o a t  x; f l o a t  y; ) p; 
record ( 
i n t  t a g ;  f l o a t  x; f l o a t  y; ) q; 
A subsequent assignment x = p . 
x 
+ q. 
x ; sets variable x to the sum of the fields 
named x in the records p  and q. Note that the relative address of x in p differs 
from the relative address of x  in q. 
For convenience, record types will encode both the types and relative ad- 
dresses of their fields, using a symbol table for the record type. A record type 
has the form record(t), where record is the type constructor, and t is a symbol- 
table object that holds information about the fields of this record type. 
The translation scheme in Fig. 6.18 consists of a single production to be 
added to the productions for T in Fig. 6.15. This production has two semantic 
actions. The embedded action before D saves the existing symbol table, denoted 
by top and sets top to a fresh symbol table. It also saves the current ofset, and 
sets oflset to 0. The declarations generated by D will result in types and relative 
addresses being put in the fresh symbol table. The action after D creates a 
record type using top, before restoring the saved symbol table and offset. 
T + record 
'
C
'
 
{ Env.push(top); top = 
new Env(); 
Stack.push(ofset); oflset = 0; } 
D '3' 
{ T.type = record(top); T.width = oaset; 
top = 
Env.pop(); ofset = Stack.pop(); ) 
Figure 6.18: Handling of field names in records 
For concreteness, the actions in Fig. 6.18 give pseudocode for a specific im- 
plementation. Let class Env implement symbol tables. The call Env.push(top) 
pushes the current symbol table denoted by top onto a stack. Variable top is 
then set to a new symbol table. Similarly, o$set is pushed onto a stack called 
Stack. Variable ofset is then set to 0. 
378 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
After the declarations in D have been translated, the symbol table top holds 
the types and relative addresses of the fields in this record. Further, ogset gives 
the storage needed for all the fields. The second action sets T. 
type to record(top) 
and T. 
width to offset. Variables top and ogset are then restored to their pushed 
values to complete the translation of this record type. 
This discussion of storage for record types carries over to classes, since no 
storage is reserved for methods. See Exercise 6.3.2. 
6.3.7 Exercises for Section 6.3 
Exercise 6.3.1 : 
Determine the types and relative addresses for the identifiers 
in the following sequence of declarations: 
f l o a t  x; 
record ( f l o a t  x; 
f l o a t  y; ) p; 
record ( i n t  t a g ;  f l o a t  x; f l o a t  y ;  ) q; 
! Exercise 6.3.2 : 
Extend the handling of field names in Fig. 6.18 to classes and 
single-inheritance class hierarchies. 
a) Give an implementation of class Enu that allows linked symbol tables, so 
that a subclass can either redefine a field name or refer directly to a field 
name in a superclass. 
b) Give a translation scheme that allocates a contiguous data area for the 
fields in a class, including inherited fields. Inherited fields must maintain 
the relative addresses they were assigned in the layout for the superclass. 
6.4 
Translation of Expressions 
The rest of this chapter explores issues that arise during the translation of ex- 
pressions and statements. We begin in this section with the translation of ex- 
pressions into three-address code. An expression with more than one operator, 
like a + 
b * c, will translate into instructions with at most one operator per in- 
struction. An array reference A[i] 
[j] 
will expand into a sequence of three-address 
instructions that calculate an address for the reference. We shall consider type 
checking of expressions in Section 6.5 and the use of boolean expressions to 
direct the flow of control through a program in Section 6.6. 
6.4.1 Operations Within Expressions 
The syntax-directed definition in Fig. 6.19 builds up the three-address code for 
an assignment statement S using attribute code for S and attributes addr and 
code for an expression E. Attributes S.code and E.code denote the three-address 
code for S and E, respectively. Attribute E.addr denotes the address that will 
6.4. TRANSLATION OF EXPRESSIONS 
379 
PRODUCTION 
S + i d = E ;  
E.addr = new Temp () 
E.code = El .code 1 )  
gen(E. 
addr '=' 'minus' El. 
addr) 
SEMANTIC RULES 
S.code=E.codeII 
gen(top.get(id. 
lexeme) '=' E. 
addr) 
E -+ El + E2 
E. 
addr = 
new Temp () 
E.code = El.code 1 1  E2.code 1 1  
gen(E. 
addr '=I 
El. 
addr '+I 
E2. 
addr) 
Figure 6.19: Three-address code for expressions 
1 
( El 
hold the value of E. Recall from Section 6.2.1 that an address can be a name, 
a constant, or a compiler-generated temporary. 
Consider the last production, E -+ id, in the syntax-directed definition in 
Fig. 6.19. When an expression is a single identifier, say x, then x itself holds the 
value of the expression. The semantic rules for this production define E.addr 
to point to the symbol-table entry for this instance of id. Let top denote the 
current symbol table. Function top.get retrieves the entry when it is applied to 
the string representation id.lexeme of this instance of id. E. 
code is set to the 
empty string. 
When E + ( El ) , the translation of E is the same as that of the subex- 
pression El. Hence, E .  
addr equals El. 
addr, and E. 
code equals El. 
code. 
The operators + and unary - 
in Fig. 6.19 are representative of the operators 
in a typical language. The semantic rules for E + 
El + E2, generate code to 
compute the value of E from the values of El and E2. Values are computed 
into newly generated temporary names. If El is computed into El.addr and 
Ez into Ez. 
addr, then El + E2 
translates into t = 
El. 
addr + 
E2. 
addr, where t is 
a new temporary name. E.addr is set to t. A sequence of distinct temporary 
names tl , 
t z  
, 
. 
. . is created by successively executing new Temp(). 
For convenience, we use the notation gen(x ' = I  y '+' z )  to represent the 
three-address instruction x = 
y + 
z. Expressions appearing in place of variables 
like x, y, and z are evaluated when passed to gen, and quoted strings like ' = I  
are taken literally.5 Other three-address instructions will be built up similarly 
E.addr = El. 
addr 
E. 
code = El. 
code 
5 ~ n  
syntax-directed definitions, gen builds an instruction and returns it. In translation 
schemes, gen builds an instruction and incrementally emits it by putting it into the stream 
380 
CHAPTER 6. INTERMEDIATE- 
CODE GENERATION 
by applying gen to a combination of expressions and strings. 
When we translate the production E -+ El + E2, 
the semantic rules in 
Fig. 6.19 build up E. 
code by concatenating El. 
code, E2. 
code, and an instruc- 
tion that adds the values of El and E2. The instruction puts the result of the 
addition into a new temporary name for E, denoted by E.addr. 
The translation of E -+ -El is similar. The rules create a new temporary 
for E and generate an instruction to perform the unary minus operation. 
Finally, the production S -+ id 
= 
E ;  generates instructions that assign the 
value of expression E to the identifier id. The semantic rule for this production 
uses function top.get to determine the address of the identifier represented by 
id, as in the rules for E -+ id. S.code consists of the instructions to compute 
the value of E into an address given by E.addr, followed by an assignment to 
the address top.get(id.lexeme) for this instance of id. 
Example 6.11 : The syntax-directed definition in Fig. 6.19 translates the as- 
signment statement a = b + - 
c ; into the three-address code sequence 
tl = minus c 
t 2  = b + tl 
a = t 2  
6.4.2 Incremental Translation 
Code attributes can be long strings, so they are usually generated incremen- 
tally, as discussed in Section 5.5.2. Thus, instead of building up E.code as in 
Fig. 6.19, we can arrange to generate only the new three-address instructions, 
as in the translation scheme of Fig. 6.20. In the incremental approach, gen not 
only constructs a three-address instruction, it appends the instruction to the 
sequence of instructions generated so far. The sequence may either be retained 
in memory for further processing, or it may be output incrementally. 
The translation scheme in Fig. 6.20 generates the same code as the syntax- 
directed definition in Fig. 6.19. With the incremental approach, the code at- 
tribute is not used, since there is a single sequence of instructions that is created 
by successive calls to gen. For example, the semantic rule for E + 
El + 
E
2
 in 
Fig. 6.20 simply calls gen to generate an add instruction; the instructions to 
compute El into El. 
addr and E2 
into E2. 
addr have already been generated. 
The approach of Fig. 6.20 can also be used to build a syntax tree. The new 
semantic action for E -+ El + E2 
creates a node by using a constructor, as in 
E -+ El + E2 { E.addr = new Node('+', El 
.addr, E2.addr); ) 
Here, attribute addr represents the address of a node rather than a variable or 
const 
ant. 
of generated instructions. 
6.4. TRANSLATION OF EXPRESSIONS 
E -t El+E2 {E.addr=new Temp(); 
gen(E.addr '=I El 
.addr '+I 
E2. 
addr); } 
( 
-El 
{ E. 
addr = new Temp 0; 
gen(E. 
addr '=I 
'minus' El. 
addr) 
; ] 
Figure 6.20: Generating three-address code for expressions incrementally 
6.4.3 Addressing Array Elements 
Array elements can be accessed quickly if they are stored in a block of consecu- 
tive locations. In C and Java, array elements are numbered O , 1 ,  . . . , 
n - 
1, 
for 
an array with n elements. If the width of each array element is w, then the ith 
element of array A begins in location 
base + 
i x w 
(6.2) 
where base is the relative address of the storage allocated for the array. That 
is, base is the relative address of A[O]. 
The formula (6.2) generalizes to two or more dimensions. In two dimensions, 
we write A[iz][i2] 
in C and Java for element i2 in row il. Let wl be the width 
of a row and let w
2
 be the width of an element in a row. The relative address 
of A[il] 
[iz] 
can then be calculated by the formula 
base + 
il x wl + 
i2 x w
a
 
(6.3) 
In I% dimensions, the formula is 
base + 
il x wl + 
i2 x w
2
 + 
- + 
ik x w
k
 
(6.4) 
where wj, for 1 5 j _< k ,  
is the generalization of wl and wz in (6.3). 
Alternatively, the relative address of an array reference can be calculated 
in terms of the numbers of elements nj along dimension $ of the array and the 
width w = w
k
 of a single element of the array. In two dimensions (i.e., k = 2 
and w = 
w2), the location for A[il] 
[i2] 
is given by 
base + (il x n2 + 
iq) 
x w 
(6.5) 
In k dimensions, the following formula calculates the same address as (6.4) 
: 
382 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
More generally, array elements need not be numbered starting at 0. In a 
one-dimensional array, the array elements are numbered low, low + 1, 
. . 
. , 
high 
and base is the relative address of A[low]. Formula (6.2) for the address of A[i] 
is replaced by: 
base + (i - 
low) x w 
(6.7) 
The expressions (6.2) and (6.7) can be both be rewritten as i x w + 
c, where 
the subexpression c = base - 
low x w can be precalculated at compile time. 
Note that c = base when low is 0. We assume that c is saved in the symbol 
table entry for A, so the relative address of A[i] is obtained by simply adding 
i x w to c. 
Compile-time precalculation can also be applied to address calculations for 
elements of multidimensional arrays; see Exercise 6.4.5. However, there is one 
situation where we cannot use compile-time precalculation: when the array's 
size is dynamic. If we do not know the values of low and high (or their gen- 
eralizations in many dimensions) at compile time, then we cannot compute 
constants such as c. Then, formulas like (6.7) must be evaluated as they are 
written, when the program executes. 
The above address calculations are based on row-major layout for arrays, 
which is used in C and Java. A two-dimensional array is normally stored in 
one of two forms, either row-major (row-by-row) or column-major (column-by- 
column). Figure 6.21 shows the layout of a 2 x 3 array A in (a) row-major form 
and (b) column-major form. Column-major form is used in the Fortran family 
of languages. 
4
1
1
 
1
1
 
First ?El 
row 
4
1
1
 
21 
secoi 
row E
l
 
C 
1 
First y h m n  
(a) Row Major 
(b) Column Major 
4
1
1
 
3
1
 
Figure 6.21: Layouts for a two-dimensional array. 
Third 4 
column 
We can generalize row- or column-major form to many dimensions. The 
generalization of row-major form is to store the elements in such a way that, 
as we scan down a block of storage, the rightmost subscripts appear to vary 
fastest, like the numbers on an odometer. Column-major form generalizes to 
the opposite arrangement, with the leftmost subscripts varying fastest. 
4
2
1
 
3
1
 
6.4. TRANSLATION OF EXPRESSIONS 
6.4.4 
Translation of Array References 
The chief problem in generating code for array references is to relate the address- 
calculation formulas in Section 6.4.3 to a grammar for array references. Let 
nonterminal L generate an array name followed by a sequence of index expres- 
sions: 
As in C and Java, assume that the lowest-numbered array element is 0. 
Let us calculate addresses based on widths, using the formula (6.4), rather 
than on numbers of elements, as in (6.6). The translation scheme in Fig. 6.22 
generates three-address code for expressions with array references. It consists of 
the productions and semantic actions from Fig. 6.20, together with productions 
involving nonterminal L 
. 
I L = E ; 
{ gen(L. 
addr. 
base '[' 
L. 
addr '1' 
I=' 
E. 
addr); 
} 
E i; E l + E 2  
{E.addr=newTemp(); 
gen(E. 
addr ' = I  El. 
addr ' + I  E2. 
addr) 
; } 
I
L
 
{ E.addr = new Temp 0; 
gen(E.addr ' = I  L.array. 
base 'P L.addr I ] ' ) ;  } 
L -+ id [ E I 
{ L.array = top.get(id.lexeme); 
L.type = L.array.type. 
elem; 
L. 
addr = new Temp 0; 
gen(L.addr 
' = I  E.addr I*' L.type.width); } 
/ 
L1 [ E 1 { L.array = Ll .array; 
L.type = Ll .type.elem; 
t = new Temp () 
; 
L. 
addr = new Temp (); 
gen(t I=' 
E.addr ' * I  L.type.width); } 
gen(L. 
addr '=I Ll. 
addr I+' t); 
} 
Figure 6.22: Semantic actions for array references 
Nonterminal L has three synthesized attributes: 
I .  L.addr denotes a temporary that is used while computing the offset for 
the array reference by summing the terms ij x w
j
 
in (6.4). 
384 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
2. L.array is a pointer to the symbol-table entry for the array name. The 
base address of the array, say, L. 
array. 
base is used to determine the actual 
1-value of an array reference after all the index expressions are analyzed. 
3. L. 
type is the type of the subarray generated by L. For any type t, we 
assume that its width is given by t.width. We use types as attributes, 
rather than widths, since types are needed anyway for type checking. For 
any array type t, 
suppose that t.elem gives the element type. 
The production S -+ 
id 
= 
E ;  represents an assignment to a nonarray vari- 
able, which is handled as usual. The semantic action for S --+ L = E; generates 
an indexed copy instruction to assign the value denoted by expression E to the 
location denoted by the array reference L. Recall that attribute L. array gives 
the symbol-table entry for the array. The array's base address - 
the address 
of its 0th element - 
is given by L. 
array. base. Attribute L. 
addr denotes the 
temporary that holds the offset for the array reference generated by L. The 
location for the array reference is therefore L. 
array. 
base[L. 
addr] 
. The generated 
instruction copies the r-value from address E.addr into the location for L. 
Productions E -+ El +E2 and E --+ 
id are the same as before. The se- 
mantic action for the new production E -+ L generates code to copy the 
value from the location denoted by L into a new temporary. This location is 
L. 
array. 
base[L. 
addr], as discussed above for the production S -+ L = E ; 
. Again, 
attribute L. 
array gives the array name, and L. 
array. 
base gives its base address. 
Attribute L.addr denotes the temporary that holds the offset. The code for the 
array reference places the r-value at the location designated by the base and 
offset into a new temporary denoted by E.addr. 
Example 6.12 : Let a denote a 2 x 3 array of integers, and let c, i, 
and 
j all denote integers. Then, the type of a is array(2, array(3, integer)). Its 
width w is 24, assuming that the width of an integer is 4. The type of a[i] is 
array(3, 
integer), of width w
l
 
= 12. The type of a[il [jl is integer. 
An annotated parse tree for the expression c + a [il [ 
j I is shown in Fig. 6.23. 
The expression is translated into the sequence of three-address instructions in 
Fig. 6.24. As usual, we have used the name of each identifier to refer to its 
symbol-table entry. 
6
.
4
.
5
 Exercises 
f
o
r
 Section 
6
.
4
 
Exercise 6.4.1 : Add to the translation of Fig. 6.19 rules for the following 
productions: 
b) E -+ + El (unary plus). 
Exercise 6.4.2 : 
Repeat Exercise 6.4.1 for the incremental translation of Fig. 
6.20. 
6.4. TRANSLATION OF EXPRESSIONS 
E. 
addr = 
t 5  
E. 
addr = 
t4 
I 
L.array = a 
L.type = integer 
L.addr = 
ts 
L.array = 
a 
/ 
L.type = array(3, 
integer) 
[ 
E.addr = 
j 
1 
L.addr = 
tl 
/ /  
\ \  
I 
j 
I
:
 
E.addr = i 
a. 
type 
1 
= array(2, array(3, 
integer)) 
I 
i 
Figure 6.23: Annotated parse tree for c + a[i] [j] 
Figure 6.24: Three-address code for expression c + aCi] [j] 
Exercise 6.4.3 : Use the translation of Fig. 6.22 to translate the following 
assignments: 
! 
Exercise 6.4.4 : 
Revise the translation of Fig. 6.22 for array references of the 
Fortran style, that is, id[E1, 
E2, 
. 
. . , 
En] 
for an n-dimensional array. 
Exercise 6.4.5 : 
Generalize formula (6.7) to multidimensional arrays, and in- 
dicate what values can be stored in the symbol table and used to compute 
offsets. Consider the following cases: 
a) An array A of two dimensions, in row-major form. The first dimension 
has indexes running from lI to hl, 
and the second dimension has indexes 
from 1
2
 to ha. 
The width of a single array element is w. 
386 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
Symbolic Type Widths 
The intermediate code should be relatively independent of the target ma- 
chine, so the optimizer does not have to change much if the code generator 
is replaced by one for a different machine. However, as we have described 
the calculation of type widths, an assumption regarding how basic types 
is built into the translation scheme. For instance, Example 6.12 assumes 
that each element of an integer array takes four bytes. Some intermediate 
codes, e.g., P-code for Pascal, leave it to the code generator to fill in the 
size of array elements, so the intermediate code is independent of the size 
of a machine word. We could have done the same in our translation scheme 
if we replaced 4 (as the width of an integer) by a symbolic constant. 
b) The same as (a), but with the array stored in column-major form. 
! 
c) An array A of k dimensions, stored in row-major form, with elements of 
size w. The jth dimension has indexes running from l j  to hj. 
! 
d) The same as (c) but with the array stored in column-major form. 
Exercise 6.4.6 : 
An integer array A[i, 
j] has index i ranging from 1 
to 10 and 
index j ranging from 1 
to 20. Integers take 4 bytes each. Suppose array A is 
stored starting at byte 0. Find the location of: 
Exercise 6.4.7: Repeat Exercise 6.4.6 if A is stored in column-major order. 
Exercise 6.4.8 : 
A real array A[i, 
j, 
k] has index i ranging from 1 
to 4, index 
j ranging from 0 to 4, and index k ranging from 5 to 10. Reals take 8 bytes 
each. Suppose array A is stored starting at byte 0. Find the location of: 
Exercise 6.4.9 
: 
Repeat Exercise 6.4.8 if A is stored in column-major order. 
6.5 
Type Checking 
To do type checking a compiler needs to assign a type expression to each com- 
ponent of the source program. The compiler must then determine that these 
type expressions conform to a collection of logical rules that is called the type 
system for the source language. 
Type checking has the potential for catching errors in programs. In principle, 
any check can be done dynamically, if the target code carries the type of an 
6.5. TYPE CHECKING 
387 
element along with the value of the element. A sound type system eliminates the 
need for dynamic checking for type errors, because it allows us to determine 
statically that these errors cannot occur when the target program runs. An 
implementation of a language is strongly tyfled if a compiler guarantees that the 
programs it accepts will run without type errors. 
Besides their use for compiling, ideas from type checking have been used 
to improve the security of systems that allow software modules to be imported 
and executed. Java programs compile into machine-independent bytecodes that 
include detailed type information about the operations in the bytecodes. Im- 
ported code is checked before it is allowed to execute, to guard against both 
inadvertent errors and malicious misbehavior. 
6.5.1 Rules for Type Checking 
Type checking can take on two forms: synthesis and inference. Type synthesis 
builds up the type of an expression from the types of its subexpressions. It 
requires names to be declared before they are used. The type of El + E2 is 
defined in terms of the types of El and E2. A typical rule for type synthesis 
has the form 
if f has type s -+ 
t and x has type s, 
then expression f (x) has type t 
(6.8) 
Here, f and x denote expressions, and s -+ t denotes a function from s to t. 
This rule for functions with one argument carries over to functions with several 
arguments. The rule (6.8) can be adapted for El + 
E2 by viewing it as a function 
application add(E1 
, 
E2) 
.6 
Type inference determines the type of a language construct from the way it 
is used. Looking ahead to the examples in Section 6.5.4, let null be a function 
that tests whether a list is empty. Then, from the usage null(x), we can tell 
that x must be a list. The type of the elements of x is not known; all we know 
is that x must be a list of elements of some type that is presently unknown. 
Variables representing type expressions allow us to talk about unknown 
types. We shall use Greek letters a, 
P, . 
- - for type variables in type expressions. 
A typical rule for type inference has the form 
if f 
(x) is an expression, 
then for some a and ,B, f has type a -+ P and x has type a 
(6.9) 
Type inference is needed for languages like ML, which check types, but do not 
require names to be declared. 
6
~
e
 
shall use the term "synthesis" even if some context information is used to determine 
types. With overloaded functions, where the same name is given to more than one function, 
the context of El $ 
E2 may also need to be considered in some languages. 
388 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
In this section, we consider type checking of expressions. The rules for 
checking statements are similar to those for expressions. For example, we treat 
the conditional statement "if (E) 
S;" as if it were the application of a function 
if to E and S. Let the special type void denote the absence of a value. Then 
function if expects to be applied to a boolean and a void; the result of the 
application is a void. 
6.5.2 
Type Conversions 
Consider expressions like x + 
i, where x is of type float and i is of type inte- 
ger. Since the representation of integers and floating-point numbers is different 
within a computer and different machine instructions are used for operations 
on integers and floats, the compiler may need to convert one of the operands of 
+ 
to ensure that both operands are of the same type when the addition occurs. 
Suppose that integers are converted to floats when necessary, using a unary 
operator (float). For example, the integer 2 is converted to a float in the code 
for the expression 2 * 3 .14: 
tl = (float) 2 
t 2  = t l  * 3.14 
We can extend such examples to consider integer and float versions of the 
operators; for example, int* for integer operands and float* for floats. 
Type synthesis will be illustrated by extending the scheme in Section 6.4.2 
for translating expressions. We introduce another attribute E.type, whose value 
is either integer or float. The rule associated with E 
,,-+ El + 
E2 
builds on the 
pseudocode 
if ( El.type = integer and E2.type = integer ) E.type = integer; 
else if ( El .type = 
float and E2. 
type = integer ) - . 
As the number of types subject to conversion increases, the number of cases 
increases rapidly. Therefore with large numbers of types, careful organization 
of the semantic actions becomes important. 
Type conversion rules vary from language to language. The rules for Java 
in Fig. 6.25 distinguish between widening conversions, which are intended to 
preserve information, and narrowing conversions, which can lose information. 
The widening rules are given by the hierarchy in Fig. 6.25(a): any type lower 
in the hierarchy can be widened to a higher type. Thus, a char can be widened 
to an int or to a float, but a char cannot be widened to a short. The narrowing 
rules are illustrated by the graph in Fig. 6.25(b): a type s can be narrowed to a 
type t if there is a path from s to t. Note that char, short, and byte are pairwise 
convertible to each other. 
Conversion from one type to another is said to be implicit if it is done 
automatically by the compiler. Implicit type conversions, also called coercions, 
6.5. TYPE CHECKING 
double 
I 
float 
I 
long 
I 
iznt 
\ 
short 
char 
I 
byte 
double 
4 
float 
1 
long 
1 
char - 
short - 
byte 
u 
(a) Widening conversions 
(b) Narrowing conversions 
Figure 6.25: Conversions between primitive types in Java 
are limited in many languages to widening conversions. Conversion is said to 
be explicit if the programmer must write something to cause the conversion. 
Explicit conversions are also called casts. 
The semantic action for checking E -+ El + 
E2 uses two functions: 
1. max(tl, t2) takes two types tl and tz and returns the maximum (or least 
upper bound) of the two types in the widening hierarchy. It declares an 
error if either tl or ta is not in the hierarchy; e.g., if either type is an array 
or a pointer type. 
2. widen(a, t, 
w) generates type conversions if needed to widen an address 
a of type t into a value of type w. It returns a itself if t and w are the 
same type. Otherwise, it generates an instruction to do the conversion 
and place the result in a temporary t, which is returned as the result. 
Pseudocode for widen, assuming that the only types are .integer and float, 
appears in Fig. 6.26. 
Addr widen(Addr a, Type t, Type w) 
if ( t = 
w ) return a; 
else if ( t = integer and w = 
float ) { 
temp = new Temp(); 
gen(ternp '=' '(float)' a); 
return temp; 
I- 
else error; 
1 
Figure 6.26: Pseudocode for function widen 
390 
CHAPTER 6. INTERMEDIATE- 
CODE GENERATION 
The semantic action for E -+ El + E2 in Fig. 6.27 illustrates how type 
conversions can be added to the scheme in Fig. 6.20 for translating expressions. 
In the semantic action, temporary variable a1 is either El.addr, if the type of 
El does not need to be converted to the type of E, 
or a new temporary variable 
returned by widen if this conversion is necessary. Similarly, a2 is either E2.addr 
or a new temporary holding the type-converted value of E2. Neither conversion 
is needed if both types are integer or both are float. In general, however, we 
could find that the only way to add values of two different types is to convert 
them both to a third type. 
E -+ El+E2 {E.type = max(El.type,E2.type); 
a1 = widen(El . 
addr, El 
.type, 
E.type); 
a2 = widen(E2. 
addr, E2 
.type, 
E. 
type); 
E.addr = new Temp 0; 
gen(E. 
addr '=I 
a1 '+I 
a2); 
) 
Figure 6.27: Introducing type conversions into expression evaluation 
6.5.3 
Overloading of Functions and Operators 
An overloaded symbol has different meanings depending on its context. Over- 
loading is resolved when a unique meaning is determined for each occurrence 
of a name. In this section, we restrict attention to overloading that can be 
resolved by looking only at the arguments of a function, as in Java. 
Example 6.13 : 
The + operator in Java denotes either string concatenation 
or addition, depending on the types of its operands. User-defined functions can 
be overloaded as well, as in 
void err() ( 
3 
void err(String s )  ( - . -  3 
Note that we can choose between these two versions of a function err 
by looking 
at their arguments. 
The following is a type-synthesis rule for overloaded functions: 
iff can have type si + 
ti, for 1 
5 i 5 n, where si # s j  for i # j 
and x has type s k ,  for some 1 
5 k 5 n 
(6.10) 
then expression f (x) has type tk 
The value-number method of Section 6.1.2 can be applied to type expres- 
sions to resolve overloading based on argument types, efficiently. In a DAG 
representing a type expression, we assign an integer index, called a value num- 
ber, to each node. Using Algorithm 6.3, we construct a signature for a node, 
6.5. TYPE CHECKING 
391 
consisting of its label and the value numbers of its children, in order from left to 
right. The signature for a function consists of the function name and the types 
of its arguments. The assumption that we can resolve overloading based on 
the types of arguments is equivalent to saying that we can resolve overloading 
based on signatures. 
It is not always possible to resolve overloading by looking only at the argu- 
ments of a function. In Ada, instead of a single type, a subexpression standing 
alone may have a set of possible types for which the context must provide suffi- 
cient information to narrow the choice down to a single type (see Exercise 6.5.2). 
6
.
5
.
4
 Type Inference and Polymorphic Functions 
Type inference is useful for a language like ML, which is strongly typed, but 
does not require names to be declared before they are used. Type inference 
ensures that names are used consistently. 
The term "polymorphic" refers to any code fragment that can be executed 
with arguments of different types. In this section, we consider parametric poly- 
morphism, where the polymorphism is characterized by parameters or type 
variables. The running example is the ML program in Fig. 6.28, which defines 
a function length. The type of length can be described as, "for any type a, 
length maps a list of elements of type a to an integer." 
fun length(x) = 
if null(x) then 0 else length(tl(x)) + 
1; 
Figure 6.28: ML program for the length of a list 
Example 6.14 : 
In Fig. 6.28, the keyword fun introduces a function definition; 
functions can be recursive. The program fragment defines function length with 
one parameter x. The body of the function consists of a conditional expression. 
The predefined function null tests whether a list is empty, and the predefined 
function tl (short for "tail") returns the remainder of a list after the first element 
is removed. 
The function length determines the length or number of elements of a list 
x. All elements of a list must have the same type, but length can be applied to 
lists whose elements are of any one type. In the following expression, length is 
applied to two different types of lists (list elements are enclosed within "[" and 
): 
The list of strings has length 3 and the list of integers has length 4, so expres- 
sion (6.11) evaluates to 7. 
392 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
Using the symbol V (read as "for any type") and the type constructor list, 
the type of length can be written as 
Va. list(a) + 
integer 
(6.12) 
The V symbol is the universal quantifier, and the type variable to which it 
is applied is said to be bound by it. Bound variables can be renamed at will, 
provided all occurrences of the variable are renamed. Thus, the type expression 
VP. list(P) -+ integer 
is equivalent to (6.12). A type expression with a V symbol in it will be referred 
to informally as a "polymorphic type." 
Each time a polymorphic function is applied, its bound type variables can 
denote a different type. During type checking, at each use of a polymorphic 
type we replace the bound variables by fresh variables and remove the universal 
quantifiers. 
The next example informally infers a type for length, implicitly using type 
inference rules like (6.9), which is repeated here: 
if f (x) is an expression, 
then for some a and p, f has type a -+ P and x has type a 
Example 6.15 : 
The abstract syntax tree in Fig. 6.29 represents the definition 
of length in Fig. 6.28. The root of the tree, labeled fun, represents the function 
definition. The remaining nonleaf nodes can be viewed as function applications. 
The node labeled + represents the application of the operator + to a pair of 
children. Similarly, the node labeled if represents the application of an operator 
if to a triple formed by its children (for type checking, it does not matter that 
either the then or the else part will be evaluated, but not both). 
fun \ 
length 
" /;\ 
apply 
0 
+ 
/ \ 
/ \ 
null 
x 
apply 
1 
/ \ 
length 
apply 
/ \ 
Figure 6.29: Abstract syntax tree for the function definition in Fig. 6.28 
F'rom the body of function length, we can infer its type. Consider the children 
of the node labeled if, from left to right. Since null expects to be applied to 
lists, x must be a list. Let us use variable a as a placeholder for the type of the 
list elements; that is, x has type "list of a." 
6.5. TYPE CHECE(ING 
393 
Substitutions, Instances, and Unification 
If t is a type expression and S is a substitution (a mapping from type vari- 
ables to type expressions), then we write S(t) for the result of consistently 
replacing all occurrences of each type variable a in t by S(a). S(t) is 
called an instance of t. For example, list(integer) is an instance of list(a), 
since it is the result of substituting integer for a in list(a) 
. Note, however, 
that integer -+ float is not an instance of a -+ a, 
since a substitution must 
replace all occurrences of a by the same type expression. 
Substitution S is a uniifier of type expressions tl and t2 if S(tl) = 
S(t2). S is the most general unifier of tl and t2 if for any other unifier of 
tl and t2, 
say St, 
it is the case that for any t, S1(t) 
is an instance of S(t). 
In words, St 
imposes more constraints on t than S does. 
If null(x) is true, then length(x) is 0. Thus, the type of length must be 
"function from list of a to integer." This inferred type is consistent with the 
usage of length in the else part, length(tl(x)) + 1. 
Since variables can appear in type expressions, we have to re-examine the 
notion of equivalence of types. Suppose El of type s -+ st is applied to E2 of 
type t. Instead of simply determining the equality of s and t, we must "unify" 
them. Informally, we determine whether s and t can be made structurally 
equivalent by replacing the type variables in s and t by type expressions. 
A substitution is a mapping from type variables to type expressions. We 
write S(t) for the result of applying the substitution S to the variables in type 
expression t; see the box on "Substitutions, Instances, and Unification." Two 
type expressions tl and t2 unify if there exists some substitution S such that 
S(tl) = 
S(t2). In practice, we are interested in the most general unifier, which 
is a substitution that imposes the fewest constraints on the variables in the 
expressions. See Section 6.5.5 for a unification algorithm. 
Algorithm 6.16 : 
Type inference for polymorphic functions. 
INPUT: A program consisting of a sequence of function definitions followed by 
an expression to be evaluated. An expression is made up of function applications 
and names, where names can have predefined polymorphic types. 
OUTPUT: Inferred types for the names in the program. 
METHOD: For simplicity, we shall deal with unary functions only. The type of a 
function f (xl, 
x2) with two parameters can be represented by a type expression 
sl x s 2  -+ t, where sl and s2 are the types of xl and x2, respectively, and t is the 
type of the result f (xl, 
22). An expression f (a, 
b) can be checked by matching 
the type of a with sl and the type of b with s2. 
394 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
Check the function definitions and the expression in the input sequence. Use 
the inferred type of a function if it is subsequently used in an expression. 
For a function definition fun idl 
(id2) = E, create fresh type variables a 
and ,8. Associate the type a -+ ,8 with the function idl, and the type a 
with the parameter id2. Then, infer a type for expression E. Suppose 
a denotes type s and ,8 denotes type t after type inference for E. The 
inferred type of function idl is s -+ t. Bind any type variables that remain 
unconstrained in s -+ t by 'if quantifiers. 
For a function application El 
(E2), 
infer types for El and E2. Since El is 
used as a function, its type must have the form s -+ st. (Technically, the 
type of El must unify with ,
8
 -+ y, where ,8 and y are new type variables). 
Let t be the inferred type of El. Unify s and t. If unification fails, the 
expression has a type error. Otherwise, the inferred type of El 
(E2) is st. 
For each occurrence of a polymorphic function, replace the bound vari- 
ables in its type by distinct fresh variables and remove the 'if quantifiers. 
The resulting type expression is the inferred type of this occurrence. 
For a name that is encountered for the first time, introduce a fresh variable 
for its type. 
Example 6.17: In Fig. 6.30, we infer a type for function length. The root of 
the syntax tree in Fig. 6.29 is for a function definition, so we introduce variables 
,8 and y, 
associate the type ,8 -+ 
y 
with function length, and the type ,8 with x; 
see lines 1-2 of Fig. 6.30. 
At the right child of the root, we view if as a polymorphic function that is 
applied to a triple, consisting of a boolean and two expressions that represent 
the then and else parts. Its type is Va. boolean x a x a -+ a .  
Each application of a polymorphic function can be to a different type, so we 
make up a fresh variable ai (where i is from "if") and remove the 'd; see line 3 
of Fig. 6.30. The type of the left child of if must unify with boolean, and the 
types of its other two children must unify with ai. 
The predefined function null has type Va. list(a) -+ boolean. We use a fresh 
type variable an 
(where n is for "null") in place of the bound variable a; 
see 
line 4. From the application of null to x, we infer that the type ,8 of x must 
match list(a,); see line 5 .  
At the first child of if, the type boolean for null(x) matches the type expected 
by if. At the second child, the type ai unifies with integer; see line 6. 
Now, consider the subexpression length(tl(x)) + 1. We make up a fresh 
variable at (where t is for "tail") for the bound variable a in the type of tl; see 
line 8. From the application tl(x), we infer list(at) = 
,
O
 = list(an); 
see line 9. 
Since length(tl(x)) is an operand of +, its type y must unify with integer; 
see line 10. It follows that the type of length is list(a,) -+ 
integer. After the 
6.5. TYPE CHECKING 
395 
x : p  
if : boolean x ai x ai -+ ai 
null : list(an) 
-+ 
boolean 
null($) : boolean 
0 : integer 
+ : integer x integer -+ integer 
tl : list(at) 
-+ Eist(at) 
tl(x) : list(at) 
length(tl(x)) 
: y 
1 : integer 
list(&,) 
= p 
ai = integer 
UNIFY 
LINE 
1) 
list(at) 
= list(an) 
I 
y = integer 
EXPRESSION 
: TYPE 
length : ,
8
 -+ y 
Figure 6.30: Inferring a type for the function length of Fig. 6.28 
12) 
13) 
function definition is checked, the type variable a, remains in the type of length. 
Since no assumptions were made about a,, any type can be substituted for it 
when the function is used. We therefore make it a bound variable and write 
length(tl(x)) 
+ 1 : integer 
if( 
- - ) : integer 
Van. 
list(an) 
-+ integer 
for the type of length. 
6.5.5 
An Algorithm for Unification 
Informally, unification is the problem of determining whether two expressions 
s and t can be made identical by substituting expressions for the variables in 
s and t. Testing equality of expressions is a special case of unification; if s 
and t have constants but no variables, then s and t unify if and only if they 
are identical. The unification algorithm in this section extends to graphs with 
cycles, so it can be used to test structural equivalence of circular types.7 
We shall implement a graph-theoretic formulation of unification, where types 
are represented by graphs. Type variables are represented by leaves and type 
constructors are represented by interior nodes. Nodes are grouped into equiv- 
alence classes; if two nodes are in the same equivalence class, then the type 
expressions they represent must unify. Thus, all interior nodes in the same 
class must be for the same type constructor, and their corresponding children 
must be equivalent. 
Example 6.18 : 
Consider the two type expressions 
7 ~ n  
some applications, it is an error to unify a variable with an expression containing that 
variable. Algorithm 6.19 permits such substitutions. 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
The following substitution S is the most general unifier for these expressions 
This substitution maps the two type expressions to the following expression 
The two expressions are represented by the two nodes labeled -+: 1 
in Fig. 6.31. 
The integers at the nodes indicate the equivalence classes that the nodes belong 
to after the nodes numbered 1 
are unified. 
+: 
1 
/ \ 
x : 2  
list : 8 
list : 6 
/ \ 
+: / 7 
.
.
 
,--list: 
6 
/ \ 
a1 : 4 
a
2
 : 5 
a
3
 : 
4 
a
4
 : 
5 
Figure 6.3 
1 
: Equivalence classes after unification 
Algorithm 6.19: Unification of a pair of nodes in a type graph. 
INPUT: A graph representing a type and a pair of nodes m and n to be unified. 
OUTPUT: Boolean value true if the expressions represented by the nodes m 
and n unify; false, otherwise. 
METHOD: A node is implemented by a record with fields for a binary operator 
and pointers to the left and right children. The sets of equivalent nodes are 
maintained using the set field. One node in each equivalence class is chosen to be 
the unique representative of the equivalence class by making its set field contain 
a null pointer. The set fields of the remaining nodes in the equivalence class will 
point (possibly indirectly through other nodes in the set) to the representative. 
Initially, each node n is in an equivalence class by itself, with n as its own 
representative node. 
The unification algorithm, shown in Fig. 6.32, uses the following two oper- 
ations on nodes: 
6.5. TYPE CHECKING 
boolean unzfy(Node m, Node n) { 
s = 
find(m); t = 
find(n); 
if ( s = t ) return true; 
else if ( nodes s and t represent the same basic type ) return true; 
else if (s is an op-node with children sl and sz and 
t is an op-node with children tl and t2) { 
union(s , 
t) 
; 
return unify(sl, tl) and unif?l(sz, 
t2); 
1 
else if s or t represents a variable { 
union(s, t) 
; 
return true; 
1 
else return false; 
Figure 6.32: Unification algorithm. 
find(n) returns the representative node of the equivalence class currently 
containing node n. 
union(m, 
n) merges the equivalence classes containing nodes m and n. I
f
 
one of the representatives for the equivalence classes of m and n is a non- 
variable node, union makes that nonvariable node be the representative 
for the merged equivalence class; otherwise, union makes one or the other 
of the original representatives be the new representative. This asymme- 
try in the specification of union is important because a variable cannot 
be used as the representative for an equivalence class for an expression 
containing a type constructor or basic type. Otherwise, two inequivalent 
expressions may be unified through that variable. 
The union operation on sets is implemented by simply changing the set field 
of the representative of one equivalence class so that it points to the represen- 
tative of the other. To find the equivalence class that a node belongs to, we 
follow the set pointers of nodes until the representative (the node with a null 
pointer in the set field) is reached. 
Note that the algorithm in Fig. 6.32 uses s = 
find(m) and t = 
find(n) rather 
than m and n, respectively. The representative nodes s and t are equal if m 
and n are in the same equivalence class. If s and t represent the same basic 
type, the call unzfy(m, n) returns true. If s and t are both interior nodes for a 
binary type constructor, we merge their equivalence classes on speculation and 
recursively check that their respective children are equivalent. By merging first, 
we decrease the number of equivalence classes before recursively checking the 
children, so the algorithm terminates. 
398 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
The substitution of an expression for a variable is implemented by adding 
the leaf for the variable to the equivalence class containing the node for that 
expression. Suppose either rn or n is a leaf for a variable. Suppose also that 
this leaf has been put into an equivalence class with a node representing an 
expression with a type constructor or a basic type. Then find will return 
a representative that reflects that type constructor or basic type, so that a 
variable cannot be unified with two different expressions. 
Example 6.20 : 
Suppose that the two expressions in Example 6.18 are repre- 
sented by the initial graph in Fig. 6.33, where each node is in its own equiv- 
alence class. When Algorithm 6.19 is applied to compute unify(l,9), it notes 
that nodes 1 
and 9 both represent the same operator. It therefore merges 1 
and 
9 into the same equivalence class and calls unify(2,lO) and unify(8,14). The 
result of computing unify(l, 
9) is the graph previously shown in Fig. 6.31. 
+: 
1 
+: 
9 
/ \ 
x : 2  
list : 
8 
x  : 10 
as : 14 
/ \ 
list : 
6 
/ \ 
: , 
, - + - - + i s t :  
13 
/ \ 
a1 : 4  
a2 : 5 
a3 : 7 
a4 : 
12 
Figure 6.33: Initial graph with each node in its own equivalence class 
If Algorithm 6.19 returns true, we can construct a substitution S that acts 
as the unifier, as follows. For each variable a, 
find(a) gives the node n that 
is the representative of the equivalence class of a .  The expression represented 
by n is S(u). For example, in Fig. 6.31, we see that the representative for 
as is node 4, which represents 01. The representative for as is node 8, which 
represents list(az). The resulting substitution S is as in Example 6.18. 
6.5.6 Exercises for Section 6.5 
Exercise 6.5.1 : Assuming that function widen in Fig. 6.26 can handle any 
of the types in the hierarchy of Fig. 6.25(a), translate the expressions below. 
Assume that c and d are characters, s and t are short integers, i and j are 
integers, and x is a float. 
c) x = ( S  + C) * (t + d). 
6.6. CONTROL FLOW 
399 
Exercise 6.5.2 : 
As in Ada, suppose that each expression must have a unique 
type, but that from a subexpression, by itself, all we can deduce is a set of pos- 
sible types. That is, the application of function El to argument Ez 
, 
represented 
by E i 
El ( E2 
), has the associated rule 
E.type = { t / for some s in E2. 
type, s i 
t is in El .type } 
Describe an SDD that determines a unique type for each subexpression by 
using an attribute type to synthesize a set of possible types bottom-up, and, 
once the unique type of the overall expression is determined, proceeds top-down 
to determine attribute unique for the type of each subexpression. 
6.6 Control Flow 
The translation of statements such as if-else-st 
atements and while-statements 
is tied to the translation of boolean expressions. In programming languages, 
boolean expressions are often used to 
1. Alter the flow of control. Boolean expressions are used as conditional 
expressions in statements that alter the flow of control. The value of such 
boolean expressions is implicit in a position reached in a program. For 
example, in if (E) S, the expression E must be true if statement S is 
reached. 
2. Compute logical values. A boolean expression can represent true or false 
as values. Such boolean expressions can be evaluated in analogy to arith- 
metic expressions using three-address instructions with logical operators. 
The intended use of boolean expressions is determined by its syntactic con- 
text. For example, an expression following the keyword if is used to alter the 
flow of control, while an expression on the right side of an assignment is used 
to denote a logical value. Such syntactic contexts can be specified in a number 
of ways: we may use two different nonterminals, use inherited attributes, or 
set a flag during parsing. Alternatively we may build a syntax tree and invoke 
different procedures for the two different uses of boolean expressions. 
This section concentrates on the use of boolean expressions to alter the flow 
of control. For clarity, we introduce a new nonterminal B for this purpose. 
In Section 6.6.6, we consider how a compiler can allow boolean expressions to 
represent logical values. 
6.6.1 Boolean Expressions 
Boolean expressions are composed of the boolean operators (which we denote 
&&, I I, and !, using the C convention for the operators AND, OR, and NOT, 
respectively) applied to elements that are boolean variables or relational ex- 
pressions. Relational expressions are of the form El re1 E2, where El and 
400 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
E2 
are arithmetic expressions. In this section, we consider boolean expressions 
generated by the following grammar: 
B -+ 
B I I B  ( B & & B  
( ! B  
I ( B )  1 E r e l E  1 true 1 false 
We use the attribute rel.op to indicate which of the six comparison operators 
<, <=, =, ! =, >, or >= is represented by rel. As is customary, we assume 
that I I and && are left-associative, and that I I has lowest precedence, then 
&&, then !. 
Given the expression B1 I I B2, if we determine that B1 is true, then we 
can conclude that the entire expression is true without having to evaluate B2. 
Similarly, given B1&&B2, 
if B1 is false, then the entire expression is false. 
The semantic definition of the programming language determines whether 
all parts of a boolean expression must be evaluated. If the language definition 
permits (or requires) portions of a boolean expression to go unevaluated, then 
the compiler can optimize the evaluation of boolean expressions by computing 
only enough of an expression to determine its value. Thus, in an expression 
such as B1 I I B2, 
neither B1 nor B2 is necessarily evaluated fully. If either B1 
or B2 is an expression with side effects (e.g., it contains a function that changes 
a global variable), then an unexpected answer may be obtained. 
6.6.2 Short-Circuit Code 
In short-circuit (or jumping) code, the boolean operators &&, I I ,  and ! trans- 
late into jumps. The operators themselves do not appear in the code; instead, 
the value of a boolean expression is represented by a position in the code se- 
quence. 
Example 6.2 1 : 
The statement 
might be translated into the code of Fig. 6.34. In this translation, the boolean 
expression is true if control reaches label L2. If the expression is false, control 
goes immediately to L1, skipping L2 and the assignment x = 0. 
Figure 6.34: Jumping code 
6.6. CONTROL FLOW 
40 
1 
6.6.3 Flow-of-Control Statements 
We now consider the translation of boolean expressions into three-address code 
in the context of statements such as those generated by the following grammar: 
S 4 i f ( B ) S 1  
S 4 if ( B ) S1 else S2 
S + while ( B ) S1 
In these productions, nonterminal B represents a boolean expression and non- 
terminal S represents a statement. 
This grammar generalizes the running example of while expressions that we 
introduced in Example 5.19. As in that example, both B and S have a synthe- 
sized attribute code, which gives the translation into three-address instructions. 
For simplicity, we build up the translations B. 
code and S. 
code as strings, us- 
ing syntax-directed definitions. The semantic rules defining the code attributes 
could be implemented instead by building up syntax trees and then emitting 
code during a tree traversal, or by any of the approaches outlined in Section 5.5. 
The translation of if (B) S1 consists of B. 
code followed by Sl. 
code, as illus- 
trated in Fig. 6.35(a). Within B. 
code are jumps based on the value of B. If B 
is true, control flows to the first instruction of S1 
.code, and if B is false, control 
flows to the instruction immediately following Sl 
.code. 
B. 
true : 
Sl 
. 
code 
B. 
true : 
./I 
B.false . 
B.false : 
(a) if 
begin : 
\ 
d B .  
true 
(b) if-else 
B. 
true : 
Sl . 
code 
-
1
 
goto begin 
B. 
false : 
(c) while 
Figure 6.35: Code for if-, if-else-, and while-statements 
The labels for the jumps in B.code and S.code are managed using inherited 
attributes. With a boolean expression B, we associate two labels: B.true, the 
402 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
label to which control flows if B is true, and B.false, the label to which control 
flows if B is false. With a statement S, we associate an inherited attribute 
S.next denoting a label for the instruction immediately after the code for S. 
In some cases, the instruction immediately following S.code is a jump to some 
label L. A jump to a jump to L from within S.code is avoided using S.next. 
The syntax-directed definition in Fig. 6.36-6.37 produces t 
hree-address code 
for boolean expressions in the context of if-, if-else-, and while-st atements. 
S -+ if ( B ) S1 else S2 
S + assign 
S + while ( B ) S1 
S. 
code = assign. 
code 
B.true = newlabel() 
B.false = Sl.next = S.next 
S. 
code = B. 
code ( 1  label(B.true) 
/ ( Sl. 
code 
B.true = newlabel() 
B.false = newlabel() 
Sl .next = S2. 
next = S.next 
S. 
code = B.code 
I / label(B.true) 
I I Sl . 
code 
I 
I gen('gotol S. 
next) 
I I label(B. 
false) 1 I S2. 
code 
begin = newlabel() 
B.true = newlabel() 
B.false = S.next 
&.next = begin 
S.code = label(begin) 
( 1  B.code 
I 
I 
/ label(B.true) 
1 I Sl. 
code 
I 
I I gen('got 
o1 begin) 
Figure 6.36: Syntax-directed definition for flow-of-control statements. 
We assume that newlabelo creates a new label each time it is called, and that 
label(L) 
attaches label L to the next three-address instruction to be generated.8 
'
1
f
 implemented literally, the semantic rules will generate lots of labels and may attach 
more than one labe1 
to a three-address instruction. The backpatching approach of Section 6.7 
6.6. CONTROL FLOW 
403 
A program consists of a statement generated by P -+ 
S. The semantic rules 
associated with this production initialize S.next to a new label. P.code consists 
of S.code followed by the new label S.next. Token assign in the production 
S -+ 
assign is a placeholder for assignment statements. The translation of 
assignments is as discussed in Section 6.4; for this discussion of control flow, 
S. 
code is simply assign. 
code. 
In translating S -+ if (B) S1, 
the semantic rules in Fig. 6.36 create a new 
label B.true and attach it to the first three-address instruction generated for 
the statement S1, 
as illustrated in Fig. 6.35(a). Thus, jumps to B.true within 
the code for B will go to the code for S1. Further, by setting B.false to S.next, 
we ensure that control will skip the code for S1 if B evaluates to false. 
In translating the if-else-statement S -+ if (B) S1 else S2, 
the code for the 
boolean expression B has jumps out of it to the first instruction of the code for 
S1 if B is true, and to the first instruction of the code for S2 
if B is false, as 
illustrated in Fig. 6.35(b). Further, control flows from both Sl and S2 
to the 
three-address instruction immediately following the code for S - 
its label is 
given by the inherited attribut,e 
S.next. An explicit g o t  
o S.next appears after 
the code for S1 to skip over the code for S2. No goto is needed after S2, 
since 
S2. 
next is the same as S. 
next. 
The code for S -+ 
while (B) S1 
is formed from B. 
code and Sl 
.code as shown 
in Fig. 6.35(c). We use a local variable begin to hold a new label attached to 
the first instruction for this while-statement, which is also the first instruction 
for B. We use a variable rather than an attribute, because begin is local to 
the semantic rules for this production. The inherited label S.next marks the 
instruction that control must flow to if B is false; hence, B. 
false is set to be 
S.next. A new label B. 
true is attached to the first instruction for S1; the code 
for B generates a jump to this label if B is true. After the code for S1 we place 
the instruction goto begin, which causes a jump back to the beginning of the 
code for the boolean expression. Note that S1 
.next is set to this label begin, so 
jumps from within Sl. 
code can go directly to begin. 
The code for S + 
S
1
 S
2
 consists of the code for S1 followed by the code for 
S2. The semantic rules manage the labels; the first instruction after the code 
for S
1
 is the beginning of the code for S2 
; 
and the instruction after the code for 
S
z
 is also the instruction after the code for S. 
We discuss the translation of flow-of-control statements further in Section 
6.7. There we shall see an alternative method, called "backpatching," which 
emits code for statements in one pass. 
6.6.4 Control-Flow Translation of Boolean Expressions 
The semantic rules for boolean expressions in Fig. 6.37 complement the semantic 
rules for statements in Fig. 6.36. As in the code layout of Fig. 6.35, a boolean 
expression B is translated into three-address instructions that evaluate B using 
creates labels only when they are needed. Alternatively, unnecessary labels can be eliminated 
during a subsequent optimization phase. 
404 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
conditional and unconditional jumps to one of two labels: B.true if B is true, 
and B.fa1se if B is false. 
Bl .false = new label() 
B2. 
true = B. 
true 
B2 
.false = B. 
false 
B.code = Bl .code I 
I label(B1 
.false) ( 1  B
2
 .code 
Bl .true = B.false 
Bl .false = B. 
true 
B.code = Bl.code 
B --+ true 
B -+ El re1 E2 
B. 
code = gen('gotol B. 
true) 
B. 
code = El. 
code ( 1  E2. 
code 
( 1  gen('if1 
El. 
addr rel. 
op &. 
addr 'goto' B. 
true) 
I 
I gen('got 
o
'
 B.false) 
B --+ false 
I B.code = gen('gotol B.false) 
Figure 6.37: Generating three-address code for booleans 
The fourth production in Fig. 6.37, B -+ El re1 E2, 
is translated directly 
into a comparison three-address instruction with jumps to the appropriate 
places. For instance, B of the form a < b translates into: 
The remaining productions for B are translated as follows: 
1. Suppose B is of the form B1 I I Bz. If B1 is true, then we immediately 
know that B itself is true, so Bl.true is the same as B.true. If B1 is false, 
then B2 must be evaluated, so we make Bl.false be the label of the first 
instruction in the code for Bz. The true and false exits of B2 are the same 
as the true and false exits of B, respectively. 
6.6. CONTROL FLOW 
2. The translation of Bl && B2 is similar. 
3. No code is needed for an expression B of the form ! 
B1: just interchange 
the true and false exits of B to get the true and false exits of B1. 
4. The constants true and false translate into jumps to B.true and B.false, 
respectively. 
Example 6.22 : 
Consider again the following statement from Example 6.21: 
Using the syntax-directed definitions in Figs. 6.36 and 6.37 we would obtain 
the code in Fig. 6.38. 
Figure 6.38: Control-flow translation of a simple if-st 
atement 
The statement (6.13) constitutes a program generated by P -+ S from 
Fig. 6.36. The semantic rules for the production generate a new label L1 for 
the instruction after the code for S. Statement S 
has the form if (B) S1, 
where 
S1 is x = O;, so the rules in Fig. 6.36 generate a new label L2 and attach it to 
the first (and only, in this case) instruction in Sl.code, which is x = 0. 
Since I I has lower precedence than &&, the boolean expression in (6.13) 
has the form B1 I I B2, where B1 is z < 100. Following the rules in Fig. 6.37, 
Bl .true is La, 
the label of the assignment x = 0 ; 
. Bl 
.false is a new label LS , 
attached to the first instruction in the code for B2. 
Note that the code generated is not optimal, in that the translation has 
three more instructions (goto's) than the code in Example 6.21. The instruction 
goto L3 is redundant, since L3 is the label of the very next instruction. The 
two goto L1 instructions can be eliminated by using i f  
False instead of i f  
instructions, as in Example 6.21. 
6.6.5 Avoiding Redundant Gotos 
In Example 6.22, the comparison x > 200 translates into the code fragment: 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
Instead, consider the instruction: 
This i f  
False instruction takes advantage of the natural flow from one instruc- 
tion to the next in sequence, so control simply "falls through" to label L4 if 
x > 200 is false, thereby avoiding a jump. 
In the code layouts for if- and while-statements in Fig. 6.35, the code for 
statement S1 immediately follows the code for the boolean expression B. By 
using a special label fall (i.e., "don't generate any jump"), we can adapt the 
semantic rules in Fig. 6.36 and 6.37 to allow control to fall through from the 
code for B to the code for S1. The new rules for S -+ if (B) S1 in Fig. 6.36 set 
B.true to fall: 
B.true = 
fall 
B.fa1se = Sl 
.next = S.next 
S.code = B.code I 
( Sl 
.code 
Similarly, the rules for if-else- and while-statements also set B. 
true to fall. 
We now adapt the semantic rules for boolean expressions to allow control to 
fall through whenever possible. The new rules for B -+ 
re1 & in Fig. 6.39 
generate two instructions, as in Fig. 6.37, if both B.true and B.false are explicit 
labels; that is, neither equals fall. Otherwise, if B.true is an explicit label, then 
B.fa1se must be fall, so they generate an i f  instruction that lets control fall 
through if the condition is false. Conversely, if B.false is an explicit label, then 
they generate an i f  
False instruction. In the remaining case, both B. 
true and 
B,false are fall, so no jump in generated.' 
In the new rules for B -+ B1 I 1 B2 in Fig. 6.40, note that the meaning of 
label fall for B is different from its meaning for B1. Suppose B.true is fall; i.e, 
control falls through B, if B evaluates to true. Although B evaluates to true if 
B1 does, Bl.true must ensure that control jumps over the code for B2 to get to 
the next instruction after B. 
On the other hand, if B1 
evaluates to false, the truth-value of B is de- 
termined by the value of B2, so the rules in Fig. 6.40 ensure that Bl.false 
corresponds to control falling through from B1 to the code for B2. 
The semantic rules are for B -+ 
B1 && B2 are similar to those in Fig. 6.40. 
We leave them as an exercise. 
Example 6.23 : 
With the new rules using the special label fall, the program 
(6.13) from Example 6.21 
' ~ n  
C and Java, expressions may contain assignments within them, so code must be gen- 
erated for the subexpressions El and E2, 
even if both B.true and B.false are fall. If desired, 
dead code can be eliminated during an optimization phase. 
6.6. CONTROL FLOW 
test = El .addr rel. 
op E2 
.addr 
s = if B.true # fall and B .  
false # fall then 
gen('ifl test ' g o t  
o' B. 
true) I 
( gen('got o' B.false) 
else if B.true # fall then gen('if1 
test 'goto' B.true) 
else if B.false # fall then gen('if ~ a l s e '  
test 'goto' B.false) 
else ' 
' 
B.code = El .code ( 1  E2. 
code I ( s 
Figure 6.39: Semantic rules for B -+ El re1 E2 
Bl.true = if B.true # fall then B.true else newlabel() 
Bl .false = 
fall 
B2.true = B.true 
B2.false = B.false 
B.code = if B.true # fall then B1 
.code 1 1  B2. 
code 
else Bl . 
code 1  I B2. 
code I I label(Bl . 
true) 
Figure 6.40: Semantic rules for B -+ B1 I I B2 
translates into the code of Fig. 6.41. 
Figure 6.41: If-statement translated using the fall-through technique 
As in Example 6.22, the rules for P -+ 
S create label L1. The difference from 
Example 6.22 is that the inherited attribute B.true is fall when the semantic 
rules for B -+ B
1
 
I I B
2
 are applied (B.false is L1). The rules in Fig. 6.40 
create a new label L2 to allow a jump over the code for B2 if B1 evaluates to 
true. Thus, Bl . 
true is Lz and Bl .false is fall, since B2 must be evaluated if B1 
is false. 
The production B -+ El re1 E2 that generates x < 100 is therefore reached 
with B.true = L2 and B. 
false = 
fall. With these inherited labels, the rules in 
Fig. 6.39 therefore generate a single instruction if 
x < 100 g o t o  L2. 
408 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
6.6.6 Boolean Values and Jumping Code 
The focus in this section has been on the use of boolean expressions t? alter 
the flow of control in statements. A boolean expression may also be evaluated 
for its value, as in assignment statements such as x = true; 
or x = acb;. 
A clean way of handling both roles of boolean expressions is to first build a 
syntax tree for expressions, using either of the following approaches: 
1. Use two passes. Construct a complete syntax tree for the input, and then 
walk the tree in depth-first order, computing the translations specified by 
the semantic rules. 
2. Use one pass for statements, but two passes for expressions. With this 
approach, we would translate E in while (E) S1 before S1 is examined. 
The translation of E, however, would be done by building its syntax tree 
and then walking the tree. 
The following grammar has a single nonterminal E for expressions: 
S -+ id = E ;  I i f ( E ) S  1 w h i l e ( E ) S  I S S  
E + EIIE ( E & & E  
( E r e l E  
( E + E  
( ( E )  
( i d 1  
truelfalse 
Nonterminal E governs the flow of control in S -+ 
while (E) Sl. The same 
nonterminal E denotes a value in S + 
id = E ; and E -+ E + E. 
We can handle these two roles of expressions by using separate code-genera- 
tion functions. Suppose that attribute E.n denotes the syntax-tree node for an 
expression E and that nodes are objects. Let method jump generate jumping 
code at an expression node, and let method rualue generate code to compute 
the value of the node into a temporary. 
When E appears in S + while (E) S1, method jump is called at node 
E.n. The implementation of jump is based on the rules for boolean expressions 
in Fig. 6.37. Specifically, jumping code is generated by calling E.n.jump(t, f), 
where t is a new label for the first instruction of Sl.code and f is the label 
S. 
next. 
When E appears in S 
-+ id = E ;, method rualue is called at node E n .  If E 
has the form El + 
E2, 
the method call E.n. 
rualue() generates code as discussed 
in Section 6.4. If E has the form El && E2, 
we first generate jumping code for 
E and then assign true or false to a new temporary t at the true and false exits, 
respectively, from the jumping code. 
For example, the assignment x = a < b && c < d can be implemented by the 
code in Fig. 6.42. 
6.6.7 Exercises for Section 6.6 
Exercise 6.6.1 : Add rules to the syntax-directed definition of Fig. 6.36 for 
the following control-flow constructs: 
a) A repeat-statment repeat S while B 
6.6. CONTROL FLOW 
i f F a l s e  a < b goto L1 
i f F a l s e  c > d goto L1 
t = t r u e  
got0 L2 
L1 
: t = f a l s e  
L2: 
x = t  
Figure 6.42: Translating a boolean assignment by computing the value of a 
temporary 
! 
b) A for-loop for (S1 
; B; S2) 
S3. 
Exercise 6.6.2: Modern machines try to execute many instructions at the 
same time, including branching instructions. Thus, there is a severe cost if the 
machine speculatively follows one branch, when control actually goes another 
way (all the speculative work is thrown away). It is therefore desirable to min- 
imize the number of branches. Notice that the implementation of a while-loop 
in Fig. 6.35(c) has two branches per interation: one to enter the body from 
the condition B and the other to jump back to the code for B. As a result, 
it is usually preferable to implement while (B) S as if it were if (B) { re- 
peat S until !(B) 
). 
Show what the code layout looks like for this translation, 
and revise the rule for while-loops in Fig. 6.36. 
! Exercise 6.6.3 : Suppose that there were an "exclusive-or" operator (true if 
and only if exactly one of its two arguments is true) in C. Write the rule for 
this operator in the style of Fig. 6.37. 
Exercise 6.6.4 : Translate the following expressions using the goto-avoiding 
translation scheme of Section 6.6.5: 
Exercise 6.6.5 : 
Give a translation scheme based on the syntax-directed defi- 
nition in Figs. 6.36 and 6.37. 
Exercise 6.6.6 
: Adapt the semantic rules in Figs. 6.36 and 6.37 to allow 
control to fall through, using rules like the ones in Figs. 6.39 and 6.40. 
! Exercise 6.6.7 : 
The semantic rules for statements in Exercise 6.6.6 generate 
unnecessary labels. Modify the rules for statements in Fig. 6.36 to create labels 
as needed, using a special label deferred to mean that a label has not yet been 
created. Your rules must generate code similar to that in Example 6.21. 
410 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
!! Exercise 6.6.8 : 
Section 6.6.5 talks about using fall-through code to minimize 
the number of jumps in the generated intermediate code. However, it does not 
take advantage of the option to replace a condition by its complement, e.g., re- 
place i f  a < b goto L1 
; goto L2 by i f  b >= a goto La 
; goto L1. Develop 
a SDD that does take advantage of this option when needed. 
6.7 Backpatching 
A key problem when generating code for boolean expressions and flow-of-control 
statements is that of matching a jump instruction with the target of the jump. 
For example, the translation of the boolean expression B in i f  ( 
B ) S contains 
a jump, for when B is false, to the instruction following the code for S. In a 
one-pass translation, B must be translated before S is examined. What then 
is the target of the goto that jumps over the code for S? In Section 6.6 we 
addressed this problem by passing labels as inherited attributes to where the 
relevant jump instructions were generated. But a separate pass is then needed 
to bind labels to addresses. 
This section takes a complementary approach, called backpatching, in which 
lists of jumps are passed as synthesized attributes. Specifically, when a jump 
is generated, the target of the jump is temporarily left unspecified. Each such 
jump is put on a list of jumps whose labels are to be filled in when the proper 
label can be determined. All of the jumps on a list have the same target label. 
6.7.1 One-Pass Code Generation Using Backpatching 
Backpatching can be used to generate code for boolean expressions and flow- 
of-control statements in one pass. The translations we generate will be of the 
same form as those in Section 6.6, except for how we manage labels. 
In this section, synthesized attributes truelist and falselist of nonterminal B 
are used to manage labels in jumping code for boolean expressions. In particu- 
lar, B.truelist will be a list of jump or conditional jump instructions into which 
we must insert the label to which control goes if B is true. B.falselist likewise is 
the list of instructions that eventually get the label to which control goes when 
B is false. As code is generated for B, 
jumps to the true and false exits are left 
incomplete, with the label field unfilled. These incomplete jumps are placed 
on lists pointed to by B.truelist and B.falselist, as appropriate. Similarly, a 
statement S has a synthesized attribute S.nextlist, denoting a list of jumps to 
the instruction immediately following the code for S. 
For specificity, 
we generate instructions into an instruction array, and labels 
will be indices into this array. To manipulate lists of jumps, we use three 
functions: 
1. makelist(i) creates a new list containing only i, an index into the array of 
instructions; makelist returns a pointer to the newly created list. 
2. merge(pl 
, 
p2) concatenates the lists pointed to by pl and p2 , and returns 
a pointer to the concatenated list. 
3. backpatch(p, 
i )  
inserts i as the target label for each of the instructions on 
the list pointed to by p. 
6.7.2 Backpatching for Boolean Expressions 
We now construct a translation scheme suitable for generating code for boolean 
expressions during bottom-up parsing. A marker nonterminal M in the gram- 
mar causes a semantic action to pick up, at appropriate times, the index of the 
next instruction to be generated. The grammar is as follows: 
B -+ B1 I I MB2 
1 B1 && M B2 1 ! B1 I ( B 1 )  
( El re1 E2 I true 1 false 
M + €  
The translation scheme is in Fig. 6.43. 
1) B -+ B1 I l M B2 
{ backpatch(B1.falselist, 
M.instr); 
B. 
truelist = merge(B1. 
truelist, 
B2. 
truelist); 
B. 
falselist = 
B2. 
falselist; ) 
2) B -+ B1 && M B2 { backpatch(B1 
. 
truelist, 
M. 
instr); 
B. 
truelist = 
B2 
. 
truelist; 
B. 
falselist = merge(Bl. 
falselist, 
B2 
. 
falselist); } 
3) B + ! B1 
{ B. 
truelist = 
Bl . 
falselist; 
B. 
falselist = 
Bl . 
truelist; ) 
4) 
B - + ( B 1 )  
{ B, 
truelist = 
Bl . 
truelist; 
B. 
falselist = Bl .falselist; ) 
5) 
B -+ El re1 E2 
{ B. 
truelist = makelist(nextinstr) 
; 
B. 
falselist = makelist(nextinstr 
+ 
I ) ;  
emit('ifl 
El .addr rel.op E2.addr 
'goto -I); 
emit('goto - I ) ;  ) 
6 )  B -+ true 
{ B . 
truelist = makelist(nextinstr) 
; 
emit('goto -I); 
) 
7) B -+ false 
{ B .falselist = makelist(nextinstr) 
; 
emit('goto - I ) ;  ) 
Figure 6.43: Translation scheme for boolean expressions 
Consider semantic action (1) for the production B i 
B1 I I M B2. If B1 is 
true, then B is also true, so the jumps on B1. 
truelist become part of B.truelist. 
If B1 is false, however, we must next test B2, so the target for the jumps 
412 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
Bl.falselist must be the beginning of the code generated for B2. This target is 
obtained using the marker nonterminal M .  That nonterminal produces, as a 
synthesized attribute M.instr, the index of the next instruction, just before B2 
code starts being generated. 
To obtain that instruction index, we associate with the production M -+ c 
the semantic action 
{ M. 
instr = nextinstr; } 
The variable nextinstr holds the index of the next instruction to follow. This 
value will be backpatched onto the Bl .falselist (i.e., each instruction on the 
list Bl. 
falselist will receive M.instr as its target label) when we have seen the 
remainder of the production B -+ B1 I I M B2. 
Semantic action (2) for B -+ B1 
&& M BZ 
is similar to (I). Action (3) for 
B -+ ! 
B swaps the true and false lists. Action (4) ignores parentheses. 
For simplicity, semantic action (5) generates two instructions, a conditional 
goto and an unconditional one. Neither has its target filled in. These instruc- 
tions are put on new lists, pointed to by B.truelist and B.falselist, respectively. 
Figure 6.44: Annotated parse tree for x < 100 1 I x > 200 && x ! = y 
Example 6.24 : 
Consider again the expression 
An annotated parse tree is shown in Fig. 6.44; for readability, attributes tru- 
elist, falselist, and instr are represented by their initial letters. The actions are 
performed during a depth-first traversal of the tree. Since all actions appear at 
the ends of right sides, they can be performed in conjunction with reductions 
during a bottom-up parse. In response to the reduction of x < 100 to B by 
production (5), the two instructions 
are generated. (We arbitrarily start instruction numbers at 100.) The marker 
nonterminal M in the production 
records the value of nextinstr, which at this time is 102. The reduction of 
x > 200 to B by production (5) generates the instructions 
The subexpression x > 200 corresponds to B1 in the production 
The marker nonterminal M 
records the current value of nextinstr, which is now 
104. Reducing x ! 
= y into B by production (5) generates 
We now reduce by B -+ B1 && M B2. The corresponding semantic ac- 
tion calls backpatch(B1 
.truelist, 
M.instr) to bind the true exit of Bl to the first 
instruction of B2. Since B1. 
truelist is (102) and M. 
instr is 104, this call to 
backpatch fills in 104 in instruction 102. The six instructions generated so far 
are thus as shown in Fig. 6.45(a). 
The semantic action associated with the final reduction by B -+ B1 I I M B2 
calls backpatch({101},102) which leaves the instructions as in Fig. 6.45(b). 
The entire expression is true if and only if the gotos of instructions 100 
or 104 are reached, and is false if and only if the gotos of instructions 103 or 
105 are reached. These instructions will have their targets filled in later in 
the compilation, when it is seen what must be done depending on the truth or 
falsehood of the expression. 
E
I
 
6.7.3 Flow-of-Control Statements 
We now use backpatching to translate flow-of-control statements in one pass. 
Consider statements generated by the following grammar: 
Here S denotes a statement, L a statement list, A an assignment-statement, 
and B a boolean expression. Note that there must be other productions, such as 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
(a) After backpatching 104 into instruction 102. 
(b) After backpatching 102 into instruction 101. 
Figure 6.45: Steps in the backpatch process 
those for assignment-statements. The productions given, however, are sufficient 
to illustrate the techniques used to translate flow-of-control 
statements. 
The code layout for if-, if-else-, and while-statements is the same as in 
Section 6.6. We make the tacit assumption that the code sequence in the 
instruction array reflects the natural flow of control from one instruction to the 
next. If not, then explicit jumps must be inserted to implement the natural 
sequential flow of control. 
The translation scheme in Fig. 6.46 maintains lists of jumps that are filled in 
when their targets are found. As in Fig. 6.43, boolean expressions generated by 
nonterminal B have two lists of jumps, B. 
truelist and B.falselist, corresponding 
to the true and false exits from the code for B, respectively. Statements gener- 
ated by nonterminals S and L have a list of unfilled jumps, given by attribute 
nextlist, that must eventually be completed by backpatching. S.next1ist is a list 
of all conditional and unconditional jumps to the instruction following the code 
for statement S in execution order. L.nextlist is defined similarly. 
Consider the semantic action (3) 
in Fig. 6.46. The code layout for production 
S -+ while ( B  
) S1 is as in Fig. 6.35(c). The two occurrences of the marker 
nonterminal M in the production 
S -+ while n/l; ( B 
Ad2 SI 
record the instruction numbers of the beginning of the code for B and the 
beginning of the code for S1. The corresponding labels in Fig. 6.35(c) are begin 
and B. 
true, respectively. 
1) S + 
if ( B ) M Sl { backpateh(B.truelist, 
M.instr); 
S. 
nextlist = merge(B.falselist, Sl . 
nextlist); 
) 
2) S -+ if ( B ) Ml S1 
N else M2 
S2 
{ backpatch(B. 
truelist, Ml . 
instr); 
backpatch(l3 
.falselist, M2. 
instr) 
; 
temp = merge(&. 
nextlist, N. 
nextlist) 
; 
S.nextlist = merge(temp, S2. 
nextlist); 
) 
3) S -+ while Ml ( 
B ) M2 
S1 
{ backpatch(S1. 
nextlist, Ml . 
instr) 
; 
bachpatch(B. 
truelist, M2. 
instr) 
; 
S.nextlist = B.falselist; 
emit('got 
o' MI. 
instr) 
; } 
5 )  S - + A ;  
{ S.nextlist = null; ) 
Figure 6.46: Translation of statements 
Again, the only production for M is M -+ 
6. Action (6) 
in Fig. 6.46 sets 
attribute M.instr to the number of the next instruction. After the body Sl 
of the while-statement is executed, control flows to the beginning. Therefore, 
when we reduce while MI ( 
B ) M2 
Sl to S, we backpatch Sl.nextlist to make 
all targets on that list be MI 
.instr. An explicit jump to the beginning of the 
code for B is appended after the code for S1 because control may also "fall out 
the bottom." B.truelist is backpatched to go to the beginning of Sl by making 
jumps an B. 
truelist go to M2 
. 
instr. 
A more compelling argument for using S.next1ist and L.nextlist comes when 
code is generated for the conditional statement if ( B ) S1 else S2. If control 
"falls out the bottom" of Sl, as when Sl is an assignment, we must include 
at the end of the code for S
1
 a jump over the code for S2. We use another 
marker nonterminal to generate this jump after Sl . Let nonterminal N be this 
416 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
marker with production N -+ E .  N has attribute N.nextlist, which will be a list 
consisting of the instruction number of the jump goto - that is generated by 
the semantic action (7) for N. 
Semantic action (2) in Fig. 6.46 deals with if-else-statements with the syntax 
We backpatch the jumps when B is true to the instruction Ml.instr; the latter 
is the beginning of the code for S1. Similarly, we backpatch jumps when B is 
false to go to the beginning of the code for S2. The list S.nextlist includes all 
jumps out of S1 and S2, 
as well as the jump generated by N. (Variable temp is 
a temporary that is used only for merging lists.) 
Semantic actions (8) and (9) handle sequences of statements. In 
the instruction following the code for Ll in order of execution is the beginning 
of S. Thus the Ll .nextlist list is backpatched to the beginning of the code for 
S, 
which is given by M. 
instr. In L -+ S, 
L. 
nextlist is the same as S.nextEist. 
Note that no new instructions are generated anywhere in these semantic 
rules, except for rules (3) and (7). All other code is generated by the semantic 
actions associated with assignment-st atement 
s and expressions. The flow of 
control causes the proper backpatching so that the assignments and boolean 
expression evaluations will connect properly. 
6.7.4 
Break-, Continue-, and Goto-Statements 
The most elementary programming language construct for changing the flow of 
control in a program is the goto-statement. In C, a statement like goto L 
sends 
control to the statement labeled L - 
there must be precisely one statement with 
label L in this scope. Goto-statements can be implemented by maintaining a 
list of unfilled jumps for each label and then backpatching the target when it 
is known. 
Java does away with goto-statements. However, Java does permit disci- 
plined jumps called break-statements, which send control out of an enclosing 
construct, and continue-statements, which trigger the next iteration of an en- 
closing loop. The following excerpt from a lexical analyzer illustrates simple 
break- and continue-st 
atement 
s: 
1) f o r  ( ; ; readch() ) ( 
2) 
i f (  peek == ' ' I I peek == '\t' ) continue; 
3) 
e l s e  if( 
peek == )\n) 
) l i n e  = l i n e  + 1; 
4) 
e l s e  break; 
5 )  
1 
Control jumps from the break-statement on line 4 to the next statement after 
the enclosing for loop. Control jumps from the continue-statement on line 2 to 
code to evaluate readch() and then to the if-statement on line 2. 
If S is the enclosing construct, then a break-statement is a jump to the first 
instruction after the code for S. We can generate code for the break by (1) 
keeping track of the enclosing statement S, (2) generating an unfilled jump for 
the break-statement , and (3) putting this unfilled jump on S. 
nextlist, where 
nextlist is as discussed in Section 6.7.3. 
In a two-pass front end that builds syntax trees, S.next1ist can be imple- 
mented as a field in the node for S. We can keep track of S by using the 
symbol table to map a special identifier break to the node for the enclosing 
statement S. This approach will also handle labeled break-statements in Java, 
since the symbol table can be used to map the label to the syntax-tree node for 
the enclosing construct. 
Alternatively, instead of using the symbol table to access the node for S, 
we can put a pointer to S.nextlist in the symbol table. Now, when a break- 
statement is reached, we generate an unfilled jump, look up nextlist through 
the symbol table, and add the jump to the list, where it will be backpatched as 
discussed in Section 6.7.3. 
Continue-statements can be handled in a manner analogous to the break- 
statement. The main difference between the two is that the target of the gen- 
erated jump is different. 
6.7.5 Exercises for Section 6.7 
Exercise 6.7.1 : 
Using the translation of Fig. 6.43, translate each of the fol- 
lowing expressions. Show the true and false lists for each subexpression. You 
may assume the address of the first instruction generated is 100. 
Exercise 6.7.2 : 
In Fig. 6.47(a) is the outline of a program, and Fig. 6.47(b) 
sketches the structure of the generated three-address code, using the backpatch- 
ing translation of Fig. 6.46. Here, il through i8 are the labels of the generated 
instructions that begin each of the "Code" sections. When we implement this 
translation, we maintain, for each boolean expression E, two lists of places in 
the code for E, which we denote by E.true and E.false. The places on list 
E.true are those places where we eventually put the label of the statement to 
which control must flow whenever E is true; E.false similarly lists the places 
where we put the label that control flows to when E is found to be false. Also, 
we maintain for each statement S, 
a list of places where we must put the label 
to which control flows when S is finished. Give the value (one of il through is) 
that eventually replaces each place on each of the following lists: 
(a) E3.false 
(b) S2 
.next (c) E4.false (d) Sl 
.next (e) Ez. 
true 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
while (El) { 
if (E2) 
while (E3) 
s
1
;
 
else { 
if (E4) 
s 2  
; 
s
3
 
il 
: Code for El 
i2: Code for E2 
i3: Code for E3 
i4: Code for S1 
is: Code for E4 
i6: Code for S2 
i7: Code for S3 
is: ... 
Figure 6.47: Control-flow structure of program for Exercise 6.7.2 
Exercise 6.7.3 
: 
When performing the translatiofi of Fig. 6.47 using the scheme 
of Fig. 6.46, we create lists S. 
next for each statement, starting with the assign- 
ment-statements S1, S2, 
and S3, 
and proceeding to progressively larger if- 
statements, if-else-statements, while-statements, and statement blocks. There 
are five constructed statements of this type in Fig. 6.47: 
S4: while (E3) S1. 
$6: The block consisting of S5 
and S3. 
S7: The statement if S4 else Ss. 
Sg 
: The entire program. 
For each of these constructed statements, there is a rule that allows us 
to construct &.next in terms of other Sj.next lists, and the lists Ek.true and 
Ek.false 
for the expressions in the program. Give the rules for 
(a) S4. 
next (b) S5. 
next (c) S6 
.next (d) S7 
.next (e) S8. 
next 
6.8 Switch-Statements 
The "switch" or "case" statement is available in a variety of languages. Our 
switch-statement syntax is shown in Fig. 6.48. There is a selector expression 
E, which is to be evaluated, followed by n constant values V
l
 , 
V2, 
. 
. 
- , 
Vn that 
the expression might take, perhaps including a default "value," which always 
matches the expression if no other value does. 
6.8. S 
WITCH-STATEMENTS 
switch ( E ) ( 
case Vl: S1 
case V
2
 : S2 
... 
case Vn-l: SnV1 
default: S
,
 
3 
Figure 6.48: Switch-statement syntax 
6.8.1 Translation of Switch-Statements 
The intended translation of a switch is code to: 
1. Evaluate the expression E. 
2. Find the value V
,
 
in the list of cases that is the same as the value of the 
expression. Recall that the default value matches the expression if none 
of the values explicitly mentioned in cases does. 
3. Execute the statement Sj associated with the value found. 
Step (2) is an n-way branch, which can be implemented in one of several 
ways. If the number of cases is small, say 10 at most, then it is reasonable to 
use a sequence of conditional jumps, each of which tests for an individual value 
and transfers to the code for the corresponding statement. 
A compact way to implement this sequence of conditional jumps is to create 
a table of pairs, each pair consisting of a value and a label for the corresponding 
statement's code. The value of the expression itself, paired with the label for the 
default statement is placed at the end of the table at run time. A simple loop 
generated by the compiler compares the value of the expression with each value 
in the table, being assured that if no other match is found, the last (default) 
entry is sure to match. 
If the number of values exceeds 10 or so, it is more efficient to construct a 
hash table for the values, with the labels of the various statements as entries. 
If no entry for the value possessed by the switch expression is found, a jump to 
the default statement is generated. 
There is a common special case that can be implemented even more effi- 
ciently than by an n-way branch. If the values all lie in some small range, 
say rnin to max, and the number of different values is a reasonable fraction of 
max - 
min, then we can construct an array of max - 
min "buckets," where 
bucket j - 
min contains the label of the statement with value j ;  any bucket 
that would otherwise remain unfilled contains the default label. 
To perform the switch, evaluate the expression to obtain the value j ;  check 
that it is in the range min to mas and transfer indirectly to the table entry at 
offset j - 
min. For example, if the expression is of type character, a table of, 
420 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
say, 128 
entries (depending on the character set) may be created and transferred 
through with no range testing. 
6.8.2 Syntax-Directed Translation of Switch-Statements 
The intermediate code in Fig. 6.49 is a convenient translation of the switch- 
statement in Fig. 6.48. The tests all appear at the end so that a simple code 
generator can recognize the multiway branch and generate efficient code for it, 
using the most appropriate implementation suggested at the beginning of this 
section. 
code to evaluate E into t 
goto t e s t  
L1: 
code for S1 
goto next 
: 
code for Sz 
goto next 
. . . 
L 
: 
code for Sn-1 
goto next 
L,: 
code for Sn 
goto next 
t e s t :  
i f  t = V
l
 goto L1 
i f  t = V2 goto L2 
... 
i f  t = T/,-l goto LnV1 
got0 Ln 
next : 
Figure 6.49: Translation of a switch-statement 
The more straightforward sequence shown in Fig. 6.50 would require the 
compiler to do extensive analysis to find the most efficient implementation. Note 
that it is inconvenient in a one-pass compiler to place the branching statements 
at the beginning, because the compiler could not then emit code for each of the 
statements Si as it saw them. 
To translate into the form of Fig. 6.49, when we see the keyword switch, we 
generate two new labels t e s t  and next, and a new temporary t. Then, as we 
parse the expression E ,  
we generate code to evaluate E into t. After processing 
E, we generate the jump goto t e s t .  
Then, as we see each case keyword, we create a new label Li and enter it into 
the symbol table. We place in a queue, used only to store cases, a value-label 
pair consisting of the value V, 
of the case constant and Li (or a pointer to the 
symbol-table entry for L i )  We process each statement case V, 
: Si by emitting 
the label Li attached to the code for Si7 
followed by the jump goto next. 
6.8. S 
WITCH-STATEMENTS 
code to evaluate E into t 
i f  t != Vl goto L1 
code for S
1
 
goto next 
L1: 
i f  t != V2 goto L2 
code for S2 
goto next 
L2: 
L,-2: 
i f  t != VnW1 
goto Ln-i 
code for Sn-1 
goto next 
LnVl : 
code for S, 
next : 
Figure 6.50: Another translation of a switch statement 
When the end of the switch is found, we are ready to generate the code for 
the n-way branch. Reading the queue of value-label pairs, we can generate a 
sequence of three-address statements of the form shown in Fig. 6.51. There, t 
is the temporary holding the value of the selector expression E, and L, is the 
label for the default statement. 
case t Vl L1 
case t V
2
 L2 
case t Vn-l Ln-l 
case t t L, 
label next 
Figure 6.51: Case three-address-code instructions used to translate a switch- 
statement 
The case t Vi 
Li instruction is a synonym for i f  t 
= V
i
 goto Li in Fig. 6.49, 
but the case instruction is easier for the final code generator to detect as a 
candidate for special treatment. At the code-generation phase, these sequences 
of case statements can be translated into an n-way branch of the most efficient 
type, depending on how many there are and whether the values fall into a small 
range. 
6.8.3 Exercises for Section 6.8 
! 
Exercise 6.8.1 
: In order to translate a switch-statement into a sequence of 
case-statements as in Fig. 6.51, the translator needs to create the list of value- 
422 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
label pairs, as it processes the source code for the switch. We can do so, using 
an additional translation that accumulates just the pairs. Sketch a syntax- 
direction definition that produces the list of pairs, while also emitting code for 
the statements Si 
that are the actions for each case. 
6.9 Intermediate Code for Procedures 
Procedures and their implementation will be discussed at length in Chapter 7, 
along with the run-time management of storage for names. We use the term 
function in this section for a procedure that returns a value. We briefly discuss 
function declarations and three-address code for function calls. In three-address 
code, a function call is unraveled into the evaluation of parameters in prepa- 
ration for a call, followed by the call itself. For simplicity, we assume that 
parameters are passed by value; parameter-passing methods are discussed in 
Section 1.6.6. 
Example 6.25 : 
Suppose that a is an array of integers, and that f is a function 
from integers to integers. Then, the assignment 
might translate into the following three-address code: 
1) t l = i * 4  
2) 
t 2  = a 
tl 1 
3) 
param t 2  
4) 
t 3  = call f, 1 
5) n = t 3  
The first two lines compute the value of the expression a[il into temporary 
t2, as discussed in Section 6.4. Line 3 makes ta an actual parameter for the 
call on line 4 of f with one parameter. Line 5 assigns the value returned by the 
function call to t3. 
Line 6 assigns the returned value to n. 
The productions in Fig. 6.52 allow function definitions and function calls. 
(The syntax generates unwanted commas after the last parameter, but is good 
enough for illustrating translation.) Nonterminals D and T generate declara- 
tions and types, respectively, as in Section 6.3. A function definition gener- 
ated by D consists of keyword define, a return type, the function name, for- 
mal parameters in parentheses and a function body consisting of a statement. 
Nonterminal F generates zero or more formal parameters, where a formal pa- 
rameter consists of a type followed by an identifier. Nonterminals S and E 
generate statements and expressions, respectively. The production for S 
adds a 
statement that returns the value of an expression. The production for E adds 
function calls, with actual parameters generated by A. An actual parameter is 
an expression. 
6.9. INTERMEDIATE CODE FOR PROCEDLTRES 
D + define T id ( F ) ( 
S ) 
F + 
c 
1 T i d , F  
S + return E ; 
E + i d ( A )  
A + 
€
1
 E , A  
Figure 6.52: Adding functions to the source language 
Function definitions and function calls can be translated using concepts that 
have already been introduced in this chapter. 
Function types. The type of a function must encode the return type and 
the types of the formal parameters. Let void be a special type that repre- 
sents no parameter or no return type. The type of a function pop() that 
returns an integer is therefore "function from void to integer." Function 
types can be represented by using a constructor fun applied to the return 
type and an ordered list of types for the parameters. 
Symbol tables. Let s 
be the top symbol table when the function definition 
is reached. The function name is entered into s 
for use in the rest of the 
program. The formal parameters of a function can be handled in analogy 
with field names in a record (see Fig. 6.18. In the production for D, after 
seeing define and the function name, we push s and set up a new symbol 
table 
Env.push(top); top = 
new Env(top); 
Call the new symbol table, t. Note that top is passed as a parameter in 
new Env(top), so the new symbol table t can be linked to the previous 
one, s. The new table t is used to translate the function body. We revert 
to the previous symbol table s after the function body is translated. 
Type checking. Within expressions, a function is treated like any other 
operator. The discussion of type checking in Section 6.5.2 therefore carries 
over, including the rules for coercions. For example, iff is a function with 
a parameter of type real, then the integer 2 is coerced to a real in the call 
f (2). 
Function calls. When generating three-address instructions for a function 
call id(E, 
6,. 
. 
. , 
E), 
it is sufficient to generate the three-address instruc- 
tions for evaluating or reducing the parameters E to addresses, followed 
by a param instruction for each parameter. If we do not want to mix 
the parameter-evaluating instructions with the param instructions, the 
attribute E.addr for each expression E can be saved in a data structure 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
such as a queue. Once all the expressions are translated, the param in- 
structions can be generated as the queue is emptied. 
The procedure is such an important and frequently used programming con- 
struct that it is imperative for a compiler to good code for procedure calls and 
returns. The run-time routines that handle procedure parameter passing, calls, 
and returns are part of the run-time support package. Mechanisms for run-time 
support are discussed in Chapter 7. 
6.10 Summary of Chapter 6 
The techniques in this chapter can be combined to build a simple compiler front 
end, like the one in Appendix A. The front end can be built incrementally: 
+ Pick an intermediate representation: An intermediate representation is 
typically some combination of a graphical notation and three-address 
code. As in syntax trees, a node in a graphical notation represents a 
construct; the children of a node represent its subconstructs. Three ad- 
dress code takes its name from instructions of the form x = 
y op z, with 
at most one operator per instruction. There are additional instructions 
for control flow. 
+ Translate expressions: Expressions with built-up operations can be un- 
wound into a sequence of individual operations by attaching actions to 
each production of the form E -+ El op E2. The action either creates 
a node for E with the nodes for El and E2 as children, or it generates 
a three-address instruction that applies op to the addresses for El and 
E2 and puts the result into a new temporary name, which becomes the 
address for E. 
+ Check types: The type of an expression El op Ez is determined by the 
operator op and the types of El and Ez. A coercion is an implicit type 
conversion, such as from integer to float. Intermediate code contains ex- 
plicit type conversions to ensure an exact match between operand types 
and the types expected by an operator. 
+ Use a symbol table to zmplement declarations: A declaration specifies the 
type of a name. The width of a type is the amount of storage needed for 
a name with that type. Using widths, the relative address of a name at 
run time can be computed as an offset from the start of a data area. The 
type and relative address of a name are put into the symbol table due to 
a declaration, so the translator can subsequently get them when the name 
appears in an expression. 
+ Flatten arrays: For quick access, array elements are stored in consecutive 
locations. Arrays of arrays are flattened so they can be treated as a one- 
6.11. REFERENCES FOR CHAPTER 6 
425 
dimensional array of individual elements. The type of an array is used to 
calculate the address of an array element relative to the base of the array. 
4 Generate jumping code for boolean expressions: In short-circuit or jump- 
ing code, the value of a boolean expression is implicit in the position 
reached in the code. Jumping code is useful because a boolean expression 
B is typically used for control flow, as in if (B) S. Boolean values can be 
computed by jumping to t = 
t r u e  or t = false, 
as appropriate, where t is 
a temporary name. Using labels for jumps, a boolean expression can be 
translated by inheriting labels corresponding to its true and false exits. 
The constants true and false translate into a jump to the true and false 
exits, respectively. 
4 Implement statements using control Bow: Statements can be translated 
by inheriting a label next, where next marks the first instruction after the 
code for this statement. The conditional S -+ if (B) 
S1 can be translated 
by attaching a new label marking the beginning of the code for S1 and 
passing the new label and S.next for the true and false exits, respectively, 
of B. 
4 Alternatively, use backpatching: Backpatching is a technique for generat- 
ing code for boolean expressions and statements in one pass. The idea 
is to maintain lists of incomplete jumps, where all the jump instructions 
on a list have the same target. When the target becomes known, all the 
instructions on its list are completed by filling in the target. 
4 Implement records: Field names in a record or class can be treated as a 
sequence of declarations. A record type encodes the types and relative 
addresses of the fields. A symbol table object can be used for this purpose. 
6.11 References for Chapter 6 
Most of the techniques in this chapter stem from the flurry of design and im- 
plementation activity around Algol 60. Syntax-directed translation into inter- 
mediate code was well established by the time Pascal [Ill and C [6, 9
1
 were 
created. 
UNCOL (for Universal Compiler Oriented Language) is a mythical universal 
intermediate language, sought since the mid 1950's. Given an UNCOL, com- 
pilers could be constructed by hooking a front end for a given source language 
with a back end for a given target language [lo]. The bootstrapping techniques 
given in the report [lo] are routinely used to retarget compilers. 
The UNCOL ideal of mixing and matching front ends with back ends has 
been approached in a number of ways. A retargetable compiler consists of one 
front end that can be put together with several back ends to implement a given 
language on several machines. Neliac was an early example of a language with 
a retargetable compiler [5] written in its own language. Another approach is to 
426 
CHAPTER 6. INTERMEDIATE-CODE GENERATION 
retrofit a front end for a new language onto an existing compiler. Feldman [2] 
describes the addition of a Fortran 77 front end to the C compilers [6] and 
[9]. GCC, the GNU Compiler Collection [3], supports front ends for C, C++, 
Objective-C, Fortran, Java, and Ada. 
Value numbers and their implementation by hashing are from Ershov [I]. 
The use of type information to improve the security of Java bytecodes is 
described by Gosling [4]. 
Type inference by using unification to solve sets of equations has been re- 
discovered several times; its application to ML is described by Milner [7]. See 
Pierce [8] for a comprehensive treatment of types. 
1. Ershov, A. P., "On programming of arithmetic operations," Comm. ACM 
1:8 (1958), pp. 3-6. See also Comm. ACM 1:9 (1958), p. 16. 
2. Feldman, S. I., "Implementation of a portable Fortran 77 compiler using 
modern tools," ACM SIGPLAN Notices 14:8 (1979), 
pp. 98-106 
3. GCC home page http: 
//gcc .gnu. org/, Free Software Foundation. 
4. Gosling, J., "Java intermediate bytecodes," Proc. A 
CM SIGPLA 
N Work- 
shop on Intermediate Representations (1995), pp. 11 
1-1 18. 
5 .  Huskey, H. D., M. H. Halstead, and R. McArthur, "Neliac - 
a dialect of 
Algol," Comm. A 
CM 3:8 (1960), pp. 463-468. 
6. Johnson, S. C., "A tour through the portable C compiler," Bell Telephone 
Laboratories, Inc., Murray Hill, N. J., 1979. 
7. Milner, R., "A theory of type polymorphism in programming," J. Com- 
puter and System Sciences 17:3 (1978), pp. 348-375. 
8. Pierce, B. C., Types and Programming Languages, MIT Press, Cambridge, 
Mass., 2002. 
9. Ritchie, D. M., "A tour through the UNIX C compiler," Bell Telephone 
Laboratories, Inc., Murray Hill, N. J., 1979. 
10. Strong, J., J. Wegstein, A. Tritter, J. Olsztyn, 0 .  Mock, and T. Steel, 
"The problem of programming communication with changing machines: 
a proposed solution," Comm. ACM 1:8 (1958), pp. 12-18. Part 2: 1:9 
(1958), pp. 9-15. Report of the Share Ad-Hoc committee on Universal 
Languages. 
11. Wirth, N. "The design of a Pascal compiler," Softurare-Practice 
and 
Experience 1:4 (1971), pp. 309-333. 
Chapter 7 
Run-Time Environments 
A compiler must accurately implement the abstractions embodied in the source- 
language definition. These abstractions typically include the concepts we dis- 
cussed in Section 1.6 such as names, scopes, bindings, data types, operators, 
procedures, parameters, and flow-of-control constructs. The compiler must co- 
operate with the operating system and other systems software to support these 
abstractions on the target machine. 
To do so, the compiler creates and manages a run-time environment in which 
it assumes its target programs are being executed. This environment deals with 
a variety of issues such as the layout and allocation of storage locations for the 
objects named in the source program, the mechanisms used by the target pro- 
gram to access variables, the linkages between procedures, the mechanisms for 
passing parameters, and the interfaces to the operating system, input/output 
devices, and other programs. 
The two themes in this chapter are the allocation of storage locations and 
access to variables and data. We shall discuss memory management in some 
detail, including stack allocation, heap management, and garbage collection. In 
the next chapter, we present techniques for generating target code for many 
common language constructs. 
Storage Organization 
From the perspective of the compiler writer, the executing target program runs 
in its own logical address space in which each program value has a location. The 
management and organization of this logical address space is shared between 
the compiler, operating system, and target machine. The operating system 
maps the logical addresses into physical addresses, which are usually spread 
throughout memory. 
The run-time representation of an object program in the logical address 
space consists of data and program areas as shown in Fig. 7.1. A compiler for a 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
language like C++ on an operating system like Linux might subdivide memory 
in this way. 
/ 
Static 
1 
I 
Free Memory 
/ 
Stack 
Figure 7.1: Typical subdivision of run-time memory into code and data areas 
Throughout this book, we assume the run-time storage comes in blocks of 
contiguous bytes, where a byte is the smallest unit of addressable memory. A 
byte is eight bits and four bytes form a machine word. Multibyte objects are 
stored in consecutive bytes and given the address of the first byte. 
As discussed in Chapter 6, the amount of storage needed for a name is de- 
termined from its type. An elementary data type, such as a character, integer, 
or float, can be stored in an integral number of bytes. Storage for an aggre- 
gate type, such as an array or structure, must be large enough to hold all its 
components. 
The storage layout for data objects is strongly influenced by the addressing 
constraints of the target machine. On many machines, instructions to add 
integers may expect integers to be aligned, that is, placed at an address divisible 
by 4. Although an array of ten characters needs only enough bytes to hold ten 
characters, a compiler may allocate 12 
bytes to get the proper alignment, leaving 
2 bytes unused. Space left unused due to alignment considerations is referred 
to as padding. When space is at a premium, a compiler may pack data so that 
no padding is left; additional instructions may then need to be executed at run 
time to position packed data so that it can be operated on as if it were properly 
aligned. 
The size of the generated target code is fixed at compile time, so the com- 
piler can place the executable target code in a statically determined area Code, 
usually in the low end of memory. Similarly, the size of some program data 
objects, such as global constants, and data generated by the compiler, such as 
information to support garbage collection, may be known at compile time, and 
these data objects can be placed in another statically determined area called 
Static. One reason for statically allocating as many data objects as possible is 
7.1. STORAGE ORGANIZATION 
that the addresses of these objects can be compiled into the target code. In 
early versions of Fortran, all data objects could be allocated statically. 
To maximize the utilization of space at run time, the other two areas, Stack 
and Heap, are at the opposite ends of the remainder of the address space. These 
areas are dynamic; their size can change as the program executes. These areas 
grow towards each other as needed. The stack is used to store data structures 
called activation records that get generated during procedure calls. 
In practice, the stack grows towards lower addresses, the heap towards 
higher. However, throughout this chapter and the next we shall assume that 
the stack grows towards higher addresses so that we can use positive offsets for 
notational convenience in all our examples. 
As we shall see in the next section, an activation record is used to store 
information about the status of the machine, such as the value of the program 
counter and machine registers, when a procedure call occurs. When control 
returns from the call, the activation of the calling procedure can be restarted 
after restoring the values of relevant registers and setting the program counter 
to the point immediately after the call. Data objects whose lifetimes are con- 
tained in that of an activation can be allocated on the stack along with other 
information associated with the activation. 
Many programming languages allow the programmer to allocate and deal- 
locate data under program control. For example, C has the functions malloc 
and free that can be used to obtain and give back arbitrary chunks of stor- 
age. The heap is used to manage this kind of long-lived data. Section 7.4 will 
discuss various memory-management algorithms that can be used to maintain 
the heap. 
7.1.1 Static Versus Dynamic Storage Allocation 
The layout and allocation of data to memory locations in the run-time envi- 
ronment are key issues in storage management. These issues are tricky because 
the same name in a program text can refer to multiple locations at run time. 
The two adjectives static and dynamic distinguish between compile time and 
run time, respectively. We say that a storage-allocation decision is static, if it 
can be made by the compiler looking only at the text of the program, not at 
what the program does when it executes. Conversely, a decision is dynamic if 
it can be decided only while the program is running. Many compilers use some 
combination of the following two strategies for dynamic storage allocation: 
1. Stack storage. Names local to a procedure are allocated space on a stack. 
We discuss the "run-time stack" starting in Section 7.2. The stack sup- 
ports the normal call/return policy for procedures. 
2. Heap storage. Data that may outlive the call to the procedure that cre- 
ated it is usually allocated on a "heap" of reusable storage. We discuss 
heap management starting in Section 7.4. The heap is an area of virtual 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
memory that allows objects or other data elements to obtain storage when 
they are created and to return that storage when they are invalidated. 
To support heap management, "garbage collection" enables the run-time 
system to detect useless data elements and reuse their storage, even if the pro- 
grammer does not return their space explicitly. Automatic garbage collection 
is an essential feature of many modern languages, despite it being a difficult 
operation to do efficiently; it may not even be possible for some languages. 
7.2 Stack Allocation of Space 
Almost all compilers for languages that use procedures, functions, or methods 
as units of user-defined actions manage at least part of their run-time memory 
as a stack. Each time a procedure1 is called, space for its local variables is 
pushed onto a stack, and when the procedure terminates, that space is popped 
off the stack. As we shall see, this arrangement not only allows space to be 
shared by procedure calls whose durations do not overlap in time, but it allows 
us to compile code for a procedure in such a way that the relative addresses of its 
nonlocal variables are always the same, regardless of the sequence of procedure 
calls. 
7.2.1 Activation Trees 
Stack allocation would not be feasible if procedure calls, or activations of pro- 
cedures, did not nest in time. The following example illustrates nesting of 
procedure calls. 
Example 7.1 : 
Figure 7.2 contains a sketch of a program that reads nine inte- 
gers into an array a and sorts them using the recursive quicksort algorithm. 
The main function has three tasks. It calls readArray, sets the sentinels, and 
then calls quicksort on the entire data array. Figure 7.3 suggests a sequence of 
calls that might result from an execution of the program. In this execution, the 
call to partition(l,9) returns 4, so a[l] through a[3] 
hold elements less than its 
chosen separator value v, 
while the larger elements are in a[5] through a[9]. 
In this example, as is true in general, procedure activations are nested in 
time. If an activation of procedure p calls procedure q, then that activation of 
q must end before the activation of p can end. There are three common cases: 
1. The activation of q terminates normally. Then in essentially any language, 
control resumes just after the point of p at which the call to q was made. 
2. The activation of q, or some procedure q called, either directly or indi- 
rectly, aborts; i.e., it becomes impossible for execution to continue. In 
that case, p ends simultaneously with q. 
'~ecall 
we use "procedure7' as a generic term for function, procedure, method, or subrou- 
tine. 
7.2. STACK ALLOCATION OF SPACE 
int a 
[ill 
; 
void readArray() { /* 
Reads 9 integers into a[l], 
..., 
a[9]. */ 
int i; 
. 
. . 
3 
int partition(int m y  
int n) { 
/* Picks a separator value u, and partitions a[m .. 
n] 
so that 
a[m ..p - 
1
1
 are less than u, a[p] = 
u, and a[p + 
1 
.. 
n] 
are 
equal to or greater than u. Returns p. */ 
... 
void quicksort(int m, int n) ( 
int i; 
if (n > m) ( 
i = partition(m, n); 
quicksort 
(my 
i-I) 
; 
quicksort 
(i+l, 
n) 
; 
> 
1 
main() ( 
readArray 
() ; 
a[O] = -9999; 
a[lO] = 9999; 
quicksort 
(I, 
9) 
; 
3 
Figure 7.2: Sketch of a quicksort program 
3. The activation of q terminates because of an exception that q cannot han- 
dle. Procedure p may handle the exception, in which case the activation 
of q has terminated while the activation of p continues, although not nec- 
essarily from the point at which the call to q was made. If p cannot handle 
the exception, then this activation of p terminates at the same time as the 
activation of q, and presumably the exception will be handled by some 
other open activation of a procedure. 
We therefore can represent the activations of procedures during the running 
of an entire program by a tree, called an activation tree. Each node corresponds 
to one activation, and the root is the activation of the "main" procedure that 
initiates execution of the program. At a node for an activation of procedure p, 
the children correspond to activations of the procedures called by this activation 
of p. We show these activations in the order that they are called, from left to 
right. Notice that one child must finish before the activation to its right can 
begin. 
432 
CHAPTER 7. RUN- TIME ENVIRONMENTS 
A Version of Quicksort 
The sketch of a quicksort program in Fig. 7.2 uses two auxiliary functions 
readArray and partition. The function readArray is used only to load the 
data into the array a. The first and last elements of a are not used for 
data, but rather for "sentinels" set in the main function. We assume a[O] 
is set to a value lower than any possible data value, and a[10] 
is set to a 
value higher than any data value. 
The function partition divides a portion of the array, delimited by the 
arguments rn and n, so the low elements of a[m] 
through a[n] 
are at the 
beginning, and the high elements are at the end, although neither group is 
necessarily in sorted order. We shall not go into the way partition works, 
except that it may rely on the existence of the sentinels. One possible 
algorithm for partition is suggested by the more detailed code in Fig. 9.1. 
Recursive procedure quicksort first decides if it needs to sort more 
than one element of the array. Note that one element is always "sorted," 
so quicksort has nothing to do in that case. If there are elements to sort, 
quicksort first calls partition, which returns an index i to separate the low 
and high elements. These two groups of elements are then sorted by two 
recursive calls to quicksort. 
Example 7.2 : One possible activation tree that completes the sequence of 
calls and returns suggested in Fig. 7.3 is shown in Fig. 7.4. Functions are 
represented by the first letters of their names. Remember that this tree is only 
one possibility, since the arguments of subsequent calls, and also the number of 
calls along any branch is influenced by the values returned by partition. 
The use of a run-time stack is enabled by several useful relationships between 
the activation tree and the behavior of the program: 
1. The sequence of procedure calls corresponds to a preorder traversal of the 
activation tree. 
2. The sequence of returns corresponds to a postorder traversal of the acti- 
vation tree. 
3. Suppose that control lies within a particular activation of some procedure, 
corresponding to a node N of the activation tree. Then the activations 
that are currently open (live) are those that correspond to node N and its 
ancestors. The order in which these activations were called is the order 
in which they appear along the path to N, starting at the root, and they 
will return in the reverse of that order. 
7.2. STACK ALLOCATION OF SPACE 
e n t e r  main ( 
) 
enter readArray () 
leave readArray () 
enter quicksort ( I ,  
9) 
enter partition(1,g) 
leave partition(1,g) 
enter quicksort(l,3) 
... 
leave quicksort ( l , 3 )  
enter quicksort(5,g) 
. 
. . 
leave quicksort(5,g) 
leave quicksort (l,9) 
leave main() 
Figure 7.3: Possible activations for the program of Fig. 7.2 
Figure 7.4: Activation tree representing calls during an execution of quicksort 
7.2.2 Activation Records 
Procedure calls and returns are usually managed by a run-time stack called the 
control stack. Each live activation has an activation record (sometimes called a 
frame) on the control stack, with the root of the activation tree at the bottom, 
and the entire sequence of activation records on the stack corresponding to the 
path in the activation tree to the activation where control currently resides. 
The latter activation has its record at the top of the stack. 
Example 7.3 : If control is currently in the activation q(2,3) of the tree of 
Fig. 7.4, then the activation record for q(2,3) is at the top of the control stack. 
Just below is the activation record for q(1,3), 
the parent of q(2,3) in the tree. 
Below that is the activation record q(l,9), 
and at the bottom is the activation 
record for m, the main function and root of the activation tree. 
434 
CHAPTER 7. RUN- 
TIME ENVIRONMENTS 
We shall conventionally draw control stacks with the bottom of the stack 
higher than the top, so the elements in an activation record that appear lowest 
on the page are actually closest to the top of the stack. 
The contents of activation records vary with the language being imple- 
mented. Here is a list of the kinds of data that might appear in an activation 
record (see Fig. 7.5 for a summary and possible order for these elements): 
I 
Actual parameters 
- - - - - - - - - - - - - - - -  1 
Returned values 
- - - - - - - - - - - - - - - - - -  
Control link 
Access link 
- - - - - - - - - - - - - - - - - -  
Saved machine status 
- - - - - - - - - - - - - - - - - -  
Local data 
- - - - - - - - - - - - - - - - - -  
Temporaries 
Figure 7.5: A general activation record 
1. Temporary values, such as those arising from the evaluation of expres- 
sions, in cases where those temporaries cannot be held in registers. 
2. Local data belonging to the procedure whose activation record this is. 
3. A saved machine status, with information about the state of the machine 
just before the call to the procedure. This information typically includes 
the return address (value of the program counter, to which the called 
procedure must return) and the contents of registers that were used by 
the calling procedure and that must be restored when the return occurs. 
4. An "access link" may be needed to locate data needed by the called proce- 
dure but found elsewhere, e.g., in another activation record. Access links 
are discussed in Section 7.3.5. 
5. A control link, pointing to the activation record of the caller. 
6. Space for the return value of the called function, if any. Again, not all 
called procedures return a value, and if one does, we may prefer to place 
that value in a register for efficiency. 
7. The actual parameters used by the calling procedure. Commonly, these 
values are not placed in the activation record but rather in registers, when 
possible, for greater efficiency. However, we show a space for them to be 
completely general. 
7.2. STACK ALLOCATION OF SPACE 
435 
Example 7.4: Figure 7.6 shows snapshots of the run-time stack as control 
flows through the activation tree of Fig. 7.4. Dashed lines in the partial trees 
go to activations that have ended. Since array a is global, space is allocated for 
it before execution begins with an activation of procedure main, as shown in 
Fig. 7.6(a). 
main 
main 
main 
(a) Frame for main 
integer a 11 
F T  
main 
I 
main 
I 
(b) r is activated 
integer a 11 
m 
main 
I 
main 
I 
I 
integer i 
I 
(c) r has been popped and q(1,9) pushed 
(d) Control returns to q(l,3) 
Figure 7.6: Downward-growing stack of activation records 
When control reaches the first call in the body of main, procedure r is 
activated, and its activation record is pushed onto the stack (Fig. 7.6(b)). The 
activation record for r contains space for local variable i. Recall that the top of 
stack is at the bottom of diagrams. When control returns from this activation, 
its record is popped, leaving just the record for main on the stack. 
Control then reaches the call to q (quicksort) with actual parameters 1 
and 
9, and an activation record for this call is placed on the top of the stack, as in 
Fig. 7.6(c). The activation record for q contains space for the parameters m 
and n and the local variable i, following the general layout in Fig. 7.5. Notice 
that space once used by the call of r is reused on the stack. No trace of data 
local to r will be available to q(l,9). When q(l,9) returns, the stack again has 
only the activation record for main. 
Several activations occur between the last two snapshots in Fig. 7.6. A 
recursive call to q(1,3) was made. Activations p(l,3) and q(1,O) have begun 
and ended during the lifetime of q(l,3), 
leaving the activation record for q(l,3) 
436 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
on top (Fig. 7.6(d)). Notice that when a procedure is recursive, it is normal to 
have several of its activation records on the stack at the same time. 
7.2.3 Calling Sequences 
Procedure calls are implemented by what are known as calling sequences, which 
consists of code that allocates an activation record on the stack and enters 
information into its fields. A return sequence is similar code to restore the state 
of the machine so the calling procedure can continue its execution after the call. 
Calling sequences and the layout of activation records may differ greatly, 
even among implementations of the same language. The code in a calling se- 
quence is often divided between the calling procedure (the "caller") and the 
procedure it calls (the "callee"). There is no exact division of run-time tasks 
between caller and callee; the source language, the target machine, and the op- 
erating system impose requirements that may favor one solution over another. 
In general, if a procedure is called from n different points, then the portion of 
the calling sequence assigned to the caller is generated n times. However, the 
portion assigned to the callee is generated only once. Hence, it is desirable to 
put as much of the calling sequence into the callee as possible - 
whatever the 
callee can be relied upon to know. We shall see, however, that the callee cannot 
know everything. 
When designing calling sequences and the layout of activation records, the 
following principles are helpful: 
1. Values communicated between caller and callee are generally placed at the 
beginning of the callee7s 
activation record, so they are as close as possible 
to the caller's activation record. The motivation is that the caller can 
compute the values of the actual parameters of the call and place them 
on top of its own activation record, without having to create the entire 
activation record of the callee, or even to know the layout of that record. 
Moreover, it allows for the use of procedures that do not always take 
the same number or type of arguments, such as C's printf function. 
The callee knows where to place the return value, relative to its own 
activation record, while however many arguments are present will appear 
sequentially below that place on the stack. 
2. Fixed-length items are generally placed in the middle. F'rom Fig. 7.5, such 
items typically include the control link, the access link, and the machine 
status fields. If exactly the same components of the machine status are 
saved for each call, then the same code can do the saving and restoring 
for each. Moreover, if we standardize the machine's status information, 
then programs such as debuggers will have an easier time deciphering the 
stack contents if an error occurs. 
3. Items whose size may not be known early enough are placed at the end 
of the activation record. Most local variables have a fixed length, which 
7.2. STACK ALLOCATION OF SPACE 
437 
can be determined by the compiler by examining the type of the variable. 
However, some local variables have a size that cannot be determined until 
the program executes; the most common example is a dynamically sized 
array, where the value of one of the callee's parameters determines the 
length of the array. Moreover, the amount of space needed for tempo- 
raries usually depends on how successful the code-generation phase is in 
keeping temporaries in registers. Thus, while the space needed for tem- 
poraries is eventually known to the compiler, it may not be known when 
the intermediate code is first generated. 
4. We must locate the top-of-stack pointer judiciously. A common approach 
is to have it point to the end of the fixed-length fields in the activation 
record. Fixed-length data can then be accessed by fixed offsets, known to 
the intermediate-code generator, relative to the top-of-stack pointer. A 
consequence of this approach is that variable-length fields in the activation 
records are actually "above" the top-of-stack. Their offsets need to be 
calculated at run time, but they too can be accessed from the top-of- 
stack pointer, by using a positive offset. 
\ / Parameters and returned value 
. . . . . . . . . . . . . . . . . . . . . .  
Y Control link 
Links and saved status 
. . . . . . . . . . . . . . . . . . . . . .  
i I 
Temporaries and local data 
Parameters and returned value 
v------ 
. . . . . . . . . . . . . . . . . . . . . .  
Control link 
Links and saved status 
top-sp 
. . . . . . . . . . . . . . . . . . . . . .  
Temporaries and local data 
T 
Caller's 
activation 
recrd 
responsibility 
"
l
"
 
$. 
Callee's 
activation 
record 
Callee's 
responsibility 1 
Figure 7.7: Division of tasks between caller and callee 
An example of how caller and callee might cooperate in managing the stack 
is suggested by Fig. 7.7. A register top-sp points to the end of the machine- 
status field in the current top activation record. This position within the callee's 
activation record is known to the caller, so the caller can be made responsible 
for setting top-sp before control is passed to the callee. The calling sequence 
and its division between caller and callee is as follows: 
1. The caller evaluates the actual parameters. 
438 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
2. The caller stores a return address and the old value of top-sp into the 
callee's activation record. The caller then increments top-sp to the po- 
sition shown in Fig. 7.7. That is, top-sp is moved past the caller's local 
data and temporaries and the callee's parameters and status fields. 
3. The callee saves the register values and other status information. 
4. The callee initializes its local data and begins execution. 
A suitable, corresponding return sequence is: 
1. The callee places the return value next to the parameters, as in Fig. 7.5. 
2. Using information in the machine-status field, the callee restores top-sp 
and other registers, and then branches to the return address that the 
caller placed in the status field. 
3. Although top-sp has been decremented, the caller knows where the return 
value is, relative to the current value of top-sp; the caller therefore may 
use that value. 
The above calling and return sequences allow the number of arguments of 
the called procedure to vary from call to call (e.g., as in C's printf function). 
Note that at compile time, the target code of the caller knows the number and 
types of arguments it is supplying to the callee. Hence the caller knows the size 
of the parameter area. The target code of the callee, however, must be prepared 
to handle other calls as well, so it waits until it is called and then examines the 
parameter field. Using the organization of Fig. 7.7, information describing the 
parameters must be placed next to the status field, so the callee can find it. 
For example, in the printf function of C, the first argument describes the 
remaining arguments, so once the first argument has been located, the caller 
can find whatever other arguments there are. 
7.2.4 Variable-Length Data on the Stack 
The run-time memory-management system must deal frequently with the allo- 
cation of space for objects the sizes of which are not known at compile time, 
but which are local to a procedure and thus may be allocated on the stack. In 
modern languages, objects whose size cannot be determined at compile time are 
allocated space in the heap, the storage structure that we discuss in Section 7.4. 
However, it is also possible to allocate objects, arrays, or other structures of 
unknown size on the stack, and we discuss here how to do so. The reason to 
prefer placing objects on the stack if possible is that we avoid the expense of 
garbage collecting their space. Note that the stack can be used only for an 
object if it is local to a procedure and becomes inaccessible when the procedure 
returns. 
A common strategy for allocating variable-length arrays (i.e., arrays whose 
size depends on the value of one or more parameters of the called procedure) is 
7.2. STACK ALLOCATION OF SPACE 
439 
shown in Fig. 7.8. The same scheme works for objects of any type if they are 
local to the procedure called and have a size that depends on the parameters 
of the call. 
In Fig. 7.8, procedure p has three local arrays, whose sizes we suppose cannot 
be determined at compile time. The storage for these arrays is not part of the 
activation record for p, although it does appear on the stack. Only a pointer 
to the beginning of each array appears in the activation record itself. Thus, 
when p is executing, these pointers are at known offsets from the top-of-stack 
pointer, so the target code can access array elements through these pointers. 
Figure 7.8: Access to dynamically allocated arrays 
' 
Control link and saved status 
- - - - - - - - - - - - - - - - - - - - - - - -  
. . 
. 
- - - - - - - - - - - - - - - - - - - - - - - -  
Pojnt_e: _to_ _a - - - - - - -> 
- - - - - - - - 
- - - - - - - - 
flo_iint_er: 
_to 
_b- - 
- - - - -\_ 
Also shown in Fig. 7.8 is the activation record for a procedure q, called by p. 
The activation record for q begins after the arrays of p, and any variable-length 
arrays of q are located beyond that. 
Access to the data on the stack is through two pointers, top and top-sp. 
Here, top marks the actual top of stack; it points to the position at which 
the next activation record will begin. The second, top-sp is used to find local, 
fixed-length fields of the top activation record. For consistency with Fig. 7.7, 
we shall suppose that top-sp points to the end of the machine-status field. In 
Fig. 7.8, top-sp points to the end of this field in the activation record for q. 
From there, we can find the control-link field for q, which leads us to the place 
in the activation record for p where top-sp pointed when p was on top. 
The code to reposition top and top-sp can be generated at compile time, 
Activation record 
- - - - - - - - 
Po_int_er 
t o  
c 
- - - - - - 
... 
Array a 
- - - - - - - - - - - - - - - - - - - - - - - -  
Array b -I Arrays 
fOl 
of p 
. . . . . . . . . . . . . . . . . . . . . . .  
Array c 
. . . . . . . . . . . . . . . . . . . . . . .  
I 
top-sp +- 
t 
Arrays of q 
top 
' 
Control link and saved status 
- - - - - - - - - - - - - - - - - - 
- - - - - 
Activation record for 
procedure q called by p 
440 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
in terms of sizes that will become known at run time. When q returns, top-sp 
can be restored from the saved control link in the activation record for q. The 
new value of top is (the old unrestored value of) top-sp minus the length of the 
machine-status, control and access link, return-value, and parameter fields (as 
in Fig. 7.5) in q's activation record. This length is known at compile time to 
the caller, although it may depend on the caller, if the number of parameters 
can vary across calls to q. 
7.2.5 Exercises for Section 7.2 
Exercise 7.2.1 
: 
Suppose that the program of Fig. 7.2 uses a partition function 
that always picks a[m] as the separator u. Also, when the array a[m], 
. . 
. , 
a[n] 
is reordered, assume that the order is preserved as much as possible. That is, 
first come all the elements less than u, in their original order, then all elements 
equal to v, and finally all elements greater than v, in their original order. 
a) Draw the activation tree when the numbers 9,8,7,6,5,4,3,2,1 
are sorted. 
b) What is the largest number of activation records that ever appear together 
on the stack? 
Exercise 7.2.2 
: Repeat Exercise 7.2.1 when the initial order of the numbers 
is 1,3,5,7,9,2,4,6,8. 
Exercise 7.2.3: In Fig. 7.9 is C code to compute Fibonacci numbers recur- 
sively. Suppose that the activation record for f includes the following elements 
in order: (return value, argument n, local s, local t); 
there will normally be 
other elements in the activation record as well. The questions below assume 
that the initial call is f(5). 
a) Show the complete activation tree. 
b) What does the stack and its activation records look like the first time f (1) 
is about to return? 
! 
c) What does the stack and its activation records look like the fifth time 
f (1) 
is about to return? 
Exercise 7.2.4 
: 
Here is a sketch of two C functions f and g: 
i n t  f ( i n t  x) { i n t  i ;  
return i+1; ... 
i n t  g ( i n t  y) { i n t  j; . - .  
f ( j + l )  
1 
That is, function g calls f . Draw the top of the stack, starting with the acti- 
vation record for g, after g calls f ,  and f is about to return. You can consider 
only return values, parameters, control links, and space for local variables; you 
do not have to consider stored state or temporary or local values not shown in 
the code sketch. However, you should indicate: 
7.3. ACCESS TO NONLOCAL DATA ON THE STACK 
i n t  f  ( i n t  n) ( 
i n t  t ,  s; 
i f  (n < 2) return 1; 
s = f (n-1); 
t = f (n-2) ; 
r e t u r n  s + t ;  
3 
Figure 7.9: Fibonacci program for Exercise 7.2.3 
a) Which function creates the space on the stack for each element? 
b) Which function writes the value of each element? 
c) To which activation record does the element belong? 
Exercise 7.2.5 : 
In a language that passes parameters by reference, there is a 
function f (x, 
y) that does the following: 
x  = x  + 1 ;  y  = y + 2; r e t u r n  x+y; 
If a is assigned the value 3, and then f (a, 
a) is called, what is returned? 
Exercise 7.2.6: The C function f is defined by: 
i n t  f  ( i n t  x, *py , **ppz) ( 
**ppz += 1; *py += 2; x  += 3; return x+y+z; 
Variable a is a pointer to b; variable b is a pointer to c, and c is an integer 
currently with value 4. If we call f (c, 
b, a), what is returned? 
7
.
3
 Access to Nonlocal Data on the Stack 
In this section, we consider how procedures access their data. Especially im- 
portant is the mechanism for finding data used within a procedure p but that 
does not belong to p. Access becomes more complicated in languages where 
procedures can be declared inside other procedures. We therefore begin with 
the simple case of C functions, and then introduce a language, ML, that permits 
both nested function declarations and functions as "first-class objects;" that is, 
functions can take functions as arguments and return functions as values. This 
capability can be supported by modifying the implementation of the run-time 
stack, and we shall consider several options for modifying the stack frames of 
Section 7.2. 
442 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
7.3.1 Data Access Without Nested Procedures 
In the C family of languages, all variables are defined either within a single 
function or outside any function ( 
"globally"). Most importantly, it is impossible 
to declare one procedure whose scope is entirely within another procedure. 
Rather, a global variable v has a scope consisting of all the functions that follow 
the declaration of v, except where there is a local definition of the identifier v. 
Variables declared within a function have a scope consisting of that function 
only, or part of it, if the function has nested blocks, as discussed in Section 1.6.3. 
For languages that do not allow nested procedure declarations, allocation of 
storage for variables and access to those variables is simple: 
1. Global variables are allocated static storage. The locations of these vari- 
ables remain fixed and are known at compile time. So to access any 
variable that is not local to the currently executing procedure, we simply 
use the statically determined address. 
2. Any other name must be local to the activation at the top of the stack. 
We may access these variables through the top-sp pointer of the stack. 
An important benefit of static allocation for globals is that declared proce- 
dures may be passed as parameters or returned as results (in C, a pointer to 
the function is passed), with no substantial change in the data-access strategy. 
With the C static-scoping rule, and without nested procedures, any name non- 
local to one procedure is nonlocal to all procedures, regardless of how they are 
activated. Similarly, if a procedure is returned as a result, then any nonlocal 
name refers to the storage statically allocated for it. 
7.3.2 Issues With Nested Procedures 
Access becomes far more complicated when a language allows procedure dec- 
larations to be nested and also uses the normal static scoping rule; that is, a 
procedure can access variables of the procedures whose declarations surround 
its own declaration, following the nested scoping rule described for blocks in 
Section 1.6.3. The reason is that knowing at compile time that the declaration 
of p is immediately nested within q does not tell us the relative positions of 
their activation records at run time. In fact, since either p or q or both may be 
recursive, there may be several activation records of p and/or q on the stack. 
Finding the declaration that applies to a nonlocal name x in a nested pro- 
cedure p is a static decision; it can be done by an extension of the static-scope 
rule for blocks. Suppose x is declared in the enclosing procedure q. Finding 
the relevant activation of q from an activation of p is a dynamic decision; it re- 
quires additional run-time information about activations. One possible solution 
to this problem is to use LLaccess 
links," which we introduce in Section 7.3.5. 
7.3. ACCESS TO NONLOCAL DATA ON THE STACK 
443 
7.3.3 A Language With Nested Procedure Declarations 
The C family of languages, and many other familiar languages do not support 
nested procedures, so we introduce one that does. The history of nested pro- 
cedures in languages is long. Algol 60, an ancestor of C, had this capability, 
as did its descendant Pascal, a once-popular teaching language. Of the later 
languages with nested procedures, one of the most influential is ML, and it 
is this language whose syntax and semantics we shall borrow (see the box on 
"More about ML" for some of the interesting features of ML): 
ML is a functional language, meaning that variables, once declared and 
initialized, are not changed. There are only a few exceptions, such as the 
array, whose elements can be changed by special function calls. 
Variables are defined, and have their unchangeable values initialized, by 
a statement of the form: 
v a l  (name) = (expression) 
Functions are defined using the syntax: 
fun (name) ( (arguments) 1 = (body) 
For function bodies we shall use let-statements of the form: 
l e t  (list of definitions) i n  (statements) end 
The definitions are normally v a l  or fun statements. The scope of each 
such definition consists of all following definitions, up to 
the in, 
and all the 
statements up to the end. Most importantly, function definitions can be 
nested. For example, the body of a function p can contain a let-statement 
that includes the definition of another (nested) function q. Similarly, q 
can have function definitions within its own body, leading to arbitrarily 
deep nesting of functions. 
7.3.4 
Nesting Depth 
Let us give nesting depth 1 
to procedures that are not nested within any other 
procedure. For example, all C functions are at nesting depth 1. However, if a 
procedure p is defined immediately within a procedure at nesting depth i, 
then 
give p the nesting depth i + 1. 
Example 7.5 : 
Figure 7.10 contains a sketch in ML of our running quicksort 
example. The only function at nesting depth 1 
is the outermost function, sort, 
which reads an array a of 9 integers and sorts them using the quicksort algo- 
rithm. Defined within sort, at line (2), is the array a itself. Notice the form 
444 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
More About ML 
In addition to being almost purely functional, ML presents a number of 
other surprises to the programmer who is used to C and its family. 
ML supports higher-order functions. That is, a function can take 
functions as arguments, and can construct and return other func- 
tions. Those functions, in turn, can take functions as arguments, to 
any level. 
ML has essentially no iteration, as in C7s 
for- and while-statements, 
for instance. Rather, the effect of iteration is achieved by recur- 
sion. This approach is essential in a functional language, since 
we cannot change the value of an iteration variable like i in 
"for (i=O 
; 
i<10 
; 
i++) 
" of C. Instead, ML would make i a function 
argument, and the function would call itself with progressively higher 
values of i until the limit was reached. 
ML supports lists and labeled tree structures as primitive data 
types. 
ML does not require declaration of variable types. Rather, it deduces 
types at compile time, and treats it as an error if it cannot. For 
example, v a l  x = I evidently makes x have integer type, and if we 
also see v a l  y = 2*x7 
then we know y is also an integer. 
of the ML declaration. The first argument of array says we want the array 
to have 1
1
 elements; all ML arrays are indexed by integers starting with 0, so 
this array is quite similar to the C array a from Fig. 7.2. The second argument 
of array says that initially, all elements of the array a hold the value 0. This 
choice of initial value lets the ML compiler deduce that a is an integer array, 
since 0 is an integer, so we never have to declare a type for a. 
Also declared within sort are several functions: readArray, exchange, and 
quicksort. On lines (4) and (6) we suggest that r e a d A m y  and exchange each 
access the array a. Note that in ML, array accesses can violate the functional 
nature of the language, and both these functions actually change values of a's 
elements, as in the C version of quicksort. Since each of these three functions is 
defined immediately within a function at nesting depth 1, 
their nesting depths 
are all 2. 
Lines (7) through (11) show some of the detail of quicksort. Local value v, 
the pivot for the partition, is declared at line (8). Function partition is defined 
at line (9). In line (10) we suggest that partition accesses both the array a 
and the pivot value v, and also calls the function exchange. Since p a r t  
it 
ion is 
defined immediately within a function at nesting depth 2, it is at depth 3. Line 
7.3. ACCESS TO NONLOCAL DATA ON THE STACK 
1) 
fun sort 
(inputFile, 
outputFile) = 
let 
2) 
val a = array(II,O); 
3) 
fun readArray(inputFi1e) = ... ; 
4) 
... a ... . 
5 
> 
fun exchange(i, 
j) = 
6) 
. . 
. a ... . 
7) 
fun quicksort(m,n) = 
let 
8) 
v a l v =  
... . 
9) 
fun partition(y,z) = 
10) 
. . . a . . . v . . .  
exchange . . . 
in 
11) 
. . . a . . . v . . .  
partition - - . quicksort 
end 
in 
l2> 
a - .  
- readArray .. quicksort - 
end 
; 
Figure 7.10: A version of quicksort, in ML style, using nested functions 
(11) suggests that quicksort accesses variables a and v, the function partition, 
and itself recursively. 
Line (12) suggests that the outer function sort accesses a and calls the two 
procedures readArray and quicksort. 
7.3.5 Access Links 
A direct implementation of the normal static scope rule for nested functions is 
obtained by adding a pointer called the access link to each activation record. If 
procedure p is nested immediately within procedure q in the source code, then 
the access link in any activation of p points to the most recent activation of q. 
Note that the nesting depth of q must be exactly one less than the nesting depth 
of p. Access links form a chain from the activation record at the top of the stack 
to a sequence of activations at progressively lower nesting depths. Along this 
chain are all the activations whose data and procedures are accessible to the 
currently executing procedure. 
Suppose that the procedure p at the top of the stack is at nesting depth n,, 
and p needs to access x, which is an element defined within some procedure q 
that surrounds p and has nesting depth n,. Note that n, 5 n,, with equality 
only if p and q are the same procedure. To find x, we start at the activation 
record for p at the top of the stack and follow the access link n, - 
n, times, 
from activation record to activation record. Finally, we wind up at an activation 
record for q, and it will always be the most recent (highest) activation record 
446 
CHAPTER 7. RUN- TIME ENVIRONMENTS 
for q that currently appears on the stack. This activation record contains the 
element x that we want. Since the compiler knows the layout of activation 
records, x will be found at some fixed offset from the position in q7s 
activation 
record that we can reach by following the last access link. 
Example 7.6 : 
Figure 7.11 shows a sequence of stacks that might result from 
execution of the function sort of Fig. 7.10. As before, we represent function 
names by their first letters, and we show some of the data that might appear in 
the various activation records, as well as the access link for each activation. In 
Fig. 7.11(a), we see the situation after sort has called readArray to load input 
into the array a and then called quicksort(l,9) to sort the array. The access link 
from quicksort(179) 
points to the activation record for sort, not because sort 
called quicksort but because sort is the most closely nested function surrounding 
quicksort in the program of Fig. 7.10. 
S 
- - - - - - - - 
- 
access link 
- - - - - - - - 
- 
a 
4
1
7
 
9) 
- - - - - - - 
- 
- 
access link 
- - - - - - - - 
- 
v 
q(17 3) 
- - - - - - - - - 
access link 
- - - - - - - 
- - - - - - - 
access link 
* - - - I  
access link 
Figure 7.11: Access links for finding nonlocal data 
In successive steps of Fig. 7.11 we see a recursive call to quicksort(173), 
followed by a call to partition, which calls exehange. Notice that quicksort(173)'s 
access link points to sort, for the same reason that quicksort(179)'s 
does. 
In Fig. 7.11 
(d), 
the access link for exchange bypasses the activation records 
for quicksort and partition, since exchange is nested immediately within sort. 
That arrangement is fine, since exchange needs to access only the array a, and 
the two elements it must swap are indicated by its own parameters i and j. 
7,3. ACCESS TO NONLOCAL DATA ON THE STACK 
7.3.6 Manipulating Access Links 
How are access links determined? The simple case occurs when a procedure 
call is to a particular procedure whose name is given explicitly in the procedure 
call. The harder case is when the call is to a procedure-parameter; in that 
case, the particular procedure being called is not known until run time, and the 
nesting depth of the called procedure may differ in different executions of the 
call. Thus, let us first consider what should happen when a procedure q calls 
pracedure p, explicitly. There are three cases: 
1. Procedure p is at a higher nesting depth than q. Then p must be defined 
immediately within q, or the call by q would not be at a position that 
is within the scope of the procedure name p. Thus, the nesting depth of 
p is exactly one greater than that of q, and the access link from p must 
lead to q. It is a simple matter for the calling sequence to include a step 
that places in the access link for p a pointer to the activation record of q. 
Examples include the call of quicksort by sort to set up Fig. 7.11 
(a), and 
the call of partition by quicksort to create Fig. 7.11(c). 
2. The call is recursive, that is, p = qS2 
Then the access link for the new acti- 
vation record is the same as that of the activation record below it. An ex- 
ample is the call of quicksort(l,3) by quicksort(l,9) to set up Fig. 7.11(b). 
3. The nesting depth n, of p is less than the nesting depth n, of q. In 
order for the call within q to be in the scope of name p, procedure q 
must be nested within some procedure r ,  
while p is a procedure defined 
immediately within r. The top activation record for r can therefore be 
found by following the chain of access links, starting in the activation 
record for q, for n, - 
n, + 
1 
hops. Then, the access link for p must go to 
this activation of r .  
Example 7.7 : 
For an example of case (3), notice how we go from Fig. 7.11(c) 
to Fig. 7.11(d). The nesting depth 2 of the called function exchange is one 
less than the depth 3 of the calling function partition. Thus, we start at the 
activation record for partition and follow 3 - 
2 + 
1 
= 
2 access links, which takes 
us from partition's activation record to that of quicksort(l,3) to that of sort. 
The access link for exchange therefore goes to the activation record for sort, as 
we see in Fig. 7.11(d). 
An equivalent way to discover this access link is simply to follow access links 
for n, - 
n, hops, and copy the access link found in that record. In our example, 
we would go one hop to the activation record for quicksort(l,3) and copy its 
access link to sort. Notice that this access link is correct for exchange, even 
though exchange is not in the scope of quicksort, these being sibling functions 
nested within sort. 
2~~ 
allows mutually recursive functions, which would be handled the same way. 
448 
CHAPTER 7. RUN- 
TIME ENVIRONMENTS 
7.3.7 Access Links for Procedure Parameters 
When a procedure p is passed to another procedure q as a parameter, and q then 
calls its parameter (and therefore calls p in this activation of q), it is possible 
that q does not know the context in which p appears in the program. If so, it is 
impossible for q to know how to set the access link for p. The solution to this 
problem is as follows: when procedures are used as parameters, the caller needs 
to pass, along with the name of the procedure-parameter, the proper access link 
for that parameter. 
The caller always knows the link, since if p is passed by procedure r as an 
actual parameter, then p must be a name accessible to r, and therefore, r can 
determine the access link for p exactly as if p were being called by r directly. 
That is, we use the rules for constructing access links given in Section 7.3.6. 
Example 7.8: In Fig. 7.12 we see a sketch of an ML function a that has 
functions b and c nested within it. Function b has a function-valued parameter 
f, 
which it calls. Function c defines within it a function d, and c then calls b 
with actual parameter d. 
fun a(x) = 
l e t  
fun b(f) = 
... f ... 
fun c(y) = 
l e t  
fun d(z) = ... 
i n  
. . 
. b(d) . 
. 
. 
end 
i n  ... c ( i )  ... 
end ; 
Figure 7.12: Sketch of ML program that uses function-parameters 
Let us trace what happens when a is executed. First, a calls c, so we place 
an activation record for c above that for a on the stack. The Access link for 
c points to the record for a, since c is defined immediately within a. Then c 
calls b(d). The calling sequence sets up an activation record for b, as shown in 
Fig. 7.13(a). 
Within this activation record is the actual parameter d and its access link, 
which together form the value of formal parameter f in the activation record 
for b. Notice that c knows about d, since d is defined within c, and therefore 
c passes a pointer to its own activation record as the access link. No matter 
where d was defined, if c is in the scope of that definition, then one of the three 
rules of Section 7.3.6 must apply, and c can provide the link. 
7.3. ACCESS TO NONLOCAL DATA ON THE STACK 
d 
access link 
C 
- - - - - - - - -  
access link 
Figure 7.13: Actual parameters carry their access link with them 
f : (d, 
-)- - 
Now, let us look at what b does. We know that at some point, it uses 
its parameter f ,  
which has the effect of calling d. An activation record for d 
appears on the stack, as shown in Fig. 7.13(b). The proper access link to place 
in this activation record is found in the value for parameter f; 
the link is to 
the activation record for c, since c immediately surrounds the definition of d. 
Notice that b is capable of setting up the proper link, even though b is not in 
the scope of c7s 
definition. 
/ 
- -  
7.3.8 Displays 
One problem with the access-link approach to nonlocal data is that if the nesting 
depth gets large, we may have to follow long chains of links to reach the data 
we need. A more efficient implementation uses an auxiliary array d, called the 
display, which consists of one pointer for each nesting depth. We arrange that, 
at all times, d[i] is a pointer to the highest activation record on the stack for 
any procedure at nesting depth i. Examples of a display are shown in Fig. 7.14. 
For instance, in Fig. 7.14(d), we see the display d, with d[l] holding a pointer 
to the activation record for sort, the highest (and only) activation record for a 
function at nesting depth 1. Also, d[2] holds a pointer to the activation record 
for exchange, the highest record at depth 2, and d[3] points to partition, the 
highest record at depth 3. 
The advantage of using a display is that if procedure p is executing, and 
it needs to access element x belonging to some procedure q, we need to look 
only in d[i], where i is the nesting depth of q; we follow the pointer d[i] to the 
activation record for q, wherein x is found at a known offset. The compiler 
knows what i is, so it can generate code to access x using d[i] 
and the offset of 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
Figure 7.14: Maintaining the display 
7.3. ACCESS TO NONLOCAL DATA ON THE STACK 
451 
x from the top of the activation record for q. Thus, the code never needs to 
follow a long chain of access links. 
In order to maintain the display correctly, we need to save previous values 
of display entries in new activation records. If procedure p at depth n, is called, 
and its activation record is not the first on the stack for a procedure at depth 
n,, then the activation record for p needs to hold the previous value of d[np], 
while d[n,] itself is set to point to this activation of p. When p returns, and its 
activation record is removed from the stack, we restore d[n,] to have its value 
prior to the call of p. 
Example 7.9: Several steps of manipulating the display are illustrated in 
Fig. 7.14. In Fig. 7.14(a), sort at depth 1 
has called quicksort(1, 9) at depth 2. 
The activation record for quicksort has a place to store the old value of d[2], 
indicated as saved d[2], 
although in this case since there was no prior activation 
record at depth 2, this pointer is null. 
In Fig. 7.14(b), quicksort(1, 9) calls quicksort(l,3). Since the activation 
records for both calls are at depth 2, we must store the pointer to quicksort(l,9), 
which was in d[2], in the record for quicksort(l,3). Then, d[2] is made to point 
to quicksort(173). 
Next, partition is called. This function is at depth 3, so we use the slot d[3] 
in the display for the first time, and make it point to the activation record for 
partition. The record for partition has a slot for a former value of d[3], but in 
this case there is none, so the pointer remains null. The display and stack at 
this time are shown in Fig. 7.14(c). 
Then, partition calls exchange. That function is at depth 2, so its activa- 
tion record stores the old pointer d[2], which goes to the activation record for 
quicksort(l, 
3). Notice that the display pointers "cross"; that is; d[3] points 
further down the stack than d[2] does. However, that is a proper situation; 
exchange can only access its own data and that of sort, via d[I]. 
7.3.9 Exercises for Section 7.3 
Exercise 7.3.1 : 
In Fig. 7.15 is a ML function main that computes Fibonacci 
numbers in a nonstandard way. Function f ibO will compute the nth Fibonacci 
number for any n 2 
0. Nested within in is f ibl, which computes the nth 
Fibonacci number on the assumption n > 2, and nested within f i b l  is f ib2, 
which assumes n 2 
4. Note that neither f i b l  nor f ib2 need to check for the 
basis cases. Show the stack of activation records that result from a call to main, 
up until the time that the first call (to f ib0 
(1) 
) is about to return. Show the 
access link in each of the activation records on the stack. 
Exercise 7.3.2 : Suppose that we implement the functions of Fig. 7.15 using 
a display. Show the display at the moment the first call to f ib0 
(1) 
is about to 
return. Also, indicate the saved display entry in each of the activation records 
on the stack at that time. 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
fun main () C 
l e t  
fun f ib0 
(n) = 
l e t  
fun f i b l ( n )  = 
l e t  
fun fib2(n) = f ibl(n-1) + f ibl(n-2) 
i n  
i f  n >= 4 then fib2(n) 
e l s e  fibO(n-1) + fibO(n-2) 
end 
i n  
i f  n >= 2 then f i b l ( n )  
e l s e  1 
end 
i n  
f ib0 
(4) 
end ; 
Figure 7.15: Nested functions computing Fibonacci numbers 
7.4 Heap Management 
The heap is the portion of the store that is used for data that lives indefinitely, or 
until the program explicitly deletes it. While local variables typically become 
inaccessible when their procedures end, many languages enable us to create 
objects or other data whose existence is not tied to the procedure activation 
that creates them. For example, both C++ and Java give the programmer new 
to create objects that may be passed - 
or pointers to them may be passed - 
from procedure to procedure, so they continue to exist long after the procedure 
that created them is gone. Such objects are stored on a heap. 
In this section, we discuss the memory manager, the subsystem that allo- 
cates and deallocates space within the heap; it serves as an interface between 
application programs and the operating system. For languages like C or C++ 
that deallocate chunks of storage manually (i.e., by explicit statements of the 
program, such as f r e e  or delete), 
the memory manager is also responsible for 
implementing deallocation. 
In Section 7.5, we discuss garbage collection, which is the process of finding 
spaces within the heap that are no longer used by the program and can therefore 
be reallocated to house other data items. For languages like Java, it is the 
garbage collector that deallocates memory. When it is required, the garbage 
collector is an important subsystem of the memory manager. 
7.4. HEAP MANAGEMENT 
7.4.1 
The Memory Manager 
The memory manager keeps track of all the free space in heap storage at all 
times. It performs two basic functions: 
Allocation. When a program requests memory for a variable or ~ b j e c t , ~  
the memory manager produces a chunk of contiguous heap memory of 
the requested size. If possible, it satisfies an allocation request using free 
space in the heap; if no chunk of the needed size is available, it seeks to 
increase the heap storage space by getting consecutive bytes of virtual 
memory from the operating system. If space is exhausted, the memory 
manager passes that information back to the application program. 
Deallocation. The memory manager returns deallocated space to the pool 
of free space, so it can reuse the space to satisfy other allocation requests. 
Memory managers typically do not return memory to the operating sys- 
tem, even if the program's heap usage drops. 
Memory management would be simpler if (a) all allocation requests were 
for chunks of the same size, and (b) storage were released predictably, say, 
first-allocated first-deallocated. There are some languages, such as Lisp, for 
which condition (a) holds; pure Lisp uses only one data element - 
a two- 
pointer cell - 
from which all data structures are built. Condition (b) also 
holds in some situations, the most common being data that can be allocated 
on the run-time stack. However, in most languages, neither (a) nor (b) holds 
in general. Rather, data elements of different sizes are allocated, and there is 
no good way to predict the lifetimes of all allocated objects. 
Thus, the memory manager must be prepared to service, in any order, allo- 
cation and deallocation requests of any size, ranging from one byte to as large 
as the program's entire address space. 
Here are the properties we desire of memory managers: 
Space Efficiency. A memory manager should minimize the total heap 
space needed by a program. Doing so allows larger programs to run in 
a fixed virtual address space. Space efficiency is achieved by minimizing 
"fragmentation," discussed in Section 7.4.4. 
Program Efficiency. A memory manager should make good use of the 
memory subsystem to allow programs to run faster. As we shall see in 
Section 7.4.2, the time taken to execute an instruction can vary widely 
depending on where objects are placed in memory. Fortunately, programs 
tend to exhibit "locality," a phenomenon discussed in Section 7.4.3, which 
refers to the nonrandom clustered way in which typical programs access 
memory. By attention to the placement of objects in memory, the memory 
manager can make better use of space and, hopefully, make the program 
run faster. 
3 ~ n  
what follows, we shall refer to things requiring memory space as "objects," even if they 
are not true objects in the "object-oriented programming" sense. 
CHAPTER 7. RUN- TIME ENVIRONMENTS 
Low Overhead. Because memory allocations and deallocations are fre- 
quent operations in many programs, it is important that these operations 
be as efficient as possible. That is, we wish to minimize the overhead - 
the fraction of execution time spent performing allocation and dealloca- 
tion. Notice that the cost of allocations is dominated by small requests; 
the overhead of managing large objects is less important, because it usu- 
ally can be amortized over a larger amount of computation. 
7.4.2 The Memory Hierarchy of a Computer 
Memory management and compiler optimization must be done with an aware- 
ness of how memory behaves. Modern machines are designed,so that program- 
mers can write correct programs without concerning themselves with the details 
of the memory subsystem. However, the efficiency of a program is determined 
not just by the number of instructions executed, but also by how long it takes 
to execute each of these instructions. The time taken to execute an instruction 
can vary significantly, since the time taken to access different parts of memory 
can vary from nanoseconds to milliseconds. Data-intensive programs can there- 
fore benefit significantly from optimizations that make good use of the memory 
subsystem. As we shall see in Section 7.4.3, they can take advantage of the 
phenomenon of "locality" - 
the nonrandom behavior of typical programs. 
The large variance in memory access times is due to the fundamental limi- 
tation in hardware technology; we can build small and fast storage, or large and 
slow storage, but not storage that is both large and fast. It is simply impos- 
sible today to build gigabytes of storage with nanosecond access times, which 
is how fast high-performance processors run. Therefore, practically all modern 
computers arrange their storage as a memory hierarchy. A memory hierarchy, 
as shown in Fig. 7.16, consists of a series of storage elements, with the smaller 
faster ones "closer" to the processor, and the larger slower ones further away. 
Typically, a processor has a small number of registers, whose contents are 
under software control. Next, it has one or more levels of cache, usually made 
out of static RAM, that are kilobytes to several megabytes in size. The next 
level of the hierarchy is the physical (main) memory, made out of hundreds of 
megabytes or gigabytes of dynamic RAM. The physical memory is then backed 
up by virtual memory, which is implemented by gigabytes of disks. Upon a 
memory access, the machine first looks for the data in the closest (lowest-level) 
storage and, if the data is not there, looks in the next higher level, and so on. 
Registers are scarce, so register usage is tailored for the specific applications 
and managed by the code that a compiler generates. All the other levels of the 
hierarchy are managed automatically; in this way, not only is the programming 
task simplified, but the same program can work effectively across machines 
with different memory configurations. With each memory access, the machine 
searches each level of the memory in succession, starting with the lowest level, 
until it locates the data. Caches are managed exclusively in hardware, in order 
to keep up with the relatively fast RAM access times. Because disks are rela- 
7.4. HEAP MANAGEMENT 
Typical Sizes 
Typical Access Times 
256MB - 2GB 
I 
Physical Memory 
1 
100 - 150 ns 
128KB - 4MB 
I 
2nd-Level Cache 
1 
40 - 60 ns 
3 - 1 5 m s  
> 2GB 
16 - 64KB 
I 
1st-Level Cache 
I 
Virtual Memory (Disk) 
Figure 7.16: Typical Memory Hierarchy Configurations 
32 Words 
tively slow, the virtual memory is managed by the operating system, with the 
assistance of a hardware structure known as the "translation lookaside buffer." 
Data is transferred as blocks of contiguous storage. To amortize the cost 
of access, larger blocks are used with the slower levels of the hierarchy. Be- 
Registers (Processor) 
tween main memory and cache, data is transferred in blocks known as cache 
lines, which are typically from 32 to 256 bytes long. Between virtual memory 
(disk) and main memory, data is transferred in blocks known as pages, typically 
between 4K and 64K bytes in size. 
1 
ns 
7.4.3 Locality in Programs 
Most programs exhibit a high degree of locality; that is, they spend most of 
their time executing a relatively small fraction of the code and touching only 
a small fraction of the data. We say that a program has temporal locality if 
the memory locations it accesses are likely to be accessed again within a short 
period of time. We say that a program has spatial locality if memory locations 
close to the location accessed are likely also to be accessed within a short period 
of time. 
The conventional wisdom is that programs spend 90% of their time executing 
10% of the code. Here is why: 
Programs often contain many instructions that are never executed. Pro- 
grams built with components and libraries use only a small fraction of the 
provided functionality. Also as requirements change and programs evolve, 
legacy systems often contain many instructions that are no longer used. 
456 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
Static and Dynamic RAM 
Most random-access memory is dynamic, which means that it is built of 
very simple electronic circuits that lose their charge (and thus "forget" 
the bit they were storing) in a short time. These circuits need to be 
refreshed - 
that is, their bits read and rewritten - 
periodically. On 
the other hand, static RAM is designed with a more complex circuit for 
each bit, and consequently the bit stored can stay indefinitely, until it is 
changed. Evidently, a chip can store more bits if it uses dynamic-RAM 
circuits than if it uses static-RAM circuits, so we tend to see large main 
memories of the dynamic variety, while smaller memories, like caches, are 
made from static circuits. 
Only a small fraction of the code that could be invoked is actually executed 
in a typical run of the program. For example, instructions to handle 
illegal inputs and exceptional cases, though critical to the correctness of 
the program, are seldom invoked on any particular run. 
The typical program spends most of its time executing innermost loops 
and tight recursive cycles in a program. 
Locality allows us to take advantage of the memory hierarchy of a modern 
computer, as shown in Fig. 7.16. By placing the most common instructions and 
data in the fast-but-small storage, while leaving the rest in the slow-but-large 
storage, we can lower the average memory-access 
time of a program significantly. 
It has been found that many programs exhibit both temporal and spatial 
locality in how they access both instructions and data. Data-access patterns, 
however, generally show a greater variance than instruction-access patterns. 
Policies such as keeping the most recently used data in the fastest hierarchy 
work well for common programs but may not work well for some data-intensive 
programs - 
ones that cycle through very large arrays, for example. 
We often cannot tell, just from looking at the code, which sections of the 
code will be heavily used, especially for a particular input. Even if we know 
which instructions are executed heavily, the fastest cache often is not large 
enough to hold all of them at the same time. We must therefore adjust the 
contents of the fastest storage dynamically and use it to hold instructions that 
are likely to be used heavily in the near future. 
Optimization Using the Memory Hierarchy 
The policy of keeping the most recently used instructions in the cache tends 
to work well; in other words, the past is generally a good predictor of future 
memory usage. When a new instruction is executed, there is a high proba- 
bility that the next instruction also will be executed. This phenomenon is an 
7.4. HEAP MANAGEMENT 
457 
Cache Architectures 
How do we know if a cache line is in a cache? It would be too expensive 
to check every single line in the cache, so it is common practice to restrict 
the placement of a cache line within the cache. This restriction is known 
as set associativity. A cache is k-way set associative if a cache line can 
reside only in k locations. The simplest cache is a 1-way associative cache, 
also known as a direct-mapped cache. In a direct-mapped cache, data with 
memory address n can be placed only in cache address n mod s, where s 
is the size of the cache. Similarly, a k-way set associative cache is divided 
into k sets, where a datum with address n can be mapped only to the 
location n mod (slk) in each set. Most instruction and data caches have 
associativity between 1 and 8. When a cache line is brought into the 
cache, and all the possible locations that can hold the line are occupied, 
it is typical to evict the line that has been the least recently used. 
example of spatial locality. One effective technique to improve the spatial lo- 
cality of instructions is to have the compiler place basic blocks (sequences of 
instructions that are always executed sequentially) that are likely to follow each 
other contiguously - 
on the same page, or even the same cache line, if possi- 
ble. Instructions belonging to the same loop or same function also have a high 
probability of being executed t ~ g e t h e r . ~  
We can also improve the temporal and spatial locality of data accesses in 
a program by changing the data layout or the order of the computation. For 
example, programs that visit large amounts of data repeatedly, each time per- 
forming a small amount of computation, do not perform well. It is better if we 
can bring some data from a slow level of the memory hierarchy to a faster level 
(e.g., disk to main memory) once, and perform all the necessary computations 
on this data while it resides at the faster level. This concept can be applied 
recursively to reuse data in physical memory, in the caches and in the registers. 
7.4.4 Reducing Fragmentation 
At the beginning of program execution, the heap is one contiguous unit of free 
space. As the program allocates and deallocates memory, this space is broken 
up into free and used chunks of memory, and the free chunks need not reside in 
a contiguous area of the heap. We refer to the free chunks of memory as holes. 
With each allocation request, the memory manager must place the requested 
chunk of memory into a large-enough hole. Unless a hole of exactly the right 
size is found, we need to split some hole, creating a yet smaller hole. 
4
~
s
 
a machine fetches a word in memory, it is relatively inexpensive to prefetch the next 
several contiguous words of memory as well. Thus, a common memory-hierarchy feature is 
that a multiword block is fetched from a level of memory each time that level is accessed. 
458 
CHAPTER 7. RUN-TIME ENVIRONn(lENTS 
With each deallocation request, the freed chunks of memory are added back 
to the pool of free space. We coalesce contiguous holes into larger holes, as the 
holes can only get smaller otherwise. If we are not careful, the memory may 
end up getting fragmented, consisting of large numbers of small, noncontiguous 
holes. It is then possible that no hole is large enough to satisfy a future request, 
even though there may be sufficient aggregate free space. 
Best-Fit and 
Next-Fit Object Placement 
We reduce fragmentation by controlling how the memory manager places new 
objects in the heap. It has been found empirically that a good strategy for mini- 
mizing fragmentation for real-life programs is to allocate the requested memory 
in the smallest available hole that is large enough. This best-fit algorithm tends 
to spare the large holes to satisfy subsequent, larger requests. An alternative, 
called first-fit, where an object is placed in the first (lowest-address) hole in 
which it fits, takes less time to place objects, but has been found inferior to 
best-fit in overall performance. 
To implement best-fit placement more efficiently, we can separate free space 
into bins, according to their sizes. One practical idea is to have many more bins 
for the smaller sizes, because there are usually many more small objects. For 
example, the Lea memory manager, used in the GNU C compiler gcc, aligns 
all chunks to 8-byte boundaries. There is a bin for every multiple of 8-byte 
chunks from 16 bytes to 512 bytes. Larger-sized bins are logarithmically spaced 
(i.e., the minimum size for each bin is twice that of the previous bin), and 
within each of these bins the chunks are ordered by their size. There is always 
a chunk of free space that can be extended by requesting more pages from the 
operating system. Called the wilderness chunk, this chunk is treated by Lea as 
the largest-sized bin because of its extensibility. 
Binning makes it easy to find the best-fit chunk. 
If, as for small sizes requested from the Lea memory manager, there is a 
bin for chunks of that size only, we may take any chunk from that bin. 
For sizes that do not have a private bin, we find the one bin that is 
allowed to include chunks of the desired size. Within that bin, we can use 
either a first-fit or a best-fit strategy; i.e., we either look for and select 
the first chunk that is sufficiently large or, we spend more time and find 
the smallest chunk that is sufficiently large. Note that when the fit is not 
exact, the remainder of the chunk will generally need to be placed in a 
bin with smaller sizes. 
However, it may be that the target bin is empty, or all chunks in that 
bin are too small to satisfy the request for space. In that case, we simply 
repeat the search, using the bin for the next larger size(s). Eventually, 
we either find a chunk we can use, or we reach the "wilderness7' 
chunk, 
from which we can surely obtain the needed space, possibly by going to 
the operating system and getting additional pages for the heap. 
7.4. HEAP MANAGEMENT 
459 
While best-fit placement tends to improve space utilization, it may not be 
the best in terms of spatial locality. Chunks allocated at about the same time by 
a program tend to have similar reference patterns and to have similar lifetimes. 
Placing them close together thus improves the program's spatial locality. One 
useful adaptation of the best-fit algorithm is to modify the placement in the 
case when a chunk of the exact requested size cannot be found. In this case, we 
use a next-fit strategy, trying to allocate the object in the chunk that has last 
been split, whenever enough space for the new object remains in that chunk. 
Next-fit also tends to improve the speed of the allocation operation. 
Managing and Coalescing Free Space 
When an object is deallocated manually, the memory manager must make its 
chunk free, so it can be allocated again. In some circumstances, it may also be 
possible to combine (coalesce) that chunk with adjacent chunks of the heap, to 
form a larger chunk. There is an advantage to doing so, since we can always 
use a large chunk to do the work of small chunks of equal total size, but many 
small chunks cannot hold one large object, as the combined chunk could. 
If we keep a bin for chunks of one fixed size, as Lea does for small sizes, 
then we may prefer not to coalesce adjacent blocks of that size into a chunk of 
double the size. It is simpler to keep all the chunks of one size in as many pages 
as we need, and never coalesce them. Then, a simple allocation/deallocation 
scheme is to keep a bitmap, with one bit for each chunk in the bin. A 1 
indicates 
the chunk is occupied; 0 indicates it is free. When a chunk is deallocated, we 
change its 1 
to a 0. When we need to allocate a chunk, we find any chunk with 
a 0 bit, change that bit to a I, and use the corresponding chunk. If there are 
no free chunks, we get a new page, divide it into chunks of the appropriate size, 
and extend the bit vector. 
Matters are more complex when the heap is managed as a whole, without 
binning, or if we are willing to coalesce adjacent chunks and move the resulting 
chunk to a different bin if necessary. There are two data structures that are 
useful to support coalescing of adjacent free blocks: 
Boundary Tags. At both the low and high ends of each chunk, whether 
free or allocated, m
7
e
 keep vital information. At both ends, we keep a 
free/used bit that tells whether or not the block is currently allocated 
(used) or available (free). Adjacent to each free/used bit is a count of the 
total number of bytes in the chunk. 
A Doubly Linked, Embedded Free List. The free chunks (but not the 
allocated chunks) are also linked in a doubly linked list. The pointers for 
this list are within the blocks themselves, say adjacent to the boundary 
tags at either end. Thus, no additional space is needed for the free list, 
although its existence does place a lower bound on how small chunks can 
get; they must accommodate two boundary tags and two pointers, even 
if the object is a single byte. The order of chunks on the free list is left 
460 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
unspecified. For example, the list could be sorted by size, thus facilitating 
best-fit placement. 
Example 7.10 : 
Figure 7.17 shows part of a heap with three adjacent chunks, 
A, B, and C. Chunk B, of size 100, has just been deallocated and returned to 
the free list. Since we know the beginning (left end) of B, we also know the 
end of the chunk that happens to be immediately to B's left, namely A in this 
example. The freelused bit at the right end of A is currently 0, so A too is free. 
We may therefore coalesce A and B into one chunk of 300 bytes. 
Chunk A 
Chunk B 
Chunk C 
Figure 7.17: Part of a heap and a doubly linked free list 
It might be the case that chunk C, the chunk immediately to B's right, 
is also free, in which case we can combine all of A, B, and C. Note that if 
we always coalesce chunks when we can, then there can never be two adjacent 
free chunks, so we never have to look further than the two chunks adjacent to 
the one being deallocated. In the current case, we find the beginning of C by 
starting at the left end of B, which we know, and finding the total number of 
bytes in B, which is found in the left boundary tag of B and is 100 bytes. With 
this information, we find the right end of B and the beginning of the chunk to 
its right. At that point, we examine the freelused bit of C and find that it is 1 
for used; hence, C is not available for coalescing. 
Since we must coalesce A and B, we need to remove one of them from the free 
list. The doubly linked free-list structure lets us find the chunks before and after 
each of A and B. Notice that it should not be assumed that physical neighbors 
A and B are also adjacent on the free list. Knowing the chunks preceding and 
following A and B on the free list, it is straightforward to manipulate pointers 
on the list to replace A and B by one coalesced chunk. 
Automatic garbage collection can eliminate fragmentation altogether if it 
moves all the allocated objects to contiguous storage. The interaction between 
garbage collection and memory management is discussed in more detail in Sec- 
tion 7.6.4. 
7.4.5 
Manual Deallocation Requests 
We close this section with manual memory management, where the programmer 
must explicitly arrange for the deallocation of data, as in C and C++. Ideally, 
any storage that will no longer be accessed should be deleted. Conversely, any 
storage that may be referenced must not be deleted. Unfortunately, it is hard to 
enforce either of these properties. In addition to considering the difficulties with 
7.4. 
HEAP 
MANAGEMENT 
manual deallocation, we shall describe some of the techniques programmers use 
to help with the difficulties. 
Problems with Manual Deallocation 
Manual memory management is error-prone. The common mistakes take two 
forms: failing ever to delete data that cannot be referenced is called a rnernory- 
leak error, and referencing deleted data is a dangling-pointer-dereference error. 
It is hard for programmers to tell if a program will never refer to some stor- 
age in the future, so the first common mistake is not deleting storage that will 
never be referenced. Note that although memory leaks may slow down the exe- 
cution of a program due to increased memory usage, they do not affect program 
correctness, as long as the machine does not run out of memory. Many pro- 
grams can tolerate memory leaks, especially if the leakage is slow. However, for 
long-running programs, and especially nonstop programs like operating systems 
or server code, it is critical that they not have leaks. 
Automatic garbage collection gets rid of memory leaks by deallocating all 
the garbage. Even with automatic garbage collection, a program may still use 
more memory than necessary. A programmer may know that an object will 
never be referenced, even though references to that object exist somewhere. In 
that case, the programmer must deliberately remove references to objects that 
will never be referenced, so the objects can be deallocated automatically. 
Being overly zealous about deleting objects can lead to even worse problems 
than memory leaks. The second common mistake is to delete some storage and 
then try to refer to the data in the deallocated storage. Pointers to storage that 
has been deallocated are known as dangling pointers. Once the freed storage 
has been reallocated to a new variable, any read, write, or deallocation via 
the dangling pointer can produce seemingly random effects. We refer to any 
operation, such as read, write, or deallocate, that follows a pointer and tries to 
use the object it points to, as dereferencing the pointer. 
Notice that reading through a dangling pointer may return an arbitrary 
value. Writing through a dangling pointer arbitrarily changes the value of the 
new variable. Deallocating a dangling pointer's storage means that the storage 
of the new variable may be allocated to yet another variable, and actions on 
the old and new variables may conflict with each other. 
Unlike memory leaks, dereferencing a dangling pointer after the freed storage 
is reallocated almost always creates a program error that is hard to debug. As 
a result, programmers are more inclined not to deallocate a variable if they are 
not certain it is unreferencable. 
A related form of programming error is to access an illegal 
.address. Common 
examples of such errors include dereferencing null pointers and accessing an 
out-of-bounds array element. It is better for such errors to be detected than to 
have the program silently corrupt the results. In fact, many security violations 
exploit programming errors of this type, where certain program inputs allow 
unintended access to data, leading to a "hacker" taking control of the program 
462 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
An Example: Purify 
Rational's Purify is one of the most popular commercial tools that helps 
programmers find memory access errors and memory leaks in programs. 
Purify instruments binary code by adding additional instructions to check 
for errors as the program executes. It keeps a map of memory to indicate 
where all the freed and used spaces are. Each allocated object is bracketed 
with extra space; accesses to unallocated locations or to spaces between 
objects are flagged as errors. This approach finds some dangling pointer 
references, but not when the memory has been reallocated and a valid 
object is sitting in its place. This approach also finds some out-of-bound 
array accesses, if they happen to land in the space inserted at the end of 
the objects. 
Purify also finds memory leaks at the end of a program execution. It 
searches the contents of all the allocated objects for possible pointer values. 
Any object without a pointer to it is a leaked chunk of memory. Purify 
reports the amount of memory leaked and the locations of the leaked 
objects. We may compare Purify to a "conservative garbage collector," 
which will be discussed in Section 7.8.3. 
and machine. One antidote is to have the compiler insert checks with every 
access, to make sure it is within bounds. The compiler's optimizer can discover 
and remove those checks that are not really necessary because the optimizer 
can deduce that the access must be within bounds. 
Programming Conventions and Tools 
We now present a few of the most popular conventions and tools that have been 
developed to help programmers cope with the complexity in managing memory: 
Object ownership is useful when an object's lifetime can be statically rea- 
soned about. The idea is to associate an owner with each object at all 
times. The owner is a pointer to that object, presumably belonging to 
some function invocation. The owner (i.e., its function) is responsible for 
either deleting the object or for passing the object to another owner. It 
is possible to have other, nonowning pointers to the same object; these 
pointers can be overwritten any time, and no deletes should ever be ap- 
plied through them. This convention eliminates memory leaks, as well as 
attempts to delete the same object twice. However, it does not help solve 
the dangling-pointer-reference problem, because it is possible to follow a 
nonowning pointer to an object that has been deleted. 
Reference counting is useful when an object's lifetime needs to be deter- 
mined dynamically. The idea is to associate a count with each dynamically 
7.5. INTRODUCTION TO GARBAGE COLLECTION 
463 
allocated object. Whenever a reference to the object is created, we incre- 
ment the reference count; whenever a reference is removed, we decrement 
the reference count. When the count goes to zero, the object can no longer 
be referenced and can therefore be deleted. This technique, however, does 
not catch useless, circular data structures, where a collection of objects 
cannot be accessed, but their reference counts are not zero, since they 
refer to each other. For an illustration of this problem, see Example 7.11. 
Reference counting does eradicate all dangling-pointer references, since 
there are no outstanding references to any deleted objects. Reference 
counting is expensive because it imposes an overhead on every operation 
that stores a pointer. 
Region-based allocation is useful for collections of objects whose lifetimes 
are tied to specific phases in a computation.When objects are created to 
be used only within some step of a computation, we can allocate all such 
objects in the same region. We then delete the entire region once that 
computation step completes. This region-based allocation technique has 
limited applicability. However, it is very efficient whenever it can be used; 
instead of deallocating objects one at a time, it deletes all objects in the 
region in a wholesale fashion. 
7.4.6 
Exercises for Section 7.4 
Exercise 7.4.1 : 
Suppose the heap consists of seven chunks, starting at address 
0. The sizes of the chunks, in order, are 80, 30, 60, 50, 70, 20, 40 bytes. When 
we place an object in a chunk, we put it at the high end if there is enough 
space remaining to form a smaller chunk (so that the smaller chunk can easily 
remain on the linked list of free space). However, we cannot tolerate chunks 
of fewer that 8 bytes, so if an object is almost as large as the selected chunk, 
we give it the entire chunk and place the object at the low end of the chunk. 
If we request space for objects of the following sizes: 32, 64, 48, 16, in that 
order, what does the free space list look like after satisfying the requests, if the 
method of selecting chunks is 
a) First fit. 
b) Best fit. 
7.5 
Introduction to Garbage Collection 
Data that cannot be referenced is generally known as garbage. Many high-level 
programming languages remove the burden of manual memory management 
from the programmer by offering automatic garbage collection, which deallo- 
cates unreachable data. Garbage collection dates back to the initial implemen- 
tation of Lisp in 1958. Other significant languages that offer garbage collection 
include Java, Perl, ML, Modula-3, Prolog, and Smalltalk. 
464 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
In this section, we introduce many of the concepts of garbage collection. 
The notion of an object being "reachable" is perhaps intuitive, but we need to 
be precise; the exact rules are discussed in Section 7.5.2. We also discuss, in 
Section 7.5.3, a simple, but imperfect, method of automatic garbage collection: 
reference counting, which is based on the idea that once a program has lost all 
references to an object, it simply cannot and so will not reference the storage. 
Section 7.6 covers trace-based collectors, which are algorithms that discover 
all the objects that are still useful, and then turn all the other chunks of the 
heap into free space. 
7.5.1 Design Goals for Garbage Collectors 
Garbage collection is the reclamation of chunks of storage holding objects that 
can no longer be accessed by a program. We need to assume that objects have 
a type that can be determined by the garbage collector at run time. From the 
type information, we can tell how large the object is and which components of 
the object contain references (pointers) to other objects. We also assume that 
references to objects are always to the address of the beginning of the object, 
never pointers to places within the object. Thus, all references to an object 
have the same value and can be identified easily. 
A user program, which we shall refer to as the mutator, modifies the col- 
lection of objects in the heap. The mutator creates objects by acquiring space 
from the memory manager, and the mutator may introduce and drop references 
to existing objects. Objects become garbage when the mutator program cannot 
"reach77 
them, in the sense made precise in Section 7.5.2. The garbage collector 
finds these unreachable objects and reclaims their space by handing them to 
the memory manager, which keeps track of the free space. 
A Basic Requirement: Type Safety 
Not all languages are good candidates for automatic garbage collection. For a 
garbage collector to work, it must be able to tell whether any given data element 
or component of a data element is, or could be used as, a pointer to a chunk of 
allocated memory space. A language in which the type of any data component 
can be determined is said to be type safe. There are type-safe languages like 
ML, for which we can determine types at compile time. There are other type- 
safe languages, like Java, whose types cannot be determined at compile time, 
but can be determined at run time. The latter are called dynamically typed 
languages. If a language is neither statically nor dynamically type safe, then it 
is said to be unsafe. 
Unsafe languages, which unfortunately include some of the most impor- 
tant languages such as C and C++, are bad candidates for automatic garbage 
collection. In unsafe languages, memory addresses can be manipulated arbi- 
trarily: arbitrary arithmetic operations can be applied to pointers to create 
new pointers, and arbitrary integers can be cast as pointers. Thus a program 
7.5. INTRODUCTION TO GARBAGE COLLECTION 
theoretically could refer to any location in memory at any time. Consequently, 
no memory location can be considered to be inaccessible, and no storage can 
ever: be reclaimed safely. 
In practice, most C and C++ programs do not generate pointers arbitrarily, 
and a theoretically unsound garbage collector that works well empirically has 
been developed and used. We shall discuss conservative garbage collection for 
C and C++ in Section 7.8.3. 
Performance Metrics 
Garbage collection is often so expensive that, although it was invented decades 
ago and absolutely prevents memory leaks, it has yet to be adopted by many 
mainstream programming languages. Many different approaches have been pro- 
posed over the years, and there is not one clearly best garbage-collection algo- 
rithm. Before exploring the options, let us first enumerate the performance 
metrics that must be considered when designing a garbage collector. 
Overall Execution Time. Garbage collection can be very slow. It is impor- 
tant that it not significantly increase the total run time of an application. 
Since the garbage collector necessarily must touch a lot of data, its perfor- 
mance is determined greatly by how it leverages the memory subsystem. 
Spa& Usage. It is important that garbage collection avoid fragmentation 
and make the best use of the available memory. 
Pause Time. Simple garbage collectors are notorious for causing pro- 
grams - 
the mutators - 
to pause suddenly for an extremely long time, 
as garbage collection kicks in without warning. Thus, besides minimiz- 
ing the overall execution time, it is desirable that the maximum pause 
time be minimized. As an important special case, real-time applications 
require certain computations to be completed within a time limit. We 
must either suppress garbage collection while performing real-time tasks, 
or restrict maximum pause time. Thus, garbage collection is seldom used 
in real-time applications. 
Program Locality. We cannot evaluate the speed of a garbage collector 
solely by its running time. The garbage collector controls the placement 
of data and thus influences the data locality of the mutator program. It 
can improve a mutator's temporal locality by freeing up space and reusing 
it; it can improve the mutator's spatial locality by relocating data used 
together in the same cache or pages. 
Some of these design goals conflict with one another, and tradeoffs must be 
made carefully by considering how programs typically behave. Also objects of 
different characteristics may favor different treatments, requiring a collector to 
use different techniques for different kinds of objects. 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
For example, the number of objects allocated is dominated by small objects, 
so allocation of small objects must not incur a large overhead. On the other 
hand, consider garbage collectors that relocate reachable objects. Relocation is 
expensive when dealing with large objects, but less so with small objects. 
As another example, in general, the longer we wait to collect garbage in a 
trace-based collector, the larger the fraction of objects that can be collected. 
The reason is that objects often "die young," so if we wait a while, many of 
the newly allocated objects will become unreachable. Such a collector thus 
costs less on the average, per unreachable object collected. On the other hand, 
infrequent collection increases a program's memory usage, decreases its data 
locality, and increases the length of the pauses. 
In contrast, a reference-counting collector, by introducing a constant over- 
head to many of the mutator's operations, can slow down the overall execution 
of a program significantly. On the other hand, reference counting does not cre- 
ate long pauses, and it is memory efficient, because it finds garbage as soon 
as it is produced (with the exception of certain cyclic structures discussed in 
Section 7.5.3). 
Language design can also affect the characteristics of memory usage. Some 
languages encourage a programming style that generates a lot of garbage. For 
example, programs in functional or almost functional programming languages 
create more objects to avoid mutating existing objects. In Java, all objects, 
other than base types like integers and references, are allocated on the heap 
and not the stack, even if their lifetimes are confined to that of one function 
invocation. This design frees the programmer from worrying about the lifetimes 
of variables, at the expense of generating more garbage. Compiler optimizations 
have been developed to analyze the lifetimes of variables and allocate them on 
the stack whenever possible. 
7.5.2 Reachability 
We refer to all the data that can be accessed directly by a program, without 
having to dereference any pointer, as the root set. For example, in Java the root 
set of a program consists of all the static field members and all the variables 
on its stack. A program obviously can reach any member of its root set at 
any time. Recursively, any object with a reference that is stored in the field 
members or array elements of any reachable object is itself reachable. 
Reachability becomes a bit more complex when the program has been op- 
timized by the compiler. First, a compiler may keep reference variables in 
registers. These references must also be considered part of the root set. Sec- 
ond, even though in a type-safe language programmers do not get to manipulate 
memory addresses directly, a compiler often does so for the sake of speeding up 
the code. Thus, registers in compiled code may point to the middle of an object 
or an array, or they may contain a value to which an offset will be applied to 
compute a legal address. Here are some things an optimizing compiler can do 
to enable the garbage collector to find the correct root set: 
7.5. INTRODUCTION TO GARBAGE COLLECTION 
The compiler can restrict the invocation of garbage collection to only 
certain code points in the program, when no "hidden" references exist. 
The compiler can write out information that the garbage collector can 
use to recover all the references, such as specifying which registers contain 
references, or how to compute the base address of an object that is given 
an internal address. 
The compiler can assure that there is a reference to the base address of 
all reachable objects whenever the garbage collector may be invoked. 
The set of reachable objects changes as a program executes. It grows as new 
objects get created and shrinks as objects become unreachable. It is important 
to remember that once an object becomes unreachable, it cannot become reach- 
able again. There are four basic operations that a mutator performs to change 
the set of reachable objects: 
Object Allocations. These are performed by the memory manager, which 
returns a reference to each newly allocated chunk of memory. This oper- 
ation adds members to the set of reachable objects. 
Parameter Passing and Return Values. References to objects are passed 
from the actual input parameter to the corresponding formal parameter, 
and from the returned result back to the callee. Objects pointed to by 
these references remain reachable. 
Reference Assignments. Assignments of the form u = v, where u and v 
are references, have two effects. First, u is now a reference to the object 
referred to by v. As long as u is reachable, the object it refers to is surely 
reachable. Second, the original reference in u is lost. If this reference is 
the last to some reachable object, then that object becomes unreachable. 
Any time an object becomes unreachable, all objects that are reachable 
only through references contained in that object also become unreachable. 
Procedure Returns. As a procedure exits, the frame holding its local 
variables is popped off the stack. If the frame holds the only reachable 
reference to any object, that object becomes unreachable. Again, if the 
now unreachable objects hold the only references to other objects, they 
too become unreachable, and so on. 
In summary, new objects are introduced through object allocations. Param- 
eter passing and assignments can propagate reachability; assignments and ends 
of procedures can terminate reachability. As an object becomes unreachable, it 
can cause more objects to become unreachable. 
There are two basic ways to find unreachable objects. Either we catch the 
transitions as reachable objects turn unreachable, or we periodically locate all 
the reachable objects and then infer that all the other objects are unreachable. 
Reference counting, introduced in Section 7.4.5, is a well-known approximation 
468 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
Survival of Stack Objects 
When a procedure is called, a local variable v, whose object is allocated 
on the stack, may have pointers to v placed in nonlocal variables. These 
pointers will continue to exist after the procedure returns, yet the space for 
v disappears, resulting in a dangling-reference situation. Should we ever 
allocate a local like v on the stack, as C does for example? The answer 
is that the semantics of many languages requires that local variables cease 
to exist when their procedure returns. Retaining a reference to such a 
variable is a programming error, and the compiler is not required to fix 
the bug in the program. 
to the first approach. We maintain a count of the references to an object, as 
the mutator performs actions that may change the reachability set. When the 
count goes to zero, the object becomes unreachable. We discuss this approach 
in more detail in Section 7.5.3. 
The second approach computes reachability by tracing all the references 
transitively. A trace-based garbage collector starts by labeling ("marking") all 
objects in the root set as "reachable," examines iteratively all the references 
in reachable objects to find more reachable objects, and labels them as such. 
This approach must trace all the references before it can determine any object 
to be unreachable. But once the reachable set is computed, it can find many 
unreachable objects all at once and locate a good deal of free storage at the same 
time. Because all the references must be analyzed at the same time, we have 
an option to relocate the reachable objects and thereby reduce fragmentation. 
There are many different trace-based algorithms, and we discuss the options in 
Sections 7.6 and 7.7.1. 
7.5.3 Reference Counting Garbage Collectors 
We now consider a simple, although imperfect, garbage collector, based on 
reference counting, which identifies garbage as an object changes from being 
reachable to unreachable; the object can be deleted when its count drops to 
zero. With a reference-counting garbage collector, every object must have a 
field for the reference count. Reference counts can be maintained as follows: 
1. Object Allocation. The reference count of the new object is set to 1. 
2. Parameter Passing. The reference count of each object passed into a 
procedure is incremented. 
3. Reference Assignments. For statement u = v, where u and v are refer- 
ences, the reference count of the object referred to by v goes up by one, 
and the count for the old object referred to by u goes down by one. 
7.5. INTRODUCTION TO GARBAGE COLLECTION 
469 
4. Procedure Returns. As a procedure exits, all the references held by the 
local variables of that procedure activation record must also be decre- 
mented. If several local variables hold references to the same object, that 
object's count must be decremented once for each such reference. 
5. Transitive Loss o
f
 Reachability. Whenever the reference count of an object 
becomes zero, we must also decrement the count of each object pointed 
to by a reference within the object. 
Reference counting has two main disadvantages: it cannot collect unreach- 
able, cyclic data structures, and it is expensive. Cyclic data structures are quite 
plausible; data structures often point back to their parent nodes, or point to 
each other as cross references. 
Example 7.11 : 
Figure 7.18 shows three objects with references among them, 
but no references from anywhere else. If none of these objects is part of the 
root set, then they are all garbage, but their reference counts are each greater 
than 0. Such a situation is tantamount to a memory leak if we use reference 
counting for garbage collection, since then this garbage and any structures like 
it are never deallocated. 
_ - - -  
- - - - - - - -  
- - - - 
No pointers 
outside 
Figure 7.18: An unreachable, cyclic data structure 
The overhead of reference counting is high because additional operations are 
introduced with each reference assignment, and at procedure entries and exits. 
This overhead is proportional to the amount of computation in the program, and 
not just to the number of objects in the system. Of particular concern are the 
updates made to references in the root set of a program. The concept of deferred 
reference counting has been proposed as a means to eliminate the overhead 
associated with updating the reference counts due to local stack accesses. That 
is, reference counts do not include references from the root set of the program. 
An object is not considered to be garbage until the entire root set is scanned 
and no references to the object are found. 
The advantage of reference counting, on the other hand, is that garbage col- 
lection is performed in an incremental fashion. Even though the total overhead 
can be large, the operations are spread throughout the mutator's computation. 
470 
CHAPTER 7. RUN- TIME ENVIRONMENTS 
Although removing one reference may render a large number of objects un- 
reachable, the operation of recursively modifying reference counts can easily be 
deferred and performed piecemeal across time. Thus, reference counting is par- 
ticularly attractive algorithm when timing deadlines must be met, as well as for 
interactive applications where long, sudden pauses are unacceptable. Another 
advantage is that garbage is collected immediately, keeping space usage low. 
Figure 7.19: A network of objects 
7.5.4 Exercises for Section 7.5 
Exercise 7.5.1 : 
What happens to the reference counts of the objects in Fig. 
7.19 if: 
a) The pointer from A to B is deleted. 
b) The pointer from X to A is deleted. 
c) The node C is deleted. 
Exercise 7.5.2 : 
What happens to reference counts when the pointer from A 
to D in Fig. 7.20 is deleted? 
7.6 Introduction to Trace-Based Collection 
Instead of collecting garbage as it is created, trace-based collectors run periodi- 
cally to find unreachable objects and reclaim their space. Typically, we run the 
7.6. INTRODUCTION TO TRACE-BASED COLLECTION 
Figure 7.20: Another network of objects 
trace-based collector whenever the free space is exhausted or its amount drops 
below some threshold. 
We begin this section by introducing the simplest "mark-and-sweep" gar- 
bage collection algorithm. We then describe the variety of trace-based algo- 
rithms in terms of four states that chunks of memory can be put in. This 
section also contains a number of improvements on the basic algorithm, includ- 
ing those in which object relocation is a part of the garbage-collection function. 
7.6.1 A Basic Mark-and-Sweep Collector 
Mark-and-sweep garbage-collection algorithms are straightforward, stop-the- 
world algorithms that find all the unreachable objects, and put them on the list 
of free space. Algorithm 7.12 visits and "marks" all the reachable objects in the 
first tracing step and then "sweeps" the entire heap to free up unreachable ob- 
jects. Algorithm 7.14, which we consider after introducing a general framework 
for trace-based algorithms, is an optimization of Algorithm 7.12. By using an 
additional list to hold all the allocated objects, it visits the reachable objects 
only once. 
Algorithm 7.12 : 
Mark-and-sweep garbage collection. 
INPUT: A root set of objects, a heap, and a free list, called Free, with all the 
unallocated chunks of the heap. As in Section 7.4.4, all chunks of space are 
marked with boundary tags to indicate their free/used status and size. 
OUTPUT: A modified Free list after all the garbage has been removed. 
METHOD: The algorithm, shown in Fig. 7.21, uses several simple data struc- 
tures. List Free holds objects known to be free. A list called Unscanned, holds 
objects that we have determined are reached, but whose successors we have not 
yet considered. That is, we have not scanned these objects to see what other 
CHAPTER 7. RUN-TME ENVIRONMENTS 
/* 
marking phase */ 
set the reached-bit to 1 
and add to list Unscanned each object 
referenced by the root set; 
while ( Unscanned # 0) { 
remove some object o from Unscanned; 
for (each object o' referenced in o) { 
if (0' is unreached; i.e., its reached-bit is 0) { 
set the reached-bit of o' to 1; 
put o' in Unscanned; 
1 
I 
I 
/* 
sweeping phase */ 
Free = 0; 
for (each chunk of memory o in the heap) { 
if (o is unreached, i.e., its reached-bit is 0) add o to Free; 
else set the reached-bit of o to 0; 
3 
Figure 7.21: A Mark-and-Sweep Garbage Collector 
objects can be reached through them. The Unscanned list is empty initially. 
Additionally, each object includes a bit to indicate whether it has been reached 
(the reached- 
bit). Before the algorithm begins, all allocated objects have the 
reached-bit set to 0. 
In line (1) 
of Fig. 7.21, we initialize the Unscanned list by placing there all 
the objects referenced by the root set. The reached-bit for these objects is also 
set to 1. Lines (2) through (7) are a loop, in which we, in turn, examine each 
object o that is ever placed on the Unscanned list. 
The for-loop of lines (4) through (7) implements the scanning of object o. 
We examine each object o' for which we find a reference within o. If o' has 
already been reached (its reached-bit is I), 
then there is no need to do anything 
about 0
'
;
 
it either has been scanned previously, or it is on the Unscanned list 
to be scanned later. However, if o' was not reached already, then we need to 
set its reached-bit to 1 
in line (6) and add o' to the Unscanned list in line (7). 
Figure 7.22 illustrates this process. It shows an Unscanned list with four objects. 
The first object on this list, corresponding to object o in the discussion above, 
is in the process of being scanned. The dashed lines correspond to the three 
kinds of objects that might be reached from o: 
1. A previously scanned object that need not be scanned again. 
2. An object currently on the Unscanned list. 
3. An item that is reachable, but was previously thought to be unreached. 
7.6. INTRODUCTION TO TRACE-BASED COLLECTION 
Free and unreached objects 
reached bit = 0 
Unscanned and previously scanned objects 
reached bit = 1 
Figure 7.22: The relationships among objects during the marking phase of a 
mark-and-sweep garbage collector 
Lines (8) through (ll), 
the sweeping phase, reclaim the space of all the 
objects that remain unreached at the end of the marking phase. Note that 
these will include any objects that were on the Free list originally. Because the 
set of unreached objects cannot be enumerated directly, the algorithm sweeps 
through the entire heap. Line (10) puts free and unreached objects on the 
Free list, one at a time. Line (11) handles the reachable objects. We set their 
reached-bit to 0, in order to maintain the proper preconditions for the next 
execution of the garbage-collection algorithm. 
7.6.2 
Basic Abstraction 
All trace-based algorithms compute the set of reachable objects and then take 
the complement of this set. Memory is therefore recycled as follows: 
a) The program or mutator runs and makes allocation requests. 
b) The garbage collector discovers reachability by tracing. 
c) The garbage collector reclaims the storage for unreachable objects. 
This cycle is illustrated in Fig. 7.23 in terms of four states for chunks of memory: 
Free, Unreached, Unscanned, and Scanned. The state of a chunk might be stored 
in the chunk itself, or it might be implicit in the data structures used by the 
garbage-collection algorithm. 
While trace- 
based algorithms may differ in their implement 
ation, they can 
all be described in terms of the following states: 
1. Free. A chunk is in the Free state if it is ready to be allocated. Thus, a 
Free chunk must not hold a reachable object. 
2. Unreached. Chunks are presumed unreachable, unless proven reachable by 
tracing. A chunk is in the Unreached state at any point during garbage 
CHAPTER 7. RUN-'TIME ENVIRONMENTS 
allocate 
Free 
(a) Before tracing: action of mutator 
pointers 
root set 
(b) Discovering reachability by tracing 
- - ' ready for 
next collection 
(c) Reclaiming storage 
Figure 7.23: States of memory in a garbage collection cycle 
collection if its reachability has not yet been established. Whenever a 
chunk is allocated by the memory manager, its state is set to Unreached 
as illustrated in Fig. 7.23(a). Also, after a round of garbage collection, 
the state of a reachable object is reset to Unreached to get ready for the 
next round; see the transition from Scanned to Unreached, which is shown 
dashed to emphasize that it prepares for the next round. 
3. Unscanned, Chunks that are known to be reachable are either in state 
Unscanned or state Scanned. A chunk is in the Unscanned state if it is 
known to be reachable, but its pointers have not yet been scanned. The 
transition to Unscanned from Unreached occurs when we discover that a 
chunk is reachable; see Fig. 7.23(b). 
4. Scanned. Every Unscanned object will eventually be scanned and tran- 
sition to the Scanned state. To scan an object, we examine each of the 
pointers within it and follow those pointers to the objects to which they 
refer. If a reference is to an Unreached object, then that object is put in 
the Unscanned 
state. When the scan of an object is completed, that object 
is placed in the Scanned state; see the lower transition in Fig. 7.23(b). A 
Scanned object can only contain references to other Scanned or Unscanned 
objects, and never to Unreached objects. 
7.6. INTRODUCTION TO TRACE-BASED COLLECTION 
475 
When no objects are left in the Unscanned state, the computation of reach- 
ability is complete. Objects left in the Unreached state at the end are truly 
unreachable. The garbage collector reclaims the space they occupy and places 
the chunks in the Free state, as illustrated by the solid transition in Fig. 7.23(c). 
To get ready for the next cycle of garbage collection, objects in the Scanned state 
are returned to the Unreached state; see the dashed transition in Fig. 7.23(c). 
Again, remember that these objects really are reachable right now. The Un- 
reachable state is appropriate because we shall want to start all objects out 
in this state when garbage collection next begins, by which time any of the 
currently reachable objects may indeed have been rendered unreachable. 
Example 7.13: Let us see how the data structures of Algorithm 7.12 relate 
to the four states introduced above. Using the reached-bit and membership on 
lists Free and Unscanned, we can distinguish among all four states. The table 
of Fig. 7.24 summarizes the characterization of the four states in terms of the 
data structure for Algorithm 7.12. 
Figure 7.24: Representation of states in Algorithm 7.12 
STATE 
Free 
Unreached 
Unscanned 
Scanned 
7.6.3 Optimizing Mark-and-Sweep 
ON Free 
ON Unscanned 
REACHED-BIT 
Yes 
No 
0 
No 
No 
0 
No 
Yes 
1 
No 
No 
1 
The final step in the basic mark-and-sweep algorithm is expensive because there 
is no easy way to find only the unreachable objects without examining the entire 
heap. An improved algorithm, due to Baker, keeps a list of all allocated objects. 
To find the set of unreachable objects, which we must return to free space, we 
take the set difference of the allocated objects and the reached objects. 
Algorithm 7.14 : 
Baker's mark-and-sweep collector. 
\ 
INPUT: A root set of objects, a heap, a free list Free, and a list of allocated 
objects, which we refer to as Unreached. 
OUTPUT: Modified lists Free and Unreached, which holds allocated objects. 
METHOD: In this algorithm, shown in Fig. 7.25, the data structure for garbage 
collection is four lists named Free, Unreached, Unscanned, and Scanned, each 
of which holds all the objects in the state of the same name. These lists may 
be implemented by embedded, doubly linked lists, as was discussed in Sec- 
tion 7.4.4. A reached-bit in objects is not used, but we assume that each object 
476 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
contains bits telling which of the four states it is in. Initially, Free is the free 
list maintained by the memory manager, and all allocated objects are on the 
Unreached list (also maintained by the memory manager as it allocates chunks 
to objects). 
I) 
Scanned = 0; 
2) 
Unscanned = 
set of objects referenced in the root set; 
3) 
while (Unscanned # 0) { 
4) 
move object o from Unscanned to Scanned; 
5) 
for (each object o' referenced in o) { 
6) 
if (0' is in Unreached) 
7 )  
move o' from Unreached to Unscanned; 
1 
1 
8) 
Free = Free U Unreached; 
9) 
Unreached = Scanned; 
Figure 7.25: Baker's mark-and-sweep algorithm 
Lines (1) 
and (2) initialize Scanned to be the empty list, and Unscanned to 
have only the objects reached from the root set. Note that these objects were 
presumably on the list Unreached and must be removed from there. Lines (3) 
through (7) are a straightforward implementation of the basic marking algo- 
rithm, using these lists. That is, the for-loop of lines (5) through (7) examines 
the references in one unscanned object o, and if any of those references o' have 
not yet been reached, line (7) changes o' to the Unscanned state. 
At the end, line (8) takes those objects that are still on the Unreached list 
and deallocates their chunks, by moving them to the Free list. Then, line (9) 
takes all the objects in state Scanned, which are the reachable objects, and 
reinitializes the Unreached list to be exactly those objects. Presumably, as the 
memory manager creates new objects, those too will be added to the Unreached 
list and removed from the Free list. 
In both algorithms of this section, we have assumed that chunks returned 
to the free list remain as they were before deallocation. However, as discussed 
in Section 7.4.4, it is often advantageous to combine adjacent free chunks into 
larger chunks. If we wish to do so, then every time we return a chunk to the 
free list, either at line (10) of Fig. 7.21 or line (8) of Fig. 7.25, we examine the 
chunks to its left and right, and merge if one is free. 
7.6.4 Mark-and-Compact Garbage Collectors 
Relocating collectors move reachable objects around in the heap to eliminate 
memory fragmentation. It is common that the space occupied by reachable ob- 
jects is much smaller than the freed space. Thus, after identifying all the holes, 
7.6. INTRODUCTION T O  TRACE-BASED COLLECTION 
477 
instead of freeing them individually, one attractive alternative is to relocate all 
the reachable objects into one end of the heap, leaving the entire rest of the 
heap as one free chunk. After all, the garbage collector has already analyzed 
every reference within the reachable objects, so updating them to point to the 
new locations does not require much more work. These, plus the references in 
the root set, are all the references we need to change. 
Having all the reachable objects in contiguous locations reduces fragmen- 
tation of the memory space, making it easier to house large objects. Also, by 
making the data occupy fewer cache lines and pages, relocation improves a pro- 
gram's temporal and spatial locality, since new objects created at about the 
same time are allocated nearby chunks. Objects in nearby chunks can bene- 
fit from prefetching if they are used together. Further, the data structure for 
maintaining free space is simplified; instead of a free list, all we need is a pointer 
free to the beginning of the one free block. 
Relocating collectors vary in whether they relocate in place or reserve space 
ahead of time for the relocation: 
A mark-and-compact collector, described in this section, compacts objects 
in place. Relocating in place reduces memory usage. 
The more efficient and popular copying collector in Section 7.6.5 moves 
objects from one region of memory to another. Reserving extra space for 
relocation allows reachable objects to be moved as they are discovered. 
The mark-and-compact collector in Algorithm 7.15 has three phases: 
1. First is a marking phase, similar to that of the mark-and-sweep algorithms 
described previously. 
2. Second, the algorithm scans the allocated section of the heap and com- 
putes a new address for each of the reachable objects. New addresses are 
assigned from the low end of the heap, so there are no holes between reach- 
able objects. The new address for each object is recorded in a structure 
called NewLocation. 
3. Finally, the algorithm copies objects to their new locations, updating all 
references in the objects to point to the corresponding new locations. The 
needed addresses are found in NewLocation. 
Algorithm 7.15 : 
A mark-and-compact garbage collector. 
INPUT: A root set of objects, a heap, and free, a pointer marking the start of 
free space. 
OUTPUT: The new value of pointer free. 
METHOD: The algorithm is in Fig. 7.26; it uses the following data structures: 
1. An Unscanned list, as in Algorithm 7.12. 
478 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
2. Reached bits in all objects, also as in Algorithm 7.12. To keep our de- 
scription simple, we refer to objects as "reached" or "unreached," when 
we mean that their reached-bit is 1 
or 0, respectively. Initially, all objects 
are unreached. 
3. The pointer free, which marks the beginning of undlocated space in the 
heap. 
4. The table NewLocation. This structure could be a hash table, search tree, 
or another structure that implements the two operations: 
(a) Set NewLocation(o) to a new address for object o. 
(b) Given object o, get the value of NewLocation(o). 
We shall not concern ourselves with the exact structure used, although 
you may assume that NewLocation is a hash table, and therefore, the 
"set" and "get" operations are each performed in average constant time, 
independent of how many objects are in the heap. 
The first, or marking, phase of lines (I) through (7) is essentially the same 
as the first phase of Algorithm 7.12. The second phase, lines (8) through (12), 
visits each chunk in the allocated part of the heap, from the left, or low end. As 
a result, chunks are assigned new addresses that increase in the same order as 
their old addresses. This ordering is important, since when we relocate objects, 
we can do so in a way that assures we only move objects left, into space that 
was formerly occupied by objects we have moved already. 
Line (8) starts the free pointer at the low end of the heap. In this phase, 
we use free to indicate the first available new address. We create a new address 
only for those objects o that are marked as reached. Object o is given the next 
available address at line (lo), and at line (11) we increment free by the amount 
of storage that object o requires, so free again points to the beginning of free 
space. 
In the final phase, lines (13) through (17), we again visit the reached objects, 
in the same from-the-left order as in the second phase. Lines (15) and (16) 
replace all internal pointers of a reached object o by their proper new values, 
using the ~ew~ocation 
table to determine the replacement. Then, line (17) 
moves the object o, with the revised internal references, to its new location. 
Finally, lines (18) and (19) retarget pointers in the elements of the root set that 
are not themselves heap objects, e.g., statically allocated or stack-allocated 
objects. Figure 7.27 suggests how the reachable objects (those that are not 
shaded) are moved down the heap, while the internal pointers are changed to 
point to the new locations of the reached objects. 
7.6.5 Copying collectors 
A copying collector reserves, ahead of time, space to which the objects can 
move, thus breaking the dependency between tracing and finding free space. 
7.6. INTRODUCTION TO TRACE-BASED COLLECTION 
/* mark */ 
Unscanned = set of objects referenced by the root set; 
while ( Unscanned # 0) { 
remove object o from Unscanned; 
for (each object o' referenced in o) { 
if (0' is unreached) { 
mark o' as reached; 
put o' on list Unscanned; 
1 
1 
1 
/* 
compute new locations */ 
free = starting location of heap storage; 
for (each chunk of memory o in the heap, from the low end) { 
if (o is reached { 
NewLocation(o) = 
free; 
free = 
free + sixeof(o); 
1 
1 
/* 
retarget references and move reached objects */ 
for (each chunk of memory o in the heap, from the low end) { 
if (o is reached) { 
for (each reference o.r in o) 
copy o to NewLocation(o); 
1 
I- 
for (each reference r in the root set) 
r = NewLocation(r); 
Figure 7.26: A Mark-and-Compact Collector 
The memory space is partitioned into two semispaces, A and B. The mutator 
allocates memory in one semispace, say A, until it fills up, at which point the 
mutator is stopped and the garbage collector copies the reachable objects to 
the other space, say B. When garbage collection completes, the roles of the 
semispaces are reversed. The mutator is allowed to resume and allocate objects 
in space B, and the next round of garbage collection moves reachable objects 
to space A. The following algorithm is due to C. J. Cheney. 
Algorithm 7.16 : 
Cbeney 
's copying collector. 
INPUT: A root set of objects, and a heap consisting of the From semispace, 
containing allocated objects, and the To semispace, all of which is free. 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
\ 
/ 
\ 
, 
I 
- - _ _ - #  
free 
Figure 7.27: Moving reached objects to the front of the heap, while preserving 
internal pointers 
OUTPUT: At the end, the To semispace holds the allocated objects. A free 
pointer indicates the start of free space remaining in the To semispace. The 
From semispace is completely free. 
METHOD: The algorithm is shown in Fig. 7.28. Cheney's algorithm finds 
reachable objects in the From semispace and copies them, as soon as they are 
reached, to the To semispace. This placement groups related objects toget 
her 
and may improve spatial locality. 
Before examining the algorithm it 
self, which is the function Copying 
Collec- 
tor in Fig. 7.28, consider the auxiliary function LookupNewLocation in lines (11) 
through (16). This function takes an object o and finds a new location for it 
in the To space if o has no location there yet. All new locations are recorded 
in a structure NewLocation, and a value of NULL indicates o has no assigned 
10cation.~ 
As in Algorithm 7.15, the exact form of structure NewLocation may 
vary, but it is fine to assume that it is a hash table. 
If we find at line (12) that o has no location, then it is assigned the beginning 
of the free space within the To semispace, at line (13). Line (14) increments 
the free pointer by the amount of space taken by o, and at line (15) we copy o 
from the From space to the To space. Thus, the movement of objects from one 
semispace to the other occurs as a side effect, the first time we loolr up the new 
location for the object. Regardless of whether the location of o was or was not 
previously established, line (16) returns the location of o in the To space. 
Now, we can consider the algorithm itself. Line (2) establishes that none of 
the objects in the From space have new addresses yet. At line (3), we initialize 
two pointers, unscanned and free, to the beginning of the To semispace. Pointer 
free will always indicate the beginning of free space within the To space. As we 
add objects to the T
o
 space, those with addresses below unscanned will be in 
the Scanned state, while those between unscanned and free are in the Unscanned 
5 ~ n  
a typical data structure, such as a hash table, if o is not assigned a location, then there 
simply would be no mention of it in the structure. 
7.6. INTRODUCTION TO TRACE-BASED COLLECTION 
481 
Copying 
Collector () { 
for (all objects o in From space) NewLocation(o) =NULL; 
unscanned = 
free = starting address of T
o
 space; 
for (each reference r in the root set) 
replace r with LookupNewLocation(r) 
; 
while '(unscanned # free) { 
o = object at location unscanned; 
for (each reference o.r within o) 
unscanned = unscanned + sizeof(o); 
1 
1 
/* 
Look up the new location for object if it has been moved. */ 
/* 
Place object in Unscanned state otherwise. */ 
LookupNewLocation(o) { 
if (NewLocation(o) = NULL) { 
NewLocation(o) = 
free; 
free = 
free + sizeof(o); 
copy o to NewLocation(o); 
1 
return NewLocatzon(o) 
; 
1 
Figure 7.28: A Copying Garbage Collector 
state. Thus, free always leads unscanned, and when the latter catches up to 
the former, there are no more Unscanned objects, and we are done with the 
garbage collection. Notice that we do our work within the To space, although 
all references within objects examined at line (8) lead us back to the From 
space. 
Lines (4) and (5) handle the objects reachable from the root set. Note that 
as a side effect, some of the calls to LookupNewLocation at line (5) will increase 
free, as chunks for these objects are allocated within To. Thus, the loop of lines 
(6) through (10) will be entered the first time it is reached, unless there are no 
objects referenced by the root set (in which case the entire heap is garbage). 
This loop then scans each of the objects that has been added to To and is in the 
Unscanned state. Line (7) takes the next unscanned object, o. Then, at lines 
(8) and (9), each reference within o is translated from its value in the From 
semispace to its value in the To semispace. Notice that, as a side effect, if a 
reference within o is to an object we have not reached previously, then the call 
to LookupNewLocatzon at line (9) creates space for that object in the To space 
and moves the object there. Finally, line (10) increments unscanned to point 
to the next object, just beyond o in the To space. 
0 
482 
CHAPTER 7. RUN- 
TIME ENVIRONMENTS 
7.6.6 Comparing Costs 
Cheney's algorithm has the advantage that it does not touch any of the un- 
reachable objects. On the other hand, a copying garbage collector must move 
the contents of all the reachable objects. This process is especially expensive for 
large objects and for long-lived objects that survive multiple rounds of garbage 
collection. We can summarize the running time of each of the four algorithms 
described in this section, as follows. Each estimate ignores the cost of processing 
the root set. 
Basic Mark-and-Sweep (Algorithm 7.12) 
: Proportional to the number of 
chunks in the heap. 
Baker's Mark-and-Sweep (Algorithm 7.14): Proportional to the number 
of reached objects. 
Baszc Mark-and-Compact (Algorithm 7.15): Proportional to the number 
of chunks in the heap plus the total size of the reached objects. 
Cheney's Copying Collector (Algorithm 7.16): Proportional to the total 
size of the reached objects. 
7.6.7 Exercises for Section 7.6 
Exercise 7.6.1 : 
Show the steps of a mark-and-sweep garbage collector on 
a) Fig. 7.19 with the pointer A -+ B deleted. 
b) Fig. 7.19 with the pointer A -+ 
C deleted. 
c) Fig. 7.20 with the painter A -+ D deleted. 
d) Fig. 7.20 with the object B deleted. 
Exercise 7.6.2 
: 
The Baker mark-and-sweep algorithm moves objects among 
four lists: Free, Unreached, Unscanned, and Scanned. For each of the object 
networks of Exercise 7.6.1, indicate for each object the sequence of lists on 
which it finds itself from just before garbage collection begins until just after it 
finishes. 
Exercise 7.6.3 : 
Suppose we perform a mark-and-compact garbage collection 
on each of the networks of Exercise 7.6.1. Also, suppose that 
i. Each object has size 100 bytes, and 
ii. Initially, the nine objects in the heap are arranged in alphabetical order, 
starting at byte 0 of the heap. 
What is the address of each object after garbage collection? 
7.7. SHORT-PA USE GARBAGE COLLECTION 
483 
Exercise 7.6.4 : Suppose we execute Cheney7s 
copying garbage collection al- 
gorithm on each of the networks of Exercise 7.6.1. Also, suppose that 
i. Each object has size 100 bytes, 
ii. The unscanned list is managed as a queue, and when an object has more 
than one pointer, the reached objects are added to the queue in alpha- 
betical order, and 
iii. The From semispace starts at location 0, and the To semispace starts at 
location 10,000. 
What is the value of NewLocation(o) for each object o that remains after garbage 
collection? 
7.7 Short-Pause Garbage Collection 
Simple trace-based collectors do stop-the-world-style garbage collection, which 
may introduce long pauses into the execution of user programs. We can reduce 
the length of the pauses by performing garbage collection one part at a time. 
We can divide the work in time, by interleaving garbage collection with the 
mutation, or we can divide the work in space by collecting a subset of the 
garbage at a time. The former is known as incremental collection and the 
latter is known as partial collection. 
An incremental collector breaks up the reachability analysis into smaller 
units, allowing the mutator to run between these execution units. The reachable 
set changes as the mutator executes, so incremental collection is complex. As 
we shall see in Section 7.7.1, finding a slightly conservative answer can make 
tracing more efficient. 
The best known of partial-collection algorithms is generational garbage col- 
lection; it partitions objects according to how long they have been allocated 
and collects the newly created objects more often because they tend to have a 
shorter lifetime. An alternative algorithm, the train algorithm, also collects a 
subset of garbage at a time, and is best applied to more mature objects. These 
two algorithms can be used together to create a partial collector that handles 
younger and older objects differently. We discuss the basic algorithm behind 
partial collection in Section 7.7.3, and then describe in more detail how the 
generational and train algorithms work. 
Ideas from both incremental and partial collection can be adapted to cre- 
ate an algorithm that collects objects in parallel on a multiprocessor; see Sec- 
tion 7.8.1. 
7.7.1 Incremental Garbage Collection 
Incremental collectors are conservative. While a garbage collector must not 
collect objects that are not garbage, it does not have to collect all the garbage 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
in each round. We refer to the garbage left behind after collection as floating 
garbage. Of course it is desirable to minimize floating garbage. In particular, 
an incremental collector should not leave behind any garbage that was not 
reachable at the beginning of a collection cycle. If we can be sure of such 
a collection guarantee, then any garbage not collected in one round will be 
collected in the next, and no memory is leaked because of this approach to 
garbage collection. 
In other words, incremental collectors play it safe by overestimating the set 
of reachable objects. They first process the program's root set atomically, with- 
out interference from the mutator. After finding the initial set of unscanned 
objects, the mutator's actions are interleaved with the tracing step. During this 
period, any of the mutator's actions that may change reachability are recorded 
succinctly, in a side table, so that the collector can make the necessary ad- 
justments when it resumes execution. If space is exhausted before tracing com- 
pletes, the collector completes the tracing process, without allowing 
the mutator 
to execute. In any event, when tracing is done, space is reclaimed atomically. 
Precision of Incremental Collection 
Once an object becomes unreachable, it is not possible for the object to become 
reachable again. Thus, as garbage collection and mutation proceed, the set of 
reachable objects can only 
1. Grow due to new objects allocated after garbage collection starts, and 
2. Shrink by losing references to allocated objects. 
Let the set of reachable objects at the beginning of garbage collection be R; 
let New be the set of allocated objects during garbage collection, and let Lost 
be the set of objects that have become unreachable due to lost references since 
tracing began. The set of objects reachable when tracing completes is 
( R  
u 
New) - 
Lost. 
It is expensive to reestablish an object's reachability every time a mutator 
loses a reference to the object, so incremental collectors do not attempt to 
collect all the garbage at the end of tracing. Any garbage left behind - 
floating 
garbage - 
should be a subset of the Lost objects. Expressed formally, the set 
S of objects found by tracing must satisfy 
( R  
U New) - 
Lost C 
S C ( R  
U New) 
Simple Increment 
a
1
 Tracing 
We first describe a straightforward tracing algorithm that finds the upper bound 
R  
U New. The behavior of the mutator is modified during the tracing as follows: 
7.7. SHORT-PAUSE GARBAGE COLLECTION 
485 
a All references that existed before garbage collection are preserved; that is, 
before the mutator overwrites a reference, its old value is remembered and 
treated like an additional unscanned object containing just that reference. 
a All objects created are considered reachable immediately and are placed 
in the Unscanned state. 
This scheme is conservative but correct, because it finds R, the set of all the 
objects reachable before garbage collection, plus New, the set of all the newly 
allocated objects. However, the cost is high, because the algorithm intercepts 
all write operations and remembers all the overwritten references. Some of this 
work is unnecessary because it may involve objects that are unreachable at the 
end of garbage collection. We could avoid some of this work and also improve 
the algorithm's precision if we could detect when the overwritten references 
point to objects that are unreachable when this round of garbage collection 
ends. The next algorithm goes fairly far in these two directions. 
7.7.2 Incremental Reachability Analysis 
If we interleave the mutator with a basic tracing algorithm, such as Algo- 
rithm 7.12, then some reachable objects may be misclassified as unreachable. 
The problem is that the actions of the mutator can violate a key invariant of 
the algorithm; namely, a Scanned object can only contain references to other 
Scanned or Unscanned objects, never to Unreached objects. Consider the fol- 
lowing scenario: 
1. The garbage collector finds object 01 reachable and scans the pointers 
within 01, thereby putting 01 in the Scanned state. 
2. The mutator stores a reference to an Unreached (but reachable) object o 
into the Scanned object 01. It does so by copying a reference to o from 
an object 02 that is currently in the Unreached or Unscanned state. 
3. The mutator loses the reference to o in object 02. It may have overwrit- 
ten 02's reference to o before the reference is scanned, or 0 2  may have 
become unreachable and never have reached the Unscanned state to have 
its references scanned. 
Now, o is reachable through object 01, but the garbage collector may have seen 
neither the reference to o in 01 nor the reference to o in 02. 
The key to a more precise, yet correct, incremental trace is that we must 
note all copies of references to currently unreached objects from an object that 
has not been scanned to one that has. To intercept problematic transfers of 
references, the algorithm can modify the mutator's action during tracing in any 
of the following ways: 
CHAPTER 7. RUN- 
TIME ENVIRONMENTS 
Write Barriers. Intercept writes of references into a Scanned object 01, 
when the reference is to an Unreached object o. In this case, classify o 
as reachable and place it in the Unscanned set. Alternatively, place the 
written object 01 back in the Unscanned set so we can rescan it. 
Read Barriers. Intercept the reads of references in Unreached or Un- 
scanned objects. Whenever the mutator reads a reference to an object o 
from an object in either the Unreached or Unscanned state, classify o as 
reachable and place it in the Unscanned set. 
Transfer Barriers. Intercept the loss of the original reference in an Un- 
reached or Unscanned object. Whenever the mutator overwrites a ref- 
erence in an Unreached or Unscanned object, save the reference being 
overwritten, classify it as reachable, and place the reference itself in the 
Unscanned set. 
None of the options above finds the smallest set of reachable objects. If the 
tracing process determines an object to be reachable, it stays reachable even 
though all references to it are overwritten before tracing completes. That is, 
the set of reachable objects found is between ( R  
U New) - 
Lost and ( R  
U New). 
Write barriers are the most efficient of the options outlined above. Read 
barriers are more expensive because typically there are many more reads than 
there are writes. Transfer barriers are not competitive; because many objects 
"die young," this approach would retain many unreachable objects. 
Implementing Write Barriers 
We can implement write barriers in two ways. The first approach is to re- 
member, during a mutation phase, all new references written into the Scanned 
objects. We can place all these references in a list; the size of the list is propor- 
tional to the number of write operations to Scanned objects, unless duplicates 
are removed from the list. Note that references on the list may later be over- 
written themselves and potentially could be ignored. 
The second, more efficient approach is to remember the locations where the 
writes occur. We may remember them as a list of locations written, possibly 
with duplicates eliminated. Note it is not important that we pinpoint the 
exact locations written, as long as all the locations that have been written are 
rescanned. Thus, there are several techniques that allow us to remember less 
detail about exactly where the rewritten locations are. 
Instead of remembering the exact address or the object and field that is 
written, we can remember just the objects that hold the written fields. 
We can divide the address space into fixed-size blocks, known as cards, 
and use a bit array to remember the cards that have been written into. 
7.7. SHORT-PAUSE GARBAGE COLLECTION 
487 
We can choose to remember the pages that contain the written locations. 
We can simply protect the pages containing Scanned objects. Then, any 
writes into Scanned objects will be detected without executing any ex- 
plicit instructions, because they will cause a protection violation, and the 
operating system will raise a program exception. 
In general, by coarsening the granularity at which we remember the written 
locations, less storage is needed, at the expense of increasing the amount of 
rescanning performed. In the first scheme, all references in the modified objects 
will have to be rescanned, regardless of which reference was actually modified. 
In the last two schemes, all reachable objects in the modified cards or modified 
pages need to be rescanned at the end of the tracing process. 
Combining Incremental and Copying Techniques 
The above methods are sufficient for mark-and-sweep garbage collection. Copy- 
ing collection is slightly more complicated, because of its interaction with the 
mutator. Objects in the Scanned or Unscanned states have two addresses, one 
in the From semispace and one in the To semispace. As in Algorithm 7.16, we 
must keep a mapping from the old address of an object to its relocated address. 
There are two choices for how we update the references. First, we can have 
the mutator make all the changes in the From space, and only at the end of 
garbage collection do we update all the pointers and copy all the contents over 
to the To space. Second, we can instead make changes to the representation in 
the To space. Whenever the mutator dereferences a pointer to the From space, 
the pointer is translated to a new location in the To space if one exists. All the 
pointers need to be translated to point to the To space in the end. 
7.7.3 Partial-Collection Basics 
The fundamental fact is that objects typically "die young." It has been found 
that usually between 80% and 98% of all newly allocated objects die within a 
few million instructions, or before another megabyte has been allocated. That 
is, objects often become unreachable before any garbage collection is invoked. 
Thus, is it quite cost effective to garbage collect new objects frequently. 
Yet, objects that survive a collection once are likely to survive many more 
collections. With the garbage collectors described so far, the same mature 
objects will be found to be reachable over and over again and, in the case 
of copying collectors, copied over and over again, in every round of garbage 
collection. Generational garbage collection works most frequently on the area 
of the heap that contains the youngest objects, so it tends to collect a lot of 
garbage for relatively little work. The train algorithm, on the other hand, does 
not spend a large proportion of time on young objects, but it does limit the 
pauses due to garbage collection. Thus, a good combination of strategies is 
to use generational collection for young objects, and once an object becomes 
488 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
sufficiently mature, to "promote" it to a separate heap that is managed by the 
train algorithm. 
We refer to the set of objects to be collected on one round of partial collection 
as the target set and the rest of the objects as the stable set. Ideally, a partial 
collector should reclaim all objects in the target set that are unreachable from 
the program's root set. However, doing so would require tracing all objects, 
which is what we try to avoid in the first place. Instead, partial collectors 
conservatively reclaim only those objects that cannot be reached through either 
the root set of the program or the stable set. Since some objects in the stable 
set may themselves be unreachable, it is possible that we shall treat as reachable 
some objects in the target set that really have no path from the root set. 
We can adapt the garbage collectors described in Sections 7.6.1 and 7.6.4 to 
work in a partial manner by changing the definition of the "root set." Instead of 
referring to just the objects held in the registers, stack and global variables, the 
root set now also includes all the objects in the stable set that point to objects 
in the target set. References from target objects to other target objects are 
traced as before to find all the reachable objects. We can ignore all pointers to 
stable objects, because these objects are all considered reachable in this round 
of partial collection. 
To identify those stable objects that reference target objects, we can adopt 
techniques similar to those used in incremental garbage collection. In incremen- 
tal collection, we need to remember all the writes of references from scanned 
objects to unreached objects during the tracing process. Here we need to re- 
member all the writes of references from the stable objects to the target objects 
throughout the mutator's execution. Whenever the mutator stores into a sta- 
ble object a reference to an object in the target set, we remember either the 
reference or the location of the write. We refer to the set of objects holding 
references from the stable to the target objects as the remembered set for this 
set of target objects. As discussed in Section 7.7.2, we can compress the repre- 
sentation of a remembered set by recording only the card or page in which the 
written object is found. 
Partial garbage collectors are often implemented as copying garbage collec- 
tors. Noncopying collectors can also be implemented by using linked lists to 
keep track of the reachable objects. The "generational" scheme described below 
is an example of how copying may be combined with partial collection. 
7.7.4 Generational Garbage Collect 
ion 
Generational garbage collection is an effective way to exploit the property that 
most objects die young. The heap storage in generational garbage collection is 
separated into a series of partitions. We shall use the convention of numbering 
them 0,1,2, 
. . 
. , 
n, with the lower-numbered partitions holding the younger 
objects. Objects are first created in partition 0. When this partition fills up, 
it is garbage collected, and its reachable objects are moved into partition 1. 
Now, with partition 0 empty again, we resume allocating new objects in that 
7.7. SHORT-PAUSE GARBAGE COLLECTION 
489 
partition. When partition 0 again fills,6 it is garbage collected and its reachable 
objects copied into partition 1, where they join the previously copied objects. 
This pattern repeats until partition 1 also fills up, at which point garbage 
collection is applied to partitions 0 and 1. 
In general, each round of garbage collection is applied to all partitions num- 
bered i or below, for some i; 
the proper i to choose is the highest-numbered 
partition that is currently full. Each time an object survives a collection (i.e., 
it is found to be reachable), it is promoted to the next higher partition from 
the one it occupies, until it reaches the oldest partition, the one numbered n. 
Using the terminology introduced in Section 7.7.3, when partitions i and 
below are garbage collected, the partitions from 0 through i make up the target 
set, and all partitions above i comprise the stable set. To support finding root 
sets for all possible partial collections, we keep for each partition i a remembered 
set, consisting of all the objects in partitions above i that point to objects in set 
i. The root set for a partial collection invoked on set i includes the remembered 
sets for partition i and below. 
In this scheme, all partitions below i are collected whenever we collect i. 
There are two reasons for this policy: 
1. Since younger generations contain more garbage and are collected more 
often anyway, we may as well collect them along with an older generation. 
2. Following this strategy, we need to remember only the references pointing 
from an older generation to a newer generation. That is, neither writes 
to objects in the youngest generation nor promoting objects to the next 
generation causes updates to any remembered set. If we were to collect 
a partition without a younger one, the younger generation would become 
part of the stable set, and we would have to remember references that 
point from younger to older generations as well. 
In summary, this scheme collects younger generations more often, and col- 
lections of these generations are particularly cost effective, since LLobjects 
die 
young.." Garbage collection of older generations takes more time, since it in- 
cludes the collection of all the younger generations and contains proportionally 
less garbage. Nonetheless, older generations do need to be collected once in 
a while to remove unreachable objects. The oldest generation holds the most 
mature objects; its collection is expensive because it is equivalent to a full collec- 
tion. That is, generational collectors occasionally require that the full tracing 
step be performed and therefore can introduce long pauses into a program's 
execution. An alternative for handling mature objects only is discussed next. 
'~echnicall~, 
partitions do not "fill," since they can be expanded with additional disk 
blocks by the memory manager, if desired. However, there is normally a limit on the size of a 
partition, other than the last. We shall refer to reaching this limit as "filling" the partition. 
490 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
7.7.5 
The Train Algorithm 
While the generational approach is very efficient for the handling of immature 
objects, it is less efficient for the mature objects, since mature objects are moved 
every time there is a collection involving them, and they are quite unlikely to 
be garbage. A different approach to incremental collection, called the train 
algorithm, was developed to improve the handling of mature objects. It can be 
used for collecting all garbage, but it is probably better to use the generational 
approach for immature objects and, only after they have survived a few rounds 
of collection, "promote" them to another heap, managed by the train algorithm. 
Another advantage to the train algorithm is that we never have to do a complete 
garbage collection, as we do occasionally for generational garbage collection. 
To motivate the train algorithm, let us look at a simple example of why it is 
necessary, in the generational approach, to have occasional all-inclusive rounds 
of garbage collection. Figure 7.29 shows two mutually linked objects in two 
partitions i and j, where j > i. Since both objects have pointers from outside 
their partition, a collection of only partition i or only partition j could never 
collect either of these objects. Yet they may in fact be part of a cyclic garbage 
structure with no links from the outside. In general, the "links" between the 
objects shown may involve many objects and long chains of references. 
Partition i - 
Partition j 
Figure 7.29: A cyclic structure across partitions that may be cyclic garbage 
In generational garbage collection, we eventually collect partition j, and 
since i < j, we also collect i at that time. Then, the cyclic structure will be 
completely contained in the portion of the heap being collected, and we can 
tell if it truly is garbage. However, if we never have a round of collection that 
includes both i and j, we would have a problem with cyclic garbage, just as we 
did with reference counting for garbage collection. 
The train algorithm uses fixed-length partitions, called cars; a car might be 
a single disk block, provided there are no objects larger than disk blocks, or the 
car size could be larger, but it is fixed once and for all. Cars are organized into 
trains. There is no limit to the number of cars in a train, and no limit to the 
number of trains. There is a lexicographic order to cars: first order by train 
number, and within a train, order by car number, as in Fig. 7.30. 
There are two ways that garbage is collected by the train algorithm: 
The first car in lexicographic order (that is, the first remaining car of the 
first remaining train) is collected in one incremental garbage-collection 
step. This step is similar to collection of the first partition in the gener- 
ational algorithm, since we maintain a "remembered" list of all pointers 
7.7. SHORT-PAUSE GARBAGE COLLECTION 
Train 1 
Train 2 
Train 3 
Figure 7.30: Organization of the heap for the train algorithm 
from outside the car. Here, we identify objects with no references at all, 
as well as garbage cycles that are contained completely within this car. 
Reachable objects in the car are always moved to some other car, so each 
garbage-collected car becomes empty and can be removed from the train. 
a Sometimes, the first train has no external references. That is, there are 
no pointers from the root set to any car of the train, and the remembered 
sets for the cars contain only references from other cars in the train, not 
from other trains. In this situation, the train is a huge collection of cyclic 
garbage, and we delete the entire train. 
Remembered Sets 
We now give the details of the train algorithm. Each car has a remembered set 
consisting of all references to objects in the car from 
a) Objects in higher-numbered cars of the same train, and 
b) Objects in higher-numbered trains. 
In addition, each train has a remembered set consisting of all references from 
higher-numbered trains. That is, the remembered set for a train is the union of 
the remembered sets for its cars, except for those references that are internal 
to the train. It is thus possible to represent both kinds of remembered sets 
by dividing the remembered sets for the cars into "internal" (same train) and 
"external" (other trains) portions. 
Note that references to objects can come from anywhere, not just from 
lexicographically higher cars. However, the two garbage-collection processes 
deal with the first car of the first train, and the entire first train, respectively. 
Thus, when it is time to use the remembered sets in a garbage collection, there 
is nothing earlier from which references could come, and therefore there is no 
point in remembering references to higher cars at any time. We must be careful, 
of course, to manage the remembered sets properly, changing them whenever 
the mutator modifies references in any object. 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
Managing Trains 
Our objective is to draw out of the first train all objects that are not cyclic 
garbage. Then, the first train either becomes nothing but cyclic garbage and is 
therefore collected at the next round of garbage collection, or if the garbage is 
not cyclic, then its cars may be collected one at a time. 
We therefore need to start new trains occasionally, even though there is no 
limit on the number of cars in one train, and we could in principle simply add 
new cars to a single train, every time we needed more space. For example, we 
could start a new train after every k object creations, for some k. That is, in 
general, a new object is placed in the last car of the last train, if there is room, 
or in a new car that is added to the end of the last train, if there is no room. 
However, periodically, we instead start a new train with one car, and place the 
new object there. 
Garbage Collecting a Car 
The heart of the train algorithm is how we process the first car of the first 
train during a round of garbage collection. Initially, the reachable set is taken 
to be the objects of that car with references from the root set and those with 
references in the remembered set for that car. We then scan these objects as 
in a mark-and-sweep collector, but we do not scan any reached objects outside 
the one car being collected. After this tracing, some objects in the car may 
be identified as garbage. There is no need to reclaim their space, because the 
entire car is going to disappear anyway. 
However, there are likely to be some reachable objects in the car, and these 
must be moved somewhere else. The rules for moving an object are: 
If there is a reference in the remembered set from any other train (which 
will be higher-numbered than the train of the car being collected), then 
move the object to one of those trains. If there is room, the object can 
go in some existing car of the train from which a reference emanates, or 
it can go in a new, last car if there is no room. 
If there is no reference from other trains, but there are references from 
the root set or from the first train, then move the object to any other car 
of the same train, creating a new, last car if there is no room. If possible, 
pick a car from which there is a reference, to help bring cyclic structures 
to a single car. 
After moving all the reachable objects from the first car, we delete that car. 
Panic Mode 
There is one problem with the rules above. In order to be sure that all garbage 
will eventually be collected, we need to be sure that every train eventually 
becomes the first train, and if this train is not cyclic garbage, then eventually 
7.7. SHORT-PA 
USE GARBAGE COLLECTION 
493 
all cars of that train are removed and the train disappears one car at a time. 
However, by rule (2) above, collecting the first car of the first train can produce 
a new last car. It cannot produce two or more new cars, since surely all the 
objects of the first car can fit in the new, last car. However, could we be in a 
situation where each collection step for a train results in a new car being added, 
and we never get finished with this train and move on to the other trains? 
The answer is, unfortunately, that such a situation is possible. The problem 
arises if we have a large, cyclic, nongarbage structure, and the mutator manages 
to change references in such a way that we never see, at the time we collect 
a car, any references from higher trains in the remembered set. If even one 
object is removed from the train during the collection of a car, then we are OK, 
since no new objects are added to the first train, and therefore the first train 
will surely run out of objects eventually. However, there may be no garbage 
at all that we can collect at a stage, and we run the risk of a loop where we 
perpetually garbage collect only the current first train. 
To avoid this problem, we need to behave differently whenever we encounter 
a futile garbage collection, that is, a car from which not even one object can be 
deleted as garbage or moved to another train. In this "panic mode," we make 
two changes: 
1. When a reference to an object in the first train is rewritten, we maintain 
the reference as a new member of the root set. 
2. When garbage collecting, if an object in the first car has a reference from 
the root set, including dummy references set up by point (I), 
then we 
move that object to another train, even if it has no references from other 
trains. It is not important which train we move it to, as long as it is not 
the first train. 
In this way, if there are any references from outside the first train to objects 
in the first train, these references are considered as we collect every car, and 
eventually some object will be removed from that train. We can then leave panic 
mode and proceed normally, sure that the current first train is now smaller than 
it was. 
7.7.6 
Exercises for Section 7.7 
Exercise 7.7.1 : 
Suppose that the network of objects from Fig. 7.20 is managed 
by an incremental algorithm that uses the four lists Unreached, Unscanned, 
Scanned, and Free, as in Baker's algorithm. To be specific, the Unscanned list 
is managed as a queue, and when more than one object is to be placed on this list 
due to the scanning of one object, we do so in alphabetical order. Suppose also 
that we use write barriers to assure that no reachable object is made garbage. 
Starting with A and B on the Unscanned list, suppose the following events 
occur: 
i. A is scanned. 
494 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
ii. The pointer A -+ D is rewritten to be A -+ H. 
iii. B is scanned. 
iv. D is scanned. 
v. The pointer B -+ 
C is rewritten to be B -+ I. 
Simulate the entire incremental garbage collection, assuming no more pointers 
are rewritten. Which objects are garbage? Which objects are placed on the 
Free list? 
Exercise 7.7.2 : 
Repeat Exercise 7.7.1 on the assumption that 
a) Events (ii) and (v) are interchanged in order. 
b) Events (ii) and (v) occur before (i), (iii), and (iv). 
Exercise 7.7.3 
: Suppose the heap consists of exactly the nine cars on three 
trains shown in Fig. 7.30 (i.e., ignore the ellipses). Object o in car I1 has 
references from cars 12, 23, and 32. When we garbage collect car 11, where 
might o wind up? 
Exercise 7.7.4 : 
Repeat Exercise 7.7.3 for the cases that o has 
a) Only references from cars 22 and 31. 
b) No references other than from car 11. 
Exercise 7.7.5 : Suppose the heap consists of exactly the nine cars on three 
trains shown in Fig. 7.30 (i.e., ignore the ellipses). We are currently in panic 
mode. Object 01 in car 11 
has only one reference, from object 02 in car 12. That 
reference is rewritten. When we garbage collect car 11, what could happen to 
ol? 
7.8 Advanced Topics in Garbage Collection 
We close our investigation of garbage collection with brief treatments of four 
additional topics: 
1. Garbage collection in parallel environments. 
2. Partial relocations of objects. 
3. Garbage collection for languages that are not type-safe. 
4. The interaction between programmer-controlled and automatic garbage 
collect 
ion. 
7.8. ADVANCED TOPICS IN GARBAGE COLLECTION 
7.8.1 Parallel and Concurrent Garbage Collection 
Garbage collection becomes even more challenging when applied to applications 
running in parallel on a multiprocessor machine. It is not uncommon for server 
applications to have thousands of threads running at the same time; each of 
these threads is a mutator. Typically, the heap will consist of gigabytes of 
memory. 
Scalable garbage-collection algorithms must take advantage of the presence 
of multiple processors. We say a garbage collector is parallel if it uses multiple 
threads; it is concurrent if it runs simultaneously with the mutator. 
We shall describe a parallel, and mostly concurrent, collector that uses a 
concurrent and parallel phase that does most of the tracing work, and then a 
stop-the-world phase that guarantees all the reachable objects are found and re- 
claims the storage. This algorithm introduces no new basic concepts in garbage 
collection per se; it shows how we can combine the ideas described so far to 
create a full solution to the parallel-and-concurrent collection problem. How- 
ever, there are some new implementation issues that arise due to the nature 
of parallel execution. We shall discuss how this algorithm coordinates multiple 
threads in a parallel computation using a rather common work-queue model. 
To understand the design of the algorithm we must keep in mind the scale 
of the problem. Even the root set of a parallel application is much larger, 
consisting of every thread's stack, register set and globally accessible variables. 
The amount of heap storage can be very large, and so is the amount of reachable 
data. The rate at which mutations take place is also much greater. 
To reduce the pause time, we can adapt the basic ideas developed for in- 
cremental analysis to overlap garbage collection with mutation. Recall that an 
incremental analysis, as discussed in Section 7.7, performs the following three 
steps: 
1. Find the root set. This step is normally performed atomically, that is, 
with the mutator(s) stopped. 
Interleave the tracing of the reachable objects with the execution of the 
mutator(s). In this period, every time a mutator writes a reference that 
points from a Scanned object to an Unreached object, we remember that 
reference. As discussed in Section 7.7.2, we have options regarding the 
granularity with which these references are remembered. In this section, 
we shall assume the card-based scheme, where we divide the heap into 
sections called "cards" and maintain a bit map indicating which cards are 
dirty (have had one or more references within them rewritten). 
3. Stop the mutator(s) again to rescan all the cards that may hold references 
to unreached objects. 
For a large multithreaded application, the set of objects reached by the root 
set can be very large. It is infeasible to take the time and space to visit all such 
objects while all mutations cease. Also, due to the large heap and the large 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
number of mutation threads, many cards may need to be rescanned after all 
objects have been scanned once. It is thus advisable to scan some of these cards 
in parallel, while the mutators are allowed to continue to execute concurrently. 
To implement the tracing of step (2) above, in parallel, we shall use multiple 
garbage-collecting threads concurrently with the mutator threads to trace most 
of the reachable objects. Then, to implement step (3), we stop the mutators 
and use parallel threads to ensure that all reachable objects are found. 
The tracing of step (2) is carried out by having each mutator thread per- 
form part of the garbage collection, along with its own work. In addition, we 
use threads that are dedicated purely to collecting garbage. Once garbage col- 
lection has been initiated, whenever a mutator thread performs some memory- 
allocation operation, it also performs some tracing computation. The pure 
garbage-collecting threads are put to use only when a machine has idle cycles. 
As in incremental analysis, whenever a mutator writes a reference that points 
from a Scanned object to an Unreached 
object, the card that holds this reference 
is marked dirty and needs to be rescanned. 
Here is an outline of the parallel, concurrent garbage-collection algorithm. 
1. Scan the root set for each mutator thread, and put all objects directly 
reachable from that thread into the Unscanned state. The simplest incre- 
mental approach to this step is to wait until a mutator thread calls the 
memory manager, and have it scan its own root set if that has not already 
been done. If some mutator thread has not called a memory allocation 
function, but all the rest of tracing is done, then this thread must be 
interrupted to have its root set scanned. 
2. Scan objects that are in the Unscanned state. To support parallel com- 
putation, we use a work queue of fixed-size work packets, each of which 
holds a number of Unscanned objects. Unscanned objects are placed in 
work packets as they are discovered. Threads looking for work will de- 
queue these work packets and trace the Unscanned objects therein. This 
strategy allows the work to be spread evenly among workers in the tracing 
process. If the system runs out of space, and we cannot find the space to 
create these work packets, we simply mark the cards holding the objects 
to force them to be scanned. The latter is always possible because the bit 
array holding the marks for the cards has already been allocated. 
3. Scan the objects in dirty cards. When there are no more Unscanned ob- 
jects left in the work queue, and all threads' root sets have been scanned, 
the cards are rescanned for reachable objects. As long as the mutators 
continue to execute, dirty cards continue to be produced. Thus, we need 
to stop the tracing process using some criterion, such as allowing cards to 
be rescanned only once or a fixed number of times, or when the number 
of outstanding cards is reduced to some threshold. As a result, this paral- 
lel and concurrent step normally terminates before completing the trace, 
which is finished by the final step, below. 
7.8. ADVANCED TOPICS IN GARBAGE COLLECTION 
4. The final step guarantees that all reachable objects are marked as reached. 
With all the mutators stopped, the root sets for all the threads can now 
be found quickly using all the processors in the system. Because the 
reachability of most objects has been traced, only a small number of 
objects are expected to be placed in the Unscanned state. All the threads 
then participate in tracing the rest of the reachable objects and rescanning 
all the cards. 
It is important that we control the rate at which tracing takes place. The 
tracing phase is like a race. The mutators create new objects and new references 
that must be scanned, and the tracing tries to scan all the reachable objects and 
rescan the dirty cards generated in the meanwhile. It is not desirable to start 
the tracing too much before a garbage collection is needed, because that will 
increase the amount of floating garbage. On the other hand, we cannot wait 
until the memory is exhausted before the tracing starts, because then mutators 
will not be able to make forward progress and the situation degenerates to that 
of a stop-the-world collector. Thus, the algorithm must choose the time to 
commence the collection and the rate of tracing appropriately. An estimate 
of the mutation rate from previous cycles of collection can be used to help in 
the decision. The tracing rate is dynamically adjusted to account for the work 
performed by the pure garbage-collecting threads. 
7.8.2 Partial Object Relocation 
As discussed starting in Section 7.6.4, copying or compacting collectors are ad- 
vantageous because they eliminate fragmentation. However, these collectors 
have nontrivial overheads. A compacting collector requires moving all objects 
and updating all the references at the end of garbage collection. A copying 
collector figures out where the reachable objects go as tracing proceeds; if trac- 
ing is performed incrementally, we need either to translate a mutator's every 
reference, or to move all the objects and update their references at the end. 
Both options are very expensive, especially for a large heap. 
We can instead use a copying generational garbage collector. It is effective in 
collecting immature objects and reducing fragmentation, but can be expensive 
when collecting mature objects. We can use the train algorithm to limit the 
amount of mature data analyzed each time. However, the overhead of the train 
algorithm is sensitive to the size of the remembered set for each partition. 
There is a hybrid collection scheme that uses concurrent tracing to reclaim 
all the unreachable objects and at the same time moves only a part of the 
objects. This method reduces fragmentation without incurring the full cost of 
relocation in each collection cycle. 
1. Before tracing begins, choose a part of the heap that will be evacuated. 
2. As the reachable objects are marked, also remember all the references 
pointing to objects in the designated area. 
498 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
3. When tracing is complete, sweep the storage in parallel to reclaim the 
space occupied by unreachable objects. 
4. Finally, evacuate the reachable objects occupying the designated area and 
fix up the references to the evacuated objects. 
7.8.3 Conservative Collection for Unsafe Languages 
As discussed in Section 7.5.1, it is impossible to build a garbage collector that is 
guaranteed to work for all C and C++ programs. Since we can always compute 
an address with arithmetic operations, no memory locations in C and C++ can 
ever be shown to be unreachable. However, many C or C++ programs never 
fabricate addresses in this way. It has been demonstrated that a conservative 
garbage collector - 
one that does not necessarily discard all garbage - 
can be 
built to work well in practice for this class of programs. 
A conservative garbage collector assumes that we cannot fabricate an ad- 
dress, or derive the address of an allocated chunk of memory without an ad- 
dress pointing somewhere in the same chunk. We can find all the garbage in 
programs satisfying such an assumption by treating as a valid address any bit 
pattern found anywhere in reachable memory, as long as that bit pattern may 
be construed as a memory location. This scheme may classify some data erro- 
neously as addresses. It is correct, however, since it only causes the collector to 
be conservative and keep more data than necessary. 
Object relocation, requiring all references to the old locations be updated to 
point to the new locations, is incompatible with conservative garbage collection. 
Since a conservative garbage collector does not know if a particular bit pattern 
refers to an actual address, it cannot change these patterns to point to new 
addresses. 
Here is how a conservative garbage collector works. First, the memory 
manager is modified to keep a data map of all the allocated chunks of memory. 
This map allows us to find easily the starting and ending boundary of the chunk 
of memory that spans a certain address. The tracing starts by scanning the 
program's root set to find any bit pattern that looks like a memory location, 
without worrying about its type. By looking up these potential addresses in the 
data map, we can find the starting addresses of those chunks of memory that 
might be reached, and place them in the Unscanned state. We then scan all the 
unscanned chunks, find more (presumably) reachable chunks of memory, and 
place them on the work list until the work list becomes empty. After tracing 
is done, we sweep through the heap storage using the data map to locate and 
free all the unreachable chunks of memory. 
7.8.4 Weak References 
Sometimes, programmers use a language with garbage collection, but also wish 
to manage memory, or parts of memory, themselves. That is, a programmer 
may know that certain objects are never going to be accessed again, even though 
7.8. ADVANCED TOPICS IN GARBAGE COLLECTION 
499 
references to the objects remain. An example from compiling will suggest the 
problem. 
Example 7.17 : 
We have seen that the lexical analyzer often manages a sym- 
bol table by creating an object for each identifier it sees. These objects may 
appear as lexical values attached to leaves of the parse tree representing those 
identifiers, for instance. However, it is also useful to create a hash table, keyed 
by the identifier's string, to locate these objects. That table makes it easier for 
the lexical analyzer to find the object when it encounters a lexeme that is an 
identifier. 
When the compiler passes the scope of an identifier I, its symbol-table 
object no longer has any references from the parse tree, or probably any other 
intermediate structure used by the compiler. However, a reference to the object 
is still sitting in the hash table. Since the hash table is part of the root set of the 
compiler, the object cannot be garbage collected. If another identifier with the 
same lexeme as I 
is encountered, then it will be discovered that I 
is out of scope, 
and the reference to its object will be deleted. However, if no other identifier 
with this lexeme is encountered, then I's object may remain as uncollectable, 
yet useless, throughout compilation. 
O 
If the problem suggested by Example 7.17 is important, then the compiler 
writer could arrange to delete from the hash table all references to objects as 
soon as their scope ends. However, a technique known as weak references allows 
the programmer to rely on automatic garbage collection, and yet not have the 
heap burdened with reachable, yet truly unused, objects. Such a system allows 
certain references to be declared "weak." An example would be all the references 
in the hash table we have been discussing. When the garbage collector scans 
an object, it does not follow weak references within that object, and does not 
make the objects they point to reachable. Of course, such an object may still 
be reachable if there is another reference to it that is not weak. 
7.8.5 
Exercises for Section 7.8 
! Exercise 7.8.1 : 
In Section 7.8.3 we suggested that it was possible to garbage 
collect for C programs that do not fabricate expressions that point to a place 
within a chunk unless there is an address that points somewhere within that 
same chunk. Thus, we rule out code like 
because, while p might point to some chunk accidentally, there could be no other 
pointer to that chunk. On the other hand, with the code above, it is more likely 
that p points nowhere, and executing that code will result in a segmentation 
fault. However, in C it is possible to write code such that a variable like p is 
guaranteed to point to some chunk, and yet there is no pointer to that chunk. 
Write such a program. 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
7.9 Summary of Chapter 7 
+ Run-Time Organixation. To implement the abstractions embodied in the 
source language, a compiler creates and manages a run-time environment 
in concert with the operating system and the target machine. The run- 
time environment has static data areas for the object code and the static 
data objects created at compile time. It also has dynamic stack and heap 
areas for managing objects created and destroyed as the target program 
executes. 
+ Control Stack. Procedure calls and returns are usually managed by a run- 
time stack called the control stack. We can use a stack because procedure 
calls or activations nest in time; that is, if p calls q, then this activation 
of q is nested within this activation of p. 
+ Stack Allocation. Storage for local variables can allocated on a run-time 
stack for languages that allow or require local variables to become inacces- 
sible when their procedures end. For such languages, each live activation 
has an activation record (or frame) on the control stack, with the root of 
the activation tree at the bottom, and the entire sequence of activation 
records on the stack corresponding to the path in the activation tree to 
the activation where control currently resides. The latter activation has 
its record at the top of the stack. 
+ Access to Nonlocal Data on the Stack. For languages like C that do not 
allow nested procedure declarations, the location for a variable is either 
global or found in the activation record on top of the run-time stack. For 
languages with nested procedures, we can access nonlocal data on the 
stack through access links, which are pointers added to each activation 
record. The desired nonlocal data is found by following a chain of access 
links to the appropriate activation record. A display is an auxiliary array, 
used in conjunction with access links, that provides an efficient short-cut 
alternative to a chain of access links. 
+ Heap Management. The heap is the portion of the store that is used for 
data that can live indefinitely, or until the program deletes it explicitly. 
The memory manager allocates and deallocates space within the heap. 
Garbage collection finds spaces within the heap that are no longer in use 
and can therefore be reallocated to house other data items. For languages 
that require it, the garbage collector is an important subsystem of the 
memory manager. 
+ Exploiting Locality. By making good use of the memory hierarchy, mem- 
ory managers can influence the run time of a program. The time taken to 
access different parts of memory can vary from nanoseconds to millisec- 
onds. Fortunately, most programs spend most of their time executing a 
relatively small fraction of the code and touching only a small fraction of 
7.9. SUMMARY OF CHAPTER 7 
the data. A program has temporal locality if it is likely to access the same 
memory locations again soon; it has spatial locality if it is likely to access 
nearby memory locations soon. 
+ Reducing Fragmentation. As the program allocates and deallocates mem- 
ory, the heap may get fragmented, or broken into large numbers of small 
noncontiguous free spaces or holes. The best fit strategy - 
allocate the 
smallest available hole that satisfies a request - 
has been found empir- 
ically to work well. While best fit tends to improve space utilization, it 
may not be best for spatial locality. Fragmentation can be reduced by 
combining or coalescing adjacent holes. 
+ Manual Deallocation. Manual memory management has two common 
failings: not deleting data that can not be referenced is a memory-leak 
error, and referencing deleted data is a dangling-pointer-dereference error. 
+ Reachability. Garbage is data that cannot be referenced or reached. There 
are two basic ways of finding unreachable objects: either catch the tran- 
sition as a reachable object turns unreachable, or periodically locate all 
reachable objects and infer that all remaining objects are unreachable. 
+ Reference-Counting Collectors maintain a count of the references to an ob- 
ject; when the count transitions to zero, the object becomes unreachable. 
Such collectors introduce the overhead of maintaining references and can 
fail to find "cyclic" garbage, which consists of unreachable objects that 
reference each other, perhaps through a chain of references. 
+ Trace- 
Based Garbage Collectors iteratively examine or trace all references 
to find reachable objects, starting with the root set consisting of objects 
that can be accessed directly without having to dereference any pointers. 
+ Mark-and-Sweep Collectors visit and mark all reachable objects in a first 
tracing step and then sweep the heap to free up unreachable objects. 
+ Mark-and-Compact Collectors improve upon mark-and-sweep; they relo- 
cate reachable objects in the heap to eliminate memory fragmentation. 
+ Copying Collectors break the dependency between tracing and finding 
free space. They partition the memory into two semispaces, A and B. 
Allocation requests are satisfied from one semispace, say A, until it fills 
up, at which point the garbage collector takes over, copies the reachable 
objects to the other space, say B, and reverses the roles of the semispaces. 
+ Incremental Collectors. Simple trace-based collectors stop the user pro- 
gram while garbage is collected. Incremental collectors interleave the 
actions of the garbage collector and the mutator or user program. The 
mutator can interfere with incremental reachability analysis, since it can 
502 
CHAPTER 7. RUN-TIME ENVIRONMENTS 
change the references within previously scanned objects. Incremental col- 
lectors therefore play it safe by overestimating the set of reachable objects; 
any "floating garbage" can be picked up in the next round of collection. 
+ Partial Collectors also reduce pauses; they collect a subset of the garbage 
at a time. The best known of partial-collection algorithms, generational 
garbage collection, partitions objects according to how long they have 
been allocated and collects the newly created objects more often because 
they tend to have shorter lifetimes. An alternative algorithm, the train 
algorithm, uses fixed length partitions, called cars, that are collected into 
trains. Each collection step is applied to the first remaining car of the first 
remaining train. When a car is collected, reachable objects are moved out 
to other cars, so this car is left with garbage and can be removed from 
the train. These two algorithms can be used together to create a partial 
collector that applies the generational algorithm to younger objects and 
the train algorithm to more mature objects. 
7.10 References for Chapter 7 
In mathematical logic, scope rules and parameter passing by substitution date 
back to Frege [8]. Church's lambda calculus [3] uses lexical scope; it has been 
used as a model for studying programming languages. Algol 60 and its succes- 
sors, including C and Java, use lexical scope. Once introduced by the initial 
implementation of Lisp, dynamic scope became a feature of the language; Mc- 
Carthy [14] gives the history. 
Many of the concepts related to stack allocation were stimulated by blocks 
and recursion in Algol 60. The idea of a display for accessing nonlocals in 
a lexically scoped language is due to Dijkstra [5]. A detailed description of 
stack allocation, the use of a display, and dynamic allocation of arrays appears 
in Randell and Russell [16]. Johnson and Ritchie [lo] discuss the design of a 
calling sequence that allows the number of arguments of a procedure to vary 
from call to call. 
Garbage collection has been an active area of investigation; see for example 
Wilson [17]. Reference counting dates back to Collins [4]. Trace-based collection 
dates back to McCarthy [13], who describes a mark-sweep algorithm for fixed- 
length cells. The boundary-tag for managing free space was designed by Knuth 
in 1962 and published in [ll]. 
Algorithm 7.14 is based on Baker [I]. Algorithm 7.16 is based on Cheney's [2] 
nonrecursive version of Fenichel and Yochelson's [7] copying collector. 
Incremental reachability analysis is explored by Dijkstra et al. [6]. Lieber- 
man and Hewitt [12] present a generational collector as an extension of copying 
collection. The train algorithm began with Hudson and Moss [9]. 
I. Baker, H. G. Jr., "The treadmill: real-time garbage collection without 
motion sickness," ACM SIGPLAN Notices 27:3 
(Mar., 1992), 
pp. 66-70. 
7.10. REFERENCES FOR CHAPTER 7 
503 
2. Cheney, C. J., "A nonrecursive list compacting algorithm," Comm. ACM 
13:ll (Nov., 1970), pp. 677-678. 
3. Church, A., The Calculi of Lambda Conversion, Annals of Math. Studies, 
No. 6, Princeton University Press, Princeton, N. J., 1941. 
4. Collins, G. E., "A method for overlapping and erasure of lists," Comm. 
ACM 2:12 (Dec., 1960), pp. 655-657. 
5. Dijkstra, E. W ., "Recursive programming," Numerische Math. 2 (1960), 
pp. 312-318. 
6. Dijkstra, E. W., L. Lamport, A. J. Martin, C. S. Scholten, and E. F. 
M. Steffens, "On-the-fly garbage collection: an exercise in cooperation," 
Comm. ACM 21:ll (1978), pp. 966-975. 
7. Fenichel, R. R. and J. C. Yochelson, "A Lisp garbage-collector for virtual- 
memory computer systems", Comm. ACM 12:11 (1969), pp. 611-612. 
8. Frege, G., "Begriffsschrift, a formula language, modeled upon that of 
arithmetic, for pure thought," (1879). In J. van Heijenoort, From Frege 
to Godel, Harvard Univ. Press, Cambridge MA, 1967. 
9. Hudson, R. L. and J. E. B. Moss, "Incremental Collection of Mature 
Objects", Proc. Intl. Workshop on Memory Management, Lecture Notes 
In Computer Science 637 (1992), pp. 388-403. 
10. Johnson, S. C. and D. M. Ritchie, "The C language calling sequence," 
Computing Science Technical Report 102, Bell Laboratories, Murray Hill 
NJ, 1981. 
11. Knuth, D. E., Art of Computer Programming, Volume I :  Fundamental 
Algorithms, Addison-Wesley, Boston MA, 1968. 
12. Lieberman, H. and C. Hewitt, "A real-time garbage collector based on 
the lifetimes of objects," Comm. ACM 26:6 (June 1983), pp. 419-429. 
13. McCarthy, J., "Recursive functions of symbolic expressions and their com- 
putation by machine," Comm. ACM 3:4 (Apr., 1960), pp. 184-195. 
14. McCarthy, J., L'History 
of Lisp." See pp. 173-185 in R. L. Wexelblat (ed.), 
History of Programming Languages, Academic Press, New York, 1981. 
15. Minsky, M., "A LISP garbage collector algorithm using secondary stor- 
age," A. I. Memo 58, MIT Project MAC, Cambridge MA, 1963. 
16. Randell, B. and L. J. Russell, Algol 60 Implementation, Academic Press, 
New York, 1964. 
17. Wilson, P. R., "Uniprocessor garbage collection techniques," 
Chapter 8 
Code Generation 
The final phase in our compiler model is the code generator. It takes as input 
the intermediate representation (IR) produced by the front end of the com- 
piler, along with relevant symbol table information, and produces as output a 
semantically equivalent target program, as shown in Fig. 8.1. 
The requirements imposed on a code generator are severe. The target pro- 
gram must preserve the semantic meaning of the source program and be of 
high quality; that is, it must make effective use of the available resources of the 
target machine. Moreover, the code generator it 
self must run efficiently. 
The challenge is that, mathematically, the problem of generating an optimal 
target program for a given source program is undecidable; many of the subprob- 
lems encountered in code generation such as register allocation are computa- 
tionally intractable. In practice, we must be content with heuristic techniques 
that generate good, but not necessarily optimal, code. Fortunately, heuristics 
have matured enough that a carefully designed code generator can produce code 
that is several times faster than code produced by a naive one. 
Compilers that need to produce efficient target programs, include an op- 
timization phase prior to code generation. The optimizer maps the IR into 
IR from which more efficient code can be generated. In general, the code- 
optimization and code-generation phases of a compiler, often referred to as the 
back end, may make multiple passes over the IR before generating the target 
program. Code optimization is discussed in detail in Chapter 9. The tech- 
niques presented in this chapter can be used whether or not an optimization 
phase occurs before code generation. 
A code generator has three primary tasks: instruction selection, register 
 source^ FIont 
1 intermediats 
Code 
?ntermediatq 
Code 
p a r g e t  
program 
End 
code 
) Optimixer 
) 
code 
Generator program 
Figure 8.1: Position of code generator 
CHAPTER 8. CODE GENERATION 
allocation and assignment, and instruction ordering. The importance of these 
tasks is outlined in Section 8.1. Instruction selection involves choosing appro- 
priate target-machine instructions to implement the IR statements. Register 
allocation and assignment involves deciding what values to keep in which reg- 
isters. Instruction ordering involves deciding in what order to schedule the 
execution of instructions. 
This chapter presents algorithms that code generators can use to trans- 
late the IR into a sequence of target language instructions for simple register 
machines. The algorithms will be illustrated by using the machine model in Sec- 
tion 8.2. Chapter 10 covers the problem of code generation for complex modern 
machines that support a great deal of parallelism within a single instruction. 
After discussing the broad issues in the design of a code generator, we show 
what kind of target code a compiler needs to generate to support the abstrac- 
tions embodied in a typical source language. In Section 8.3, we outline imple- 
mentations of static and stack allocation of data areas, and show how names in 
the IR can be converted into addresses in the target code. 
Many code generators partition IR instructions into "basic blocks," which 
consist of sequences of instructions that are always executed together. The 
partitioning of the IR into basic blocks is the subject of Section 8.4. The 
following section presents simple local transformations that can be used to 
transform basic blocks into modified basic blocks from which more efficient 
code can be generated. These transformations are a rudimentary form of code 
optimization, although the deeper theory of code optimization will not be taken 
up until Chapter 9. An example of a useful, local transformation is the discovery 
of common subexpressions at the level of intermediate code and the resultant 
replacement of arithmetic operations by simpler copy operations. 
Section 8.6 presents a simple code-generation algorithm that generates code 
for each statement in turn, keeping operands in registers as long as possible. 
The output of this kind of code generator can be readily improved by peephole 
optimization techniques such as those discussed in the following Section 8.7. 
The remaining sections explore instruction selection and register allocation. 
8.1 Issues in the Design of a Code Generator 
While the details are dependent on the specifics of the intermediate represen- 
tation, the target language, and the run-time system, tasks such as instruction 
selection, register allocation and assignment, and instruction ordering are en- 
countered in the design of almost all code generators. 
The most important criterion for a code generator is that it produce cor- 
rect code. Correctness takes on special significance because of the number of 
special cases that a code generator might face. Given the premium on correct- 
ness, designing a code generator so it can be easily implemented, tested, and 
maintained is an important design goal. 
8.1. ISSUES IN THE DESIGN OF A CODE GENERATOR 
8.1.1 Input to the Code Generator 
The input to the code generator is the intermediate representation of the source 
program produced by the front end, along with information in the symbol table 
that is used to determine the run-time addresses of the data objects denoted 
by the names in the IR. 
The many choices for the IR include three-address representations such as 
quadruples, triples, indirect triples; virtual machine representations such as 
bytecodes and stack-machine code; linear representations such as postfix no- 
tation; and graphical representations such as syntax trees and DAG's. Many 
of the algorithms in this chapter are couched in terms of the representations 
considered in Chapter 6: three-address code, trees, and DAG7s. The techniques 
we discuss can be applied, however, to the other intermediate representations 
as well. 
In this chapter, we assume that the front end has scanned, parsed, and 
translated the source program into a relatively low-level IR, so that the values 
of the names appearing in the IR can be represented by quantities that the 
target machine can directly manipulate, such as integers and floating-point 
numbers. We also assume that all syntactic and static semantic errors have 
been detected, that the necessary type checking has taken place, and that type- 
conversion operators have been inserted wherever necessary. The code generator 
can therefore proceed on the assumption that its input is free of these kinds of 
errors. 
8.1.2 
The Target Program 
The instruction-set architecture of the target machine has a significant im- 
pact on the difficulty of constructing a good code generator that produces 
high-quality machine code. The most common target-machine architectures 
are RISC (reduced instruction set computer), CISC (complex instruction set 
computer), and stack based. 
A RISC machine typically has many registers, three-address instructions, 
simple addressing modes, and a relatively simple instruction-set architecture. 
In contrast, a CISC machine typically has few registers, two-address instruc- 
tions, a variety of addressing modes, several register classes, variable-length 
instructions, and instructions with side effects. 
In a stack-based machine, operations are done by pushing operands onto a 
stack and then performing the operations on the operands at the top of the 
stack. To achieve high performance the top of the stack is typically kept in 
registers. Stack-based machines almost disappeared because it was felt that 
the stack organization was too limiting and required too many swap and copy 
operations. 
However, stack-based architectures were revived with the introduction of 
the Java Virtual Machine (JVM). The JVM is a software interpreter for Java 
bytecodes, an intermediate language produced by Java compilers. The inter- 
CHAPTER 8. CODE GENERATION 
preter provides software compatibility across multiple platforms, a major factor 
in the success of Java. 
To overcome the high performance penalty of interpretation, which can be 
on the order of a factor of 10, just-in-time (JIT) Java compilers have been 
created. These JIT compilers translate bytecodes during run time to the native 
hardware instruction set of the target machine. Another approach to improving 
Java performance is to build a compiler that compiles directly into the machine 
instructions of the target machine, bypassing the Java bytecodes entirely. 
Producing an absolute machine-language program as output has the ad- 
vantage that it can be placed in a fixed location in memory and immediately 
executed. Programs can be compiled and executed quickly. 
Producing a relocatable machine-language program (often called an object 
module) as output allows subprograms to be compiled separately. A set of 
relocatable object modules can be linked together and loaded for execution by a 
linking loader. Although we must pay the added expense of linking and loading 
if we produce relocatable object modules, we gain a great deal of flexibility 
in being able to compile subroutines separately and to call other previously 
compiled programs from an object module. If the target machine does not 
handle relocation automatically, the compiler must provide explicit relocation 
information to the loader to link the separately compiled program modules. 
Producing an assembly-language program as output makes the process of 
code generation somewhat easier. We can generate symbolic instructions and 
use the macro facilities of the assembler to help generate code. The price paid 
is the assembly step after code generation. 
In this chapter, we shall use a very simple RISC-like computer as our target 
machine. We add to it some CISC-like addressing modes so that we can also 
discuss code-generation techniques for CISC machines. For readability, we use 
assembly code as the target language . As long as addresses can be calculated 
from offsets and other information stored in the symbol table, the code gener- 
ator can produce relocatable or absolute addresses for names just as easily as 
symbolic addresses. 
8.1.3 Instruction Selection 
The code generator must map the IR program into a code sequence that can be 
executed by the target machine. The complexity of performing this mapping is 
determined by a factors such as 
the level of the IR 
the nature of the instruction-set architecture 
the desired quality of the generated code. 
If the IR is high level, the code generator may translate each IR statement 
into a sequence of machine instructions using code templates. Such statement- 
by-statement code generation, however, often produces poor code that needs 
8.1. ISSUES IN THE DESIGN OF A CODE GENERATOR 
509 
further optimization. If the IR reflects some of the low-level details of the un- 
derlying machine, then the code generator can use this information to generate 
more efficient code sequences. 
The nature of the instruction set of the target machine has a strong effect 
on the difficulty of instruction selection. For example, the uniformity and com- 
pleteness of the instruction set are important factors. If the target machine 
does not support each data type in a uniform manner, then each exception to 
the general rule requires special handling. On some machines, for example, 
floating-point operations are done using separate registers. 
Instruction speeds and machine idioms are other important factors. If we 
do not care about the efficiency of the target program, instruction selection is 
straightforward. For each type of three-address statement, we can design a code 
skeleton that defines the target code to be generated for that construct. For 
example, every three-address statement of the form x = y + z, where x, y, and z 
are statically allocated, can be translated into the code sequence 
LD 
ROY y 
// RO = y 
(load y into register RO) 
ADDRO, ROY z 
// RO = R O  + z 
( a d d z t o ~ ~ )  
S T  x, RO 
// x = RO 
(store RO into x) 
This strategy often produces redundant loads and stores. For example, the 
sequence of t 
hree-address statements 
would be translated into 
LD 
ROY b 
// RO = b 
ADD ROY ROY c 
// RO = RO + c 
S T  a, RO 
// a = RO 
LD 
ROY a 
// RO = a 
ADD ROY ROY e 
// RO = RO + e 
S T  d, RO 
// d = RO 
Here, the fourth statement is redundant since it loads a value that has just been 
stored, and so is the third if a is not subsequently used. 
The quality of the generated code is usually determined by its speed and 
size. On most machines, a given IR program can be implemented by many 
different code sequences, with significant cost differences between the different 
implementations. A naive translation of the intermediate code may therefore 
lead to correct but unacceptably inefficient target code. 
For example, if the target machine has an "increment" instruction (INC), 
then the three-address statement a = a + 1 
may be implemented more efficiently 
by the single instruction INC a, rather than by a more obvious sequence that 
loads a into a register, adds one to the register, and then stores the result back 
into a: 
CHAPTER 8. CODE GENERATION 
LD RO, a 
// RO = a 
ADD ROY 
ROY 
#1 // RO = RO + 1 
ST a, RO 
// a = RO 
We need to know instruction costs in order to design good code sequences 
but, unfortunately, accurate cost information is often difficult to obtain. De- 
ciding which machine-code sequence is best for a given three-address construct 
may also require knowledge about the context in which that construct appears. 
In Section 8.9 we shall see that instruction selection can be modeled as a 
tree-pattern matching process in which we represent the IR and the machine 
instructions as trees. We then attempt to "tile" an IR tree with a set of sub- 
trees that correspond to machine instructions. If we associate a cost with each 
machine-instruction subtree, we can use dynamic programming to generate op- 
timal code sequences. Dynamic programming is discussed in Section 8.11. 
8.1.4 
Register Allocation 
A key problem in code generation is deciding what values to hold in what 
registers. Registers are the fastest computational unit on the target machine, 
but we usually do not have enough of them to hold all values. Values not held 
in registers need to reside in memory. Instructions involving register operands 
are invariably shorter and faster than those involving operands in memory, so 
efficient utilization of registers is particularly important. 
The use of registers is often subdivided into two subproblems: 
1. Register allocation, during which we select the set of variables that will 
reside in registers at each point in the program. 
2. Register assignment, during which we pick the specific register that a 
variable will reside in. 
Finding an optimal assignment of registers to variables is difficult, even 
with single-register machines. Mathematically, the problem is NP-complete. 
The problem is further complicated because the hardware and/or the operating 
system of the target machine may require that certain register-usage conventions 
be observed. 
Example 8.1 : 
Certain machines require register-pairs (an even and next odd- 
numbered register) for some operands and results. For example, on some ma- 
chines, integer multiplication and integer division involve register pairs. The 
multiplication instruction is of the form 
where x, the multiplicand, is the even register of an even/odd register pair and 
y, the multiplier, is the odd register. The product occupies the entire even/odd 
register pair. The division instruction is of the form 
8.1. ISSUES IN THE DESIGN OF A CODE GENERATOR 
511 
where the dividend occupies an evenlodd register pair whose even register is x; 
the divisor is y. After division, the even register holds the remainder and the 
odd register the quotient. 
Now, consider the two three-address code sequences in Fig. 8.2 in which the 
only difference in (a) and (b) is the operator in the second statement. The 
shortest assembly-code sequences for (a) and (b) are given in Fig. 8.3. 
Figure 8.2: Two three-address code sequences 
L 
R O , a  
A 
R 0 , b  
A 
R O , c  
SRDA ROY 3 2  
D 
R O , d  
ST 
R1, t 
Figure 8.3: Optimal machine-code sequences 
Ri stands for register i. SRDA stands for Shift-Right-Double-Arithmetic and 
SRDA R 0 , 3 2  shifts the dividend into R1 and clears RO so all bits equal its sign 
bit. L, ST, and A stand for load, store, and add, respectively. vote that the 
optimal choice for the register into which a is to be loaded depends on what 
will ultimately happen to t. 
Strategies for register allocation and assignment are discussed in Section 8.8. 
Section 8.10 shows that for certain classes of machines we can construct code 
sequences that evaluate expressions using as few registers as possible. 
8
.
1
.
5
 Evaluation Order 
The order in which computations are performed can affect the efficiency of the 
target code. As we shall see, some computation orders require fewer registers 
to hold intermediate results than others. However, picking a best order in 
the general case is a difficult NP-complete problem. Initially, we shall avoid 
512 
CHAPTER 8. CODE GENERATION 
the problem by generating code for the three-address statements in the order 
in which they have been produced by the intermediate code generator. In 
Chapter 10, we shall study code scheduling for pipelined machines that can 
execute several operations in a single clock cycle. 
8.2 The Target Language 
Familiarity with the target machine and its instruction set is a prerequisite 
for designing a good code generator. Unfortunately, in a general discussion of 
code generation it is not possible to describe any target machine in sufficient 
detail to generate good code for a complete language on that machine. In 
this chapter, we shall use as a target language assembly code for a simple 
computer that is representative of many register machines. However, the code- 
generation techniques presented in this chapter can be used on many other 
classes of machines as well. 
8.2.1 A Simple Target Machine Model 
Our target computer models a three-address machine with load and store oper- 
ations, computation operations, jump operations, and conditional jumps. The 
underlying computer is a byte-addressable machine with n general-purpose reg- 
isters, RO, R1, . . 
. , 
Rn - 
1. A full-fledged assembly language would have scores 
of instructions. To avoid hiding the concepts in a myriad of details, we shall 
use a very limited set of instructions and assume that all operands are integers. 
Most instructions consists of an operator, followed by a target, followed by a 
list of source operands. A label may precede an instruction. We assume the 
following kinds of instructions are available: 
Load operations: The instruction LD dst, addr loads the value in location 
addr into location dst. This instruction denotes the assignment dst = addr. 
The most common form of this instruction is LD r, 
x which loads the value 
in location x into register r. An instruction of the form LD rl, 
r2 is a 
register-to-register copy in which the contents of register r 2  are copied 
into register rl. 
Store operations: The instruction ST x, 
r stores the value in register r into 
the location x. This instruction denotes the assignment x = 
r. 
Computation operations of the form OP dst, srcl, s r e ,  
where OP is a op- 
erator like ADD or SUB, and dst, srcl , 
and src2 are locations, not necessarily 
distinct. The effect of this machine instruction is to apply the operation 
represented by OP to the values in locations srcl and src2, and place the 
result of this operation in location dst. For example, S
U
B
 rl , 
r 2 ,  r~ com- 
putes rl = r2 - 
r s  Any value formerly stored in rl is lost, but if rl is 
r 2  or r ~ ,  
the old value is read first. Unary operators that take only one 
operand do not have a src2. 
8.2. THE TARGET LANGUAGE 
Unconditional jumps: The instruction BR L causes control to branch to 
the machine instruction with label L. (BR stands for branch.) 
Conditional jumps of the form Bcond r, 
L, 
where r is a register, L is a label, 
and cond stands for any of the common tests on values in the register r. 
For example, B
L
T
Z
 r, L causes a jump to label L if the value in register r is 
less than zero, and allows control to pass to the next machine instruction 
if not. 
We assume our target machine has a variety of addressing modes: 
In instructions, a location can be a variable name x referring to the mem- 
ory location that is reserved for x (that is, the 1-value of x). 
A location can also be an indexed address of the form a(r), where a is 
a variable and r is a register. The memory location denoted by a(r) is 
computed by taking the 1-value of a and adding to it the value in register 
r. For example, the instruction LD R1, a(R2) has the effect of setting 
R l  = 
contents 
(a 
+ 
contents 
( ~ 2 ) ) ,  
where contents(x) denotes the contents 
of the register or memory location represented by x. This addressing 
mode is useful for accessing arrays, where a is the base address of the 
array (that is, the address of the first element), and r holds the number 
of bytes past that address we wish to go to reach one of the elements of 
array a. 
A memory location can be an integer indexed by a register. For ex- 
ample, LD R1, lOO(R2) has the effect of setting R 1  = contents(100 + 
contents(~2)), 
that is, of loading into R 1  the value in the memory loca- 
tion obtained by adding 100 to the contents of register R2. This feature 
is useful for following pointers, as we shall see in the example below. 
We also allow two indirect addressing modes: *r means the memory lo- 
cation found in the location represented by the contents of register r and 
*100(r) means the memory location found in the location obtained by 
adding 100 to the contents of r. For example, LD R1, * 
100 
(R2) has the 
effect of setting R 1  = contents(contents(l00 + 
contents(R2))), that is, of 
loading into R 1  the value in the memory location stored in the memory 
location obtained by adding 100 to the contents of register R2. 
Finally, we allow an immediate constant addressing mode. The constant 
is prefixed by #. The instruction LD R1, #I00 loads the integer 100 into 
register R1, and ADD R1, R1, #I00 adds the integer 100 into register R1. ' 
Comments at the end of instructions are preceded by //. 
Example 8.2 : 
The three-address statement x = y - 
z can be implemented by 
the machine instructions: 
514 
CHAPTER 8. CODE GENERATION 
LD R1, y 
// R1 = y 
LD R2, z 
// R2 = z 
SUB R1, R1, R2 
// R1 = R1 - R2 
ST x, R1 
// x = R1 
We can do better, perhaps. One of the goals of a good code-generation algorithm 
is to avoid using all four of these instructions, whenever possible. For example, 
y and/or z may have been computed in a register, and if so we can avoid the LD 
step(s). Likewise, we might be able to avoid ever storing x if its value is used 
within the register set and is not subsequently needed. 
Suppose a 
is an array whose elements are 8-byte values, perhaps real num- 
bers. Also assume elements of a 
are indexed starting at 0. We may execute the 
three-address instruction b = a 
[i] by the machine instructions: 
LD R1, i 
// R1 = i 
MUL R1, R1, 8 
// R1 = Rl * 8 
LD R2, a(R1) 
// R2 = contents(a + contents(R1)) 
ST b, R2 
// b = R2 
That is, the second step computes 8i, and the third step places in register R2 
the value in the ith element of a - 
the one found in the location that is 8i 
bytes past the base address of the array a. 
Similarly, the assignment into the array a 
represented by three-address in- 
struction a[j] = c is implemented by: 
LD R1, c 
// R1 = c 
LD R2, j 
// R2 = j 
MUL R2, R2, 8 
// R2 = R2 * 8 
ST a(R2), R1 
// contents(a + 
contents(R2)) = R1 
TO 
implement a simple pointer indirection, such as the three-address state- 
ment x = *p, we can use machine instructions like: 
The assignment through a pointer *p = y is similarly implemented in machine 
code by: 
Finally, consider a conditional-jump three-address instruction like 
8.2. THE TARGET LANGUAGE 
The machine-code equivalent would be something like: 
LD 
R1, x 
// R 1  = x 
LD 
R2, y 
// R2 = y 
SUB R1, R1, R2 
// R 1  = R 1  - R2 
BLTZ R 1 ,  M 
// i f  R 1  < 0 jump t o  M 
Here, M is the label that represents the first machine instruction generated from 
the three-address instruction that has label L. As for any three-address instruc- 
tion, we hope that we can save some of these machine instructions because the 
needed operands are already in registers or because the result need never be 
stored. 
8.2.2 Program and Instruction Costs 
We often associate a cost with compiling and running a program. Depending 
on what aspect of a program we are interested in optimizing, some common 
cost measures are the length of compilation time and the size, running time 
and power consumption of the target program. 
Determining the actual cost of compiling and running a program is a com- 
plex problem. Finding an optimal target program for a given source program is 
an undecidable problem in general, and many of the subproblems involved are 
NP-hard. As we have indicated, in code generation we must often be content 
with heuristic techniques that produce good but not necessarily optimal target 
programs. 
For the remainder of this chapter, we shall assume each target-language 
instruction has an associated cost. For simplicity, we take the cost of an in- 
struction to be one plus the costs associated with the addressing modes of the 
operands. This cost corresponds to the length in words of the instruction. 
Addressing modes involving registers have zero additional cost, while those in- 
volving a memory location or constant in them have an additional cost of one, 
because such operands have to be stored in the words following the instruction. 
Some examples: 
The instruction LD RO, R 1  copies the contents of register R 1  into register 
R
O
.
 This instruction has a cost of one because no additional memory 
words are required. 
The instruction LD ROY M loads the contents of memory location M into 
register R
O
.
 The cost is two since the address of memory location M is in 
the word following the instruction. 
The instruction LD R1, *100(R2) loads into register R 1  the value given 
by contents(contents(l00 + contents(R2))). The cost is three because the 
constant 100 is stored in the word following the instruction. 
516 
CHAPTER 8. CODE GENERATION 
In this chapter we assume the cost of a target-language program on a given 
input is the sum of costs of the individual instructions executed when the pro- 
gram is run on that input. Good code-generation algorithms seek to minimize 
the sum of the costs of the instructions executed by the generated target pro- 
gram on typical inputs. We shall see that in some situations we can actually 
generate optimal code for expressions on certain classes of register machines. 
8.2.3 Exercises for Section 8.2 
Exercise 8.2.1 
: 
Generate code for the following three-address statements as- 
suming all variables are stored in memory locations. 
e) The two statements 
Exercise 8.2.2 : 
Generate code for the following three-address statements as- 
suming a and b are arrays whose elements are 4-byte values. 
a) The four-statement sequence 
b) The t 
hree-statement sequence 
c) The three-statement sequence 
8.2. THE TARGET LANGUAGE 
517 
Exercise 8.2.3 
: Generate code for the following three-address sequence as- 
suming that p and q are in memory locations: 
Exercise 8.2.4 
: 
Generate code for the following sequence assuming that x, y, 
and z 
are in memory locations: 
i f  x < y goto L1 
z = o  
got0 L2 
Ll: z 
= 1 
Exercise 8.2.5 
: Generate code for the following sequence assuming hat n is 
in a memory location: 
Exercise 8.2.6 
: 
Determine the costs of the following instruction sequences: 
a> 
LD 
ROY 
y 
LD R1, z 
ADD ROY 
RO, R1 
ST x, RO 
b 
LD RO, i 
MUL ROY 
ROY 
8 
LD R1, a(R0) 
ST b, R1 
c> 
LD 
R O Y  
c 
LD R1, i 
MUL Rl, R1, 8 
ST a(RI), RO 
d) 
LD ROY 
p 
LD Rl, O(R0) 
ST x, R1 
CHAPTER 8. CODE GENERATION 
LD RO, p 
LD R1, x 
ST 0 
(RO) 
, 
R1 
LD 
ROY 
x 
LD 
R1, y 
SUB RO, RO, R1 
BLTZ *R3, RO 
8.3 Addresses in the Target Code 
In this section, we show how names in the IR can be converted into addresses 
in the target code by looking at code generation for simple procedure calls and 
returns using static and stack allocation. In Section 7.1, we described how each 
executing program runs in its own logical address space that was partitioned 
into four code and data areas: 
1. A statically determined area Code that holds the executable target code. 
The size of the target code can be determined at compile time. 
2. A statically determined data area Static for holding global constants and 
other data generated by the compiler. The size of the global constants 
and compiler data can also be determined at compile time. 
3. A dynamically managed area Heap for holding data objects that are allo- 
cated and freed during program execution. The size of the Heap cannot 
be determined at compile time. 
4. A dynamically managed area Stack for holding activation records as they 
are created and destroyed during procedure calls and returns. Like the 
Heap, the size of the Stack cannot be determined at compile time. 
8.3.1 
Static Allocation 
To illustrate code generation for simplified procedure calls and returns, we shall 
focus on the following three-address statements: 
call callee 
return 
h a l t  
act 
ion, which is a placeholder for other three-address statements. 
The size and layout of activation records are determined by the code gener- 
ator via the information about names stored in the symbol table. We shall first 
illustrate how to store the return address in an activation record on a procedure 
8.3. ADDRESSES IN THE TARGET CODE 
519 
call and how to return control to it after the procedure call. For convenience, 
we assume the first location in the activation holds the return address. 
Let us first consider the code needed to implement the simplest case, static 
allocation. Here, a c a l l  callee statement in the intermediate code can be im- 
plemented by a sequence of two target-machine instructions: 
ST 
callee.staticArea, #here + 
20 
BR 
cal 
lee. codeArea 
The ST instruction saves the return address at the beginning of the activation 
record for callee, and the BR transfers control to the target code for the called 
procedure callee. The attribute before callee.staticArea is a constant that gives 
the address of the beginning of the activation record for callee, and the attribute 
callee.codeArea is a constant referring to the address of the first instructiorr of 
the called procedure callee in the Code area of the run-time memory. 
The operand #here+ 20 in the ST instruction is the literal return address; it 
is the address of the instruction following the BR instruction. We assume that 
#here is the address of the current instruction and that the three constants plus 
the two instructions in the calling sequence have a length of 5 words or 20 bytes. 
The code for a procedure ends with a return to the calling procedure, except 
that the first procedure has no caller, so its final instruction is HALT, which 
returns control to the operating system. A return callee statement can be 
implemented by a simple jump instruction 
which transfers control to the address saved at the beginning of the activation 
record for callee. 
Example 8.3 : 
Suppose we have the following three-address code: 
// code for c 
act 
ionl 
c a l l  p 
act 
ionz 
halt 
// code for p 
act 
ion3 
return 
Figure 8.4 shows the target program for this three-address code. We use the 
pseudoinstruction ACTION to represent the sequence of machine instructions to 
execute the statement action, which represents three-address code that is not 
relevant for this discussion. We arbitrarily start the code for procedure c at 
address 100 and for procedure p at address 200. We that assume each ACTION 
instruction takes 20 bytes. We further assume that the activation records for 
these procedures are statically allocated starting at locations 300 and 364, re- 
spectively. 
The instructions starting at address 100 implement the statements 
CHAPTER 8. CODE GENERATION 
actionl; c a l l  p; action2; h a l t  
of the first procedure c. Execution therefore starts with the instruction ACTIONl 
at address 100. The ST instruction at address 120 saves the return address 140 
in the machine-status field, which is the first word in the activation record of p. 
The BR instruction at address 132 transfers control the first instruction in the 
target code of the called procedure p. 
// code for c 
ACTIONl 
// code for a c t  
ionl 
ST 364, #I40 
// save return address 140 in location 364 
BR 200 
// call p 
ACTION2 
H
A
L
T
 
// return to operating system 
... 
// code for p 
ACTION3 
BR *364 
// return to address saved in location 364 
// 300-363 hold activation record for c 
// return address 
// local data for c 
. 
. 
. 
// 364-451 hold activation record for p 
// return address 
// local data for p 
Figure 8.4: Target code for static allocation 
After executing ACTION3, the jump instruction at location 220 is executed. 
Since location 140 was saved at address 364 by the call sequence above, *364 
represents 140 when the BR statement at address 220 is executed. Therefore, 
when procedure p terminates, control returns to address 140 and execution of 
procedure c resumes. 
0
.
 
8.3.2 Stack Allocation 
Static allocation can become stack allocation by using relative addresses for 
storage in activation records. In stack allocation, however, the position of an 
activation record for a procedure is not known until run time. This position is 
usually stored in a register, so words in the activation record can be accessed as 
offsets from the value in this register. The indexed address mode of our target 
machine is convenient for this purpose. 
Relative addresses in an activation record can be taken as offsets from any 
known position in the activation record, as we saw in Chapter 7. For conve- 
8.3. ADDRESSES IN THE TARGET CODE 
521 
nience, we shall use positive offsets by maintaining in a register SP a pointer to 
the beginning of the activation record on top of the stack. When a procedure 
call occurs, the calling procedure increments SP and transfers control to the 
called procedure. After control returns to the caller, we decrement SP, thereby 
deallocating the activation record of the called procedure. 
The code for the first procedure initializes the stack by setting SP to the 
start of the stack area in memory: 
LD 
SP, #stackStart 
// initialize the stack 
code for the first procedure 
H
A
L
T
 
// terminate execution 
A procedure call sequence increments SP, saves the return address, and transfers 
control to the called procedure: 
ADD 
SP , SP , #caller. 
recordsize 
// increment stack pointer 
ST 
*SP , #here + 
16 
// save return address 
B
R
 
callee.codeArea 
// return to caller 
The operand #caller.recordSize represents the size of an activation record, so 
the ADD instruction makes SP point to the next activation record. The operand 
#here + 
16 in the ST instruction is the address of the instruction following BR; 
it is saved in the address pointed to by SP. 
The return sequence consists of two parts. The called procedure transfers 
control to the return address using 
BR 
*O(SP) 
// return to caller 
The reason for using *
O
 (SP) in the BR instruction is that we need two levels 
of indirection: O(SP) is the address of the first word in the activation record 
and *O(SP) is the return address saved there. 
The second part of the return sequence is in the caller, which decrements 
SP, thereby restoring SP to its previous value. That is, after the subtraction SP 
points to the beginning of the activation record of the caller: 
S
U
B
 
SP , SP , #caller. 
recordsize 
// decrement stack pointer 
Chapter 7 contains a broader discussion of calling sequences and the trade- 
offs in the division of labor between the calling and called procedures. 
Example 8.4 : The program in Fig. 8.5 is an abstraction of the quicksort 
program in the previous chapter. Procedure q is recursive, so more than one 
activation of q can be alive at the same time. 
Suppose that the sizes of the activation records for procedures m, 
p, and q 
have been determined to be msize, psize, and qsize, respectively. The first word 
in each activation record will hold a return address. We arbitrarily assume that 
the code for these procedures starts at addresses 100, 200, and 300, respectively, 
CHAPTER 8. CODE GENERATION 
// code for m 
act 
ionl 
c a l l  q 
act 
ionz 
halt 
act 
ions 
return 
// code for p 
// code for q 
act 
ion4 
c a l l  p 
act 
ion5 
c a l l  q 
act 
ion6 
c a l l  q 
return 
Figure 8.5: Code for Example 8.4 
and that the stack starts at address 600. The target program is shown in 
Figure 8.6. 
We assume that ACTION4 
contains a conditional jump to the address 456 of 
the return sequence from q; otherwise, the recursive procedure q is condemned 
to call itself forever. 
If msixe, psixe, and qsixe are 20, 40, and 60, respectively, the first instruction 
at address 100 initializes the SP to 600, the starting address of the stack. SP 
holds 620 just before control transfers from m to q, because msixe is 20. Sub- 
sequently, when q calls p, the instruction at address 320 increments SP to 680, 
where the activation record for p begins; SP reverts to 620 after control returns 
to q. If the next two recursive calls of q return immediately, the maximum value 
of SP during this execution 680. Note, however, that the last stack location used 
is 739, since the activation record of q starting at location 680 extends for 60 
bytes. 
8.3.3 Run-Time Addresses for Names 
The storage-allocation strategy and the layout of local data in an activation 
record for a procedure determine how the storage for names is accessed. In 
Chapter 6, we assumed that a name in a three-address statement is really a 
pointer to a symbol-table entry for that name. This approach has a significant 
advantage; it makes the compiler more portable, since the front end need not 
be changed even when the compiler is moved to a different machine where a 
different run-time organization is needed. On the other hand, generating the 
specific sequence of access steps while generating intermediate code can be of 
8.3. ADDRESSES IN THE TARGET CODE 
L
D
 SPY 
#600 
ACTIONl 
ADD SPY 
SPY 
#msixe 
ST *SPY 
#I52 
BR 300 
S
U
B
 SP , SPY 
#msixe 
ACTION1 
2 
H
A
L
T
 
// code for m 
// initialize the stack 
// code for act 
ionl 
// call sequence begins 
// push return address 
// call q 
// restore SP 
// code for p 
ACTION4 
ADD SPY 
SPY 
#qsixe 
ST *SPY 
#344 
B
R
 200 
S
U
B
 SPY 
SP , #qszxe 
ACTION5 
ADD SPY 
SPY 
#qsixe 
BR *SP, #396 
B
R
 300 
S
U
B
 SPY 
SPY 
#qsixe 
ACTION6 
ADD SPY 
SP, #qsixe 
ST *SPY 
#440 
B
R
 300 
S
U
B
 SPY 
SP , 
#qsixe 
B
R
 *O(SP) 
// return 
/ 
/ code for q 
// contains a conditional jump to 456 
// push return address 
// call p 
// push return address 
// call q 
// push return address 
// call q 
// return 
// stack starts here 
Figure 8.6: Target code for stack allocation 
524 
CHAPTER 8. CODE GENERATION 
significant advantage in an optimizing compiler, since it lets the optimizer take 
advantage of details it would not see in the simple three-address statement. 
In either case, names must eventually be replaced by code to access storage 
locations. We thus consider some elaborations of the simple three-address copy 
statement x = 0. After the declarations in a procedure are processed, suppose 
the symbol-table entry for x contains a relative address 12 for x. For consider 
the case in which x is in a statically allocated area beginning at address static. 
Then the actual run-time address of x is static + 
12. Although the compiler can 
eventually determine the value of static + 
12 at compile time, the position of 
the static area may not be known when intermediate code to access the name 
is generated. In that case, it makes sense to generate three-address code to 
"compute" static + 12, with the understanding that this computation will be 
carried out during the code generation phase, or possibly by the loader, before 
the program runs. The assignment x = 0 then translates into 
I
f
 the static area starts at address 100, the target code for this statement is 
8.3.4 Exercises for Section 8.3 
Exercise 8.3.1 : 
Generate code for the following three-address statements as- 
suming stack allocation where register SP 
points to the top of the stack. 
call p 
call q 
return 
call r 
return 
return 
Exercise 8.3.2 : 
Generate code for the following three-address statements as- 
suming stack allocation where register SP points to the top of the stack. 
e) The two statements 
8.4. BASIC BLOCKS AND FLOW GRAPHS 
525 
Exercise 8.3.3 : Generate code for the following three-address statements 
again assuming stack allocation and assuming a and b are arrays whose ele- 
ments are 4-byte values. 
a) The four-statement sequence 
b) The t 
hree-st 
atement sequence 
c) The three-statement sequence 
8.4 
Basic Blocks and Flow Graphs 
This section introduces a graph representation of intermediate code that is help- 
ful for discussing code generation even if the graph is not constructed explicitly 
by a code-generation algorithm. Code generation benefits from context. We 
can do a better job of register allocation if we know how values are defined 
and used, as we shall see in Section 8.8. We can do a better job of instruction 
selection by looking at sequences of three-address statements, as we shall see in 
Section 8.9. 
The representation is constructed as follows: 
1. Partition the intermediate code into basic blocks, which are maximal se- 
quences of consecutive three-address instructions with the properties that 
(a) The flow of control can only enter the basic block through the first 
instruction in the block. That is, there are no jumps into the middle 
of the block. 
(b) Control will leave the block without halting or branching, except 
possibly at the last instruction in the block. 
2. The basic blocks become the nodes of a flow graph, whose edges indicate 
which blocks can follow which other blocks. 
526 
CHAPTER 8. CODE GENERATION 
The Effect of Interrupts 
The notion that control, once it reaches the beginning of a basic block is 
certain to continue through to the end requires a bit of thought. There are 
many reasons why an interrupt, not reflected explicitly in the code, could 
cause control to leave the block, perhaps never to return. For example, an 
instruction like x = y/z appears not to affect control flow, but if x is 0 it 
could actually cause the program to abort. 
We shall not worry about such possibilities. The reason is as follows. 
The purpose of constructing basic blocks is to optimize the code. Gener- 
ally, when an interrupt occurs, either it will be handled and control will 
come back to the instruction that caused the interrupt, as if control had 
never deviated, or the program will halt with an error. In the latter case, it 
doesn't matter how we optimized the code, even if we depended on control 
reaching the end of the basic block, because the program didn't produce 
its intended result anyway. 
Starting in Chapter 9, we discuss transformations on flow graphs that turn 
the original intermediate code into "optimized" intermediate code from which 
better target code can be generated. The "optimized" intermediate code is 
turned into machine code using the code-generation techniques in this chapter. 
8.4.1 Basic Blocks 
Our first job is to partition a sequence of three-address instructions into basic 
blocks. We begin a new basic block with the first instruction and keep adding 
instructions until we meet either a jump, a conditional jump, or a label on 
the following instruction. In the absence of jumps and labels, control proceeds 
sequentially from one instruction to the next. This idea is formalized in the 
following algorithm. 
Algorithm 8.5 : 
Partitioning three-address instructions into basic blocks. 
INPUT: A sequence of three-address instructions. 
OUTPUT: A list of the basic blocks for that sequence in which each instruction 
is assigned to exactly one basic block. 
METHOD: First, we determine those instructions in the intermediate code that 
are leaders, that is, the first instructions in some basic block. The instruction 
just past the end of the intermediate program is not included as a leader. The 
rules for finding leaders are: 
1. The first three-address instruction in the intermediate code is a leader. 
8.4. BASIC BLOCKS AND FLOW GRAPHS 
527 
2. Any instruction that is the target of a conditional or unconditional jump 
is a leader. 
3. Any instruction that immediately follows a conditional or unconditional 
jump is a leader. 
Then, for each leader, its basic block consists of itself and all instructions up to 
but not including the next leader or the end of the intermediate program. 
1
7
 
Figure 8.7: Intermediate code to set a 10 x 10 matrix to an identity matrix 
Example 8.6 : 
The intermediate code in Fig. 8.7 turns a 10 x 10 matrix a into 
an identity matrix. Although it is not important where this code comes from, 
it might be the translation of the pseudocode in Fig. 8.8. In generating the 
intermediate code, we have assumed that the real-valued array elements take 8 
bytes each, and that the matrix a is stored in row-major form. 
for i from 1 
to 10 do 
for j from 1 
to 10 do 
a[i, 
j] = 0.0; 
for i from 1 
to 10 do 
a[i, 
i] = 1.0; 
Figure 8.8: Source code for Fig. 8.7 
CHAPTER 8. CODE GENERATION 
First, instruction 1 is a leader by rule (I) of Algorithm 8.5. To find the 
other leaders, we first need to find the jumps. In this example, there are three 
jumps, all conditional, at instructions 9, 11, and 17. By rule (2), the targets of 
these jumps are leaders; they are instructions 3, 2, and 13, respectively. Then, 
by rule (3), each instruction following a jump is a leader; those are instructions 
10 and 12. Note that no instruction follows 17 in this code, but if there were 
code following, the 18th instruction would also be a leader. 
We conclude that the leaders are instructions 1, 2, 3, 10, 12, and 13. The 
basic block of each leader contains all the instructions from itself until just 
before the next leader. Thus, the basic block of 1 is just 1, for leader 2 the 
block is just 2. Leader 3, however, has a basic block consisting of instructions 3 
through 9, inclusive. Instruction 10's block is 10 and 11; 
instruction 12's block 
is just 12, and instruction 13's block is 13 through 17. 
8.4.2 Next-Use Information 
Knowing when the value of a variable will be used next is essential for generating 
good code. If the value of a variable that is currently in a register will never be 
referenced subsequently, then that register can be assigned to another variable. 
The use of a name in a three-address statement is defined as follows. Suppose 
three-address statement i assigns a value to x. If statement j has x as an 
operand, and control can flow from statement i to j along a path that has 
no intervening assignments to x, 
then we say statement j uses the value of x 
computed at statement i. We further say that x 
is live at statement i. 
We wish to determine for each three-address statement x 
= 
y + 
z what the 
next uses of x, 
y, and z are. For the present, we do not concern ourselves with 
uses outside the basic block containing this three-address statement. 
Our algorithm to determine liveness and next-use information makes a back- 
ward pass over each basic block. We store the information in the symbol table. 
We can easily scan a stream of three-address statements to find the ends of ba- 
sic blocks as in Algorithm 8.5. Since procedures can have arbitrary side effects, 
we assume for convenience that each procedure call starts a new basic block. 
Algorithm 8.7: Determining the liveness and next-use information for each 
statement in a basic block. 
INPUT: A basic block B 
of three-address statements. We assume that the 
symbol table initially shows all nontemporary variables in B as being live on 
exit. 
OUTPUT: At each statement i :  
x = 
y + 
z in B, 
we attach to i the liveness and 
next-use information of x, y, and z. 
METHOD: We start at the last statement in B and scan backwards to the 
beginning of B. 
At each statement i: 
x 
= 
y + 
z in B, 
we do the following: 
1. Attach to statement i the information currently found in the symbol table 
regarding the next use and liveness of x, y, and y. 
8.4. BASIC BLOCKS AND FLOW GRAPHS 
2. In the symbol table, set x to "not live" and "no next use." 
3. In the symbol table, set y and z to "live" and the next uses of y and z to 
2. 
Here we have used + 
as a symbol representing any operator. If the three-address 
statement i is of the form x = + 
y or x = y, the steps are the same as above, 
ignoring z. Note that the order of steps (2) and (3) may not be interchanged 
because x may be y or x. 
8
.
4
.
3
 Flow Graphs 
Once an intermediate-code program is partitioned into basic blocks, we repre- 
sent the flow of control between them by a flow graph. The nodes of the flow 
graph are the basic blocks. There is an edge from block B to block C if and 
only if it is possible for the first instruction in block C to immediately follow 
the last instruction in block B. Thete are two ways that such an edge could be 
justified: 
There is a conditional or unconditional jump from the end of B to the 
beginning of C. 
C immediately follows B in the original order of the three-address instruc- 
tions, and B does not end in an unconditional jump. 
We say that B is a predecessor of C, and C is a successor of B. 
Often we add two nodes, called the entry and exit, that do not correspond 
to executable intermediate instructions. There is an edge from the entry to the 
first executable node of the flow graph, that is, to the basic block that comes 
from the first instruction of the intermediate code. There is an edge to the 
exit from any basic block that contains an instruction that could be the last 
executed instruction of the program. If the final instruction of the program is 
not an unconditional jump, then the block containing the final instruction of 
the program is one predecessor of the exit, but so is any basic block that has a 
jump to code that is not part of the program. 
Example 8.8 : The set of basic blocks constructed in Example 8.6 yields the 
flow graph of Fig. 8.9. The entry points to basic block B1, since B1 contains 
the first instruction of the program. The only successor of B1 is B2, because 
B1 does not end in an unconditional jump, and the leader of B2 immediately 
follows the end of B1. 
Block B3 has two successors. One is itself, because the leader of B3, 
instruc- 
tion 3, is the target of the conditional jump at the end of B3, 
instruction 9. The 
other successor is B4, 
because control can fall through the conditional jump at 
the end of B3 and next enter the leader of B4. 
Only Bs points to the exit of the flow graph, since the only way to get to 
code that follows the program from which we constructed the flow graph is to 
fall through the conditional jump that ends B6. 
CHAPTER 8. CODE GENERATION 
ENTRY 
B
1
 
Figure 8.9: Flow graph from Fig. 8.7 
8.4.4 Representation of Flow Graphs 
First, note from Fig. 8.9 that in the flow graph, it is normal to replace the jumps 
to instruction numbers or labels by jumps to basic blocks. Recall that every 
conditional or unconditional jump is to the leader of some basic block, and it 
is to this block that the jump will now refer. The reason for this change is that 
after constructing the flow graph, it is common to make substantial changes 
to the instructions in the various basic blocks. If jumps were to instructions, 
we would have to fix the targets of the jumps every time one of the target 
instructions was changed. 
Flow graphs, being quite ordinary graphs, can be represented by any of the 
data structures appropriate for graphs. The content of nodes (basic blocks) 
need their own representation. We might represent the content of a node by a 
8.4. BASIC BLOCKS AND FLOW GRAPHS 
531 
pointer to the leader in the array of three-address instructions, together with a 
count of the number of instructions or a second pointer to the last instruction. 
However, since we may be changing the number of instructions in a basic block 
frequently, it is likely to be more efficient to create a linked list of instructions 
for each basic block. 
8.4.5 
Loops 
Programming-language constructs like while-statements, do-while-statements, 
and for-statements naturally give rise to loops in programs. Since virtually every 
program spends most of its time in executing its loops, it is especially important 
for a compiler to generate good code for loops. Many code transformations 
depend upon the identification of "loops" in a flow graph. We say that a set of 
nodes L in a flow graph is a loop if 
1. There is a node in L called the loop entry with the property that no other 
node in L has a predecessor outside L. That is, every path from the entry 
of the entire flow graph to any node in L goes through the loop entry. 
2. Every node in L has a nonempty path, completely within L, to the entry 
of L. 
Example 8.9 : 
The flow graph of Fig. 8.9 has three loops: 
1. B3 by itself. 
2. Bg by itself. 
The first two are single nodes with an edge to the node itself. For instance, 
B3 forms a loop with B3 as its entry. Note that the second requirement for a 
loop is that there be a nonempty path from B3 to itself. Thus, a single node 
like B2, 
which does not have an edge B2 + 
B2, 
is not a loop, since there is no 
nonempty path from B2 to itself within {B2). 
The third loop, L = {B2, 
B3, 
B4), 
has B2 
as its loop entry. Note that among 
these three nodes, only B2 
has a predecessor, B1, that is not in L. Further, each 
of the three nodes has a nonempty path to B2 staying within L. For instance, 
B2 has the path B2 + 
B3 + 
B4 + 
B2. 
8.4.6 
Exercises for Section 8.4 
Exercise 8.4.1 : 
Figure 8.10 is a simple matrix-multiplication program. 
a) Translate the program into three-address statements of the type we have 
been using in this section. Assume the matrix entries are numbers that 
require 8 bytes, and that matrices are stored in row-major order. 
532 
CHAPTER 8. CODE GENERATION 
b) Construct the flow graph for your code from (a). 
c) Identify the loops in your flow graph from (b). 
f o r  (i=O; i<n; i++) 
f o r  (j=O; j<n; j++) 
c[i] [j] = 0.0; 
f o r  (i=O; i<n; i++) 
f o r  (j=O; j<n; j++) 
f o r  (k=O; k<n; k++) 
c[i] Cjl = cCil Cjl + aCil Ckl*bCkl Cjl ; 
Figure 8.10: A matrix-multiplication algorithm 
Exercise 8.4.2 : 
Figure 8.11 is code to count the number of primes from 2 to 
n, using the sieve method on a suitably large array a. That is, a[i] 
is T
R
U
E
 at 
the end only if there is no prime & 
or less that evenly divides i. We initialize 
all a[i] 
to T
R
U
E
 and then set a[j] 
to FALSE if we find a divisor of j. 
a) Translate the program into three-address statements of the type we have 
been using in this section. Assume integers require 4 bytes. 
b) Construct the flow graph for your code from (a). 
c) Identify the loops in your flow graph from (b). 
f o r  (i=2; i<=n; i++) 
aci] = TRUE; 
count = 0; 
s = s q r t  
(n) ; 
f o r  (i=2; i<=s; i++) 
i f  (a[i]) /* i has been found t o  be a prime */ C 
count++ 
; 
f o r  (j=2*i; j<=n; j = j + i )  
a[j] = FALSE; /* no multiple of i is a prime */ 
Figure 8.11: Code to sieve for primes 
8.5. OPTIMIZATION OF BASIC BLOCKS 
8.5 Optimization of Basic Blocks 
We can often obtain a substantial improvement in the running time of code 
merely by performing local optimization within each basic block by itself. More 
thorough global optimization, which looks at how information flows among the 
basic blocks of a program, is covered in later chapters, starting with Chapter 9. 
It is a complex subject, with many different techniques to consider. 
8
.
5
.
1
 The DAG Representation of Basic Blocks 
Many important techniques for local optimization begin by transforming a basic 
block into a DAG (directed acyclic graph). In Section 6.1 
.l, 
we introduced the 
DAG as a representation for single expressions. The idea extends naturally 
to the collection of expressions that are created within one basic block. We 
construct a DAG for a basic blockas follows: 
1. There is a node in the DAG for each of the initial values of the variables 
appearing in the basic block. 
2. There is a node N associated with each statement s within the block. 
The children of N are those nodes corresponding to statements that are 
the last definitions, prior to s, of the operands used by s. 
3. Node N is labeled by the operator applied at s, and also attached to N 
is the list of variables for which it is the last definition within the block. 
4. Certain nodes are designated output nodes. These are the nodes whose 
variables are live o n  exit from the block; that is, their values may be 
used later, in another block of the flow graph. Calculation of these "live 
variables" is a matter for global flow analysis, discussed in Section 9.2.5. 
The DAG representation of a basic block lets us perform several code- 
improving transformations on the code represented by the block. 
a) We can eliminate local common subexpressions, that is, instructions that 
compute a value that has already been computed. 
b) We can eliminate dead code, that is, instructions that compute a value 
that is never used. 
c) We can reorder statements that do not depend on one another; such 
reordering may reduce the time a temporary value needs to be preserved 
in a register. 
d) We can apply algebraic laws to reorder operands of three-address instruc- 
tions, and sometimes t 
hereby simplify t 
he computation. 
534 
CHAPTER 8. CODE GENERATION 
8.5.2 
Finding Local Common Subexpressions 
Common subexpressions can be detected by noticing, as a new node M is about 
to be added, whether there is an existing node N with the same children, in 
the same order, and with the same operator. If so, N computes the same value 
as M and may be used in its place. This technique was introduced as the 
"value-number" method of detecting common subexpressions in Section 6.1.1. 
Example 8.10 : 
A DAG for the block 
is shown in Fig. 8.12. When we construct the node for the third statement 
c = b + c, we know that the use of b in b + c refers to the node of Fig. 8.12 
labeled -, because that is the most recent definition of b. Thus, we do not 
confuse the values computed at statements one and three. 
Figure 8.12: DAG for basic block in Example 8.10 
However, the node corresponding to the fourth statement d = a - 
d has the 
operator - and the nodes with attached variables a and do as children. Since 
the operator and the children are the same as those for the node corresponding 
to statement two, we do not create this node, but add d to the list of definitions 
for the node labeled -. 
It might appear that, since there are only three nonleaf nodes in the DAG of 
Fig. 8.12, the basic block in Example 8.10 can be replaced by a block with only 
three statements. In fact, if b is not live on exit from the block, then we do not 
need to compute that variable, and can use d to receive the value represented 
by the node labeled -. in Fig. 8.12. The block then becomes 
8.5. OPTIMIZATION OF BASIC BLOCKS 
535 
However, if both b and d are live on exit, then a fourth statement must be 
used to copy the value from one to the other.' 
Example 8.11 : 
When we look for common subexpressions, we really are look- 
ing for expressions that are guaranteed to compute the same value, no matter 
how that value is computed. Thus, the DAG method will miss the fact that the 
expression computed by the first and fourth statements in the sequence 
is the same, namely bo + 
co. That is, even though b and c both change between 
the first and last statements, their sum remains the same, because b + c = 
( b  - 
d) + 
(c + 
d). The DAG for this sequence is shown in Fig. 8.13, but does 
not exhibit any common subexpressions. However, algebraic identities applied 
to the DAG, as discussed in Section 8.5.4, may expose the equivalence. 
Figure 8.13: DAG for basic block in Example 8.11 
8.5.3 Dead Code Elimination 
The operation on DAG's that corresponds to dead-code elimination can be im- 
plemented as follows. We delete from a DAG any root (node with no ancestors) 
that has no live variables attached. Repeated application of this transformation 
will remove all nodes from the DAG that correspond to dead code. 
Example 8.12: If, in Fig. 8.13, a and b are live but c and e are not, we can 
immediately remove the root labeled e. Then, the node labeled c becomes a 
root and can be removed. The roots labeled a and b remain, since they each 
have live variables attached. 
'1n general, we must be careful, when reconstructing code from DAG's, how we choose 
the names of variables. If a variable x 
is defined twice, or if it is assigned once and the initial 
value xo is also used, then we must make sure that we do not change the value of x until we 
have made all uses of the node whose value x previously held. 
536 
CHAPTER 8. CODE GENERATION 
8.5.4 
The Use of Algebraic Identities 
Algebraic identities represent another important class of optimizations on basic 
blocks. For example, we may apply arithmetic identities, such as 
to eliminate computations from a basic block. 
Another class of algebraic optimizations includes local reduction in strength, 
that is, replacing a more expensive operator by a cheaper one as in: 
x 
- 
- 
X X X  
2 x x 
= 
x + x  
4 2  
= 
x x 0.5 
A third class of related optimizations is constant folding. Here we evaluate 
constant expressions at compile time and replace the constant expressions by 
their  value^.^ Thus the expression 2 
* 3.14 would be replaced by 6.28. Many 
constant expressions arise in practice because of the frequent use of symbolic 
constants in programs. 
The DAG-construction process can help us apply these and other more 
general algebraic transformations such as commutativity and associativity. For 
example, suppose the language reference manual specifies 
that * is commutative; 
that is, x* 
y 
= 
y*x. 
Before we create a new node labeled * with left child M 
and 
right child N, we always check whether such a node already exists. However, 
because * is commutative, we should then check for a node having operator *, 
left child N, and right child M .  
The relational operators such as < and = sometimes generate unexpected 
common subexpressions. For example, the condition x > y can also be tested 
by subtracting the arguments and performing a test on the condition code set 
by the s~btraction.~ 
Thus, only one node of the DAG may need to be generated 
for x - 
y 
and x > y. 
Associative laws might also be applicable to expose common subexpressions. 
For example, if the source code has the assignments 
the following intermediate code might be generated: 
2~rithmetic 
expressions should be evaluated the same way at compile time as they are at 
run time. K. Thompson has suggested an elegant solution to constant folding: compile the 
constant expression, execute the target code on the spot, and replace the expression with the 
result. Thus, the compiler does not need to contain an interpreter. 
3 ~ h e  
subtraction can, however, introduce overflows and underflows while a compare in- 
struction would not. 
8.5. OPTIMIZATION OF BASIC BLOCKS 
If t is not needed outside this block, we can change this sequence to 
using both the associativity and commutativity of +. 
The compiler writer should examine the language reference manual care- 
fully to determine what rearrangements of computations are permitted, since 
(because of possible overflows or underflows) computer arithmetic does not al- 
ways obey the algebraic identities of mathematics. For example, the Fortran 
standard states that a compiler may evaluate any mathematically equivalent 
expression, provided that the integrity of parentheses is not violated. Thus, 
a compiler may evaluate x * y - 
x * x as x * (y - 
x), but it may not evaluate 
a + 
(b - 
c) as (a 
+ 
b) - 
c. A Fortran compiler must therefore keep track of where 
parentheses were present in the source language expressions if it is to optimize 
programs in accordance with the language definition. 
8.5.5 
Representation of Array References 
At first glance, it might appear that the array-indexing instructions can be 
treated like any other operator. Consider for instance the sequence of three- 
address statements: 
If we think of a [il as an operation involving a and i, 
similar to a + 
i, 
then 
it might appear as if the two uses of a[il were a common subexpression. In 
that case, we might be tempted to "optimize" by replacing the third instruction 
z = a Cil by the simpler z = x. However, since j could equal i, 
the middle 
statement may in fact change the value of a [il 
; thus, it is not legal to make 
this change. 
The proper way to represent array accesses in a DAG is as follows. 
1. An assignment from an array, like x = a Cil , 
is represented by creating a 
node with operator =[] and two children representing the initial value of 
the array, a0 in this case, and the index i. 
Variable x becomes a label of 
this new node. 
2. An assignment to an array, like a[jl = y, is represented by a new node 
with operator [I= and three children representing ao, j and y. There is 
no variable labeling this node. What is different is that the creation of 
* 
CHAPTER 8. CODE GENERATION 
this node kzlls all currently constructed nodes whose value depends on ao. 
A 
node that has been killed cannot receive any more labels; that is, it 
cannot become a common subexpression. 
Example 8.13 : 
The DAG 
for the basic block 
is shown in Fig. 8.14. The node N for x is created first, but when the node 
labeled [ 
1 
= is created, N is killed. Thus, when the node for x is created, it 
cannot be identified with N, and a new node with the same operands a0 and 
io 
must be created instead. 
Figure 8.14: The DAG 
for a sequence of array assignments 
Example 8.14 : Sometimes, a node must be killed even though none of its 
children have an array like a0 in Example 8.13 as attached variable. Likewise, 
a node can kill if it has a descendant that is an array, even though none of its 
children are array nodes. For instance, consider the three-address code 
What is happening here is that, for efficiency reasons, b has been defined to 
be a position in an array a
.
 For example, if the elements of a 
are four bytes long, 
then b represents the fourth element of a. 
If j and i represent the same value, 
then b [i] 
and b[j] represent the same location. Therefore it is important 
to have the third instruction, b[j] = y, kill the node with x as its attached 
variable. However, as we see in Fig. 8.15, both the killed node and the node 
that does the killing have z+o as a grandchild, not as a child. 
8.5. OPTIMIZATION OF BASIC BLOCKS 
Figure 8.15: A node that kills a use of an array need not have that array as a 
child 
8.5.6 Pointer Assignments and Procedure Calls 
When we assign indirectly through a pointer, as in the assignments 
we do not know what p or q point to. In effect, x = *p is a use of every 
variable whatsoever, and *q = y is a possible assignment to every variable. As 
a consequence, the operator =* must take all nodes that are currently associated 
with identifiers as arguments, which is relevant for dead-code elimination. More 
importantly, the *= operator kills all other nodes so far constructed in the DAG. 
There are global pointer analyses one could perform that might limit the set 
of variables a pointer could reference at a given place in the code. Even local 
analysis could restrict the scope of a pointer. For instance, in the sequence 
we know that x, and no other variable, is given the value of y, so we don't need 
to kill any node but the node to which x was attached. 
Procedure calls behave much like assignments through pointers. In the 
absence of global data-flow information, we must assume that a procedure uses 
and changes any data to which it has access. Thus, if variable x is in the scope 
of a procedure P, 
a call to P both uses the node with attached variable x and 
kills that node. 
8.5.7 Reassembling Basic Blocks From DAG's 
After we perform whatever optimizations are possible while constructing the 
DAG or by manipulating the DAG once constructed, we may reconstitute the 
three-address code for the basic block from which we built the DAG. For each 
540 
CHAPTER 8. CODE GENERATION 
node that has one or more attached variables, we construct a three-address 
statement that computes the value of one of those variables. We prefer to 
compute the result into a variable that is live on exit from the block. However, if 
we do not have global live-variable information to work from, we need to assume 
that every variable of the program (but not temporaries that are generated by 
the compiler to process expressions) is live on exit from the block. 
If the node has more than one live variable attached, then we have to in- 
troduce copy statements to give the correct value to each of those variables. 
Sometimes, global optimization can eliminate those copies, if we can arrange to 
use one of two variables in place of the other. 
Example 8.15 : Recall the DAG of Fig. 8.12. In the discussion following 
Example 8.10, we decided that if b is not live on exit from the block, then the 
three statements 
suffice to reconstruct the basic block. The third instruction, c = d + c, must use 
d as an operand rather than b, because the optimized block never computes b. 
If both b and d are live on exit, or if we are not sure whether or not they 
are live on exit, then we need to compute b as well as d. We can do so with the 
sequence 
This basic block is still more efficient than the original. Although the number 
of instructions is the same, we have replaced a subtraction by a copy, which 
tends to be less expensive on most machines. Further, it may be that by doing 
a global analysis, we can eliminate the use of this computation of b outside 
the block by replacing it by uses of d. In that case, we can come back to this 
basic block and eliminate b = d later. Intuitively, we can eliminate this copy if 
wherever this value of b is used, d is still holding the same value. That situation 
may or may not be true, depending on how the program recomputes d. 
When reconstructing the basic block from a DAG, we not only need to worry 
about what variables are used to hold the values of the DAG's nodes, but we 
also need to worry about the order in which we list the instructions computing 
the values of the various nodes. The rules to remember are 
1. The order of instructions must respect the order of nodes in the DAG. 
That is, we cannot compute a node's value until we have computed a 
value for each of its children. 
8.5. OPTIMIZATION OF BASIC BLOCKS 
541 
2. Assignments to an array must follow all previous assignments to, or eval- 
uations from, the same array, according to the order of these instructions 
in the original basic block. 
3. Evaluations of array elements must follow any previous (according to the 
original block) assignments to the same array. The only permutation 
allowed is that two evaluations from the same array may be done in either 
order, as long as neither crosses over an assignment to that array. 
4. Any use of a variable must follow all previous (according to the original 
block) procedure calls or indirect assignments through a pointer. 
5. Any procedure call or indirect assignment through a pointer must follow 
all previous (according to the original block) evaluations of any variable. 
That is, when reordering code, no statement may cross a procedure call or 
assignment through a pointer, and uses of the same array may cross each other 
only if both are array accesses, but not assignments to elements of the array. 
8.5.8 Exercises for Section 8.5 
Exercise 8.5.1 : 
Construct the DAG for the basic block 
Exercise 8.5.2 : 
Simplify the three-address code of Exercise 8.5.1, assuming 
a) Only a is live on exit from the block. 
b) a, b, and c are live on exit from the block. 
Exercise 8.5.3 : 
Construct the basic block for the code in block B6 of Fig. 8.9. 
Do not forget to include the comparison i 5 10. 
Exercise 8.5.4 : 
Construct the basic block for the code in block B3 of Fig. 8.9. 
Exercise 8.5.5 : 
Extend Algorithm 8.7 to process three-statements of the form 
a) a[il = b 
b) a = b [i] 
Exercise 8.5.6 : 
Construct the DAG for the basic block 
CHAPTER 8. CODE GENERATION 
on the assumption that 
a) p can point anywhere. 
b) p can point only to b or d. 
! 
Exercise 8.5.7 
: 
If a pointer or array expression, such as a Cil or *p is assigned 
and then used, without the possibility of being changed in the interim, we can 
take advantage of the situation to simplify the DAG. For example, in the 
code of Exercise 8.5.6, since p is not assigned between the second and fourth 
statements, the statement e = *p can be replaced by e = c, regardless of what 
p points to. Revise the DAG-construction algorithm to take advantage of such 
situations, and apply your algorithm to the code of Example 8.5.6. 
Exercise 8.5.8 : 
Suppose a basic block is formed from the C assignment state- 
ment 
s 
a) Give the three-address statements (only one addition per statement) for 
this block. 
b) Use the associative and commutative laws to modify the block to use the 
fewest possible number of instructions, assuming both x and y are live on 
exit from the block. 
8.6 A Simple Code Generator 
In this section, we shall consider an algorithm that generates code for a single 
basic block. It considers each three-address instruction in turn, and keeps track 
of what values are in what registers so it can avoid generating unnecessary loads 
and stores. 
One of the primary issues during code generation is deciding how to use 
registers to best advantage. There are four principal uses of registers: 
In most machine architectures, some or all of the operands of an operation 
must be in registers in order to perform the operation. 
Registers make good temporaries - 
places to hold the result of a subex- 
pression while a larger expression is being evaluated, or more generally, a 
place to hold a variable that is used only within a single basic block. 
8.6. A SIMPLE CODE GENERATOR 
Registers are used to hold (global) values that are computed in one basic 
block and used in other blocks, for example, a loop index that is incre- 
mented going around the loop and is used several times within the loop. 
Registers are often used to help with run-time storage management, for 
example, to manage the run-time stack, including the maintenance of 
stack pointers and possibly the top elements of the stack itself. 
These are competing needs, since the number of registers available is limited. 
The algorithm in this section assumes that some set of registers is available 
to hold the values that are used within the block. Typically, this set of regis- 
ters does not include all the registers of the machine, since some registers are 
reserved for global variables and managing the stack. We assume that the basic 
block has already been transformed into a preferred sequence of three-address 
instructions, by transformations such as combining common subexpressions. 
We further assume that for each operator, there is exactly one machine instruc- 
tion that takes the necessary operands in registers and performs that operation, 
leaving the result in a register. The machine instructions are of the form 
LD reg, mem 
ST mem, reg 
OP reg, reg, reg 
8.6.1 
Register and Address Descriptors 
Qur code-generation algorithm considers each three-address instruction in turn 
and decides what loads are necessary to get the needed operands into registers. 
After generating the loads, it generates the operation itself. Then, if there is a 
need to store the result into a memory location, it also generates that store. 
In order to make the needed decisions, we require a data structure that tells 
us what program variables currently have their value in a register, and which 
register or registers, if so. We also need to know whether the memory location 
for a given variable currently has the proper value for that variable, since a new 
value for the variable may have been computed in a register and not yet stored. 
The desired data structure has the following descriptors: 
1. For each available register, a register descriptor keeps track of the variable 
names whose current value is in that register. Since we shall use only those 
registers that are available for local use within a basic block, we assume 
that initially, all register descriptors are empty. As the code generation 
progresses, each register will hold the value of zero or more names. 
2. For each program variable, an address descriptor keeps track of the loca- 
tion or locations where the current value of that variable can be found. 
The location might be a register, a memory address, a stack location, or 
some set of more than one of these. The information can be stored in the 
symbol-table entry for that variable name. 
CHAPTER 8. CODE GENERATION 
8.6.2 The Code-Generation Algorithm 
An essential part of the algorithm is a function getReg(I), which selects regis- 
ters for each memory location associated with the three-address instruction I. 
Function getReg has access to the register and address descriptors for all the 
variables of the basic block, and may also have access to certain useful data-flow 
information such as the variables that are live on exit from the block. We shall 
discuss getReg after presenting the basic algorithm. While we do not know the 
total number of registers available for local data belonging to a basic block, we 
assume that there are enough registers so that, after freeing all available regis- 
ters by storing their values in memory, there are enough registers to accomplish 
any three-address operation. 
In a three-address instruction such as x = y + z, we shall treat + as a generic 
operator and ADD as the equivalent machine instruction. We do not, therefore, 
take advantage of commutativity of +. Thus, when we implement the operation, 
the value of y must be in the second register mentioned in the ADD instruction, 
never the third. A possible improvement to the algorithm is to generate code 
for both x = y + z and x = z + 
y whenever + is a commutative operator, and pick 
the better code sequence. 
Machine Instructions for Operations 
For a three-address instruction such as x = 
y + 
z, do the following: 
1. Use getReg(x = 
y + 
z) to select registers for x, y, and z. Call these R,, 
R,, and R,. 
2. If y is not in R, (according to the register descriptor for R,), then issue 
an instruction LD R,, y', where y' is one of the memory locations for y 
(according to the address descriptor for y). 
3. Similarly, if z is not in R,, issue and instruction LD R,, z', where z' is a 
location for x 
. 
4. Issue the instruction ADD R,, R, , 
RZ. 
Machine Instructions for Copy Statements 
There is an important special case: a three-address copy statement of the form 
x = y. We assume that getReg will always choose the same register for both 
x and y. If y is not already in that register R,, then generate the machine 
instruction LD R,, y. If y was already in R,, we do nothing. It is only necessary 
that we adjust the register description for R, so that it includes x as one of the 
values found there. 
8.6. A SIMPLE CODE GENERATOR 
Ending the Basic Block 
As we have described the algorithm, variables used by the block may wind up 
with their only location being a register. If the variable is a temporary used 
only within the block, that is fine; when the block ends, we can forget about 
the value of the temporary and assume its register is empty. However, if the 
variable is live on exit from the block, or if we don't know which variables are 
live on exit, then we need to assume that the value of the variable is needed 
later. In that case, for each variable x whose location descriptor does not say 
that its value is located in the memory location for x, we must generate the 
instruction ST x, R, where R is a register in which x's value exists at the end of 
the block. 
Managing Register and Address Descriptors 
As the code-generation algorithm issues load, store, and other machine instruc- 
tions, it needs to update the register and address descriptors. The rules are as 
follows: 
1. For the instruction LD R, 
x 
(a) Change the register descriptor for register R so it holds only x. 
(b) Change the address descriptor for x by adding register R as an ad- 
ditional location. 
2. For the instruction ST x, 
R, change the address descriptor for x to include 
its own memory location. 
3. For an operation such as ADD Rx, 
R,, R, implementing a three-address 
instruction x = 
y + 
x 
(a) Change the register descriptor for Rx so that it holds only x. 
(b) Change the address descriptor for x so that its only location is fix. 
Note that the memory location for x is not now in the address de- 
scriptor for x. 
(c) Remove Rx from the address descriptor of any variable other than 
x. 
4. When we process a copy statement x = y, after generating the load for y 
into register By, 
if needed, and after managing descriptors as for all load 
statements (per rule I): 
(a) Add x to the register descriptor for R,. 
(b) Change the address descriptor for x so that its only location is R, . 
Example 8.16 : 
Let us translate the basic block consisting of the three-address 
statements 
CHAPTER 8. CODE GENERATION 
Here we assume that t, 
u, and v are temporaries, local to the block, while a, b, 
c, and d are variables that are live on exit from the block. Since we have not 
yet discussed how the function getReg might work, we shall simply assume that 
there are as many registers as we need, but that when a register's value is no 
longer needed (for example, it holds only a temporary, all of whose uses have 
been passed), then we reuse its register. 
A summary of all the machine-code instructions generated is in Fig. 8.16. 
The figure also shows the register and address descriptors before and after the 
translation of each three-address instruction. 
R 1  
R2 
R 3  
a
b
c
d
t
u
v
 
LD R1, a 
LD R2, b 
SUB R2, R1, R2 
LD R3, c 
SUB R l ,  R i ,  R 3  
ADD R i ,  R3, R 1  
u 
(
t
 
(
c
 
I d  l a  (
v
 
1 
I R 2 l b  l c  ( R 1 1  
I R 3  I 
exit 
I a 
I 
b lc,~31 
d I R 2  1 R 1  1 
1 
Figure 8.16: Instructions generated and the changes in the register and address 
descriptors 
v = t + u  
ADD R3, R2, R l  
For the first three-address instruction, t = a - 
b we need to issue three in- 
structions, since nothing is in a register initially. Thus, we see a and b loaded 
8.6. A SIMPLE CODE GENERATOR 
547 
into registers R 1  and R2, and the value t produced in register R2. Notice that 
we can use R2 for t because the value b previously in R2 is not needed within 
the block. Since b is presumably live on exit from the block, had it not been 
in its own memory location (as indicated by its address descriptor), we would 
have had to store R
2
 into b first. The decision to do so, had we needed R2, 
would be taken by getReg. 
The second instruction, u = a - 
c, does not require a load of a, since it is 
already in register R1. Further, we can reuse R 1  for the result, u, since the value 
of a, previously in that register, is no longer needed within the block, and its 
value is in its own memory location if a is needed outside the block. Note that 
we change the address descriptor for a to indicate that it is no longer in R1, but 
is in the memory location called a. 
The third instruction, v = t + u, requires only the addition. Further, we can 
use R3 for the result, v, since the value of c in that register is no longer needed 
within the block, and c has its value in its own memory location. 
The copy instruction, a = d, requires a load of d, since it is not in memory. 
We show register R2's descriptor holding both a and d. The addition of a to 
the register descriptor is the result of our processing the copy statement, and 
is not the result of any machine instruction. 
The fifth instruction, d = v + u, uses two values that are in registers. Since 
u is a temporary whose value is no longer needed, we have chosen to reuse its 
register R 1  for the new value of d. Notice that d is now in only R1, and is not 
in its own memory location. The same holds for a, which is in R2 and not in 
the memory location called a. As a result, we need a "coda" to the machine 
code for the basic block that stores the live-on-exit variables a and d into their 
memory locations. We show these as the last two instructions. 
8.6.3 Design of the Function getReg 
Lastly, let us consider how to implement getReg(I), for a three-address in- 
struction I. There are many options, although there are also some absolute 
prohibitions against choices that lead to incorrect code due to the loss of the 
value of one or more live variables. We begin our examination with the case of 
an operation step, for which we again use x = y + 
x as the generic example. 
First, we must pick a register for y and a register for x. The issues are the same, 
so we shall concentrate on picking register Ry 
for y. The rules are as follows: 
1. If y is currently in a register, pick a register already containing y as R,. 
Do not issue a machine instruction to load this register, as none is needed. 
2. If y is not in a register, but there is a register that is currently empty, 
pick one such register as Ry 
. 
3. The difficult case occurs when y is not in a register, and there is no register 
that is currently empty. We need to pick one of the allowable registers 
anyway, and we need to make it safe to reuse. Let R be a candidate 
CHAPTER 8. CODE GENERATION 
register, and suppose v is one of the variables that the register descriptor 
for R says is in R. We need to make sure that v's value either is not really 
needed, or that there is somewhere else we can go to get the value of R. 
The possibilities are: 
(a) If the address descriptor for v says that v is somewhere besides R, 
then we are OK. 
(b) I
f
 v is x, the vaIue being computed by instruction I, 
and x is not 
also one of the other operands of instruction I (z in this example), 
then we are OK. The reason is that in this case, we know this value 
of x is never again going to be used, so we are free to ignore it. 
(c) Otherwise, if v is not used later (that is, after the instruction I, 
there 
are no further uses of v, and if v is live on exit from the block, then 
v is recomputed within the block), then we are OK. 
(d) If we are not OK by one of the first two cases, then we need to 
generate the store instruction ST 
v, R to place a copy of v in its own 
memory location. This operation is called a spill. 
Since R may hold several variables at the moment, we repeat the above 
steps for each such variable v. At the end, R's "score" is the number of 
store instructions we needed to generate. Pick one of the registers with 
the lowest score. 
Now, consider the selection of the register Rx. The issues and options are 
almost as for y, so we shall only mention the differences. 
1. Since a new value of x is being computed, a register that holds only x is 
always an acceptable choice for Rx. 
This statement holds even if x is one 
of y and z, since our machine instructions allows two registers to be the 
same in one instruction. 
2. If y is not used after instruction I ,  in the sense described for variable v in 
item (3c), and R, holds only y after being loaded, if necessary, then R, 
can also be used as Rx. A similar option holds regarding x and R,. 
The last matter to consider specially is the case when I 
is a copy instruction 
x = 
y. We pick the register R, as above. Then, we always choose Rx = R,. 
8.6.4 Exercises for Section 8.6 
Exercise 8.6.1 
: 
For each of the following C assignment statements 
8.7. PEEPHOLE OPTIMIZATION 
generate three-address code, assuming that all array elements are integers tak- 
ing four bytes each. In parts (d) and (e), assume that a, b, and c are constants 
giving the location of the first (0th) elements of the arrays with those names, 
as in all previous examples of array accesses in this chapter. 
! Exercise 8.6.2 
: Repeat Exercise 8.6.1 parts (d) and (e), assuming that the 
arrays a, b, and c are located via pointers, pa, pb, and pc, respectively, pointing 
to the locations of their respective first elements. 
Exercise 8.6.3 
: 
Convert your three-address code from Exercise 8.6.1 into ma- 
chine code for the machine model of this section. You may use as many registers 
as you need. 
Exercise 8.6.4 
: 
Convert your three-address code from Exercise 8.6.1 into ma- 
chine code, using the simple code-generation algorithm of this section, assuming 
three registers are available. Show the register and address descriptors after 
each step. 
Exercise 8.6.5: Repeat Exercise 8.6.4, but assuming only two registers are 
available. 
8.7 Peephole Optimization 
While most production compilers produce good code through careful instruc- 
tion selection and register allocation, a few use an alternative strategy: they 
generate naive code and then improve the quality of the target code by applying 
"optimizing" transformations to the target program. The term "optimizing" is 
somewhat misleading because there is no guarantee that the resulting code is 
optimal under any mathematical measure. Nevertheless, many simple transfor- 
mations can significantly improve the running time or space requirement of the 
target program. 
A simple but effective technique for locally improving the target code is 
peephole optimization, which is done by examining a sliding window of target 
instructions (called the peephole) and replacing instruction sequences within 
the peephole by a shorter or faster sequence, whenever possible. Peephole 
optimization can also be applied directly after intermediate code generation to 
improve the intermediate representation. 
The peephole is a small, sliding window on a program. The code in the 
peephole need not be contiguous, although some implementations do require 
this. It is characteristic of peephole optimization that each improvement may 
CHAPTER 8. CODE GENERATION 
spawn opportunities for additional improvements. In general, repeated passes 
over the target code are necessary to get the maximum benefit. In this sec- 
tion, we shall give the following examples of program transformations that are 
characteristic of peephole optimizations: 
Redundant-instruction elimination 
Flow-of-control optimizations 
Algebraic simplifications 
Use of machine idioms 
8.7.1 Eliminating Redundant Loads and Stores 
If we see the instruction sequence 
LD a, RO 
ST RO, a 
in a target program, we can delete the store instruction because whenever it is 
executed, the first instruction will ensure that the value of a has already been 
loaded into register RO. 
Note that if the store instruction had a label, we could 
not be sure that the first instruction is always executed before the second, so we 
could not remove the store instruction. Put another way, the two instructions 
have to be in the same basic block for this transformation to be safe. 
Redundant loads and stores of this nature would not be generated by the 
simple code generation algorithm of the previous section. However, a naive code 
generation algorithm like the one in Section 8.1.3 would generate redundant 
sequences such as these. 
8.7.2 
Eliminating Unreachable Code 
Another opportunity for peephole optimization is the removal of unreachable 
instructions. An unlabeled instruction immediately following an unconditional 
jump may be removed. This operation can be repeated to eliminate a sequence 
of instructions. For example, for debugging purposes, a large program may 
have within it certain code fragments that are executed only if a variable debug 
is equal to 1. In the intermediate representation, this code may look like 
i f  debug == 1 goto L1 
got0 L2 
L I : print debugging information 
L2: 
One obvious peephole optimization is to eliminate jumps over jumps. Thus, 
no matter what the value of debug, the code sequence above can be replaced 
by 
8.7. PEEPHOLE OPTIMIZATION 
if debug != 1 goto L2 
print debugging information 
L2: 
If debug is set to 0 at the beginning of the program, constant propagation 
would transform this sequence into 
if 0 != 1 goto L2 
print debugging information 
L2: 
Now the argument of the first statement always evaluates to true, so the 
statement can be replaced by goto L2. 
Then all statements that print debug- 
ging information are unreachable and can be eliminated one at a time. 
8.7.3 Flow-of-Control Optimizations 
Simple intermediate code-generation algorithms frequently produce jumps to 
jumps, jumps to conditional jumps, or conditional jumps to jumps. These 
unnecessary jumps can be eliminated in either the intermediate code or the 
target code by the following types of peephole optimizations. We can replace 
the sequence 
got0 L1 
... 
Ll: got0 L2 
by the sequence 
I
f
 there are now no jumps to L1, 
then it may be possible to eliminate the 
statement L1: goto L2 
provided it is preceded by an unconditional jump. 
Similarly, the sequence 
can be replaced by the sequence 
Finally, suppose there is only one jump to L1 and L1 is preceded by an 
unconditional goto. Then the sequence 
CHAPTER 8. CODE GENERATION 
may be replaced by the sequence 
While the number of instructions in the two sequences is the same, we sometimes 
skip the unconditional jump in the second sequence, but never in the first. Thus, 
the second sequence is superior to the first in execution time. 
8.7.4 Algebraic Simplification and Reduction in Strength 
In Section 8.5 we discussed algebraic identities that could be used to simplify 
DAG's. These algebraic identities can also be used by a peephole optimizer to 
eliminate t 
hree-address statements such as 
in the peephole. 
Similarly, reduction-in-strength transformations can be applied in the peep- 
hole to replace expensive operations by equivalent cheaper ones on the target 
machine. Certain machine instructions are considerably cheaper than others 
and can often be used as special cases of more expensive operators. For ex- 
ample, x2 is invariably cheaper to implement as x * x than as a call to an 
exponentiation routine. Fixed-point multiplication or division by a power of 
two is cheaper to implement as a shift. Floating-point division by a constant 
can be approximated as multiplication by a constant, which may be cheaper. 
8.7.5 Use of Machine Idioms 
The target machine may have hardware'instructions to implement certain spe- 
cific operations efficiently. Detecting situations that permit the use of these 
instructions can reduce execution time significantly. For example, some ma- 
chines have auto-increment and auto-decrement addressing modes. These add 
or subtract one from an operand before or after using its value. The use of the 
modes greatly improves the quality of code when pushing or popping a stack, 
as in parameter passing. These modes can also be used in code for statements 
like x=x+l. 
8.8. REGISTER ALLOCATION AND ASSIGNMENT 
8.7.6 Exercises for Section 8.7 
Exercise 8.7.1 : 
Construct an algorithm that will perform redundant-instruc- 
tion elimination in a sliding peephole on target machine code. 
Exercise 8.7.2 : 
Construct an algorithm that will do flow-of-control optimiza- 
tions in a sliding peephole on target machine code. 
Exercise 8.7.3 
: Construct an algorithm that will do simple algebraic simpli- 
fications and reductions in strength in a sliding peephole on target machine 
code. 
8.8 Register Allocation and Assignment 
Instructions involving only register operands are faster than those involving 
memory operands. On modern machines, processor speeds are often an order 
of magnitude or more faster than memory speeds. Therefore, efficient utilization 
of registers is vitally important in generating good code. This section presents 
various strategies for deciding at each point in a program what values should 
reside in registers (register allocation) and in which register each value should 
reside (register assignment). 
One approach to register allocation and assignment is to assign specific 
values in the target program to certain registers. For example, we could decide 
to assign base addresses to one group of registers, arithmetic computations to 
another, the top of the stack to a fixed register, and so on. 
This approach has the advantage that it simplifies 
the design of a code gener- 
ator. Its disadvantage is that, applied too strictly, it uses registers inefficiently; 
certain registers may go unused over substantial portions of code, while unnec- 
essary loads and stores are generated into the other registers. Nevertheless, it is 
reasonable in most computing environments to reserve a few registers for base 
registers, stack pointers, and the like, and to allow the remaining registers to 
be used by the code generator as it sees fit. 
8.8.1 Global Register Allocation 
The code generation algorithm in Section 8.6 used registers to hold values for 
the duration of a single basic block. However, all live variables were stored 
at the end of each block. To save some of these stores and corresponding 
loads, we might arrange to assign registers to frequently used variables and keep 
these registers consistent across block boundaries (globally). Since programs 
spend most of their time in inner loops, a natural approach to global register 
assignment is to try to keep a frequently used value in a fixed register throughout 
a loop. For the time being, assume that we know the loop structure of a flow 
graph, and that we know what values computed in a basic block are used outside 
that block. The next chapter covers techniques for computing this information. 
554 
CHAPTER 8. CODE GENERATION 
One strategy for global register allocation is to assign some fixed number 
of registers to hold the most active values in each inner loop. The selected 
values may be different in different loops. Registers not already allocated may 
be used to hold values local to one block as in Section 8.6. This approach 
has the drawback that the fixed number of registers is not always the right 
number to make available for global register allocation. Yet the method is 
simple to implement and was used in Fortran H, the optimizing Fortran compiler 
developed by IBM for the 360-series machines in the late 1960s. 
With early C compilers, a programmer could do some register allocation 
explicitly by using register declarations to keep certain values in registers for 
the duration of a procedure. Judicious use of register declarations did speed 
up many programs, but programmers were encouraged to first profile their 
programs to determine the program's hotspots before doing their own register 
allocation. 
8.8.2 Usage Counts 
In this section we shall assume that the savings to be realized by keeping a 
variable x in a register for the duration of a loop L is one unit of cost for each 
reference to x if x is already in a register. However, if we use the approach in 
Section 8.6 to generate code for a block, there is a good chance that after x has 
been computed in a block it will remain in a register if there are subsequent 
uses of x in that block. Thus we count a savings of one for each use of x in 
loop L that is not preceded by an assignment to x in the same block. We also 
save two units if we can avoid a store of x at the end of a block. Thus, if x 
is allocated a register, we count a savings of two for each block in loop L for 
which x is live on exit and in which x is assigned a value. 
On the debit side, if x is live on entry to the loop header, we must load x 
into its register just before entering loop L. This load costs two units. Similarly, 
for each exit block B of loop L at which x is live on entry to some successor of 
B outside of L, we must store x at a cost of two. However, on the assumption 
that the loop is iterated many times, we may neglect these debits since they 
occur only once each time we enter the loop. Thus, an approximate formula for 
the benefit to be realized from allocating a register x within loop L is 
use($, B) + 
2 * live(x, 
B) 
(8-1) 
blocks B in L 
where use(x, B) is the number of times x is used in B prior to any definition of 
x; lzue(x, 
B) is 1 
if x is live on exit from B and is assigned a value in B, and 
live(x, 
B) is 0 otherwise. Note that (8.1) is approximate, because not all blocks 
in a loop are executed with equal frequency and also because (8.1) is based 
on the assumption that a loop is iterated many times. On specific machines a 
formula analogous to (8.1), but possibly quite different from it, would have to 
be developed. 
8.8. REGISTER ALLOCATION AND ASSIGNMENT 
555 
Example 8.17 : Consider the the basic blocks in the inner loop depicted in 
Fig. 8.17, where jump and conditional jump statements have been omitted. 
Assume registers RO, R1, and R2 are allocated to hold values throughout the 
loop. Variables live on entry into and on exit from each block are shown in 
Fig. 8.17 for convenience, immediately above and below each block, respectively. 
There are some subtle points about live variables that we address in the next 
chapter. For example, notice that both e and f are live at the end of B1, 
but of 
these, only e is live on entry to B2 and only f on entry to B3. In general, the 
variables live at the end of a block are the union of those live at the beginning 
of each of its successor blocks. 
acdef 
'
7
 
cdef 
f live 
bcdef 
b, c, 
d, e, 
f live 
Figure 8.17: Flow graph of an inner loop 
To evaluate (8.1) for x = a, we observe that a is live on exit from B1 and 
is assigned a value there, but is not live on exit from B2, 
B3, or B4. Thus, 
CB 
in use(a, B) = 2. Hence the value of (8.1) for x = a is 4. That is, four 
units of cost can be saved by selecting a for one of the global registers. The 
values of (8.1) for b, c, d, e, and f are 5, 3, 6, 4, and 4, respectively. Thus, 
we may select a, b, and d for registers RO, R1, and R2, respectively. Using RO 
for e or f instead of a would be another choice with the same apparent benefit. 
Figure 8.18 shows the assembly code generated from Fig. 8.17, assuming that 
the strategy of Section 8.6 is used to generate code for each block. We do not 
show the generated code for the omitted conditional or unconditional jumps 
that end each block in Fig. 8.17, and we therefore do not show the generated 
code as a single stream as it would appear in practice. 
CHAPTER 8. CODE GENERATION 
LD Rl, b 
LD R2, d 
Figure 8.18: Code sequence using global register assignment 
t 
I 
LD R3, c 
ADD ROY 
R1, R3 
SUB R2, R2, Rl 
LD R3, f 
ADD R3, ROY 
R3 
ST e, R3 
8.8.3 Register Assignment for Outer Loops 
LD R3, f 
SUB R3, ROY 
R2 
ADD Rl, R2, R3 
ST f, R3 
J32 
LD R3, c 
SUB R3, ROY 
R3 
ST e, R3 
Having assigned registers and generated code for inner loops, we may apply the 
same idea to progressively larger enclosing loops. If an outer loop L1 contains 
an inner loop L2, the names allocated registers in L2 need not be allocated 
registers in L1 - 
L2. Similarly, if we choose to allocate x a register in L2 but 
not L1, we must load x on entrance to L2 and store x on exit from L2. We leave 
as an exercise the derivation of a criterion for selecting names to be allocated 
registers in an outer loop L, given that choices have already been made for all 
J33 
loops nested within L. 
8.8.4 
Register Allocation by Graph Coloring 
LD R3, c 
B
4
 
ADD R1, R2, R3 
When a register is needed for a computation but all available registers are in 
use, the contents of one of the used registers must be stored (spilled) into a 
memory location in order to free up a register. Graph coloring is a simple, 
systematic technique for allocating registers and managing register spills. 
In the method, two passes are used. In the first, target-machine instruc- 
tions are selected as though there are an infinite number of symbolic registers; 
in effect, names used in the intermediate code become names of registers and 
ST b y  
Rl 
ST a, R2 
4 
8.8. REGISTER ALLOCATION AND ASSIGNMENT 
the three-address instructions become machine-language instructions. If ac- 
cess to variables requires instructions that use stack pointers, display pointers, 
base registers, or other quantities that assist access, then we assume that these 
quantities are held in registers reserved for each purpose. Normally, their use is 
directly translatable into an access mode for an address mentioned in a machine 
instruction. If access is more complex, the access must be broken into several 
machine instructions, and a temporary symbolic register (or several) may need 
to be created. 
Once the instructions have been selected, a second pass assigns physical 
registers to symbolic ones. The goal is to find an assignment that minimizes 
the cost of spills. 
In the second pass, for each procedure a register-interference graph is con- 
structed in which the nodes are symbolic registers and an edge connects two 
nodes if one is live at a point where the other is defined. For example, a register- 
interference graph for Fig. 8.17 would have nodes for names a and d. In block 
B1, 
a is live at the second statement, which defines d; therefore, in the graph 
there would be an edge between the nodes for a and d. 
An attempt is made to color the register-interference graph using k colors, 
where k is the number of assignable registers. A graph is said to be colored if 
each node has been assigned a color in such a way that no two adjacent nodes 
have the same color. A color represents a register, and the color makes sure 
that no two symbolic registers that can interfere with each other are assigned 
the same physical register. 
Although the problem of determining whether a graph is k-colorable is NP- 
complete in general, the following heuristic technique can usually be used to do 
the coloring quickly in practice. Suppose a node n in a graph G has fewer than 
k neighbors (nodes connected to n by an edge). Remove n and its edges from 
G to obtain a graph GI. A k-coloring of GI can be extended to a k-coloring of 
G by assigning n a color not assigned to any of its neighbors. 
By repeatedly eliminating nodes having fewer than k edges from the register- 
interference graph, either we obtain the empty graph, in which case we can 
produce a k-coloring for the original graph by coloring the nodes in the reverse 
order in which they were removed, or we obtain a graph in which each node has 
k or more adjacent nodes. In the latter case a kcoloring is no longer possible. 
At this point a node is spilled by introducing code to store and reload the 
register. Chaitin has devised several heuristics for choosing the node to spill. 
A general rule is to avoid introducing spill code into inner loops. 
8.8.5 
Exercisesfor Section 8.8 
Exercise 8.8.1 : 
Construct the register-interference graph for the program in 
Fig. 8.17. 
Exercise 8.8.2 : Devise a register-allocation strategy on the assumption that 
we automatically store all registers on the stack before each procedure call and 
restore them after the return. 
CHAPTER 8. CODE GENERATION 
8.9 Instruction Selection by Tree Rewriting 
Instruction selection can be a large combinatorial task, especially on machines 
that are rich in addressing modes, such as CISC machines, or on machines with 
special-purpose instructions, say, for signal processing. Even if we assume that 
the order of evaluation is given and that registers are allocated by a separate 
mechanism, instruction selection - 
the problem of selecting target-language 
instructions to implement the operators in the intermediate representation - 
remains a large combinatorial task. 
In this section, we treat instruction selection as a tree-rewriting problem. 
Tree representations of target instructions have been used effectively in code- 
generator generators, which automatically construct the instruction-selection 
phase of a code generator from a high-level specification of the target machine. 
Better code might be obtained for some machines by using DAG's rather than 
trees, but DAG matching is more complex than tree matching. 
8.9.1 Tree-Translation Schemes 
Throughout this section, the input to the code-generation process will be a 
sequence of trees at the semantic level of the target machine. The trees are 
what we might get after inserting run- 
time addresses into the intermediate 
representation, as described in Section 8.3. In addition, the leaves of the trees 
contain information about the storage types of their labels. 
Example 8.18 : Figure 8.19 contains a tree for the assignment statement 
aCil = b +  
1
,
 where the array a is stored on the run-time stack and the vari- 
able b is a global in memory location Mb. The run-time addresses of locals a 
and i are given as constant offsets C, 
and Ci 
from SP, 
the register containing 
the pointer to the beginning of the current activation record. 
The assignment to a[i] is an indirect assignment in which the r-value of 
the location for a [il is set to the r-value of the expression b + 1. The addresses 
of array a and variable i are given by adding the values of the constant C, and 
Ci 
, 
respectively, to the contents of register SP 
. We simplify array-address calcu- 
lations by assuming that all values are one-byte characters. (Some instruction 
sets make special provisions for multiplications by constants, such as 2, 4, and 
8, during address calculations 
.) 
In the tree, the ind operator treats its argument as a memory address. As 
the left child of an assignment operator, the ind node gives the location into 
which the r-value on the right side of the assignment operator is to be stored. 
I
f
 an argument of a + or ind operator is a memory location or a register, then 
the contents of that memory location or register are taken as the value. The 
leaves in the tree are labeled with attributes; a subscript indicates the value of 
the attribute. 
The target code is generated by applying a sequence of tree-rewriting rules 
to reduce the input tree to a single node. Each tree-rewriting rule has the form 
8.9. INSTRUCTION SELECTION BY TREE RE 
WRITING 
Figure 8.19: Intermediate-code tree for a [i] = b 
+ 1 
replacement t 
template { action ) 
where replacement is a single node, template is a tree, and action is a code 
fragment, as in a syntax-directed translation scheme. 
A set of tree-rewriting rules is called a tree-translation scheme. 
Each tree-rewriting rule represents the translation of a portion of the tree 
given by the template. The translation consists of a possibly empty sequence of 
machine instructions that is emitted by the action associated with the template. 
The leaves of the template are attributes with subscripts, as in the input tree. 
Sometimes, certain restrictions apply to the values of the subscripts in the 
templates; these restrictions are specified as semantic predicates that must be 
satisfied before the template is said to match. For example, a predicate might 
specify that the value of a constant fall in a certain range. 
A tree-translation scheme is a convenient way to represent the instruction- 
selection phase of a code generator. As an example of a tree-rewriting rule, 
consider the rule for the register-to-register add instruction: 
Ri t 
{ ADD Ri , 
Ri , 
R j  ) 
R
i
 
/+\ 
R,i 
This rule is used as follows. If the input tree contains a subtree that matches 
this tree template, that is, a subtree whose root is labeled by the operator + 
and whose left and right children are quantities in registers i and j, then we 
can replace that subtree by a single node labeled Ri and emit the instruction 
ADD Ri, Ri, R j  as output. We call this replacement a tiling of the subtree. 
More than one template may match a subtree at a given time; we shall describe 
shortly some mechanisms for deciding which rule to apply in cases of conflict. 
Example 8.19 : 
Figure 8.20 contains tree-rewriting rules for a few instructions 
of our target machine. These rules will be used in a running example throughout 
this section. The first two rules correspond to load instructions, the next two 
560 
CHAPTER 8. CODE GENERATION 
to store instructions, and the remainder to indexed loads and additions. Note 
that rule (8) requires the value of the constant to be 1. This condition would 
be specified by a semantic predicate. 
8.9.2 Code Generation by Tiling an Input Tree 
A tree-translation scheme works as follows. Given an input tree, the templates 
in the tree-rewriting rules are applied to tile its subtrees. If a template matches, 
the matching subtree in the input tree is replaced with the replacement node of 
the rule and the action associated with the rule is done. If the action contains a 
sequence of machine instructions, the instructions are emitted. This process is 
repeated until the tree is reduced to a single node, or until no more templates 
match. The sequence of machine instructions generated as the input tree is 
reduced to a single node constitutes the output of the tree-translation scheme 
on the given input tree. 
The process of specifying a code generator becomes similar to that of us- 
ing a syntax-directed translation scheme to specify a translator. We write a 
tree-translation scheme to describe the instruction set of a target machine. In 
practice, we would like to find a scheme that causes a minimal-cost instruction 
sequence to be generated for each input tree. Several tools are available to help 
build a code generator automatically from a tree-translation scheme. 
Example 8.20 : 
Let us use the tree-translation scheme in Fig. 8.20 to generate 
code for the input tree in Fig. 8.19. Suppose that the first rule is applied to 
load the constant Ca into register R
O
:
 
The label of the leftmost leaf then changes from Ca to Ro and the instruction 
LD ROY 
#a is generated. The seventh rule now matches the leftmost subtree 
with root labeled +: 
7) 
R0 + /+\ 
{ ADD ROY R Q y  SP ) 
R
o
 
RSP 
Using this rule, we rewrite this subtree as a single node labeled Ro and generate 
the instruction ADD ROY 
ROY 
SP. 
Now the tree looks like 
8.9. INSTRUCTION SELECTION BY TREE RE 
WRITING 
R
i
 t 
M z  
I { LD 
Ri, x ) 
R
i
 t 
c a  
{ LD 
Ri, # a  ) 
4) 
Figure 8.20: Tree-rewriting rules for some target-machine instructions 
M 
t 
- 
/-\ 
ind 
R
j
 
I 
6) 
7) 
8) 
R
i
 t 
R
i
 /+\ 
ind 
I 
c a  
/+\ 
R
j
 
R
i
 t 
R
i
 /+\ 
R
j
 
R
i
 t 
/+\ 
R
i
 
C
1
 
{ ADD Ri, Ri, a ( R j )  ) 
{ ADD Ri, Ri, R j  ) 
{ I N C  Ri ) 
562 
CHAPTER 8. CODE GENERATION 
At this point, we could apply rule (5) to reduce the subtree 
ind 
to a single node labeled, say, R1. 
We could also use rule (6) to reduce the larger 
subtree 
R
o
 
/
,
\
 
ind 
I 
to a single node labeled Ro and generate the instruction ADD ROY 
ROY 
i 
(SP) 
. 
Assuming that it is more efficient to use a single instruction to compute the 
larger subtree rather than the smaller one, we choose rule (6) to get 
ind 
/ = \  
+ 
I 
R
o
 
Mb / \ 
c
1
 
In the right subtree, rule (2) applies to the leaf Mb. It generates an instruction 
to load b into register R1, 
say. Now, using rule (8) we can match the subtree 
and generate the increment instruction INC Rl. At this point, the input tree 
has been reduced to 
/ \ 
ind 
RI 
This remaining tree is matched by rule (4), which reduces the tree to a single 
node and generates the instruction ST *ROY 
R1. 
We generate the following code 
sequence: 
LD ROY 
#a 
ADD ROY 
ROY 
SP 
ADD ROY 
ROY 
i(SP) 
LD Rl, b 
INC R1 
ST *ROY 
Rl 
8.9. INSTRUCTION SELECTION BY TREE RET;VRITING 
in the process of reducing the tree to a single node. 
In order to implement the tree-reduction process in Example 8.18, we must 
address some issues related to tree-pattern matching: 
HOW 
is tree-pattern matching to be done? The efficiency of the code- 
generation process (at compile time) depends on the efficiency of the tree- 
matching algorithm. 
What do we do if more than one template matches at a given time? 
The efficiency of the generated code (at run time) may depend on the 
order in which templates are matched, since different match sequences 
will in general lead to different target-machine code sequences, some more 
efficient than others. 
If no template matches, then the code-generation process blocks. At the 
other extreme, we need to guard against the possibility of a single node being 
rewritten indefinitely, generating an infinite sequence of register move instruc- 
tions or an infinite sequence of loads and stores. 
To prevent blocking, we assume that each operator in the intermediate code 
can be implemented by one or more target-machine instructions. We further 
assume that there are enough registers to compute each tree node by itself. 
Then, no matter how the tree matching proceeds, the remaining tree can always 
be translated into target-machine instructions. 
8.9.3 Pattern Matching by Parsing 
Before considering general tree matching, we consider a specialized approach 
that uses an LR parser to do the pattern matching. The input tree can be 
treated as a string by using its prefix representation. For example, the prefix 
representation for the tree in Fig. 8.19 is 
= ind + + C, RSp 
ind + Ci 
RSp 
+ Mb C1 
The tree-translation scheme can be converted into a synt 
ax-directed trans- 
lation scheme by replacing the tree-rewriting rules with the productions of a 
context-free grammar in which the right sides are prefix representations of the 
instruction templates. 
Example 8.21 : The syntax-directed translation scheme in Fig. 8.21 is based 
on the tree-translation scheme in Fig. 8.20. 
The nonterminals of the underlying grammar are R and M. The terminal 
m represents a specific memory location, such as the location for the global 
variable b in Example 8.18. The production M --+ m in Rule (10) can be 
thought of as matching M with m prior to using one of the templates involving 
M. Similarly, we introduce a terminal sp for register SP and add the production 
R --+ SP. Finally, terminal c represents constants. 
Using these terminals, the string for the input tree in Fig. 8.19 is 
CHAPTER 8. CODE GENERATION 
1) Ri --+ c, 
{ L D  Ri, # a )  
2) 
Ri + Mz 
{ LD 
Ri, x ) 
3) 
M + = Mz Ri 
{ S T  x, R i )  
4) 
M --+ = ind Ri Rj 
{ S T  
*Ri, R j  ) 
5) Ri 
--+ ind + c, Rj 
{ LD 
Ri, a ( R j )  ) 
6) Ri --+ +Riind + c,Rj 
{ A D D  Ri, Ri, a ( R j ) )  
7) 
Ri i + Ri Rj 
{ ADD Ri, Ri, R j  ) 
8) Ri + + Ri C I  
{ INC Ri ) 
9) 
R --+ sp 
10) M --+ m 
Figure 8.21: Syntax-directed translation scheme constructed from Fig. 8.20 
= ind + + c, sp ind + ci sp + m
b
 cl 
From the productions of the translation scheme we build an LR parser using 
one of the LR-parser construction techniques of Chapter 4. The target code is 
generated by emitting the machine instruction corresponding to each reduction. 
A code-generation grammar is usually highly ambiguous, and some care 
needs to be given to how the parsing-action conflicts are resolved when the 
parser is constructed. In the absence of cost information, a general rule is to 
favor larger reductions over smaller ones. This means that in a reduce-reduce 
conflict, the longer reduction is favored; in a shift-reduce conflict, the shift 
move is chosen. This "maximal munch" approach causes a larger number of 
operations to be performed with a single machine instruction. 
There are some benefits to using LR parsing in code generation. First, 
the parsing method is efficient and well understood, so reliable and efficient 
code generators can be produced using the algorithms described in Chapter 4. 
Second, it is relatively easy to retarget the resulting code generator; a code 
selector for a new machine can be constructed by writing a grammar to describe 
the instructions of the new machine. Third, the quality of the code generated 
can be made efficient by adding special-case productions to take advantage of 
machine idioms. 
However, there are some challenges as well. A left-to-right order of evalua- 
tion is fixed by the parsing method. Also, for some machines with large numbers 
of addressing modes, the machine-description grammar and resulting parser can 
become inordinately large. As a consequence, specialized techniques are neces- 
sary to encode and process the machine-description grammars. We must also 
be careful that the resulting parser does not block (has no next move) while 
parsing an expression tree, either because the grammar does not handle some 
operator patterns or because the parser has made the wrong resolution of some 
parsing-action conflict. We must also make sure the parser does not get into an 
8.9. INSTRUCTION SELECTION BY TREE REWRITING 
565 
infinite loop of reductions of productions with single symbols on the right side. 
The looping problem can be solved using a state-splitting technique at the time 
the parser tables are generated. 
8.9.4 Routines for Semantic Checking 
In a code-generation translation scheme, the same attributes appear as in an 
input tree, but often with restrictions on what values the subscripts can have. 
For example, a machine instruction may require that an attribute value fall in 
a certain range or that the values of two attributes be related. 
These restrictions on attribute values can be specified as predicates that are 
invoked before a reduction is made. In fact, the general use of semantic actions 
and predicates can provide greater flexibility and ease of description than a 
purely grammatical specification of a code generator. Generic templates can 
be used to represent classes of instructions and the semantic actions can then 
be used to pick instructions for specific cases. For example, two forms of the 
addition instruction can be represented with one template: 
{ if ( a  
= 1) 
Ri t
.
 
/+\ 
else 
INC Ri 
Ri 
CCL 
ADD R i ,  R i ,  #a ) 
Parsing-action conflicts can be resolved by disambiguating predicates that 
can allow different selection strategies to be used in different contexts. A 
smaller description of a target machine is possible because certain aspects of 
the machine architecture, such as addressing modes, can be factored into the 
attributes. The complication in this approach is that it may become difficult 
to verify the accuracy of the translation scheme as a faithful description of the 
target machine, although this problem is shared to some degree by all code 
generators. 
8.9.5 General Tree Matching 
The LR-parsing approach to pattern matching based on prefix representations 
favors the left operand of a binary operator. In a prefix representation op El E2, 
the limited-lookahead LR parsing decisions must be made on the basis of some 
prefix of El, 
since El can be arbitrarily long. Thus, pattern matching can miss 
nuances of the target-instruction set that are due to right operands. 
Instead prefix representation, we could use a postfix representation. But, 
then an LR-parsing approach to pattern matching would favor the right oper- 
and. 
For a hand-written code generator, we can use tree templates, as in Fig. 8.20, 
as a guide and write an ad-hoc matcher. For example, if the root of the input 
tree is labeled ind, then the only pattern that could match is for rule (5); 
otherwise, if the root is labeled +, then the patterns that could match are for 
rules (6-8). 
CHAPTER 8. CODE GENERATION 
For a code-generator generator, we need a general tree-matching algorithm. 
An efficient top-down algorithm can be developed by extending the string- 
pattern-matching techniques of Chapter 3. The idea is to represent each tem- 
plate as a set of strings, where a string corresponds to a path from the root to 
a leaf in the template. We treat all operands equally by including the position 
number of a child, from left to right, in the strings. 
Example 8.22 : 
In building the set of strings for an instruction set, we shall 
drop the subscripts, since pattern matching is based on the attributes alone, 
not on their values. 
The templates in Fig. 8.22 have the following set of strings from the root to 
a leaf: 
The string C represents the template with C at the root. The string + 
1 
R 
represents the + and its left operand R in the two templates that have + at 
the root. 
R
i
 / \ 
ind 
Figure 8.22: An instruction set for tree matching 
Using sets of strings as in Example 8.22, a tree-pattern matcher can be con- 
structed by using techniques for efficiently matching multiple strings in parallel. 
In practice, the tree-rewriting process can be implemented by running the 
tree-pattern matcher during a depth-first traversal of the input tree and per- 
forming the reductions as the nodes are visited for the last time. 
Instruction costs can be taken into account by associating with each tree- 
rewriting rule the cost of the sequence of machine instructions generated if that 
rule is applied. In Section 8.11, we discuss a dynamic programming algorithm 
that can be used in conjunction with tree-pattern matching. 
By running the dynamic programming algorithm concurrently, we can select 
an optimal sequence of matches using the cost information associated with 
each rule. We may need to defer deciding upon a match until the cost of all 
alternatives is known. Using this approach, a small, efficient code generator can 
8.10. OPTIMAL CODE GENERATION FOR EXPRESSlOlVS 
567 
be constructed quickly from a tree-rewriting scheme. Moreover, the dynamic 
programming algorithm frees the code-generator designer from having to resolve 
conflicting matches or decide upon an order for the evaluation. 
8.9.6 Exercises for Section 8.9 
Exercise 8.9.1 : Construct syntax trees for each of the following statements 
assuming all nonconstant operands are in memory locations: 
Use the tree-rewriting scheme in Fig. 8.20 to generate code for each statement. 
Exercise 8.9.2 
: 
Repeat Exercise 8.9.1 above using the syntax-directed trans- 
lation scheme in Fig. 8.21 in place of the tree-rewriting scheme. 
! 
Exercise 8.9.3: Extend the tree-rewriting scheme in Fig. 8.20 to apply to 
while-statements. 
! 
Exercise 8.9.4 
: 
How would you extend tree rewriting to apply to DAG7s? 
8.10 Optimal Code Generation for Expressions 
We can choose registers optimally when a basic block consists of a single expres- 
sion evaluation, or if we accept that it is sufficient to generate code for a block 
one expression at a time. In the following algorithm, we introduce a numbering 
scheme for the nodes of an expression tree (a syntax tree for an expression) that 
allows us to generate optimal code for an expression tree when there is a fixed 
number of registers with which to evaluate the expression. 
8.10.1 Ershov Numbers 
We begin by assigning to the nodes of an expression tree a number that tells 
how many registers are needed to evaluate that node without storing any tern- 
poraries. These numbers are sometimes called Ershov numbers, after A. Ershov, 
who used a similar scheme for machines with a single arithmetic register. For 
our machine model, the rules are: 
1. Label any leaf 1. 
2. The label of an interior node with one child is the label of its child. 
3. The label of an interior node with two children is 
568 
CHAPTER 8. CODE GENERATION 
(a) The larger of the labels of its children, if those labels are different. 
(b) One plus the label of its children if the labels are the same. 
Example 8.23 : 
In Fig. 8.23 we see an expression tree (with operators omitted) 
that might be the tree for expression (a - 
b) + 
e x (c + 
d) or the three-address 
code: 
Each of the five leaves is labeled 1 
by rule (I). Then, we can label the interior 
node for t 
1 
= a 
- 
b, since both of its children are labeled. Rule (3b) applies, so 
it gets label one more than the labels of its children, that is, 2. The same holds 
for the interior node for t2 
= c + d. 
Figure 8.23: A tree labeled with Ershov numbers 
Now, we can work on the node for t 
3 
= e 
* t 
2. 
Its children have labels 1 
and 
2, so the label of the node for t3 
is the maximum, 2, by rule (3a). Finally, the 
root, the node for t 4  
= 
t 
1 
+ 
t3, 
has two children with label 2, and therefore it 
gets label 3. 
8
.
1
0
.
2
 Generating Code From Labeled Expression Trees 
It can be proved that, in our machine model, where all operands must be in 
registers, and registers can be used by both an operand and the result of an 
operation, the label of a node is the fewest registers with which the expression 
can be evaluated using no stores of temporary results. Since in this model, we 
are forced to load each operand, and we are forced to compute the result cor- 
responding to each interior node, the only thing that can make the generated 
code inferior to the optimal code is if there are unnecessary stores of tempo- 
raries. The argument for this claim is embedded in the following algorithm for 
generating code with no stores of temporaries, using a number of registers equal 
to the label of the root. 
Algorithm 8.24 : 
Generating code from a labeled expression tree. 
8.10. OPTIMAL CODE GENERATION FOR EXPRESSIONS 
INPUT: A labeled tree with each operand appearing once (that is, no common 
subexpressions) 
. 
OUTPUT: An optimal sequence of machine instructions to evaluate the root 
into a register. 
METHOD: The following 
is a recursive algorithm to generate the machine code. 
The steps below are applied, starting at the root of the tree. If the algorithm 
is applied to a node with label k, then only k registers will be used. However, 
there is a "base" b > 1 
for the registers used so that the actual registers used 
are Rb, 
Rb+1,. 
. . 
Rb+k-l. The result always appears in RbSkUl. 
1. To generate machine code for an interior node with label k and two chil- 
dren with equal labels (which must be k - 
l) 
do the following: 
(a) Recursively generate code for the right child, using base b + 1. The 
result of the right child appears in register Rb+k. 
(b) Recursively generate code for the left child, using base b; the result 
appears in Rb+k-1. 
(c) Generate the instruction O
P
 Rb+k, 
Rb+k-1, Rb+k, where O
P
 is the 
appropriate operation for the interior node in question. 
2. Suppose we have an interior node with label k and children with unequal 
labels. Then one of the children, which we'll call the "big" child, has label 
k ,  and the other child, the "little" child, has some label m < k. Do the 
following to generate code for this interior node, using base b: 
(a) Recursively generate code for the big child, using base b; the result 
appears in register Rb+k-l. 
(b) Recursively generate code for the small child, using base b; the result 
appears in register Rb+m-l. Note that since m < k, neither Rb+k-l 
nor any higher-numbered register is used. 
(c) Generate the instruction O
P
 Rb+k-l, 
Rb+m-l, Rb+k-1 or the instruc- 
tion OP Rb+iE-l, 
Rb+k-l, 
Rb+mPl, 
depending on whether the big child 
is the right or left child, respectively. 
3. For a leaf representing operand x, if the base is b generate the instruction 
LD Rb, 
x. 
Example 8.25 : Let us apply Algorithm 8.24 to the tree of Fig. 8.23. Since 
the label of the root is 3, the result will appear in Rg, 
and only R1, R2, and 
R3 will be used. The base for the root is b = 1. Since the root has children of 
equal labels, we generate code for the right child first, with base 2. 
When we generate code for the right child of the root, labeled t3, 
we find the 
big child is the right child and the little child is the left child. We thus generate 
code for the right child first, with b = 2. Applying the rules for equal-labeled 
children and leaves, we generate the following code for the node labeled t2: 
CHAPTER 8. CODE GENERATION 
LD R3, d 
LD R2, c 
ADD R3, R2, R3 
Next, we generate code for the left child of the right child of the root; this node 
is the leaf labeled e. Since b = 
2, the proper instruction is 
Now we can complete the code for the right child of the root by adding the 
instruction 
MUL R3, R2, R3 
The algorithm proceeds to generate code for the left child of the root, leaving 
the result in R2, and with base 1. The complete sequence of instructions is 
shown in Fig. 8.24. 
LD R3, d 
LD R2, c 
ADD R3, R2, R3 
LD R2, e 
MUL R3, R2, R3 
LD R2, b 
LD RI, a 
SUB R2, RI, R2 
ADD R3, R2, R3 
Figure 8.24: Optimal three-register code for the tree of Fig. 8.23 
8.10.3 Evaluating Expressions with an Insufficient Supply 
of Registers 
When there are fewer registers available than the label of the root of the tree, 
we cannot apply Algorithm 8.24 directly. We need to introduce some store 
instructions that spill values of subtrees into memory, and we then need to load 
those values back into registers as needed. Here is the modified algorithm that 
takes into account a limitation on the number of registers. 
Algorithm 8.26 : 
Generating code from a labeled expression tree. 
INPUT: A labeled tree with each operand appearing once (i.e., no common 
subexpressions) and a number of registers r > 2. 
OUTPUT: An optimal sequence of machine instructions to evaluate the root into 
a register, using no more than r registers, which we assume are R1, Rz 
, 
. . 
. , 
R, . 
8.10. OPTIMAL CODE GENERATION FOR EXPRESSIONS 
571 
METHOD: Apply the following recursive algorithm, starting at the root of the 
tree, with base b = 1. For a node N with label r or less, the algorithm is exactly 
the same as Algorithm 8.24, and we shall not repeat those steps here. However, 
for interior nodes with a label k > r, we need to work on each side of the tree 
separately and store the result of the larger subtree. That result is brought 
back into memory just before node N is evaluated, and the final step will take 
place in registers R,-l and R,. The modifications to the basic algorithm are 
as follows: 
1. Node N has at least one child with label r or greater. Pick the larger 
child (or either if their labels are the same) to be the "big" child and let 
the other child be the "little" child. 
2. Recursively generate code for the big child, using base b = 1. The result 
of this evaluation will appear in register R,. 
3. Generate the machine instruction ST tk, R,, where tk is a temporary vari- 
able used for temporary results used to help evaluate nodes with label 
k. 
4. Generate code for the little child as follows. If the little child has label r 
or greater, pick base b = 1. If the label of the little child is j < r, then 
pick b = r - 
j. Then recursively apply this algorithm to the little child; 
the result appears in R,. 
5. Generate the instruction LD 
R,-l, tk. 
6. If the big child is the right child of N, then generate the instruction 
O
P
 R,, R,, R,-l. If the big child is the left child, generate OP R,, R,-l, R,. 
Example 8.27 : 
Let us revisit the expression represented by Fig. 8.23, but now 
assume that r = 
2; that is, only registers R1 
and R2 
are available to hold tem- 
poraries used in the evaluation of expressions. When we apply Algorithm 8.26 
to Fig. 8.23, we see that the root, with label 3, has a label that is larger than 
r = 2. Thus, we need to identify one of the children as the "big" child. Since 
they have equal labels, either would do. Suppose we pick the right child as the 
big child. 
Since the label of the big child of the root is 2, there are enough registers. 
We thus apply Algorithm 8.24 to this subtree, with b = 1 and two registers. 
The result looks very much like the code we generated in Fig. 8.24, but with 
registers Rl and R2 
in place of R2 
and R3. 
This code is 
LD R2, d 
LD Rl, c 
ADD R2, R1, R2 
LD R1, e 
MUL R2, R1, R2 
572 
CHAPTER 8. CODE GENERATION 
Now, since we need both registers for the left child of the root, we need to 
generate the instruction 
Next, the left child of the root is handled. Again, the number of registers is 
sufficient for this child, and the code is 
LD R2, b 
LD Rl, a 
SUB R2, Rl, R2 
Finally, we reload the temporary that holds the right child of the root with the 
instruction 
and execute the operation at the root of the tree with the instruction 
ADD R2, R2, R1 
The complete sequence of instructions is shown in Fig. 8.25. 
LD R2, d 
LD R1, c 
ADD R2, R1, R2 
LD R1, e 
MUL R2, R1, R2 
ST t3, R2 
LD R2, b 
LD R1, a 
SUB R2, R1, R2 
LD Rl, t3 
ADD R2, R2, R1 
Figure 8.25: Optimal three-register code for the tree of Fig. 8.23, using only 
two registers 
8.10.4 Exercises for Section 8.10 
Exercise 8.10.1 : 
Compute Ershov numbers for the following expressions: 
a) a/(b+c) - 
d * (e + 
f). 
b) a + 
b * (c * (d + 
e)) 
. 
8.11. DYNAMIC PROGRAMMING CODE-GENERATION 
C) (-a + 
*p) * ( ( b  
- 
* q ) / ( - c +  * r ) ) .  
Exercise 8.10.2 : Generate optimal code using two registers for each of the 
expressions of Exercise 8.10.1. 
Exercise 8.10.3 : Generate optimal code using three registers for each of the 
expressions of Exercise 8.10.1. 
! 
Exercise 8.10.4 : 
Generalize the computation of Ershov numbers to expression 
trees with interior nodes with three or more children. 
! 
Exercise 8.10.5 : 
An assignment to an array element, such as a [i] = x, ap- 
pears to be an operator with three operands: a, 
i ,  
and x. How would you modify 
the tree-labeling scheme to generate optimal code for this machine model? 
! Exercise 8.10.6 : 
The original Ershov numbers were used for a machine that 
allowed the right operand of an expression to be in memory, rather than a 
register. How would you modify the tree-labeling scheme to generate optimal 
code for this machine model? 
! 
Exercise 8.10.7: Some machines require two registers for certain single-pre- 
cision values. Suppose that the result of a multiplication of single-register quan- 
tities requires two consecutive registers, and when we divide a/b, the value of 
a must be held in two consecutive registers. How would you modify the tree- 
labeling scheme to generate optimal code for this machine model? 
8.11 Dynamic Programming Code-Generation 
Algorithm 8.26 in Section 8.10 produces optimal code from an expression tree 
using an amount of time that is a linear function of the size of the tree. This 
procedure works for machines in which all computation is done in registers and 
in which instructions consist of an operator applied to two registers or to a 
register and a memory location. 
An algorithm based on the principle of dynamic programming can be used 
to extend the class of machines for which optimal code can be generated from 
expression trees in linear time. The dynamic programming algorithm applies 
to a broad class of register machines with complex instruction sets. 
The dynamic programming algorithm can be used to generate code for any 
machine with r interchangeable registers RO, R1,. . 
. , 
Rr-1 and load, store, and 
add instructions. For simplicity, we assume every instruction costs one unit, 
although the dynamic programming algorithm can easily be modified to work 
even if each instruction has its own cost. 
574 
CHAPTER 8. CODE GENERATION 
8.11.1 Contiguous Evaluation 
The dynamic programming algorithm partitions the problem of generating op- 
timal code for an expression into the subproblems of generating optimal code 
for the subexpressions of the given expression. As a simple example, consider 
an expression E of the form El + 
E2. An optimal program for E is formed by 
combining optimal programs for El and E2, 
in one or the other order, followed 
by code to evaluate the operator +. The subproblems of generating optimal 
code for El and E2 are solved similarly. 
An optimal program produced by the dynamic programming algorithm has 
an important property. It evaluates an expression E = El op E2 "contigu- 
ously." We can appreciate what this means by looking at the syntax tree T for 
E: 
Here TI and T2 are trees for El and E2, 
respectively. 
We say a program P evaluates a tree T contiguously if it first evaluates those 
subtrees of T that need to be computed into memory. Then, it evaluates the 
remainder of T either in the order TI, T2, and then the root, or in the order 
T2, TI, and then the root, in either case using the previously computed values 
from memory whenever necessary. As an example of noncontiguous evaluation, 
P might first evaluate part of TI leaving the value in a register (instead of 
memory), next evaluate T2, and then return to evaluate the rest of TI. 
For the register machine in this section, we can prove that given any mach- 
ine-language program P to evaluate an expression tree T, we can find an equiv- 
alent program P' such that 
1. P' is of no higher cost than P, 
2. P' uses no more registers than P, and 
3. P' evaluates the tree contiguously. 
This result implies that every expression tree can be evaluated optimally by 
a contiguous program. 
By way of contrast, machines with even-odd register pairs do not always have 
optimal contiguous evaluations; the x86 architecture uses register pairs for mul- 
tiplication and division. For such machines, we can give examples of expression 
trees in which an optimal machine language program must first evaluate into 
a register a portion of the left subtree of the root, then a portion of the right 
subtree, then another part of the left subtree, then another part of the right, 
and so on. This type of oscillation is unnecessary for an optimal evaluation of 
any expression tree using the machine in this section. 
8.11. DYNAMIC PROGRAMMING CODE-GENERATION 
The contiguous evaluation property defined above ensures that for any ex- 
pression tree T there always exists an optimal program that consists of optimal 
programs for subtrees of the root, followed by an instruction to evaluate the 
root. This property allows us to use a dynamic programming algorithm to 
generate an optimal program for T. 
8.1 
1.2 The Dynamic Programming Algorithm 
The dynamic programming algorithm proceeds in three phases (suppose the 
target machine has r registers): 
1. Compute bottom-up for each node n of the expression tree T an array C 
of costs, in which the ith component C[i] 
is the optimal cost of computing 
the subtree S 
rooted at n into a register, assuming i registers are available 
for the computation, for 1 
5 i 5 r .  
2. Traverse T ,  
using the cost vectors to determine which subtrees of T must 
be computed into memory. 
3. Traverse each tree using the cost vectors and associated instructions to 
generate the final target code. The code for the subtrees computed into 
memory locations is generated first. 
Each of these phases can be implemented to run in time linearly proportional 
to the size of the expression tree. 
The cost of computing a node n includes whatever loads and stores are 
necessary to evaluate S in the given number of registers. It also includes the 
cost of computing the operator at the root of S. The zeroth component of 
the cost vector is the optimal cost of computing the subtree S into memory. 
The contiguous evaluation property ensures that an optimal program for S can 
be generated by considering combinations of optimal programs only for the 
subtrees of the root of S. This restriction reduces the number of cases that 
need to be considered. 
In order to compute the costs C[i] at node n, we view the instructions as 
tree-rewriting rules, as in Section 8.9. Consider each template E that matches 
the input tree at node n. By examining the cost vectors at the corresponding 
descendants of n, determine the costs of evaluating the operands at the leaves 
of E. For those operands of E that are registers, consider all possible orders in 
which the corresponding subtrees of T can be evaluated into registers. In each 
ordering, the first subtree corresponding to a register operand can be evaluated 
using i available registers, the second using i - 
1 
registers, and so on. To account 
for node n, add in the cost of the instruction associated with the template E. 
The value C[i] is then the minimum cost over all possible orders. 
The cost vectors for the entire tree T can be computed bottom up in time 
linearly proportional to the number of nodes in T. It is convenient to store at 
each node the instruction used to achieve the best cost for C[i] for each value 
576 
CHAPTER 8. CODE GENERATION 
of i. The smallest cost in the vector for the root of T gives the minimum cost 
of evaluating T. 
Example 8.28 : Consider a machine having two registers RO and R l  
, and the 
following instructions, each of unit cost: 
LD 
Ri, Mj 
// Ri = M j  
op 
Ri, Ri, R j  
/ / R i Z R i  o p R j  
op 
Ri, Ri, Mj 
/ / R i = R i  o p M j  
LD 
R i ,  R j  
// Ri = R j  
ST 
M i ,  R j  
// M i  = R j  
In these instructions, Ri is either RO or R l ,  and Mj is a memory location. The 
operator op corresponds to an arithmetic operators. 
Let us apply the dynamic programming algorithm to generate optimal code 
for the syntax tree in Fig 8.26. In the first phase, we coznpute the cost vectors 
shown at each node. To illustrate this cost computation, consider the cost 
vector at the leaf a. C[O], 
the cost of computing a into memory, is 0 since it is 
already there. C[l], the cost of computing a into a register, is 1 
since we can 
load it into a register with the instruction LD RO , 
a. C[2], the cost of loading a 
into a register with two registers available, is the same as that with one register 
available. The cost vector at leaf' a is therefore (O,1,1). 
Figure 8.26: Syntax tree for (a-b) +c* (d/e) 
with cost vector at each node 
Consider the cost vector at the root. We first determine the minimum 
cost of computing the root with one and two registers available. The machine 
instruction ADD RO, RO, M matches the root, because the root is labeled with 
the operator +. Using this instruction, the minimum cost of evaluating the 
root with one register available is the minimum cost of computing its right 
subtree into memory, plus the minimum cost of computing its left subtree into 
the register, plus 1 
for the instruction. No other way exists. The cost vectors at 
the right and left children of the root show that the minimum cost of computing 
the root with one register available is 5 + 
2 + 
1 
= 
8. 
Now consider the minimum cost of evaluating the root with two registers 
available. Three cases arise depending on which instruction is used to compute 
the root and in what order the left and right subtrees of the root are evaluated. 
8.11. DYNAMIC PROGRAMMING CODE-GENERATION 
577 
1. Compute the left subtree with two registers available into register RO, 
compute the right subtree with one register available into register R1, and 
use the instruction ADD ROY 
ROY 
R 1  to compute the root. This sequence 
has cost 2 + 
5 + 1 
= 8. 
2. Compute the right subtree with two registers available into R l ,  compute 
the left subtree with one register available into RO, and use the instruction 
ADD ROY 
ROY 
R1. This sequence has cost 4+2+1 = 7. 
3. Compute the right subtree into memory location M, 
compute the left sub- 
tree with two registers available into register RO, and use the instruction 
ADD ROY 
ROY 
M. This sequence has cost 5 + 
2 + 
1 
= 
8. 
The second choice gives the minimum cost 7. 
The minimum cost of computing the root into memory is determined by 
adding one to the minimum cost of computing the root with all registers avail- 
able; that is, we compute the root into a register and then store the result. The 
cost vector at the root is therefore (8,8,7). 
From the cost vectors we can easily construct the code sequence by making 
a traversal of the tree. From the tree in Fig. 8.26, assuming two registers are 
available, an optimal code sequence is 
LD 
ROY c 
// RO = c 
LD Rl, d 
// R 1  = d 
D I V  R1, R1, e 
// R 1  = R 1  / e 
MUL ROY ROY R1 
// RO = RO * R 1  
LD R 1 ,  a 
// R 1  = a 
SUB R1, R1, b 
// R 1  = R 1  - b 
ADD Rl, R1, RO 
// R 1  = R 1  + RO 
Dynamic programming techniques have been used in a number of compilers, 
including the second version of the portable C compiler, PCC2. The technique 
facilitates retargeting because of the applicability of the dynamic programming 
technique to a broad class of machines. 
8.11.3 Exercises for Section 8.11 
Exercise 8.11.1 
: 
Augment the tree-rewriting scheme in Fig. 8.20 with costs, 
and use dynamic programming and tree matching to generate code for the 
statements in Exercise 8.9.1. 
!! Exercise 8.1 
1.2 : 
How would you extend dynamic programming to do optimal 
code generation on dags? 
5 
78 
CHAPTER 8. CODE GENERATION 
8.12 
Summary of Chapter 8 
+ Code generation is the final phase of a compiler. The code generator maps 
the intermediate representation produced by the front end, or if there is a 
code optimization phase by the code optimizer, into the target program. 
+ Instruction selection is the process of choosing target-language instruc- 
tions for each IR statement. 
+ Register allocation is the process of deciding which IR values to keep 
in registers. Graph coloring is an effective technique for doing register 
allocation in compilers. 
+ Register assignment is the process of deciding which register should hold 
a given IR value. 
+ A retargetable compiler is one that can generate code for multiple instruc- 
tion sets. 
+ A virtual machine is an interpreter for a bytecode intermediate language 
produced by languages such as Java and C#. 
4 A CISC machine is typically a two-address machine with relatively few 
registers, several register classes, and variable-length instructions with 
complex addressing modes. 
+ A RISC machine is typically a three-address machine with many registers 
in which operations are done in registers. 
A basic block is a maximal sequence of consecutive three-address state- 
ments in which flow of control can only enter at the first statement of the 
block and leave at the last statement without halting or branching except 
possibly at the last statement in the basic block. 
+ A flow graph is a graphical representation of a program in which the nodes 
of the graph are basic blocks and the edges of the graph show how control 
can flow among the blocks. 
+ A loop in a flow graph is a strongly connected region with a single entry 
point called the loop header. 
+ A DAG representation of a basic block is a directed acyclic graph in which 
the nodes of the DAG represent the statements within the block and each 
child of a node corresponds to the statement that is the last definition of 
an operand used in the statement. 
+ Peephole optimizations are local code-improving transformations that can 
be applied to a program, usually through a sliding window. 
8.13. REFERENCES FOR CHAPTER 8 
579 
+ Instruction selection can be done by a tree-rewriting process in which 
tree patterns corresponding to machine instructions are used to tile a 
syntax tree. We can associate costs with the tree-rewriting rules and 
apply dynamic programming to obtain an optimal tiling for useful classes 
of machines and expressions. 
+ An Ershov number tells how many registers are needed to evaluate an 
expression without storing any temporaries. 
+ Spill code is an instruction sequence that stores a value in a register into 
memory in order to make room to hold another value in that register. 
8.13 References for Chapter 8 
Many of the techniques covered in this chapter have their origins in the earliest 
compilers. Ershov7s 
labeling algorithm appeared in 1958 [7]. Sethi and Ullman 
[16] used this labeling in an algorithm that they prove generated optimal code 
for arithmetic expressions. Aho and Johnson [I] used dynamic programming 
to generate optimal code for expression trees on CISC machines. Hennessy 
and Patterson [12] has a good discussion on the evolution of CISC and RISC 
machine architectures and the tradeoffs involved in designing a good instruction 
set. 
RISC architectures became popular after 1990, although their origins go 
back to computers like the CDC 6600, first delivered in 1964. Many of the 
computers designed before 1990 were CISC machines, but most of the general- 
purpose computers installed after 1990 are still CISC machines because they are 
based on the Intel 80x86 architecture and its descendants, such as the Pentium. 
The Burroughs B5000 delivered in 1963 was an early stack-based machine. 
Many of the heuristics for code generation proposed in this chapter have been 
used in various compilers. Our strategy of allocating a fixed number of registers 
to hold variables for the duration of a loop was used in the implementation of 
Fortran H by Lowry and Medlock [13]. 
Efficient register allocation techniques have also been studied from the time 
of the earliest compilers. Graph coloring as a register-allocation technique was 
proposed by Cocke, Ershov [8], and Schwartz [15]. Many variants of graph- 
coloring algorithms have been proposed for register allocation. Our treatment 
of graph coloring follows Chaitin [3] [4]. Chow and Hennessy describe their 
priority-based coloring algorithm for register allocation in [5]. See [6] for a 
discussion of more recent graph-splitting and rewriting techniques for register 
allocation. 
Lexical analyzer and parser generators spurred the development of pattern- 
directed instruction selection. Glanville and Graham [ll] 
used LR-parser gen- 
eration techniques for automated instruction selection. Table-driven code gen- 
erators evolved into a variety of tree-pattern matching code-generation tools 
[14]. Aho, Ganapat 
hi, and Tjiang [2] combined efficient tree-pattern matching 
580 
CHAPTER 8. CODE GENERATION 
techniques with dynamic programming in the code generation tool twig. Fraser, 
Hanson, and Proebsting [lo] further refined these ideas in their simple efficient 
code-generator generator. 
1. Aho, A. V. and S. C. Johnson, "Optimal code generation for expression 
trees," J. ACM 23:3, pp. 488-501. 
2. Aho, A. V., M. Ganapathi, and S. W. K. Tjiang, "Code generation using 
tree matching and dynamic programming," A 
CM Trans. Programming 
Languages and Systems 11:4 (1989), pp. 491-516. 
3. Chaitin, G. J., M. A. Auslander, A. K. Chandra, J. Cocke, M. E. Hop- 
kins, and P. W. Markstein, "Register allocation via coloring," Computer 
Languages 6:l (1981), pp. 47-57. 
4. Chaitin, G. J., "Register allocation and spilling via graph coloring," A 
CM 
SIGPLAN Notices 17:6 (1982), pp. 201-207. 
5. Chow, F. and J. L. Hennessy, "The priority-based coloring approach to 
register allocation," ACM Trans. Programming Languages and Systems 
12:4 (1990), pp. 501-536. 
6. Cooper, K. D. and L. Torczon, Engineering a Compiler, Morgan Kauf- 
mann, San Francisco CA, 2004. 
7. Ershov, A. P., "On programming of arithmetic operations," Comm. A CM 
1% 
(1958), pp. 3-6. Also, Comm. ACM 1:9 (1958), p. 16. 
8. Ershov, A. P., The Alpha Automatic Programming System, Academic 
Press, New York, 1971. 
9. Fischer, C. N. and R. J. LeBlanc, Crafting a Compiler with C, Benjamin- 
Cummings, Redwood City, CA, 1991. 
10. Fraser, C. W., D. R. Hanson, and T. A. Proebsting, "Engineering a sim- 
ple, efficient code generator generator," ACM Letters on Programming 
Languages and Systems 1:3 (1992), pp. 213-226. 
11. Glanville, R. S. and S. L. Graham, "A new method for compiler code gen- 
eration," Conf. Rec. Fifth ACM Symposium on Principles of Programming 
Languages (1978), pp. 231-240. 
12. Hennessy, J. L. and D. A. Patterson, Computer Architecture: A Quanti- 
tative Approach, Third Edition, Morgan Kaufman, San Francisco, 2003. 
13. Lowry, E. S. and C. W. Medlock, "Object code optimization," Comm. 
ACM 12:l (1969), pp. 13-22. 
8.13. REFERENCES FOR CHAPTER 8 
581 
14. Pelegri-Llopart, E. and S. L. Graham, "Optimal code generation for ex- 
pressions trees: an application of BURS theory," Conf. Rec. Fifteenth An- 
nual ACM Symposium on Principles o
f
 Programming Languages (1988), 
pp. 294-308. 
15. Schwartz, J. T., On Programming: An Interim Report on the SETL 
Project, Technical Report, Courant Institute of Mathematical Sciences, 
New York, 1973. 
16. Sethi, R. and J. D. Ullman, "The generation of optimal code for arithmetic 
expressions," J. ACM 17:4 (1970), pp. 715-728. 
Chapter 9 
Machine-Independent 
Optimizations 
High-level language constructs can introduce substantial run-time overhead if 
we naively translate each construct independently into machine code. This 
chapter discusses how to eliminate many of these inefficiencies. Elimination of 
unnecessary instructions in object code, or the replacement of one sequence of 
instructions by a faster sequence of instructions that does the same thing is 
usually called "code improvement" or "code optimization." 
Local code optimization (code improvement within a basic block) was intro- 
duced in Section 8.5. This chapter deals with global code optimization, where 
improvements take into account what happens across basic blocks. We begin 
in Section 9.1 with a discussion of the principal opportunities for code improve- 
ment. 
Most global optimizations are based on data-flow analyses, which are algo- 
rithms to gather information about a program. The results of data-flow analyses 
all have the same form: for each instruction in the program, they specify some 
property that must hold every time that instruction is executed. The analyses 
differ in the properties they compute. For example, a constant-propagation 
analysis computes, for each point in the program, and for each variable used by 
the program, whether that variable has a unique constant value at that point. 
This information may be used to replace variable references by constant values, 
for instance. As another example, a liveness analysis determines, for each point 
in the program, whether the value held by a particular variable at that point is 
sure to be overwritten before it is read. If so, we do not need to preserve that 
value, either in a register or in a memory location. 
We introduce data-flow analysis in Section 9.2, including several important 
examples of the kind of information we gather globally and then use to improve 
the code. Section 9.3 introduces the general idea of a data-flow framework, 
of which the data-flow analyses in Section 9.2 are special cases. We can use 
essentially the same algorithms for all these instances of data-flow analysis, and 
584 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
we can measure the performance of these algorithms and show their correctness 
on all instances, as well. Section 9.4 is an example of the general framework that 
does more powerful analysis than the earlier examples. Then, in Section 9.5 
we consider a powerful technique, called "partial redundancy elimination," for 
optimizing the placement of each expression evaluation in the program. The 
solution to this problem requires the solution of a variety of different data-flow 
problems. 
In Section 9.6 we take up the discovery and analysis of loops in programs. 
The identification of loops leads to another family of algorithms for solving 
data-flow problems that is based on the hierarchical structure of the loops of 
a well-formed ("reducible") program. This approach to data-flow analysis is 
covered in Section 9.7. Finally, Section 9.8 uses hierarchical analysis to eliminate 
induction variables (essentially, variables that count the number of iterations 
around a loop). This code improvement is one of the most important we can 
make for programs written in commonly used programming languages. 
9.1 The Principal Sources of Optimization 
A compiler optimization must preserve the semantics of the original program. 
Except in very special circumstances, once a programmer chooses and imple- 
ments a particular algorithm, the compiler cannot understand enough about 
the program to replace it with a substantially different and more efficient al- 
gorithm. A compiler knows only how to apply relatively low-level semantic 
transformations, using general facts such as algebraic identities like i + 
0 = 
i or 
program semantics such as the fact that performing the same operation on the 
same values yields the same result. 
9.1.1 
Causes of Redundancy 
There are many redundant operations in a typical program. Sometimes the 
redundancy is available at the source level. For instance, a programmer may 
find it more direct and convenient to recalculate some result, leaving it to 
the compiler to recognize that only one such calculation is necessary. But 
more often, the redundancy is a side effect of having written the program in a 
high-level language. In most languages (other than C or C++, where pointer 
arithmetic is allowed), programmers have no choice but to refer t o  elements of 
an array or fields in a structure through accesses like A[i][j] 
or X -+ f I. 
As a program is compiled, each of these high-level data-structure accesses 
expands into a number of low-level arithmetic operations, such as the computa- 
tion of the location of the (i, 
j)th element of a matrix A. Accesses to the same 
data structure often share many common low-level operations. Programmers 
are not aware of these low-level operations and cannot eliminate the redundan- 
cies themselves. It is, in fact, preferable from a software-engineering perspec- 
tive that programmers only access data elements by their high-level names; the 
9.1. THE PRINCIPAL SO 
URCES OF OPTIMIZATION 
585 
programs are easier to write and, more importantly, easier to understand and 
evolve. By having a compiler eliminate the redundancies, we get the best of 
both worlds: the programs are both efficient and easy to maintain. 
9.1.2 
A Running Example: Quicksort 
In the following, we shall use a fragment of a sorting program called quzcksort 
to illustrate several important code-improving transformations. The C program 
in Fig. 9.1 is derived from Sedgewick,' who discussed the hand-optimization of 
such a program. We shall not discuss all the subtle algorithmic aspects of this 
program here, for example, the fact that a[O] 
must contain the smallest of the 
sorted elements, and a[max] 
the largest. 
void quicksort(int m, i n t  n) 
/* recursively s o r t s  a[m] through a[n] */ 
i 
i n t  i ,  j ;  
i n t  v, x; 
i f  (n <= m
)
 r e t u r n ;  
/* fragment begins here */ 
i = m-I; j = n; v = a[n]; 
while (I) ( 
do i = i + l ;  while (a[i] < v ) ;  
do j = j-1; while (a[j] > v ) ;  
i f  ( i  >= j )  break; 
x = a [ i l ;  aCil = a[j]; a [ j l  = x; /* swap a [ i ] ,  a[j] */ 
3 
x = a [ i l  ; a[il = a[nl ; a[nl = x; /* swap a[il , a[n] */ 
/* fragment ends here */ 
quicksort (m, j )  ; quicksort ( i + l  
,n) ; 
Figure 9.1: C code for quicksort 
Before we can optimize away the redundancies in address calculations, the 
address operations in a program first must be broken down into low-level arith- 
metic operations to expose the redundancies. In the rest of this chapter, we as- 
sume that the intermediate representation consists of three-address statements, 
where temporary variables are used to hold all the results of intermediate ex- 
pressions. Intermediate code for the marked fragment of the program in Fig. 9.1 
is shown in Fig. 9.2. 
In this example we assume that integers occupy four bytes. The assignment 
x = aCil is translated as in Section 6.4.4 into the two three-address statements 
'R. Sedgewick, "Implementing Quicksort Programs,'' Cornrn. ACM, 21,1978, pp. 847-857. 
586 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
t10 = 4*j 
art101 = x 
goto ( 5 )  
tll = 4*i 
x 
= a [ t l l ]  
t12 = 4*i 
Figure 9.2: Three-address code for fragment in Fig. 9.1 
as shown in steps (14) and (15) of Fig. 9.2. Similarly, a[j] = x becomes 
in steps (20) and (21). Notice that every array access in the original program 
translates into a pair of steps, consisting of a multiplication and an array- 
subscripting operation. As a result, this short program fragment translates 
into a rather long sequence of three-address operations. 
Figure 9.3 is the flow graph for the program in Fig. 9.2. Block B1 is the 
entry node. All conditional and unconditional jumps to statements in Fig. 9.2 
have been replaced in Fig. 9.3 by jumps to the block of which the statements 
are leaders, as in Section 8.4. In Fig. 9.3, there are three loops. Blocks B2 and 
BS are loops by themselves. Blocks B2, B3, Bq, 
and B5 together form a loop, 
with B2 the only entry point. 
9.1.3 Semantics-Preserving Transformations 
There are a number of ways in which a compiler can improve a program without 
changing the function it computes. Common-subexpression elimination, copy 
propagation, dead-code elimination, and constant folding are common examples 
of such function-preserving (or semantics-preserving) transformations; we shall 
consider each in turn. 
9.1. THE PRINCIPAL SOURCES OF OPTIMIZATION 
tll = 4*i 
x = a[tll] 
t12 = 4*i 
t13 = 4*n 
t14 = a[t13] 
a[t12] = t14 
t15 = 4*n 
a[t15] = x 
Figure 9.3: Flow graph for the quicksort fragment 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Frequently, a program will include several calculations of the same value, 
such as an offset in an array. As mentioned in Section 9.1.2, some of these 
duplicate calculations cannot be avoided by the programmer because they lie 
below the level of detail accessible within the source language. For example, 
block B5 shown in Fig. 9.4(a) recalculates 4 * 
i and 4 * j ,  
although none of these 
calculations were requested explicitly by the programmer. 
(a) Before. 
(b) After. 
Figure 9.4: Local common-subexpression elimination 
9.1.4 
Global Common Subexpressions 
An occurrence of an expression E is called a common subexpression if E was 
previously computed and the values of the variables in E have not changed since 
the previous computation. We avoid recomputing E if we can use its previously 
computed value; that is, the variable x to which the previous computation of 
E was assigned has not changed in the interim.2 
Example 9.1 : The assignments to t7 
and t 
10 in Fig. 9.4(a) compute the 
common subexpressions 4 * i and 4 * j, respectively. These steps have been 
eliminated in Fig. 9.4(b), which uses t6 
instead of t7 
and t8 
instead of t10. 
Example 9.2 : 
Figure 9.5 shows the result of eliminating both global and local 
common subexpressions from blocks B5 and B6 in the flow graph of Fig. 9.3. 
We first discuss the transformation of B5 and then mention some subtleties 
involving arrays. 
After local common subexpressions are eliminated, B5 
still evaluates 4*i and 
4 * j ,  as shown in Fig. 9.4(b). Both are common subexpressions; in particular, 
the three statements 
'1f x has changed, it may still be possible to reuse the computation of E if we assign its 
value to a new variable y, as well as to x, and use the value of y in place of a recomputation 
of E. 
9.1. THE PRINCIPAL SOURCES OF OPTIMIZATION 
Figure 9.5: B5 and B6 after common-subexpression elimination 
in B5 can be replaced by 
using t4 computed in block B3. In Fig. 9.5, observe that as control passes from 
the evaluation of 4 * 
j in B3 to Bs, 
there is no change to j and no change to t4, 
so t4 can be used if 4 * j is needed. 
Another common subexpression comes to light in B5 after t4 replaces t8. 
The new expression a[t4] 
corresponds to the value of a[j] 
at the source level. 
Not only does j retain its value as control leaves B3 and then enters B5, 
but 
590 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
a[j], a value computed into a temporary t5, does too, because there are no 
assignments to elements of the array a in the interim. The statements 
in B5 therefore can be replaced by 
Analogously, the value assigned to x in block B5 of Fig. 9.4(b) is seen to be 
the same as the value assigned to t3 in block B2. Block B5 in Fig. 9.5 is the 
result of eliminating common subexpressions corresponding to the values of the 
source level expressions a[i] and a[j] from B5 in Fig. 9.4(b). A similar series of 
transformations has been done to B6 in Fig. 9.5. 
The expression a[tl] in blocks B1 and B6 of Fig. 9.5 is not considered a 
common subexpression, although t l  can be used in both places. After control 
leaves B1 and before it reaches B6, it can go through B5, where there are 
assignments to a. Hence, a[tl] 
may not have the same value on reaching B6 as 
it did on leaving B1, and it is not safe to treat a[tl] 
as a common subexpression. 
9.1.5 Copy Propagation 
Block B5 in Fig. 9.5 can be further improved by eliminating x, using two new 
transformations. One concerns assignments of the form u = v called copy state- 
ments, or copies for short. Had we gone into more detail in Example 9.2, copies 
would have arisen much sooner, because the normal algorithm for eliminating 
common subexpressions introduces them, as do several other algorithms. 
Figure 9.6: Copies introduced during common subexpression elimination 
Example 9.3 : 
In order to eliminate the common subexpression from the state- 
ment c = d+e in Fig. 9.6(a), 
we must use a new variable t to hold the value of 
d + 
e. The value of variable t, instead of that of the expression d + 
e, is assigned 
to c in Fig. 9.6(b). Since control may reach c = d+e either after the assignment 
to a or after the assignment to b, it would be incorrect to replace c = d+e by 
either c = a o r b y c  = b. 
9.1. THE PRINCIPAL SO 
URGES OF OPTIMIZATION 
The idea behind the copy-propagation transformation is to use v for u, 
wherever possible after the copy statement u = v. For example, the assignment 
x = t3 
in block B5 of Fig. 9.5 is a copy. Copy propagation applied to B5 yields 
the code in Fig. 9.7. This change may not appear to be an improvement, but, 
as we shall see in Section 9.1.6, it gives us the opportunity to eliminate the 
assignment to x. 
Figure 9.7: Basic block B5 after copy propagation 
9.1.6 Dead-Code Elimination 
A variable is live at a point in a program if its value can be used subsequently; 
otherwise, it is dead at that point. A related idea is dead (or useless) code - 
statements that compute values that never get used. While the programmer is 
unlikely to introduce any dead code intentionally, it may appear as the result 
of previous transformations. 
Example 9.4 : 
Suppose debug is set to TRUE or FALSE at various points in the 
program, and used in statements like 
i f  (debug) p r i n t  ... 
It may be possible for the compiler to deduce that each time the program 
reaches this statement, the value of debug is FALSE. Usually, it is because there 
is one particular statement 
debug = FALSE 
that must be the last assignment to debug prior to any tests of the value of 
debug, no matter what sequence of branches the program actually takes. If 
copy propagation replaces debug by FALSE, then the print statement is dead 
because it cannot be reached. We can eliminate both the test and the print 
operation from the object code. More generally, deducing at compile time that 
the value of an expression is a constant and using the constant instead is known 
as constant folding. 
One advantage of copy propagation is that it often turns the copy state- 
ment into dead code. For example, copy propagation followed by dead-code 
elimination removes the assignment to x and transforms the code in Fig 9.7 
into 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
This code is a further improvement of block B5 in Fig. 9.5. 
9.1.7 CodeMotion 
Loops are a very important place for optimizations, especially the inner loops 
where programs tend to spend the bulk of their time. The running time of a 
program may be improved if we decrease the number of instructions in an inner 
loop, even if we increase the amount of code outside that loop. 
An important modification that decreases the amount of code in a loop is 
code motion. This transformation takes an expression that yields the same 
result independent of the number of times a loop is executed (a loop-invariant 
computation) and evaluates the expression before the loop. Note that the notion 
"before the loop" assumes the existence of an entry for the loop, that is, one 
basic block to which all jumps from outside the loop go (see Section 8.4.5). 
Example 9.5 : 
Evaluation of limit - 
2 is a loop-invariant computation in the 
following while-st 
atement : 
while ( i  <= limit-2) /* statement does not change l i m i t  */ 
Code motion will result in the equivalent code 
t = limit-2 
while ( i  <= t )  /* statement does not change l i m i t  or t */ 
Now, the computation of limit - 
2 is performed once, before we enter the loop. 
Previously, there would be n + 
1 
calculations of limit - 
2 if we iterated the body 
of the loop n times. 
9.1.8 Induction Variables and Reduction in Strength 
Another important optimization is to find induction variables in loops and 
optimize their computation. A variable x is said to be an "induction variable" 
if there is a positive or negative constant c such that each time x is assigned, its 
value increases by c. For instance, i and t2 are induction variables in the loop 
containing Bz of Fig. 9.5. Induction variables can be computed with a single 
increment (addition or subtraction) per loop iteration. The transformation of 
replacing an expensive operation, such as multiplication, by a cheaper one, 
such as addition, is known as strength reduction. But induction variables not 
only allow us sometimes to perform a strength reduction; often it is possible to 
eliminate all but one of a group of induction variables whose values remain in 
lock step as we go around the loop. 
9 . .  THE PRINCIPAL SOURCES OF OPTIMIZATION 
Figure 9.8: Strength reduction applied to 4 * j in block B3 
When processing loops, it is useful to work "inside-out" ; that is, we shall 
start with the inner loops and proceed to progressively larger, surrounding 
loops. Thus, we shall see how this optimization applies to our quicksort example 
by beginning with one of the innermost loops: B3 by itself. Note that the values 
of j and t4 remain in lock step; every time the value of j decreases by 1, the 
value of t4 decreases by 4, because 4 * j is assigned to t4. These variables, j 
and t4, thus form a good example of a pair of induction variables. 
When there are two or more induction variables in a loop, it may be possible 
to get rid of all but one. For the inner loop of B3 in Fig. 9.5, we cannot get rid of 
either j or t4 completely; t4 is used in B3 and j is used in B4. 
However, we can 
illustrate reduction in strength and a part of the process of induction-variable 
elimination. Eventually, j will be eliminated when the outer loop consisting of 
blocks Bz, 
B3, 
B4 and Bs is considered. 
594 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Example 9.6 : 
As the relationship t4 = 
4 * j surely holds after assignment to 
t4 in Fig. 9.5, and t4 is not changed elsewhere in the inner loop around B3, it 
follows that just after the statement j = j -1 the relationship t4 = 4 * j + 
4 
must hold. We may therefore replace the assignment t 4  = 4* j by t 4  = t4-4. 
The only problem is that t4 does not have a value when we enter block B3 for 
the first time. 
Since we must maintain the relationship t4 = 
4 * 
j on entry to the block B3, 
we place an initialization of t4 at the end of the block where j itself is initialized, 
shown by the dashed addition to block B1 in Fig. 9.8. Although we have added 
one more iastruction, which is executed once in block B1, the replacement of a 
multiplication by a subtraction will speed up the object code if multiplication 
takes more time than addition or subtraction, as is the case on many machines. 
Figure 9.9: Flow graph after induction-variable elimination 
We conclude this section with one more instance of induction-variable elim- 
9.1. THE PRINCIPAL SOURCES OF OPTIMIZATION 
595 
ination. This example treats i and j in the context of the outer loop containing 
B2, B3, B4 
7 and B5. 
Example 9.7 
: 
After reduction in strength is applied to the inner loops around 
B2 and B3, the only use of i and j is to determine the outcome of the test in 
block B4. We know that the values of i and t2 satisfy the relationship t2 = 
4 * 
i, 
while those of j and t4 satisfy the relationship t4 = 
4* j. Thus, the test t2 2 t4 
can substitute for i 2 j. Once this replacement is made, i in block B2 and j in 
block B3 become dead variables, and the assignments to them in these blocks 
become dead code that can be eliminated. The resulting flow graph is shown 
in Fig. 9.9. 
ENTRY 
7 
Figure 9.10: Flow graph for Exercise 9.1.1 
The code-improving transformations we have discussed have been effective. 
In Fig. 9.9, the numbers of instructions in blocks B2 and B3 have been reduced 
from 4 to 3, compared with the original flow graph in Fig. 9.3. In B5, 
the number 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
has been reduced from 9 to 3, and in B6 from 8 to 3. True, B1 has grown from 
four instructions to six, but B1 is executed only once in the fragment, so the 
total running time is barely affected by the size of B1. 
9.1.9 Exercises for Section 9.1 
Exercise 9.1.1 : 
For the flow graph in Fig. 9.10: 
a) Identify the loops of the flow graph. 
b) Statements (1) 
and (2) in B1 are both copy statements, in which a and b 
are given constant values. For which uses of a and b can we perform copy 
propagation and replace these uses of variables by uses of a constant? Do 
so, wherever possible. 
c) Identify any global common subexpressions for each loop. 
d) Identify any induction variables for each loop. Be sure to take into account 
any constants introduced in (b). 
e) Identify any loop-invariant cornput 
ations for each loop. 
Exercise 9.1.2 
: 
Apply the transformations of this section to the flow graph 
of Fig. 8.9. 
Exercise 9.1.3 
: 
Apply the transformations of this section to your flow graphs 
from (a) Exercise 8.4.1; (b) Exercise 8.4.2. 
Exercise 9.1.4: In Fig. 9.11 is intermediate code to compute the dot product 
of two vectors A and B. Optimize this code by eliminating common subexpres- 
sions, performing reduction in strength on induction variables, and eliminating 
all the induction variables you can. 
Figure 9.11: Intermediate code to compute the dot product 
9.2. INTRODUCTION TO DATA-FLO 
W ANALYSIS 
9.2 Introduction to Data-Flow Analysis 
All the optimizations introduced in Section 9.1 depend on data-flow analysis. 
"Data-flow analysis" refers to a body of techniques that derive information 
about the flow of data along program execution paths. For example, one way to 
implement global common subexpression elimination requires us to determine 
whether two textually identical expressions evaluate to the same value along 
any possible execution path of the program. As another example, if the result 
of an assignment is not used along any subsequent execution path, then we 
can eliminate the assignment as dead code. These and many other important 
questions can be answered by data-flow analysis. 
9.2.1 
The Data-Flow Abstraction 
Following Section 1.6.2, the execution of a program can be viewed as a series 
of transformations of the program state, which consists of the values of all the 
variables in the program, including those associated with stack frames below the 
top of the run-time stack. Each execution of an intermediate-code statement 
transforms an input state to a new output state. The input state is associated 
with the program point before the statement and the output state is associated 
with the program point after the statement. 
When we analyze the behavior of a program, we must consider all the pos- 
sible sequences of program points ("paths") through a flow graph that the pro- 
gram execution can take. We then extract, from the possible program states 
at each point, the information we need for the particular data-flow analysis 
problem we want to solve. In more complex analyses, we must consider paths 
that jump among the flow graphs for various procedures, as calls and returns 
are executed. However, to begin our study, we shall concentrate on the paths 
through a single flow graph for a single procedure. 
Let us see what the flow graph tells us about the possible execution paths. 
Within one basic block, the program point after a statement is the same 
as the program point before the next statement. 
o If there is an edge from block B1 to block B2, 
then the program point after 
the last statement of B1 may be followed immediately by the program 
point before the first statement of B2. 
Thus, we may define an execution path (or just path) from point pl to point p, 
to be a sequence of points p1 ,p2, . . 
. , 
Pn such that for each i = 1,2, 
. . 
. , 
n - 
1, 
either 
1. pi is the point immediately preceding a statement and pi+l is the point 
immediately following that same statement, or 
2. pi is the end of some block and pi+l is the beginning of a successor block. 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
In general, there is an infinite number of possible execution paths through a 
program, and there is no finite upper bound on the length of an execution path. 
Program analyses summarize all the possible program states that can occur at 
a point in the program with a finite set of facts. Different analyses may choose 
to abstract out different information, and in general, no analysis is necessarily 
a perfect representation of the state. 
Example 9.8 : 
Even the simple program in Fig. 9.12 describes an unbounded 
number of execution paths. Not entering the loop at all, the shortest com- 
plete execution path consists of the program points (1,2,3,4,9). The next 
shortest path executes one iteration of the loop and consists of the points 
(1,2,3,4,5,6,7,8,3,4,9). 
We know that, for example, the first time program 
point (5) is executed, the value of a is 1 due to definition dl. We say that 
dl reaches point (5) in the first iteration. In subsequent iterations, dg reaches 
point (5) and the value of a is 243. 
Figure 9.12: Example program illustrating the data-flow abstraction 
In general, it is not possible to keep track of all the program states for all 
possible paths. In data-flow analysis, we do not distinguish among the paths 
taken to reach a program point. Moreover, we do not keep track of entire states; 
rather, we abstract out certain details, keeping only the data we need for the 
purpose of the analysis. Two examples will illustrate how the same program 
states may lead to different information abstracted at a point. 
1. To help users debug their programs, we may wish to find out what are 
all the values a variable may have at a program point, and where these 
values may be defined. For instance, we may summarize all the program 
states at point (5) by saying that the value of a is one of {I, 
2431, and 
that it may be defined by one of {dl, ds). The definitions that may reach 
a program point along some path are known as reaching definitions. 
9.2. INTRODUCTION TO DATA-FLO 
W ANALYSIS 
599 
2. Suppose, instead, we are interested in implementing constant folding. If a 
use of the variable x is reached by only one definition, and that definition 
assigns a constant to x, then we can simply replace x by the constant. 
If, on the other hand, several definitions of x may reach a single program 
point, then we cannot perform constant folding on x. Thus, for constant 
folding we wish to find those definitions that are the unique definition of 
their variable to reach a given program point, no matter which execution 
path is taken. For point (5) of Fig. 9.12, there is no definition that must 
be the definition of a at that point, so this set is empty for a at point (5). 
Even if a variable has a unique definition at a point, that definition must 
assign a constant to the variable. Thus, we may simply describe certain 
variables as "not a constant," instead of collecting all their possible values 
or all their possible definitions. 
Thus, we see that the same information may be summarized differently, de- 
pending on the purpose of the analysis. 
9.2.2 The Data-Flow Analysis Schema 
In each application of data-flow analysis, we associate with every program point 
a data-flow value that represents an abstraction of the set of all possible program 
states that can be observed for that point. The set of possible data-flow values 
is the domain for this application. For example, the domain of data-flow values 
for reaching definitions is the set of all subsets of definitions in the program. 
A particular data-flow value is a set of definitions, and we want to associate 
with each point in the program the exact set of definitions that can reach that 
point. As discussed above, the choice of abstraction depends on the goal of the 
analysis; to be efficient, we only keep track of information that is relevant. 
We denote the data-flow values before and after each statement s by IN[S] 
and OUT[S], respectively. The data-flow problem is to find a solution to a set 
of constraints on the IN[s]'s and OUT[S]'s, 
for all statements s. There are two 
sets of constraints: those based on the semantics of the statements ( 
"transfer 
functions") and those based on the flow of control. 
Transfer Functions 
The data-flow values before and after a statement are constrained by the se- 
mantics of the statement. For example, suppose our data-flow analysis involves 
determining the constant value of variables at points. If variable a has value v 
before executing statement b = a, then both a and b will have the value v 
after 
the statement. This relationship between the data-flow values before and after 
the assignment statement is known as a transfer function. 
Transfer functions come in two flavors: information may propagate forward 
along execution paths, or it may flow backwards up the execution paths. In 
a forward-flow problem, the transfer function of a statement s, which we shall 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
usually denote f,, takes the data-flow value before the statement and produces 
a new data-flow value after the statement. That is, 
Conversely, in a backward-flow problem, the transfer function f, for statement 
s converts a data-flow value after the statement to a new data-flow value before 
the statement. That is, 
Control-Flow Constraints 
The second set of constraints on data-flow values is derived from the flow of 
control. Within a basic block, control flow is simple. If a block B consists of 
statements sl 
, 
s2, . 
. . , 
S, in that order, then the control-flow value out of si is 
the same as the control-flow value into si+l. That is, 
1 ~ [ ~ i + l ]  
= 
OUT[S~], 
for all i = 1 , 2 , .  
. . , 
n - 
1. 
However, control-flow edges between basic blocks create more complex con- 
straints between the last statement of one basic block and the first statement 
of the following block. For example, if we are interested in collecting all the 
definitions that may reach a program point, then the set of definitions reaching 
the leader statement of a basic block is the union of the definitions after the 
last statements of each of the predecessor blocks. The next section gives the 
details of how data flows among the blocks. 
9.2.3 Data-Flow Schemas on Basic Blocks 
While a data-flow schema technically involves data-flow values at each point in 
the program, we can save time and space by recognizing that what goes on inside 
a block is usually quite simple. Control flows from the beginning to the end of 
the block, without interruption or branching. Thus, we can restate the schema 
in terms of data-flow values entering and leaving the blocks. We denote the 
data-flow values immediately before and immediately after each basic block B 
by IN[B] 
and OUT[B], 
respectively. The constraints involving IN[B] 
and o u ~ [ B ]  
can be derived from those involving IN[S] and OUT[S] for the various statements 
s in B as follows. 
Suppose block B consists of statements sl, 
. 
. . , 
s,, in that order. If sl is the 
first statement of basic block B, then IN[B] 
= 
lN[sl], Similarly, if s, is the last 
statement of basic block B, then OUT[B] 
= OUT[S,]. The transfer function of 
a basic block B, which we denote fB, can be derived by composing the transfer 
functions of the statements in the block. That is, let fSi 
be the transfer function 
of statement si. Then fs = f,, o . . 
. o f,, o fsl. The relationship between the 
beginning and end of the block is 
9.2. INTRODUCTION TO DATA-FLOW ANALYSIS 
The constraints due to control flow between basic blocks can easily be rewrit- 
ten by substituting IN[B] 
and OUT[B] 
for lN[sl] and OUT[S,], respectively. For 
instance, if data-flow values are information about the sets of constants that 
m a y  be assigned to a variable, then we have a forward-flow problem in which 
U P  
a predecessor of B OUT[P]. 
When the data-flow is backwards as we shall soon see in live-variable analy- 
sis, the equations are similar, but with the roles of the IN'S and OUT'S reversed. 
That is, 
IN[B] 
= f~ 
(ouT[B]) 
= U
S
 a successor of B IN [S] 
. 
Unlike linear arithmetic equations, the data-flow equations usually do not 
have a unique solution. Our goal is to find the most "precise" solution that 
satisfies the two sets of constraints: control-flow and transfer constraints. That 
is, we need a solution that encourages valid code improvements, but does not 
justify unsafe transformations - 
those that change what the program com- 
putes. This issue is discussed briefly in the box on "Conservatism" and more 
extensively in Section 9.3.4. In the following subsections, we discuss some of the 
most important examples of problems that can be solved by data-flow analysis. 
9.2.4 
Reaching Definitions 
"Reaching definitions" is one of the most common, and useful data-flow schemas. 
By knowing where in a program each variable x may have been defined when 
control reaches each point pi we can determine many things about x. For just 
two examples, a compiler then knows whether x is a constant at point p, and 
a debugger can tell whether it is possible for x to be an undefined variable, 
should x be used at p. 
We say a definition d reaches a point p if there is a path from the point 
immediately following d to pi such that d is not LLkilled" 
along that path. We 
kill a definition of a variable x if there is any other definition of x anywhere 
along the path.3 Intuitively, if a definition d of some variable x reaches point 
pi then d might be the place at which the value of x used at p was last defined. 
A definition of a variable x is a statement that assigns, or may assign, a 
value to x . Procedure parameters, array accesses, and indirect references all 
may have aliases, and it is not easy to tell if a statement is referring to a 
particular variable x. Program analysis must be conservative; if we do not 
- - 
3 ~ o t e  
that the path may have loops, so we could come to another occurrence of d along 
the path, which does not "kill" d. 
602 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Detecting Possible Uses Before Definition 
Here is how we use a solution to the reaching-definitions problem to detect 
uses before definition. The trick is to introduce a dummy definition for 
each variable x in the entry to the flow graph. If the dummy definition 
of x reaches a point p where x might be used, then there might be an 
opportunity to use x before definition. Note that we can never be abso- 
lutely certain that the program has a bug, since there may be some reason, 
possibly involving a complex logical argument, why the path along which 
p is reached without a real definition of x can never be taken. 
know whether a statement s is assigning a value to x, we must assume that 
it may assign to it; that is, variable x after statement s may have either its 
original value before s or the new value created by s. For the sake of simplicity, 
the rest of the chapter assumes that we are dealing only with variables that 
have no aliases. This class of variables includes all local scalar variables in most 
languages; in the case of C and C++, local variables whose addresses have been 
computed at some point are excluded. 
Example 9.9 : 
Shown in Fig. 9.13 is a flow graph with seven definitions. Let us 
focus on the definitions reaching block B2. All the definitions in block B1 reach 
the beginning of block B2. The definition ds: j = j-1 
in block B2 also reaches 
the beginning of block B2, 
because no other definitions of j can be found in the 
loop leading back to B2. This definition, however, kills the definition d2: j = n, 
preventing it from reaching B3 or B4. The statement d4: i = i+l in B2 does 
not reach the beginning of B2 
though, because the variable i is always redefined 
by d7: i = u3. 
Finally, the definition ds 
: a = u2 
also reaches the beginning of 
block B2. 
By defining reaching definitions as we have, we sometimes allow inaccuracies. 
However, they are all in the "safe," or "conservative," direction. For example, 
notice our assumption that all edges of a flow graph can be traversed. This 
assumption may not be true in practice. For example, for no values of a and 
b can the flow of control actually reach statement 2 in the following program 
fragment: 
if (a == b) statement 1; 
else if (a == b) statement 2; 
To decide in general whether each path in a flow graph can be taken is 
an undecidable problem. Thus, we simply assume that every path in the flow 
graph can be followed in some execution of the program. In most applications 
of reaching definitions, it is conservative to assume that a definition can reach a 
point even if it might not. Thus, we may allow paths that are never be traversed 
in any execution of the program, and we may allow definitions to pass through 
ambiguous definitions of the same variable safely. 
9.2. INTRODUCTION TO DATA-FLO 
W ANALYSIS 
603 
Conservatism in Data-Flow Analysis 
Since all data-flow schemas compute approximations to the ground truth 
(as defined by all possible execution paths of the program), we are obliged 
to assure that any errors are in the "safe" direction. A policy decision is 
safe (or conservative) if it never allows us to change what the program 
computes. Safe policies may, unfortunately, cause us to miss some code 
improvements that would retain the meaning of the program, but in essen- 
tially all code optimizations there is no safe policy that misses nothing. It 
would generally be unacceptable to use an unsafe policy - 
one that sped' 
up the code at the expense of changing what the program computes. 
Thus, when designing a data-flow schema, we must be conscious of 
how the information will be used, and make sure that any approximations 
we make are in the "conservative" or "safe" direction. Each schema and 
application must be considered independently. For instance, if we use 
reaching definitions for constant folding, it is safe to think a definition 
reaches when it doesn't (we might think x is not a constant, when in fact 
it is and could have been folded), but not safe to think a definition doesn't 
reach when it does (we might replace x by a constant, when the program 
would at times have a value for x other than that constant). 
Transfer Equations for Reaching Definitions 
We shall now set up the constraints for the reaching definitions problem. We 
start by examining the details of a single statement. Consider a definition 
Here, and frequently in what follows, + is used as a generic binary operator. 
This statement "generates" a definition d of variable u and "kills" all the 
other definitions in the program that define variable u, while leaving the re- 
maining incoming definitions unaffected. The transfer function of definition d 
thus can be expressed as 
where gend = {d}, the set of definitions generated by the statement, and killd 
is the set of all other definitions of u in the program. 
As discussed in Section 9.2.2, the transfer function of a basic block can be 
found by composing the transfer functions of the statements contained therein. 
The composition of functions of the form (9.1), which we shall refer to as "gen- 
kill form," is also of that form, as we can see as follows. Suppose there are two 
functions f ~ ( x )  
= 
genl U (x - 
M111) and f2(x) 
= 
genz U (x - 
killz). Then 
604 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
ENTRY 
'----- 
gen 
= { d6 1 
B3 
kill 
={4} 
B3 
senB4 = {  
d, 1 
kill 
= { dl, d4 } 
B4 
Figure 9.13: Flow graph for illustrating reaching definitions 
This rule extends to a block consisting of any number of statements. Suppose 
block B has n statements, with transfer functions fi(x) = 
geni U ( x  
- 
killi) for 
i = 
1,2, 
. 
. . , 
n. Then the transfer function for block B may be written as: 
where 
killB = 
killl U kill2 U . 
- . U kill, 
and 
g e n ~  
= gen, U (gen,-1 - 
kill,) U (gennV2 
- 
- 
kill,) U 
- - . 
U (genl - 
killz - 
kills - 
. 
- . 
- 
kill,) 
9.2. INTRODUCTION TO DATA-FLO W ANALYSIS 
Thus, like a statement, a basic block also generates a set of definitions and 
kills a set of definitions. The gen set contains all the definitions inside the block 
that are "visible" immediately after the block - 
we refer to them as downwards 
exposed. A definition is downwards exposed in a basic block only if it is not 
"killed" by a subsequent definition to the same variable inside the same basic 
block. A basic block's kill set is simply the union of all the definitions killed by 
the individual statements. Notice that a definition may appear in both the gen 
and kill set of a basic block. If so, the fact that it is in gen takes precedence, 
because in gen-kill form, the kill set is applied before the gen set. 
Example 9.10 : 
The gen set for the basic block 
is Id2) since dl is not downwards exposed. The kill set contains both dl and 
d2, since dl kills d2 and vice versa. Nonetheless, since the subtraction of the 
kill set precedes the union operation with the gen set, the result of the transfer 
function for this block always includes definition dz. 
Control-Flow Equations 
Next, we consider the set of constraints derived from the control flow between 
basic blocks. Since a definition reaches a program point as long as there exists 
at least one path along which the definition reaches, OUT[P] 
C IN[B] 
whenever 
there is a control-flow edge from P to B. However, since a definition cannot 
reach a point unless there is a path along which it reaches, IN[B] 
needs to be no 
larger than the union of the reaching definitions of all the predecessor blocks. 
That is, it is safe to assume 
U P  
a predecessor of B OUT[P] 
We refer to union as the meet operator for reaching definitions. In any data- 
flow schema, the meet operator is the one we use to create a summary of the 
contributions from different paths at the confluence of those paths. 
Iterative Algorithm for Reaching Definitions 
We assume that every control-flow graph has two empty basic blocks, an ENTRY 
node, which represents the starting point of the graph, and an EXIT node to 
which all exits out of the graph go. Since no definitions reach the beginning 
of the graph, the transfer function for the ENTRY block is a simple constant 
function that returns 0 as an answer. That is, OUT[ENTRY] = 0. 
The reaching definitions problem is defined by the following equations: 
606 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
and for all basic blocks B other than ENTRY, 
OUT[B] 
= 
geng U (IN[B] 
- 
killB) 
U P  
a predecessor of B OUT[P]. 
These equations can be solved using the following algorithm. The result of 
the algorithm is the least fixedpoint of the equations, i.e., the solution whose 
assigned values to the IN'S and OUT'S is contained in the corresponding values 
for any other solution to the equations. The result of the algorithm below is 
acceptable, since any definition in one of the sets IN or OUT surely must reach 
the point described. It is a desirable solution, since it does not include any 
definitions that we can be sure do not reach. 
Algorithm 9.11 : 
Reaching definitions. 
INPUT: A flow graph for which killB and g e n ~  
have been computed for each 
block B. 
OUTPUT: IN[B] 
and OUT[B], 
the set of definitions reaching the entry and exit 
of each block B of the flow graph. 
METHOD: We use an iterative approach, in which we start with the "estimate" 
OUT[B] 
= 0 for all B and converge to the desired values of IN and OUT. As 
we must iterate until the IN'S (and hence the OUT'S) 
converge, we could use a 
boolean variable change to record, on each pass through the blocks, whether 
any OUT has changed. However, in this and in similar algorithms described 
later, we assume that the exact mechanism for keeping track of changes is 
understood, and we elide those details. 
The algorithm is sketched in Fig. 9.14. The first two lines initialize certain 
data-flow  value^.^ Line (3) starts the loop in which we iterate until convergence, 
and the inner loop of lines (4) through (6) applies the data-flow equations to 
every block other than the entry. 
Intuitively, Algorithm 9.11 propagates definitions as far as they will go with- 
out being killed, thus simulating all possible executions of the program. Algo- 
rithm 9.11 will eventually halt, because for every B, OUT[B] 
never shrinks; once 
a definition is added, it stays there forever. (See Exercise 9.2.6.) Since the set of 
all definitions is finite, eventually there must be a pass of the while-loop during 
which nothing is added to any OUT, and the algorithm then terminates. We 
are safe terminating then because if the OUT'S have not changed, the IN'S will 
4 ~ h e  
observant reader will notice that we could easily combine lines (1) and (2). However, 
in similar data-flow algorithms, it may be necessary to initialize the entry or exit node dif- 
ferently from the way we initialize the other nodes. Thus, we follow a pattern in all iterative 
algorithms of applying a "boundary condition" like line (1) separately from the initialization 
of line (2). 
9.2. INTRODUCTION TO DATA-FLOW ANALYSIS 
1) 
OUT[ENTRY] = 0; 
2) 
for (each basic block B other than ENTRY) 
OUT[B] 
= 
0; 
3) 
while (changes to any OUT occur) 
4) 
for (each basic block B other than ENTRY) 
{ 
5) 
= U P  
a predecessor of B OuTIP1; 
6) 
OUT[B] 
= 
geng U (IN[B] 
- 
killB); 
} 
Figure 9.14: Iterative algorithm to compute reaching definitions 
not change on the next pass. And, if the IN'S do not change, the OUT'S cannot, 
so on all subsequent passes there can be no changes. 
The number of nodes in the flow graph is an upper bound on the number of 
times around the while-loop. The reason is that if a definition reaches a point, 
it can do so along a cycle-free path, and the number of nodes in a flow graph is 
an upper bound on the number of nodes in a cycle-free path. Each time around 
the while-loop, each definition progresses by at least one node along the path 
in question, and it often progresses by more than one node, depending on the 
order in which the nodes are visited. 
In fact, if we properly order the blocks in the for-loop of line (5), there 
is empirical evidence that the average number of iterations of the while-loop 
is under 5 (see Section 9.6.7). Since sets of definitions can be represented 
by bit vectors, and the operations on these sets can be implemented by logical 
operations on the bit vectors, Algorithm 9.11 is surprisingly efficient in practice. 
Example 9.12 : We shall represent the seven definitions dl, 
d2, . . . , 
d7 in the 
flow graph of Fig. 9.13 by bit vectors, where bit i from the left represents 
definition di. The union of sets is computed by taking the logical OR of the 
corresponding bit vectors. The difference of two sets S - 
T is computed by 
complementing the bit vector of T, and then taking the logical AND of that 
complement, with the bit vector for S. 
Shown in the table of Fig. 9.15 are the values taken on by the IN and OUT 
sets in Algorithm 9.11. The initial values, indicated by a superscript 0, as 
in OUT[B]O, 
are assigned, by the loop of line (2) of Fig. 9.14. They are each 
the empty set, represented by bit vector 000 0000. The values of subsequent 
passes of the algorithm are also indicated by superscripts, and labeled 1N[BI1 
and OUT[B]' for the first pass and 1N[BI2 
and 0uT[BI2 for the second. 
Suppose the for-loop of lines (4) through (6) is executed with B taking on 
the values 
in that order. With B = B1, since OUT[ENTRY] = 0, IN[B~]' 
is the empty set, 
and O U T [ B ~ ] ~  
is geng,. This value differs from the previous value OUT[B~]~, 
so 
608 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Figure 9.15: Computation of IN and OUT 
Block B 
B1 
B2 
BS 
B4 
EXIT 
we now know there is a change on the first round (and will proceed to a second 
round). 
Then we consider B = 
B2 and compute 
This computation is summarized in Fig. 9.15. For instance, at the end of the 
first pass, 0UT[B2I1 
= 
001 1100, reflecting the fact that d4 and d5 are generated 
in B2, 
while d3 reaches the beginning of B2 and is not killed in B2. 
Notice that after the second round, 0UT[B2] 
has changed to reflect the fact 
that d6 also reaches the beginning of B2 and is not killed by B2. We did not 
learn that fact on the first pass, because the path from ds to the end of B2, 
which is B3 -+ B4 -+ B2, 
is not traversed in that order by a single pass. That is, 
by the time we learn that d6 reaches the end of B4, we have already computed 
IN[B~] 
and 0uT[B2] 
on the first pass. 
There are no changes in any of the OUT sets after the second pass. Thus, 
after a third pass, the algorithm terminates, with the IN'S and OUT'S as in the 
final two columns of Fig. 9.15. 
OUT[B]O 
000 0000 
000 0000 
000 0000 
000 0000 
000 0000 
9.2.5 
Live-Variable Analysis 
Some code-improving transformations depend on information computed in the 
direction opposite to the flow of control in a program; we shall examine one 
such example now. In live-variable analysis we wish to know for variable x and 
point p whether the value of x at p could be used along some path in the flow 
graph starting at p. If so, we say x is live at p; otherwise, x is dead at p. 
An important use for live-variable information is register allocation for basic 
blocks. Aspects of this issue were introduced in Sections 8.6 and 8.8. After a 
value is computed in a register, and presumably used within a block, it is not 
1N[BI1 
000 0000 
111 0000 
001 1100 
001 1110 
001 0111 
OUT[B]~ 
111 0000 
001 1100 
000 1110 
001 0111 
001 0111 
1N[BI2 
000 0000 
111 0111 
001 1110 
001 1110 
001 0111 
OUT[B]~ 
111 0000 
001 1110 
000 1110 
001 0111 
001 0111 
9.2. INTRODUCTION TO DATA-FLO 
W ANALYSIS 
609 
necessary to store that value if it is dead at the end of the block. Also, if all 
registers are full and we need another register, we should favor using a register 
with a dead value, since that value does not have to be stored. 
Here, we define the data-flow equations directly in terms of IN[B] and 
OUT[B], 
which represent the set of variables live at the points immediately 
before and after block B, respectively. These equations can also be derived 
by first defining the transfer functions of individual statements and composing 
them to create the transfer function of a basic block. Define 
1. defB as the set of variables defined (i.e., definitely assigned values) in B 
prior to any use of that variable in B, and 
2. u s e g  as the set of variables whose values may be used in B prior to any 
definition of the variable. 
Example 9.13 : For instance, block B2 in Fig. 9.13 definitely uses i. It also 
uses j before any redefinition of j, unless it is possible that i and j are aliases 
of one another. Assuming there are no aliases among the variables in Fig. 9.13, 
then  use^, = {i, 
j ) .  Also, B2 clearly defines i and j. Assuming there are no 
aliases, d e f g ,  = {i, 
j), as well. 
As a consequence of the definitions, any variable in  use^ must be considered 
live on entrance to block B, while definitions of variables in defB definitely 
are dead at the beginning of B. In effect, membership in defB "kills" any 
opportunity for a variable to be live becausq of paths that begin at B. 
Thus, the equations relating def and u s e  to the unknowns IN and OUT are 
defined as follows: 
and for all basic blocks B other than EXIT, 
IN[B] 
= 
u s e g  U (ouT[B] - 
defB) 
OUT[B] 
= U 
S a successor of B IN[SI 
The first equation specifies the boundary condition, which is that no variables 
are live on exit from the program. The second equation says that a variable is 
live coming into a block if either it is used before redefinition in the block or 
it is live coming out of the block and is not redefined in the block. The third 
equation says that a variable is live coming out of a block if and only if it is 
live coming into one of its successors. 
The relationship between the equations for liveness and the reaching-defin- 
itions equations should be noticed: 
610 
CHAPTER 9. MA 
CHINE-INDEPENDENT OPTIMIZATIONS 
Both sets of equations have union as the meet operator. The reason is 
that in each data-flow schema we propagate information along paths, and 
we care only about whether any path with desired properties exist, rather 
than whether something is true along all paths. 
However, information flow for liveness travels "backward," opposite to the 
direction of control flow, because in this problem we want to make sure 
that the use of a variable x at a point p is transmitted to all points prior 
to p in an execution path, so that we may know at the prior point that x 
will have its value used. 
To solve a backward problem, instead of initializing OUT[ENTRY], we ini- 
tialize IN[EXIT]. 
Sets I
N
 and OUT have their roles interchanged, and use and 
def substitute for gen and kill, respectively. As for reaching definitions, the 
solution to the liveness equations is not necessarily unique, and we want the so- 
lution with the smallest sets of live variables. The algorithm used is essentially 
a backwards version of Algorithm 9.1 
1. 
Algorithm 9.14 : 
Live-variable analysis. 
INPUT: A flow graph with def and use computed for each block. 
OUTPUT: IN[B] 
and OUT[B], 
the set of variables live on entry and exit of each 
block B of the flow graph. 
METHOD: Execute the program in Fig. 9.16. 
IN[EXIT] = 
0; 
for (each basic block B other than EXIT) 
IN[B] 
= 
0; 
while (changes to any IN occur) 
for (each basic block B other than EXIT) 
{ 
OUT[BI = US a successor of B IN I
S
1
 ; 
IN[B] 
= 
useg U (ouT[B] - 
deb); 
1 
Figure 9.16: Iterative algorithm to compute live variables 
9.2.6 
Available Expressions 
An expression x + 
y is available at a point p if every path from the entry node 
to p evaluates x + y, and after the last such evaluation prior to reaching p, 
there are no sltbsequent assignments to x or y.5 For the available-expressions 
data-flow schema we say that a block kills expression x + 
y if it assigns (or may 
5 ~ o t e  
that, as usual in this chapter, we use the operator f as a generic operator, not 
necessarily standing for addition. 
9.2. INTRODUCTION TO DATA-FLO 
W ANALYSIS 
611 
assign) x or y and does not subsequently recompute x + 
y. A block generates 
expression x + 
y if it definitely evaluates x + 
y and does not subsequently define 
x or y. 
Note that the notion of "killing" or "generating7' 
an available expression is 
not exactly the same as that for reaching definitions. Nevertheless, these notions 
of "kill7' 
and "generate7' 
behave essentially as they do for reaching definitions. 
The primary use of available-expression information is for detecting global 
common subexpressions. For example, in Fig. 9.17(a), the expression 4 * i in 
block B3 will be a common subexpression if 4 * 
i is available at the entry point 
of block BS. 
It will be available if i is not assigned a new value in block B2, 
or 
if, as in Fig. 9.17(b), 4 * i is recomputed after i is assigned in B2. 
Figure 9.17: Potential common subexpressions across blocks 
We can compute the set of generated expressions for each point in a block, 
working from beginning to end of the block. At the point prior to the block, no 
expressions are generated. If at point p set S of expressions is available, and q 
is the point after p, with statement x = y+z between them, then we form the 
set of expressions available at q by the following two steps. 
1. Add to S the expression y + 
x. 
2. Delete from S any expression involving variable x. 
Note the steps must be done in the correct order, as x could be the same as 
y or z. After we reach the end of the block, S is the set of generated expressions 
for the block. The set of killed expressions is all expressions, say y + 
t, 
such 
that either 7~ or z is defined in the block, and y + 
x is not generated by the 
block. 
Example 9.15 : 
Consider the four statements of Fig. 9.18. After the first, b + 
c 
is available. After the second statement, a - 
d becomes available, but b + 
c is 
no longer available, because b has been redefined. The third statement does 
not make b + 
c available again, because the value of c is immediately changed. 
612 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
After the last statement, a - 
d is no longer available, because d has changed. 
Thus no expressions are generated, and all expressions involving a, b, c, or d 
are killed. 
0 
Statement 
Available Expressions 
Figure 9.18: Computation of available expressions 
We can find available expressions in a manner reminiscent of the way reach- 
ing definitions are computed. Suppose U is the "universal" set of all expressions 
appearing on the right of one or more statements of the program. For each block 
B, let IN[B] 
be the set of expressions in U that are available at the point just 
before the beginning of B. Let OUT[B] 
be the same for the point following the 
end of B. Define e-gen~ 
to be the expressions generated by B and e-killB to be 
the set of expressions in U killed in B. Note that IN, OUT, e-gen, and e-kill can 
all be represented by bit vectors. The following equations relate the unknowns 
IN and OUT to each other and the known quantities e-gen and e-kill: 
and for all basic blocks B other than ENTRY, 
OUT[B] 
= 
e-geng U (IN[B] 
- 
e.killB) 
= n
p
 
a predecessor of B  OUT[^]. 
The above equations look almost identical to the equations for reaching 
definitions. Like reaching definitions, the boundary condition is OUT[ENTRY] 
= 
0, 
because at the exit of the ENTRY node, there are no available expressions. 
The most important difference is that the meet operator is intersection rather 
than union. This operator is the proper one because an expression is available 
at the beginning of a block only if it is available at the end of all its predecessors. 
In contrast, a definition reaches the beginning of a block whenever it reaches 
the end of any one or more of its predecessors. 
9.2. INTRODUCTION TO DATA-FLO 
W ANALYSIS 
613 
The use of n 
rather than U makes the available-expression equations behave 
differently from those of reaching definitions. While neither set has a unique 
solution, for reaching definitions, it is the solution with the smallest sets that 
corresponds to the definition of "reaching," and we obtained that solution by 
starting with the assumption that nothing reached anywhere, and building up 
to the solution. In that way, we never assumed that a definition d could reach 
a point p unless an actual path propagating d to p could be found. In contrast, 
for available expression equations we want the solution with the largest sets of 
available expressions, so we start with an approximation that is too large and 
work down. 
It may not be obvious that by starting with the assumption "everything 
(i.e., the set U )  
is available everywhere except at the end of the entry block" 
and eliminating only those expressions for which we can discover a path along 
which it is not available, we do reach a set of truly available expressions. In 
the case of available expressions, it is conservative to produce a subset of the 
exact set of available expressions. The argument for subsets being conservative 
is that our intended use of the information is to replace the computation of an 
available expression by a previously computed value. Not knowing an expres- 
sion is available only inhibits us from improving the code, while believing an 
expression is available when it is not could cause us to change what the program 
computes. 
Figure 9.19: Initializing the OUT sets to Q) is too restrictive. 
Example 9.16 : We shall concentrate on a single block, B2 in Fig. 9.19, to 
illustrate the effect of the initial approximation of 0uT[B2] 
on I N [ B ~ ] .  
Let G 
and K abbreviate e-gen~, 
and e-killB2, 
respectively. The data-flow equations 
for block B2 are 
These equations may be rewritten as recurrences, with ~i and 0j being the jth 
614 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
approximations of 1N[B2] 
and 0uT[B2], 
respectively: 
Starting with O0 = 0, we get I' = 
OUT[B~] 
n 
0
'
 = 
0. However, if we start 
with 0
'
 = U ,  
then we get I' = OUT[B~] 
n 
O0 = OuTIB1], as we should. Intu- 
itively, the solution obtained starting with 0
'
 = U is more desirable, because 
it correctly reflects the fact that expressions in OUTIB1] that are not killed by 
B2 are available at the end of B2. 
Algorithm 9.17 : 
Available expressions. 
INPUT: A flow graph with e-killB and e-gen~ 
computed for each block B. The 
initial block is B1. 
OUTPUT: IN[B] 
and OUT[B], 
the set of expressions available at the entry and 
exit of each block B of the flow graph. 
METHOD: Execute the algorithm of Fig. 9.20. The explanation of the steps is 
similar to that for Fig. 9.14. 
OUT[ENTRY] = 0; 
for (each basic block B other than ENTRY) OUT[B] 
= 
U ;  
while (changes to any OUT occur) 
for (each basic block B other than ENTRY) { 
INPI 
=np 
a predecessor of B OUTPI 
; 
OUT[B] 
= 
e-gen~ 
U (IN[B] 
- 
e-killB); 
1 
Figure 9.20: Iterative algorithm to compute available expressions 
9.2.7 Summary 
In this section, we have discussed three instances of data-flow problems: reach- 
ing definitions, live variables, and available expressions. As summarized in 
Fig. 9.21, the definition of each problem is given by the domain of the data- 
flow values, the direction of the data flow, the family of transfer functions, 
the boundary condition, and the meet operator. We denote the meet operator 
generically as A. 
The last row shows the initial values used in the iterative algorithm. These 
values are chosen so that the iterative algorithm will find the most precise 
solution to the equations. This choice is not strictly a part of the definition of 
9.2. INTRODUCTION TO DATA-FLO 
W ANALYSIS 
615 
the data-flow problem, since it is an artifact needed for the iterative algorithm. 
There are other ways of solving the problem. For example, we saw how the 
transfer function of a basic block can be derived by composing the transfer 
functions of the individual statements in the block; a similar compositional 
approach may be used to compute a transfer function for the entire procedure, 
or transfer functions from the entry of the procedure to any program point. We 
shall discuss such an approach in Section 9.7. 
Boundary 1 OUT[ENTRY] = 
0 
I IN[EXIT] 
= 
@ 
I OUT[ENTRY] = 
0 
Available Expressions 
Sets of expressions 
Forwards 
e - g e n ~  
u (x - 
eAiEIB) 
Live Variables 
Sets of variables 
Backwards 
useB u (x - 
defB) 
Domain 
Direction 
Transfer 
function 
Meet (A) 
Equations 
Figure 9.21: Summary of three data-flow problems 
Reaching Definitions 
Sets of definitions 
Forwards 
g e n ~  
U (x - 
 kill^) 
Initialize 
1 OUT[B] = 
0 
9.2.8 Exercises for Section 9.2 
U 
OUT[B] = 
fB (IN[B]) 
IN[B] 
= 
Exercise 9.2.1: For the flow graph of Fig. 9.10 (see the exercises for Sec- 
tion 9.1), compute 
IN[B] = 
0 
a) The gen and kill sets for each block. 
U 
IN[B] = f~ 
(OUT[B]) 
OUT[B] = 
OUT[B] = 
U 
b) The IN and OUT sets for each block. 
n 
OUT[B] = f~ 
(IN[B]) 
IN[B] 
= 
Exercise 9.2.2 
: 
For the flow graph of Fig. 9.10, compute the e-gen, e-kill, IN, 
and OUT sets for available expressions. 
Exercise 9.2.3 
: 
For the flow graph of Fig. 9.10, compute the def, use, IN; and 
OUT sets for live variable analysis. 
! 
Exercise 9.2.4 : Suppose V is the set of complex numbers. Which of the 
following operations can serve as the meet operation for a semilattice on V? 
a) Addition: (a + 
ib) A (c + 
id) = (a + 
b) + 
i(c + 
d). 
b) Multiplication: (a + 
ib) A (c + 
id) = (ac - 
bd) + 
i(ad + 
be). 
616 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Why the Available-Expressions Algorithm Works 
We need to explain why starting all OUT'S except that for the entry block 
with U ,  
the set of all expressions, leads to a conservative solution to the 
data-flow equations; that is, all expressions found to be available really 
are available. First, because intersection is the meet operation in this 
data-flow schema, any reason that an expression x + 
y is found not to be 
available at a point will propagate forward in the flow graph, along all 
possible paths, until x + y is recomputed and becomes available again. 
Second, there are only two reasons x + 
y could be unavailable: 
1. x + 
y is killed in block B because x or y is defined without a subse- 
quent computation of x + 
y. In this case, the first time we apply the 
transfer function f B  , x + 
y will be removed from o u ~ [ B ] .  
2. x + 
y is never computed along some path. Since x + 
y is never ia 
OUT[ENTRY], and it is never generated along the path in question, 
we can show by induction on the length of the path that x + 
y is 
eventually removed from IN'S and OUT'S along that path. 
Thus, after changes subside, the solution provided by the iterative algo- 
rithm of Fig. 9.20 will include only truly available expressions. 
c) Componentwise minimum: (a + 
ib) A (c 
+ 
id) = 
min(a, c) + 
i min(b, d) . 
d) Componentwise maximum: (a + 
ib) A (c + 
id) = 
max(a, c) + 
i 
max(b, d). 
! 
Exercise 9.2.5 : We claimed that if a block B consists of n statements, and 
the ith statement has gen and kill sets geni and killi, then the transfer function 
for block B has gen and kill sets g e n ~  
and killB given by 
killB = 
killl U kill2 U . - .  
U kill, 
g e n ~  
= gen, U (gen,-1 - 
kill,) U (genn-2 - 
- 
kill,) U 
. 
. U (genl - 
killa - 
kill3 - 
. 
- . - 
kill,). 
Prove this claim by induction on n. 
! 
Exercise 9.2.6 : 
Prove by induction on the number of iterations of the for-loop 
of lines (4) through (6) of Algorithm 9.11 that none of the IN'S or OUT'S ever 
shrinks. That is, once a definition is placed in one of these sets on some round, 
it never disappears on a subsequent round. 
9.2. INTRODUCTION TO DATA-FLO 
W ANALYSIS 
! 
Exercise 9.2.7: Show the correctness of Algorithm 9.11. That is, show that 
a) If definition d is put in IN[B] 
or OUT[B], 
then there is a path from d to 
the beginning or end of block B, respectively, along which the variable 
defined by d might not be redefined. 
b) If definition d is not put in IN[B] 
or OUT[B], 
then there is no path from d 
to the beginning or end of block B, respectively, along which the variable 
defined by d might not be redefined. 
! 
Exercise 9.2.8 : 
Prove the following about Algorithm 9.14: 
a) The IN'S and OUT'S never shrink. 
b) If variable x is put in IN[B] 
or OUT[B], 
then there is a path from the 
beginning or end of block B, respectively, along which x might be used. 
c) If variable x is not put in IN[B] 
or OUT[B], 
then there is no path from the 
beginning or end of block B, respectively, along which x might be used. 
! 
Exercise 9.2.9 : 
Prove the following about Algorithm 9.17: 
a) The IN'S and OUT'S never grow; that is, successive values of these sets are 
subsets (not necessarily proper) of their previous values. 
b) If expression e  
is removed from IN[B] 
or OUT[B], 
then there is a path from 
the entry of the flow graph to the beginning or end of block B, respectively, 
along which e  is either never computed, or after its last computation, one 
of its arguments might be redefined. 
c) If expression e  
remains in IN[B] 
or OUT[B], 
then along every path from the 
entry of the flow graph to the beginning or end of block B, respectively, 
e  is computed, and after the last computation, no argument of e  could be 
redefined. 
! 
Exercise 9.2.10 : 
The astute reader will notice that in Algorithm 9.11 we could 
have saved some time by initializing OUT[B] 
to g e n ~  
for all blocks B. Likewise, 
in Algorithm 9.14 we could have initialized IN[B] 
to g e n ~ .  
We did not do so for 
uniformity in the treatment of the subject, as we shall see in Algorithm 9.25. 
However, is it possible to initialize OUT[B] 
to e - g e n ~  
in Algorithm 9.17? Why 
or why not? 
! Exercise 9.2.11 
: Our data-flow analyses so far do not take advantage of the 
semantics of conditionals. Suppose we find at the end of a basic block a test 
such as 
How could we use our understanding of what the test x < 10 means to improve 
our knowledge of reaching definitions? Remember, "improve" here means that 
we eliminate certain reaching definitions that really cannot ever reach a certain 
program point. 
618 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
9.3 Foundat 
ions of Dat 
a-Flow Analysis 
Having shown several useful examples of the data-flow abstraction, we now 
study the family of data-flow schemas as a whole, abstractly. We shall answer 
several basic questions about data-flow algorithms formally: 
1. Under what circumstances is the iterative algorithm used in data-flow 
analysis correct? 
2. How precise is the solution obtained by the iterative algorithm? 
3. Will the iterative algorithm converge? 
4. What is the meaning of the solution to the equations? 
In Section 9.2, we addressed each of the questions above informally when 
describing the reaching-definitions problem. Instead of answering the same 
questions for each subsequent problem from scratch, we relied on analogies 
with the problems we had already discussed to explain the new problems. Here 
we present a general approach that answers all these questions, once and for 
all, rigorously, and for a large family of data-flow problems. We first iden- 
tify the properties desired of data-flow schemas and prove the implications of 
these properties on the correctness, precision, and convergence of the data-flow 
algorithm, as well as the meaning of the solution. Thus, to understand old 
algorithms or formulate new ones, we simply show that the proposed data-flow 
problem definitions have certain properties, and the answers to all the above 
difficult questions are available immediately. 
The concept of having a common theoretical framework for a class of sche- 
mas also has practical implications. 
The framework helps us identify the 
reusable components of the algorithm in our software design. Not only is cod- 
ing effort reduced, but programming errors are reduced by not having to recode 
similar details several times. 
A data-flow analysis framework (D, 
V, A, F) consists of 
1. A direction of the data flow D, which is either FORWARDS or BACKWARDS. 
2. A semilattice (see Section 9.3.1 for the definition), which includes a do- 
main of values V and a meet operator A. 
3. A family F of transfer functions from V to V. This family must include 
functions suitable for the boundary conditions, which are constant transfer 
functions for the special nodes ENTRY and EXIT in any flow graph. 
9.3.1 Semilattices 
A semilattice is a set V and a binary meet operator A such that for all x, y, 
and x in V: 
9.3. FOUNDATIONS OF DATA-FLO W ANALYSIS 
1. x A x = 
x (meet is idempotent). 
2. x A y = 
y A x (meet is commutative). 
3. x A (y A z) = (x A y) /\ x (meet is associative). 
A semilattice has a top element, denoted T, such that 
for all x in V, T Ax = 
x. 
Optionally, a semilattice may have a bottom element, denoted I, 
such that 
Partial Orders 
As we shall see, the meet operator of a semilattice defines a partial order on 
the values of the domain. A relation < is a partial order on a set V if for all x, 
y, and z in V: 
1. x 5 x (the partial order is reflexive). 
2. If x < y and y < x, then x = 
y (the partial order is antisymmetric). 
3. If x 5 y and y < x, then x < x (the partial order is transitive). 
The pair (V, <) is called a poset, or partially ordered set. It is also convenient 
to have a < relation for a poset, defined as 
x < 
y if and only if (z < 
y) and (x 
# 9). 
The Partial Order for a Semilattice 
It is useful to define a partial order < for a semilattice (V, A). For all x and y 
in V, we define 
x < y if and only if x A y = 
x. 
Because the meet operator A is idempotent, commutative, and associative, the 
< order as defined is reflexive, antisymmetric, and transitive. To see why, 
- 
observe that: 
Reflexivity: for all x, x 5 x. The proof is that x A x = x since meet is 
idempotent. 
Antisymmetry: if x < y and y 5 x, then x = y. In proof, x 5 y 
means x A y = x and y 5 x means y A x = y. By commutativity of A, 
x = (xA 
y) = (y Ax) = 
y. 
620 
CHAPTER 9. MA 
CHINE-INDEPENDENT OPTIMIZATIONS 
Transitivity: if x 5 y and y < z, then x 5 x. In proof, x 5 y and y 5 z 
means that x A y = x and y A x = y. Then (x A x) = ((x A y) A 2) = 
(x A (y A x)) = (x A y) = 
x, using associativity of meet. Since x A z = 
x 
has been shown, we have x 5 x, proving transitivity. 
Example 9.18 : The meet operators used in the examples in Section 9.2 are 
set union and set intersection. They are both idempotent, commutative, and 
associative. For set union, the top element is 0 and the bottom element is U, 
the universal set, since for any subset x of U, 0 u 
x = 
x and U U 
x = U. For 
set intersection, T is U and I 
is 8. V, the domain of values of the semilattice, 
is the set of all subsets of U, which is sometimes called the power set of U and 
denoted 2U. 
For all x and y in V, x U 
y = 
x implies x > y; therefore, the partial order 
imposed by set union is 2, 
set inclusion. Correspondingly, the partial order 
imposed by set intersection is C, 
set containment. That is, for set intersection, 
sets with fewer elements are considered to be smaller in the partial order. How- 
ever, for set union, sets with more elements are considered to be smaller in the 
partial order. To say that sets larger in size are smaller in the partial order is 
counterintuitive; however, this situation is an unavoidable consequence of the 
 definition^.^ 
As discussed in Section 9.2, there are usually many solutions to a set of data- 
flow equations, with the greatest solution (in the sense of the partial order _<) 
being the most precise. For example, in reaching definitions, the most precise 
among all the solutions to the data-flow equations is the one with the smallest 
number of definitions, which corresponds to the greatest element in the partial 
order defined by the meet operation, union. In available expressions, the most 
precise solution is the one with the largest number of expressions. Again, it 
is the greatest solution in the partial order defined by intersection as the meet 
operation. 
C
I
 
Greatest Lower Bounds 
There is another useful relationship between the meet operation and the partial 
ordering it imposes. Suppose (V, A) is a semilattice. A greatest lower bound (or 
glb) of domain elements x and y is an element g  such that 
2. g _ <  
y, and 
3. If x is any element such that x 5 x and x _< y, then x 5 g. 
It turns out that the meet of x and y is their only greatest lower bound. To see 
why, let g = 
x A y 
. Observe that: 
'And if we defined the partial order to be > instead of 5, 
then the problem would surface 
when the meet was intersection, although not for union. 
9.3. FOUNDATIONS OF DATA-FLO 
W ANALYSIS 
Joins, Lub's, and Lattices 
In symmetry to the glb operation on elements of a poset, we may define 
the least upper bound (or lub) of elements x and y to be that element b 
such that x < 
b, y < b, and if z is any element such that x < z and y < z, 
then b < a. One can show that there is at most one such element b if it 
exists. 
In a true lattice, there are two operations on domain elements, the 
meet A, which we have seen, and the operator join, denoted V, which 
gives the lub of two elements (which therefore must always exist in the 
lattice). We have been discussing only "semi" lattices, where only one 
of the meet and join operators exist. That is, our semilattices are meet 
semilattices. One could also speak of join semilattices, where only the join 
operator exists, and in fact some literature on program analysis does use 
the notation of join semilattices. Since the traditional data-flow literature 
speaks of meet semilattices, we shall also do so in this book. 
g 5 x because (x A y) A x = x A y. The proof involves simple uses of 
associativity, commutativity, and idempotence. That is, 
g A x = ((x 
A y) Ax) = (x 
A (y Ax)) = 
(x A (x A 
= ((x A x) A y) = 
(x A Y )  = 
9 
g < y by a similar argument. 
Suppose z is any element such that x 5 x and z < y. We claim z < g, 
and therefore, z cannot be a glb of x and y unless it is also g. In proof: 
(z 
A g) = (z 
A (x 
A y)) = ((z 
A x) 
A y). Since z < x, we know ( z  
A x) = 
z, so 
(z Ag) = (zA 
y). Since z 5 y, we know zA y = 
z, and therefore z Ag = z. 
We have proven z 5 g and conclude g = 
x A y is the only glb of x and y. 
Lattice Diagrams 
It often helps to draw the domain V as a lattice diagram, which is a graph whose 
nodes are the elements of V, and whose edges are directed downward, from x 
to y if y <_ x. For example, Fig. 9.22 shows the set V for a reaching-definitions 
data-flow schema where there are three definitions: dl, d2, 
and d3. Since <_ is 2, 
an edge is directed downward from any subset of these three definitions to each 
of its supersets. Since < is transitive, we conventionally omit the edge from x 
622 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
to y as long as there is another path from x to y left in the diagram. Thus, 
although {dl,d2,d3} 5 {dl), we do not draw this edge since it is represented 
by the path through {dl, d2}, for example. 
Figure 9.22: Lattice of subsets of definitions 
It is also useful to note that we can read the meet off such diagrams. Since 
x A y is the glb, it is always the highest x for which there are paths downward 
to z from both x and y. For example, if x is {dl) and y is {d2), then z in 
Fig. 9.22 is {dl, d2}, which makes sense, because the meet operator is union. 
The top element will appear at the top of the lattice diagram; that is, there is 
a path downward from T to each element. Likewise, the bottom element will 
appear at the bottom, with a path downward from every element to I. 
Product Lattices 
While Fig. 9.22 involves only three definitions, the lattice diagram of a typical 
program can be quite large. The set of data-flow values is the power set of the 
definitions, which therefore contains 2n elements if there are n definitions in 
the program. However, whether a definition reaches a program is independent 
of the reachability of the other definitions. We may thus express the lattice7 of 
definitions in terms of a "product lattice," built from one simple lattice for each 
definition. That is, if there were only one definition d in the program, then the 
lattice would have two elements: {I, 
the empty set, which is the top element, 
and (d), which is the bottom element. 
Formally, we may build product lattices as follows. Suppose (A, 
Aa) and 
(B, 
AB) are (semi)lattices. The product lattice for these two lattices is defined 
as follows: 
1. The domain of the product lattice is A x B. 
7 ~ n  
this discussion and subsequently, we shall often drop the "semi," since lattices like the 
one under discussion do have a join or lub operator, even if we do not make use of it. 
9.3. FOUNDATIONS OF 
DATA-FLO 
W ANALYSIS 
623 
2. The meet A for the product lattice is defined as follows. If (a, 
b) and 
(a', b') are domain elements of the product lattice, then 
(a, 
b) A (a', b') = (a 
A a', b A b'). 
(9.19) 
It is simple to express the 5 partial order for the product lattice in terms 
of the partial orders 5~ 
and SB 
for A and B 
(a, 
b) 5 (a', b') if and only if a 
a' and b SB 
b'. 
(9.20) 
To see why (9.20) follows from (9.19)) 
observe that 
(a, 
b) A (a', b') = (a AA a', b AB b'). 
So we might ask under what circumstances does (aAA 
a', bAB b') = (a, 
b)? That 
happens exactly when a AA a' = a and b AB b' = b. But these two conditions 
are the same as a LA 
a' and b <B b' . 
The product of lattices is an associative operation, so one can show that 
the rules (9.19) and (9.20) extend to any number of lattices. That is, if we are 
given lattices (Ai, 
Ai) for i = 1,2,. 
. . , 
k, then the product of all k lattices, in 
this order, has domain A1 x A2 x . 
. 
. x Ak, 
a meet operator defined by 
and a partial order defined by 
(al, 
a2,. . 
. , 
ak) < (bl, b2,. 
. 
. , 
bk) if and only if ai 5 bi for all i. 
Height of a Semilattice 
We may learn something about the rate of convergence of a data-flow analysis 
algorithm by studying the "height" of the associated semilattice. An ascending 
chain in a poset (V, 5) 
is a sequence where x1 < 
2 2  < . . 
. < xn. The height 
of a semilattice is the largest number of < relations in any ascending chain; 
that is, the height is one less than the number of elements in the chain. For 
example, the height of the reaching definitions semilattice for a program with 
n definitions is n. 
Showing convergence of an iterative data-flow algorithm is much easier if the 
semilattice has finite height. Clearly, a lattice consisting of a finite set of values 
will have a finite height; it is also possible for a lattice with an infinite number 
of values to have a finite height. The lattice used in the constant propagation 
algorithm is one such example that we shall examine closely in Section 9.4. 
9.3.2 Transfer Functions 
The family of transfer functions F : V -+ V in a data-flow framework has the 
following properties: 
624 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
1. F has an identity function I, 
such that I(x) 
= 
x for all x in V. 
2. F is closed under composition; that is, for any two functions f and g in 
F, 
the function h defined by h(x) 
= ( f  
(x)) 
is in F .  
Example 9.21 : 
In reaching definitions, F 
has the identity, the function where 
gen and bill are both the empty set. Closure under composition was actually 
shown in Section 9.2.4; we repeat the argument succinctly here. Suppose we 
have two functions 
fi 
(2) 
= 
GI U (x 
- 
K1) and f 2  (2) 
= 
G2 
U ( X  - 
K2). 
Then 
f 2  ( f i  
( 2 ) )  
= 
G
2
 U ((GI 
U (x 
- 
Kl)) 
- 
K ~ ) .  
The right side of the above is algebraically equivalent to 
(G2 U (GI 
- 
K2)) 
U ( X  - 
(Ki 
U ~ 2 ) ) .  
If we let K = Kl U K2 and G = G2 U (GI 
- 
K2), 
then we have shown that 
the composition of fl and f 2 ,  which is f (x) 
= G U (x - 
K )  
, is of the form 
that makes it a member of F. If we consider available expressions, the same 
arguments used for reaching definitions also show that F has an identity and is 
closed under composition. 
Monotone Frameworks 
To make an iterative algorithm for data-flow analysis work, we need for the 
data-flow framework to satisfy one more condition. We say that a framework 
is monotone if when we apply any transfer function f in F to two members of 
V, the first being no greater than the second, then the first result is no greater 
than the second result. 
Formally, a data-flow framework (D, 
F, V, A) is monotone if 
For all x and y in V and f in F, 
x 5 y implies f (x) 
5 f ( y ) .  
(9.22) 
Equivalently, monotonicity can be defined as 
For all z andy in V and f in F ,  f ( x A y )  
5 f ( x ) A f ( y ) .  
(9.23) 
Equation (9.23) 
says that if we take the meet of two values and then apply f , 
the,result is never greater than what is obtained by applying f to the values 
individually first and then "meeting" the results. Because the two definitions 
of monotonicity seem so different, they are both useful. We shall find one or 
the other more useful under different circumstances. Later, we sketch a proof 
to show that they are indeed equivalent. 
9.3. FOUNDATIONS OF 
DATA-FLOW ANALYSIS 
625 
We shall first assume (9.22) and show that (9.23) holds. Since x A y is the 
greatest lower bound of x and y, we know that 
Thus, by (9.22), 
Since f (x) 
A f (9) is the greatest lower bound of f (x) and f (y), we have (9.23). 
Conversely, let us assume (9.23) and prove (9.22). We suppose x 5 y and 
use (9.23) to conclude f (x) 5 f (y), thus proving (9.22). Equation (9.23) tells 
US 
But since x 5 y is assumed, x A y = 
x, by definition. Thus (9.23) says 
Since f ( x ) ~  
f (y) is the glb off (x) and f (y), 
we know f (x) A f (Y) 
F J(Y). 
Thus 
and (9.23) implies (9.22). 
Distributive Frameworks 
Often, a framework obeys a condition stronger than (9.23), which we call the 
distributivity condition, 
for all x and y in V and f in F. Certainly, if a = 
b, then a A b = 
a by idempot- 
ence, so a 5 b. Thus, distributivity implies monotonicity, although the converse 
is not true. 
Example 9.24: Let y and x be sets of definitions in the reaching-definitions 
framework. Let f be a function defined by f (x) = G U (x - 
K )  for some sets 
of definitions G and K. We can verify that the reaching-definitions framework 
satisfies the distributivity condition, by checking that 
G U ((y U z) - 
K) = (G U (y - 
K)) U (G U (x - 
K)). 
While the equation above may appear formidable, consider first those definitions 
in G. These definitions are surely in the sets defined by both the left and right 
sides. Thus, we have only to consider definitions that are not in G. In that 
case, we can eliminate G everywhere, and verify the equality 
(Y 
U z )  - 
K = (3 - 
K )  U (x - 
K). 
The latter equality is easily checked using a Venn diagram. 
626 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
9.3.3 The Iterative Algorithm for General Frameworks 
We can generalize Algorithm 9.11 to make it work for a large variety of data-flow 
problems. 
Algorit 
hrn 9.25 : 
Iterative solution to general data-flow frameworks. 
INPUT: A data-flow framework with the following components: 
1. A data-flow graph, with specially labeled ENTRY and EXIT nodes, 
2. A direction of the data-flow D, 
3. A set of values V, 
4. A meet operator A, 
5. A set of functions F, 
where f B  in F is the transfer function for block B, 
and 
6. A constant value v
,
,
,
,
 
or v
,
,
,
,
 in V, representing the boundary condition 
for forward and backward frameworks, respectively. 
OUTPUT: Values in V for IN[B] and OUT[B] 
for each block B in the data-flow 
graph. 
METHOD: The algorithms for solving forward and backward data-flow prob- 
lems are shown in Fig. 9.23(a) and 9.23(b), respectively. As with the familiar 
iterative data-flow algorithms from Section 9.2, we compute IN and OUT for 
each block by successive approximation. 
It is possible to write the forward and backward versions of Algorithm 9.25 
so that a function implementing the meet operation is a parameter, as is a 
function that implements the transfer function for each block. The flow graph 
itself and the boundary value are also parameters. In this way, the compiler 
implementor can avoid recoding the basic iterative algorithm for each data-flow 
framework used by the optimization phase of the compiler. 
We can use the abstract framework discussed so far to prove a number of 
useful properties of the iterative algorithm: 
1. If Algorithm 9.25 converges, the result is a solution to the data-flow equa- 
t 
ions. 
2. If the framework is monotone, then the solution found is the maximum 
fixedpoint (MFP) of the data-flow equations. A maxzmum fixedpoint is a 
solution with the property that in any other solution, the values of IN[B] 
and OUT[B] 
are 5 the corresponding values of the MFP. 
3. If the semilattice of the framework is monotone and of finite height, then 
the algorithm is guaranteed to converge. 
9.3. FOUNDATIONS OF DATA-FLO 
W ANALYSIS 
I) 
OUT[ENTRY] = 
VENTRy; 
2) 
for (each basic block B other than ENTRY) 
OUT[B] 
= T; 
3) while (changes to any OUT occur) 
4) 
for (each basic block B other than ENTRY) 
{ 
5 
1 
= 
A P  a predecessor of B O U T [ ~ I ;  
6) 
OUT[B] 
= 
~B(IN[B]); 
} 
(a) Iterative algorithm for a forward data-flow problem. 
I) 
IN [EXIT] = 
VEXIT ; 
2) 
for (each basic block B other than EXIT) 
IN[B] 
= T; 
3) 
while (changes to any IN occur) 
4) 
for (each basic block B other than EXIT) 
{ 
5) 
"
'
[
B
I
 
= A 
,
S
 a successor of B "[SI ; 
6) 
IN[B] 
= 
~B(ouT[B]); 
1 
(b) Iterative algorithm for a backward data-flow problem. 
Figure 9.23: Forward and backward versions of the iterative algorithm 
We shall argue these points assuming that the framework is forward. The 
case of backwards frameworks is essentially the same. The first property is easy 
to show. If the equations are not satisfied by the time the while-loop ends, then 
there will be at least one change to an OUT (in the forward case) or IN (in the 
backward case), and we must go around the loop again. 
To prove the second property, we first show that the values taken on by IN[B] 
and OUT[B] 
for any B can only decrease (in the sense of the 5 relationship for 
lattices) as the algorithm iterates. This claim can be proven by induction. 
BASIS: The base case is to show that the value of IN[B] 
and OUT[B] 
after the 
first iteration is not greater than the initialized value. This statement is trivial 
because IN[B] 
and OUT[B] 
for all blocks B # ENTRY are initialized with T. 
INDUCTION: Assume that after the kth iteration, the values are all no greater 
than those after the (k - 
1)st iteration, and show the same for iteration k + 1 
compared with iteration k. Line (5) of Fig. 9.23(a) has 
IN[B] 
= 
A 
OUT[P]. 
P a predecessor of B 
Let us use the notation IN[@ and OUT[B]~ 
to denote the values of IN[B] 
and 
OUT[B] 
after iteration i. Assuming OUT[P]~ 
5 OUT[P]'-~, we know that 
IN[B]'+' 
< IN[B] 
IC because of the properties of the meet operator. Next, line (6) 
628 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
says 
OUT[B] 
= 
fB(l~[B]). 
Since IN[B]'++' 5 IN[B]', 
we have OUT[B]'+~ < 
OUT[B]"~ 
monotonicity. 
Note that every change observed for values of IN[B] 
and OUT[B] 
is necessary 
to satisfy the equation. The meet operators return the greatest lower bound of 
their inputs, and the transfer functions return the only solution that is consis- 
tent with the block itself and its given input. Thus, if the iterative algorithm 
terminates, the result must have values that are at least as great as the corre- 
sponding values in any other solution; that is, the result of Algorithm 9.25 is 
the MFP of the equations. 
Finally, consider the third point, where the data-flow framework has finite 
height. Since the values of every IN[B] 
and OUT[B] 
decrease with each change, 
and the algorithm stops if at some round nothing changes, the algorithm is 
guaranteed to converge after a number of rounds no greater than the product 
of the height of the framework and the number of nodes of the flow graph. 
9.3.4 Meaning of a Data-Flow Solution 
We now know that the solution found using the iterative algorithm is the max- 
imum fixedpoint, but what does the result represent from a program-semantics 
point of view? To understand the solution of a data-flow framework (D, 
F, 
V, A), 
let us first describe what an ideal solution to the framework would be. We show 
that the ideal cannot be obtained in general, but that Algorithm 9.25 approxi- 
mates the ideal conservatively. 
The Ideal Solution 
Without loss of generality, we shall assume for now that the data-flow framework 
of interest is a forward-flowing problem. Consider the entry point of a basic 
block B. The ideal solution begins by finding all the possible execution paths 
leading from the program entry to the beginning of B. A path is "possible" 
only if there is some computation of the program that follows exactly that path. 
The ideal solution would then compute the data-flow value at the end of each 
possible path and apply the meet operator to these values to find their greatest 
lower bound. Then no execution of the program can produce a smaller value 
for that program point. In addition, the bound is tight; there is no greater 
data-flow value that is a glb for the value computed along every possible path 
to B in the flow graph. 
We now try to define the ideal solution more formally. For each block B in 
a flow graph, let fB be the transfer function for B. Consider any path 
p 
= 
ENTRY + 
B1 + 
Bz + 
... 
--+ Bk-1 + 
B
k
 
from the initial node ENTRY to some block Bk. The program path may have 
cycles, so one basic block may appear several times on the path P. Define the 
9.3. FOUNDATIONS OF DATA-FLOW ANALYSIS 
629 
transfer function for P, fp, to be the composition of fB, , 
fB2 
. . 
. , 
fBk-, . Note 
that fBk 
is not part of the composition, reflecting the fact that this path is 
taken to reach the beginning of block Bk, not its end. The data-flow value 
created by executing this path is thus fp(uENT,), where uENT, is the result of 
the constant transfer function representing the initial node ENTRY. The ideal 
result for block B is thus 
IDEAL[B] 
= 
A 
f~ 
  ENT TRY). 
P, a possible path from ENTRY to B 
We claim that, in terms of the lattice-theoretic partial order 5 for the framework 
in question, 
Any answer that is greater than IDEAL is incorrect. 
Any value smaller than or equal to the ideal is conservative, i.e., safe. 
Intuitively, the closer the value to the ideal the more precise it 
To see 
why solutions must be 5 the ideal solution, note that any solution greater than 
IDEAL for any block could be obtained by ignoring some execution path that 
the program could take, and we cannot be sure that there is not some effect 
along that path to invalidate any program improvement we might make based 
on the greater solution. Conversely, any solution less than IDEAL can be viewed 
as including certain paths that either do not exist in the flow graph, or that 
exist but that the program can never follow. This lesser solution will allow only 
transformations that are correct for all possible executions of the program, but 
may forbid some transformations that IDEAL would permit. 
The Meet-Over-Paths Solution 
However, as discussed in Section 9.1, finding all possible execution paths is 
undecidable. We must therefore approximate. In the dat 
a-flow abstraction, we 
assume that every path in the flow graph can be taken. Thus, we can define 
the meet-over-paths solution for B to be 
MOP[B] 
= 
A 
f~ 
(VENTRY) - 
P, a path from ENTRY to B 
Note that, as for IDEAL, the solution MOP[B] 
gives values for IN[B] 
in forward- 
flow frameworks. If we were to consider backward-flow frameworks, then we 
would think of MOP[B] 
as a value for OUT[B]. 
The paths considered in the MOP solution are a superset of all the paths 
that are possibly executed. Thus, the MOP solution meets together not only the 
data-flow values of all the executable paths, but also additional values associated 
-- 
' ~ o t e  
that in forward problems, the value IDEAL[B] 
is what we would like IN[B] 
to be. In 
backward problems, which we do not discuss here, we would define IDEAL[B] 
to be the ideal 
value of OUT[B]. 
630 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
with the paths that cannot possibly be executed. Taking the meet of the ideal 
solution plus additional terms cannot create a solution larger than the ideal. 
Thus, for all B we have MOP[B] 5 IDEAL[B], 
and we will simply say that 
MOP 5 IDEAL. 
The Maximum Fixedpoint Versus the MOP Solution 
Notice that in the MOP solution, the number of paths considered is still un- 
bounded if the flow graph contains cycles. Thus, the MOP definition does not 
lend itself to a direct algorithm. The iterative algorithm certainly does not first 
find all the paths leading to a basic block before applying the meet operator. 
Rather, 
1. The iterative algorithm visits basic blocks, not necessarily in the order of 
execution. 
2. At each confluence point, the algorithm applies the meet operator to 
the data-flow values obtained so far. Some of these values used were 
introduced artificially in the initialization process, not representing the 
result of any execution from the beginning of the program. 
So what is the relationship between the MOP solution and the solution MFP 
produced by Algorithm 9.25? 
We first discuss the order in which the nodes are visited. In an iteration, we 
may visit a basic block before having visited its predecessors. If the predecessor 
is the ENTRY node, OUT[ENTRY] would have already been initialized with the 
proper, constant value. Otherwise, it has been initialized to T, a value no 
smaller than the final answer. By monotonicity, the result obtained by using T 
as input is no smaller than the desired solution. In a sense, we can think of T 
as representing no information. 
Figure 9.24: Flow graph illustrating the effect of early meet over paths 
What is the effect of applying the meet operator early? Consider the simple 
example of Fig. 9.24, and suppose we are interested in the value of IN[B*]. 
By 
9.3. FOUNDATIONS OF DATA-FLO 
W ANALYSIS 
the definition of MOP, 
In the iterative algorithm, if we visit the nodes in the order B1, B2, 
BS, 
B4, 
then 
While the meet operator is applied at the end in the definition of MOP, the 
iterative algorithm applies it early. The answer is the same only if the data- 
flow framework is distributive. If the data-flow framework is monotone but 
not distributive, we still have IN[B~] 
5 MOP[B~]. 
Recall that in general a 
solution IN[B] 
is safe (conservative) if IN[B] 5 IDEAL[B] 
for all blocks B. Surely, 
MOP[B] 
5 IDEAL[B]. 
We now provide a quick sketch of why in general the MFP solution provided 
by the iterative algorithm is always safe. An easy induction on i shows that 
the values obtained after i iterations are smaller than or equal to the meet over 
all paths of length i or less. But the iterative algorithm terminates only if it 
arrives at the same answer as would be obtained by iterating an unbounded 
number of times. Thus, the result is no greater than the MOP solution. Since 
MOP < IDEAL and MFP 5 MOP, we know that MFP 5 IDEAL, and therefore the 
solution MFP provided by the iterative algorithm is safe. 
9.3.5 
Exercises for Section 9.3 
Exercise 9.3.1 
: 
Construct a lattice diagram for the product of three lattices, 
each based on a single definition di, 
for i = 
1,2,3. How is your lattice diagram 
related to that in Fig. 9.22? 
! Exercise 9.3.2 : In Section 9.3.3 we argued that if the framework has finite 
height, then the iterative algorithm converges. Here is an example where the 
framework does not have finite height, and the iterative algorithm does not 
converge. Let the set of values V be the nonnegative real numbers, and let the 
meet operator be the minimum. There are three transfer functions: 
i. The identity, f1(x) = 
X .  
. 
. 
22. "half," that is, the function fH(x) = 
212. 
. 
. 
. 
22%. "one." that is, the function fo(x) = 1. 
The set of transfer functions F is these three plus the functions formed by 
composing them in all possible ways. 
a) Describe the set F. 
b) What is the 5 relationship for this framework? 
632 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
c) Give an example of a flow graph with assigned transfer functions, such 
that Algorithm 9.25 does not converge. 
d) Is this framework monotone? Is it distributive? 
! 
Exercise 9.3.3 : 
We argued that Algorithm 9.25 converges if the framework is 
monotone and of finite height. Here is an example of a framework that shows 
monotonicity is essential; finite height is not enough. The domain V is {1,2}, 
the meet operator is min, and the set of functions F is only the identity (fI) 
and the "switch" function (fs(x) = 
3 - 
x) that swaps 1 
and 2. 
a) Show that this framework is of finite height but not monotone. 
b) Give an example of a flow graph and assignment of transfer functions so 
that Algorithm 9.25 does not converge. 
! 
Exercise 9.3.4 : 
Let MOP~[B] 
be the meet over all paths of length i or less from 
the entry to block B. Prove that after i iterations of Algorithm 9.25, IN[B] 
5 
MOPi[B]. Also, show that as a consequence, if Algorithm 9.25 converges, then 
it converges to something that is 5 the MOP solution. 
! 
Exercise 9.3.5: Suppose the set F of functions for a framework are all of 
gen-kill form. That is, the domain V is the power set of some set, and f (x) = 
G U (x - 
K) for some sets G and K. Prove that if the meet operator is either 
(a) union or (b) intersection, then the framework is distributive. 
9.4 Constant Propagation 
All the data-flow schemas discussed in Section 9.2 are actually simple examples 
of distributive frameworks with finite height. Thus, the iterative Algorithm 9.25 
applies to them in either its forward or backward version and produces the MOP 
solution in each case. In this section, we shall examine in detail a useful data- 
flow framework with more interesting properties. 
Recall that const 
ant propagation, or "const 
ant folding," replaces expressions 
that evaluate to the same constant every time they are executed, by that con- 
stant. The constant-propagation framework described below is different from 
all the data-flow problems discussed so far, in that 
a) it has an unbounded set of possible data-flow values, even for a fixed flow 
graph, and 
b) it is not distributive. 
Constant propagation is a forward data-flow problem. The semilattice rep- 
resenting the data-flow values and the family of transfer functions are presented 
next. 
9.4. CONSTANT PROPAGATION 
633 
9.4.1 Data-Flow Values for the Constant-Propagation 
Framework 
The set of data-flow values is a product lattice, with one component for each 
variable in a program. The lattice for a single variable consists of the following: 
1. All constants appropriate for the type of the variable. 
2. The value NAC, 
which stands for not-a-constant. A variable is mapped to 
this value if it is determined not to have a constant value. The variable 
may have been assigned an input value, or derived from a variable that is 
not a constant, or assigned different constants along different paths that 
lead to the same program point. 
3. The value UNDEF, which stands for undefined. A variable is assigned this 
value if nothing may yet be asserted; presumably, no definition of the 
variable has been discovered to reach the point in question. 
Note that NAC and UNDEF are not the same; they are essentially opposites. 
NAC says we have seen so many ways a variable could be defined that we know 
it is not constant; UNDEF says we have seen so little about the variable that we 
cannot say anything at all. 
The semilattice for a typical integer-valued variable is shown in Fig. 9.25. 
Here the top element is UNDEF, and the bottom element is NAC. 
That is, the 
greatest value in the partial order is UNDEF and the least is NAC. 
The constant 
values are unordered, but they are all less than UNDEF and greater than NAC. 
As discussed in Section 9.3.1, the meet of two values is their greatest lower 
bound. Thus, for all values u, 
UNDEF A u = 
u and NAC A u = NAC. 
For any constant c, 
and given two distinct constants cl and cz, 
C1 r\ C2 = NAC. 
A data-flow value for this framework is a map from each variable in the 
program to one of the values in the constant semilattice. The value of a variable 
u in a map m is denoted by m(u). 
9.4.2 The Meet for the Constant-Propagation Framework 
The semilattice of data-flow values is simply the product of the semilattices like 
Fig. 9.25, one for each variable. Thus, m < m' if and only if for all variables u 
we have m ( u )  5 ml(u). Put another way, m Am1 = 
m" if m"(u) = 
m ( v )  
Arn1(v) 
for all variables u. 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
UNDEF 
NAC 
Figure 9.25: Semilattice representing the possible "values" of a single integer 
variable 
9.4.3 Transfer Functions for the Constant-Propagation 
Framework 
We assume in the following that a basic block contains only one statement. 
Transfer functions for basic blocks containing several statements can be con- 
structed by camposing the functions corresponding to individual statements. 
The set F consists of certain transfer functions that accept a map of variables 
to values in the constant lattice and return another such map. 
F contains the identity function, which takes a map as input and returns 
the same map as output. F also contains the constant transfer function for 
the ENTRY node. This transfer function, given any input map, returns a map 
mo, where mo(u) = 
UNDEF, for all variables u. This boundary condition makes 
sense, because before executing any program statements there are no definitions 
for any variables. 
In general, let f, be the transfer function of statement s, and let m and m' 
represent data-flow values such that m' = 
f 
(m). We shall describe f, 
in terms 
of the relationship between m and m'. 
1. If s is not an assignment statement, 
then f, 
is simply the identity function. 
2. If s is an assignment to variable x, then m' (u) = m(u), for all variables 
u # x, provided one of the following conditions holds: 
(a) If the right-hand-side (RHS) of the statement s is a constant c, then 
ml(x) = 
c. 
(b) If the RHS is of the form y + 
r , 
theng 
m(y) + 
m(r) 
if m(y) and m(r) are constant values 
ml(x) = 
NAC 
if either m(y) or m(z) is NAC 
UNDEF 
otherwise 
(c) If the RHS is any other expression (e.g. a function call or assignment 
through a pointer), then ml(x) = NAC. 
9 ~ s  
usual, + represents a generic operator, not necessarily addition. 
9.4. CONSTANT PROPAGATION 
9.4.4 
Monotonicity of the Constant-Propagation 
Framework 
Let us show that the constant propagation framework is monotone. First, we 
can consider the effect of a function f, 
on a single variable. In all but case 2(b), 
f s  either does not change the value of m(x), or it changes the map to return a 
constant or NAC. 
In these cases, f, 
must surely be monotone. 
For case 2(b), the effect of f s  is tabulated in Fig 9.26. The first and second 
columns represent the possible input values of y and z; the last represents the 
output value of x. The values are ordered from the greatest to the smallest in 
each column or subcolumn. To show that the function is monotone, we check 
that for each possible input value of y, the value of x does not get bigger as the 
value of z gets smaller. For example, in the case where y has a constant value 
cl, as the value of z varies from UNDEF to c2 to NAC, the value of x varies from 
UNDEF, to cl + 
c2, and then to NAC, respectively. We can repeat this procedure 
for all the possible values of y. Because of symmetry, we do not even need to 
repeat the procedure for the second operand before we conclude that the output 
value cannot get larger as the input gets smaller. 
I UNDEF 11 UNDEF 
UNDEF 
UNDEF 
C l  
I UNDEF 11 
NAC 
NAC 
I 
C2 
1 1  
NAC 
Figure 9.26: The constant-propagation transfer function for x = y+z 
I 
NAC 
9.4.5 
Nondistributivity of the Constant-Propagation 
Framework 
NAC 
The constant-propagation framework as defined is monotone but not distribu- 
tive. That is, the iterative solution MFP is safe but may be smaller than the 
MOP solution. An example will prove that the framework is not distributive. 
Example 9.26 : 
In the program in Fig. 9.27, x and y are set to 2 and 3 in block 
B1, and to 3 and 2, respectively, in block B2. We know that regardless of which 
path is taken, the value of z at the end of block B3 is 5. The iterative algorithm 
does not discover this fact, however. Rather, it applies the meet operator at 
the entry of B3, getting NAC'S as the values of x and y. Since adding two NAC'S 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Figure 9.27: An example demonstrating that the const 
ant propagation frame- 
work is not distributive 
yields a NAC, the output produced by Algorithm 9.25 is that x = 
NAC at the exit 
of the program. This result is safe, but imprecise. Algorithm 9.25 is imprecise 
because it does not keep track of the correlation that whenever x is 2, y is 3, 
and vice versa. It is possible, but significantly more expensive, to use a more 
complex framework that tracks all the possible equalities that hold among pairs 
of expressions involving the variables in the program; this approach is discussed 
in Exercise 9.4.2. 
Theoretically, we can attribute this loss of precision to the nondistributivity 
of the constant propagation framework. Let f l ,  f 2 ,  and f3 be the transfer 
functions representing blocks B1, B2 
and B3, 
respectively. As shown in Fig 9.28, 
rendering the framework nondistributive. 
Figure 9.28: Example of nondistributive transfer functions 
m(z> 
UNDEF 
UNDEF 
UNDEF 
UNDEF 
NAC 
5 
5 
5 
m(y) 
UNDEF 
3 
2 
NAC 
NAC 
3 
2 
NAC 
m 
m
0
 
fl (ma) 
f2 b 0 )  
fl(m0) 
A f 2 ( m 0 )  
f3(fl 
(m0) 
A fi(m0)) 
f 3 ( f l  (m0>> 
f3 ( f 2  
(mo>> 
f 3 ( f l ( m o ) ) A f 3 ( f 2 ( m 0 ) )  
m(x> 
UNDEF 
2 
3 
NAC 
N 
AC 
2 
3 
NAC 
9.4. CONSTANT PROPAGATION 
9.4.6 Interpretation of the Results 
The value UNDEF is used in the iterative algorithm for two purposes: to initialize 
the ENTRY node and to initialize the interior points of the program before the 
iterations. The meaning is slightly different in the two cases. The first says that 
variables are undefined at the beginning of the program execution; the second 
says that for lack of information at the beginning of the iterative process, we 
approximate the solution with the top element UNDEF. At the end of the 
iterative process, the variables at the exit of the ENTRY node will still hold the 
UNDEF value, since OUT[ENTRY] never changes. 
It is possible that UNDEF'S may show up at some other program points. 
When they do, it means that no definitions have been observed for that variable 
along any of the paths leading up to that program point. Notice that with the 
way we define the meet operator, as long as there exists a path that defines a 
variable reaching a program point, the variable will not have an UNDEF value. 
If all the definitions reaching a program point have the same constant value, 
the variable is considered a constant even though it may not be defined along 
some program path. 
By assuming that the program is correct, the algorithm can find more con- 
stants than it otherwise would. That is, the algorithm conveniently chooses 
some values for those possibly undefined variables in order to make the pro- 
gram more efficient. This change is legal in most programming languages, since 
undefined variables are allowed to take on any value. If the language semantics 
requires that all undefined variables be given some specific value, then we must 
change our problem formulation accordingly. And if instead we are interested in 
finding possibly undefined variables in a program, we can formulate a different 
data-flow analysis to provide that result (see Exercise 9.4.1). 
Example 9.27 : 
In Fig. 9.29, the values of x are 10 and UNDEF at the exit of 
basic blocks B2 and BS, 
respectively. Since UNDEF A 10 = 10, the value of x is 
10 on entry to block B4. Thus, block B5, where x is used, can be optimized 
by replacing x by 10. Had the path executed been B1 -+ B3 --+ B4 -+ B5, the 
value of x reaching basic block B5 would have been undefined. So, it appears 
incorrect to replace the use of x by 10. 
However, if it is impossible for predicate Q to be false while Q' is true, 
then this execution path never occurs. While the programmer may be aware 
of that fact, it may well be beyond the capability of any data-flow analysis to 
determine. Thus, if we assume that the program is correct and that all the 
variables are defined before they are used, it is indeed correct that the value 
of x at the beginning of basic block B5 can only be 10. And if the program 
is incorrect to begin with, then choosing 10 as the value of x cannot be worse 
than allowing x to assume some random value. 
9.4.7 Exercises for Section 9.4 
! 
Exercise 9.4.1 
: 
Suppose we wish to detect all possibility of a variable being 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Figure 9.29: Meet of UNDEF and a constant 
uninitialized along any path to a point where it is used. How would you modify 
the framework of this section to detect such situations? 
! 
! 
Exercise 9.4.2 : 
An interesting and powerful data-flow-analysis framework is 
obtained by imagining the domain V to be all possible partitions of expressions, 
so that two expressions are in the same class if and only if they are certain to 
have the same value along any path to the point in question. To avoid having 
to list an infinity of expressions, we can represent V by listing only the minimal 
pairs of equivalent expressions. For example, if we execute the statements 
then the minimal set of equivalences is {a - 
b, c r 
a + 
d}. From these follow 
other equivalences, such as c E 
b + 
d and a + 
e = 
b + 
e, but there is no need to 
list these explicitly. 
a) What is the appropriate meet operator for this framework? 
b) Give a data structure to represent domain values and an algorithm to 
implement the meet operator. 
c) What are the appropriate functions to associate with statements? Explain 
the effect that a statement such as a = b+c should have on a partition of 
expressions (i.e., on a value in V) 
. 
d) Is this framework monotone? Distributive? 
9.5. PARTIAL-RED 
UNDANCY ELIMINATION 
9.5 Part 
ial-Redundancy Elimination 
In this section, we consider in detail how to minimize the number of expression 
evaluations. That is, we want to consider all possible execution sequences in 
a flow graph, and look at the nqmber of times an expression such as x + 
y is 
evaluated. By moving around the places where x + 
y is evaluated and keeping 
the result in a temporary variable when necessary, we often can reduce the 
number of evaluations of this expression along many of the execution paths, 
while not increasing that number along any path. Note that the number of 
different places in the flow graph where x + 
y is evaluated may increase, but 
that is relatively unimportant, as long as the number of evaluations of the 
expression x + 
y is reduced. 
Applying the code transformation developed here improves the performance 
of the resulting code, since, as we shall see, an operation is never applied unless 
it absolutely has to be. Every optimizing compiler implements something like 
the transformation described here, even if it uses a less "aggressive" algorithm 
than the one of this section. However, there is another motivation for discussing 
the problem. Finding the right place or places in the flow graph at which 
to evaluate each expression requires four different kinds of data-flow analyses. 
Thus, the study of "partial-redundancy elimination," as minimizing the number 
of expression evaluations is called, will enhance our understanding of the role 
data-flow analysis plays in a compiler. 
Redundancy in programs exists in several forms. As discussed in Section 
9.1.4, it may exist in the form of common subexpressions, where several evalua- 
tions of the expression produce the same value. It may also exist in the form of 
a loop-invariant expression that evaluates to the same value in every iteration 
of the loop. Redundancy may also be partial, if it is found along some of the 
paths, but not necessarily along all paths. Common subexpressions and loop- 
invariant expressions can be viewed as special cases of partial redundancy; thus 
a single partial-redundancy-elimination algorithm can be devised to eliminate 
all the various forms of redundancy. 
In the following, we first discuss the different forms of redundancy, in order 
to build up our intuition about the problem. We then describe the generalized 
redundancy-elimination problem, and finally we present the algorithm. This 
algorithm is particularly interesting, because it involves solving multiple data- 
flow problems, in both the forward and backward directions. 
9.5.1 
The Sources of Redundancy 
Figure 9.30 illustrates the three forms of redundancy: common subexpressions, 
loop-invariant expressions, and partially redundant expressions. The figure 
shows the code both before and after each optimization. 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Figure 9.30: Examples of (a) global common subexpression, (b) loop-invariant 
code motion, (c) partial-redundancy elimination. 
Global Common Subexpressions 
In Fig. 9.30(a), the expression b + 
c computed in block B4 is redundant; it has 
already been evaluated by the time the flow of control reaches B4 regardless of 
the path taken to get there. As we observe in this example, the value of the 
expression may be different on different paths. We can optimize the code by 
storing the result of the computations of b + 
c in blocks B2 and B3 in the same 
temporary variable, say t, and then assigning the value of t to the variable e in 
block B4, 
instead of reevaluating the expression. Had there been an assignment 
to either b or c after the last computation of b + 
c but before block B4, the 
expression in block B4 would not be redundant. 
Formally, we say that an expression b + 
c is (fully) redundant at point p, if 
it is an available expression, in the sense of Section 9.2.6, at that point. That 
is, the expression b + 
c has been computed along all paths reaching p, and the 
variables b and c were not redefined after the last expression was evaluated. 
The latter condition is necessary, because even though the expression b + 
c is 
textually executed before reaching the point p, the value of b + 
c computed at 
9.5. PARTIAL-RED UNDANCY ELIMINATION 
641 
Finding "Deep" Common Subexpressions 
Using available-expressions analysis to identify redundant expressions only 
works for expressions that are textually identical. For example, an appli- 
cation of common-subexpression elimination will recognize that t 
I in the 
code fragment 
t l = b + c ;  
a = t l + d ;  
has the same value as does t 2  
in 
t 2 = b + c ;  
e = t 2 + d ;  
as long as the variables b and c have not been redefined in between. It 
does not, however, recognize that a and e are also the same. It is possi- 
ble to find such "deep" common subexpressions by re-applying common 
subexpression elimination until no new common subexpressions are found 
on one round. It is also possible to use the framework of Exercise 9.4.2 to 
catch deep common subexpressions. 
point p would have been different, because the operands might have changed. 
Loop-Invariant Expressions 
Fig. 9.30(b) shows an example of a loop-invariant expression. The expression 
b + 
c is loap invariant assuming neither the variable b nor c is redefined within 
the loop. We can optimize the program by replacing all the re-executions in 
a loop by a single calculation outside the loop. We assign the computation to 
a temporary variable, say t, and then replace the expression in the loop by t. 
There is one more point we need to consider when performing "code motion" 
optimizations such as this. We should not execute any instruction that would 
not have executed without the optimization. For example, if it is possible to 
exit the loop without executing the loop-invariant instruction at all, then we 
should not move the instruction out of the loop. There are two reasons. 
1. If the instruction raises an exception, then executing it may throw an 
exception that would not have happened in the original program. 
2. When the loop exits early, the "optimized" program takes more time than 
the original program. 
To ensure that loop-invariant expressions in while-loops can be optimized, 
compilers typically represent the statement 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
while c ( 
s; 
3 
in the same way as the statement 
if c ( 
repeat 
s; 
until not c; 
3 
In this way, loop-invariant expressions can be placed just prior to the repeat- 
until construct. 
Unlike common-subexpression elimination, where a redundant expression 
computation is simply dropped, loop-invariant-expression elimination requires 
an expression from inside the loop to move outside the loop. Thus, this opti- 
mization is generally known as "loop-invariant code motion." Loop-invariant 
code motion may need to be repeated, because once a variable is determined to 
to have a loop-invariant value, expressions using that variable may also become 
loop-invariant . 
Partially Redundant Expressions 
An example of a partially redundant expression is shown in Fig. 9.30(c). The 
expression b + 
c in block B4 is redundant on the path B1 -+ 
B2 + 
B4, but not 
on the path B1 -+ 
B3 -+ 
B4. We can eliminate the redundancy on the former 
path by placing a computation of b + 
c in block B3. All the results of b + 
c are 
written into a temporary variable t, 
and the calculation in block B4 is replaced 
with t. Thus, like loop-invariant code motion, partial-redundancy elimination 
requires the placement of new expression computations. 
9.5.2 
Can All Redundancy Be Eliminated? 
Is it possible to eliminate all redundant computations along every path? The 
answer is "no," unless we are allowed to change the flow graph by creating new 
blocks. 
Example 9.28 : 
In the example shown in Fig. 9.31 
(a), the expression of b + 
c 
is computed redundantly in block B4 if the program follows the execution path 
B1 -+ B2 -+ B4. However, we cannot simply move the computation of b + 
c to 
block B3, because doing so would create an extra computation of b + 
c when 
the path B1 -+ 
B3 -+ 
B5 is taken. 
What we would like to do is to insert the computation of b + 
c only along 
the edge from block B3 to block Bq. 
We can do so by placing the instruction 
in a new black, say, B6, 
and making the flow of control from B3 go through B6 
before it reaches B4. The transformation is shown in Fig. 9.31(b). 
9.5. PARTIAL-RED 
UNDANCY ELIMINATION 
Figure 9.31: B3 + 
B4 is a critical edge 
We define a critical edge of a flow graph to be any edge leading from a 
node with more than one successor to a node with more than one predecessor. 
By introducing new blocks along critical edges, we can always find a block to 
accommodate the desired expression placement. For instance, the edge from 
B3 to B4 in Fig. 9.31(a) is critical, because B3 has two successors, and B4 has 
two predecessors. 
Adding blocks may not be sufficient to allow the elimination of all redundant 
computations. As shown in Example 9.29, we may need to duplicate code so 
as to isolate the path where redundancy is found. 
Example 9.29 : 
In the example shown in Figure 9.32(a), the expression of b + 
c 
is computed redundantly along the path B1 + 
B2 + 
B4 += B6. We would like 
to remove the redundant computation of b + 
c from block B6 in this path and 
compute the expression only along the path B1 + 
B3 + 
B4 += B6. However, 
there is no single program point or edge in the source program that corresponds 
uniquely to the latter path. To create such a program point, we can duplicate 
the pair of blocks B4 and B6, 
with one pair reached through B2 and the other 
reached through BS, 
as shown in Figure 9.32(b). The result of b + 
c is saved in 
variable t in block B2, 
and moved to variable d in Bi, the copy of B6 reached 
from B2. 
Since the number of paths is exponential in the number of conditional 
branches in the program, eliminating all redundant expressions can greatly 
increase the size of the optimized code. We therefore restrict our discussion 
of redundancy-elimination techniques to those that may introduce additional 
blocks but that do not duplicate portions of the control flow graph. 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Figure 9.32: Code duplication to eliminate redundancies 
9.5.3 
The Lazy-Code-Motion Problem 
It is desirable for programs optimized with a partial-redundancy-elimination 
algorithm to have the following properties: 
1. All redundant computations of expressions that can be eliminated without 
code duplication are eliminated. 
2. The optimized program does not perform any computation that is not in 
the original program execution. 
3. Expressions are computed at the latest possible time. 
The last property is important because the values of expressions found to be 
redundant are usually held in registers until they are used. Computing a value 
as late as possible minimizes its lifetime - 
the duration between the time the 
value is defined and the time it is last used, which in turn minimizes its usage of 
a register. We refer to the optimization of eliminating partial redundancy with 
the goal of delaying the computations as much as possible as lazy code motion. 
To build up our intuition of the problem, we first discuss how to reason about 
partial redundancy of a single expression along a single path. For convenience, 
we assume for the rest of the discussion that every statement is a basic block 
of its own. 
9.5. PARTIAL-RED 
UNDANCY ELIMINATION 
Full Redundancy 
An expression e in block B is redundant if along all paths reaching B, e has 
been evaluated and the operands of e have not been redefined subsequently. 
Let S be the set of blocks, each containing expression e, that renders e in B 
redundant. The set of edges leaving the blocks in S must necessarily form a 
cutset, which if removed, disconnects block B from the entry of the program. 
Moreover, no operands of e are redefined along the paths that lead from the 
blocks in S to B. 
Partial Redundancy 
If an expression e in block B is only partially redundant, the lazy-code-motion 
algorithm attempts to render e fully redundant in B by placing additional copies 
of the expressions in the flow graph. If the attempt is successful, the optimized 
flow graph will also have a set of basic blocks S, 
each containing expression e, 
and whose outgoing edges are a cutset between the entry and B. Like the fully 
redundant case, no operands of e are redefined along the paths that lead from 
the blocks in S to B. 
9.5.4 
Anticipation of Expressions 
There is an additional constraint imposed on inserted expressions to ensure 
that no extra operations are executed. Copies of an expression must be placed 
only at program points where the expression is anticipated. We say that an 
expression b + 
c is anticipated at point p if all paths leading from the point p 
eventually compute the value of the expression b + 
c from the values of b and c 
that are available at that point. 
Let us now examine what it takes to eliminate partial redundancy along an 
acyclic path B1 -+ B2 -+ . . 
. -+ 
B,. Suppose expression e is evaluated only in 
blocks B1 and B,, and that the operands of e are not redefined in blocks along 
the path. There are incoming edges that join the path and there are outgoing 
edges that exit the path. We see that e is not anticipated at the entry of block 
Bi if and only if there exists an outgoing edge leaving block Bj 
, 
i 5 j < n, 
that 
leads to an execution path that does not use the value of e. Thus, anticipation 
limits how early an expression can be inserted. 
We can create a cutset that includes the edge BiWl 
-+ 
Bi and that renders 
e redundant in B, if e is either available or anticipated at the entry of Bi. If e 
is anticipated but not available at the entry of Bi, we must place a copy of the 
expression e along the incoming edge. 
We have a choice of where to place the copies of the expression, since there 
are usually several cutsets in the flow graph that satisfy all the requirements. 
In the above, computation is introduced along the incoming edges to the path 
of interest and so the expression is computed as close to the use as possible, 
without introducing redundancy. Note that these introduced operations may 
themselves be partially redundant with other instances of the same expression 
646 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
in the program. Such partial redundancy may be eliminated by moving these 
computations further up. 
In summary, anticipation of expressions limits how early an expression can 
be placed; you cannot place an expression so early that it is not anticipated 
where you place it. The earlier an expression is placed, the more redundancy 
can be removed, and among all solutions that eliminate the same redundancies, 
the one that computes the expressions the latest minimizes the lifetimes of the 
registers holding the values of the expressions involved. 
9.5.5 The Lazy-Code-Motion Algorithm 
This discussion thus motivates a four-step algorithm. The first step uses an- 
ticipation to determine where expressions can be placed; the second step finds 
the earliest cutset, among those that eliminate as many redundant operations 
as possible without duplicating code and without introducing any unwanted 
computations. This step places the computations at program points where the 
values of their results are first anticipated. The third step then pushes the 
cutset down to the point where any further delay would alter the semantics of 
the program or introduce redundancy. The fourth and final step is a simple 
pass to clean up the code by removing assignments to temporary variables that 
are used only once. Each step is accomplished with a data-flow pass: the first 
and fourth are backward-flow problems, the second and third are forward-flow 
problems. 
Algorithm Overview 
1. Find all the expressions anticipated at each program point using a back- 
ward data-flow pass. 
The second step places the computation where the values of the expres- 
sions are first anticipated along some path. After we have placed copies 
of an expression where the expression is first anticipated, the expression 
would be available at program point p if it has been anticipated along all 
paths reaching p. Availability can be solved using a forward data-flow 
pass. If we wish to place the expressions at the earliest possible posi- 
tions, we can simply find those program points where the expressions are 
anticipated but are not available. 
3. Executing an expression as soon as it is anticipated may produce a value 
long before it is used. An expression is postponable at a program point if 
the expression has been anticipated and has yet to be used along any path 
reaching the program point. Postponable expressions are found using a 
forward data-flow pass. We place expressions at those program points 
where they can no longer be postponed. 
4. A simple, final backward data-flow pass is used to eliminate assignments 
to temporary variables that are used only once in the program. 
9.5. PARTIA 
L-RED 
UNDANCY ELIMINATION 
Preprocessing Steps 
We now present the full lazy-code-motion algorithm. To keep the algorithm 
simple, we assume that initially every statement is in a basic block of its own, 
and we only introduce new computations of expressions at the beginnings of 
blocks. To ensure that this simplification does not reduce the effectiveness of 
the technique, we insert a new block between the source and the destination of 
an edge if the destination has more than one predecessor. Doing so obviously 
also takes care of all critical edges in the program. 
We abstract the semantics of each block B with two sets: e- use^ is the set 
of expressions computed in B and e-killB is the set of expressions killed, that 
is, the set of expressions any of whose operands are defined in B. Example 9.30 
will be used throughout the discussion of the four data-flow analyses whose 
definitions are summarized in Fig. 9.34. 
Example 9.30 : 
In the flow graph in Fig. 9.33(a), the expression b + 
c appears 
three times. Because the block Bg is part of a loop, the expression may be 
computed many times. The computation in block Bg is not only loop invariafit; 
it is also a redundant expression, since its value already has been used in block 
B7. For this example, we need to compute b+c only twice, once in block B5 and 
once along the path after B2 and before B7. The lazy code motion algorithm 
will place the expression computations at the beginning of blocks B4 and B5. 
Anticipated Expressions 
Recall that an expression b + 
c is anticipated at a program point p if all paths 
leading from point p eventually compute the value of the expression b + 
c from 
the values of b and c that are available at that point. 
In Fig. 9.33(a), all the blocks anticipating b + 
c on entry are shown as lightly 
shaded boxes. The expression b + 
c is anticipated in blocks BQ, 
B4, 
B5, 
B6, 
B7, 
and Bg. It is not anticipated on entry to block B2, because the value of c is 
recomputed within the block, and therefore the value of b + 
c that would be 
computed at the beginning of B2 is not used along any path. The expression 
b+c is not anticipated on entry to B1, because it is unnecessary along the branch 
from B1 to Bz (although it would be used along the path B1 -+ B5 -+ B6). 
Similarly, the expression is not anticipated at the beginning of B8, because of 
the branch from B8 to Bll. The anticipation of an expression may oscillate 
along a path, as illustrated by B7 -+ B8 -+ B9. 
The data-flow equations for the anticipated-expressions problem are shown 
in Fig 9.34(a). The analysis is a backward pass. An anticipated expression at 
the exit of a block B is an anticipated expression on entry only if it is not in the 
e- kill^ set. Also a block B generates as new uses the set of e-uses expressions. 
At the exit of the program, none of the expressions are anticipated. Since we 
are interested in finding expressions that are anticipated along every subsequent 
648 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
4 
e = t 
I \ postponable 
Figure 9.33: Flow graph of Example 9.30 
path, the meet operator is set intersection. Consequently, the interior points 
must be initialized to the universal set U ,  
as was discussed for the available- 
expressions problem in Section 9.2.6. 
Available Expressions 
At the end of this second step, copies of an expression will be placed at program 
points where the expression is first anticipated. If that is the case, an expression 
will be avazlable at program point p if it is anticipated along all paths reaching 
p. This problem is similar to available-expressions described in Section 9.2.6. 
The transfer function used here is slightly different though. An expression is 
available on exit from a block if it is 
9.5. PARTIA 
L-RED 
UNDANCY ELIMINATION 
Direction 
I Backwards 
( Forwards 
(b) Available Expressions 
Sets of expressions 
Domain 
(a) Anticipated Expressions 
Sets of expressions 
Transfer 
function 
Boundary 
Meet (A) 
Equations 
f~ (4 
= 
e-useg u ( x  
- 
e - k d B )  
Initialization 
f~ (4 
= 
(anticzpated[B].in 
U x) 
- 
e-kdlB 
I N [ E X I T ]  = 0 
n 
IN[B] 
= 
f B  (ouTIB]) 
IN[B] 
= 
U 
I OUT[B] 
= 
U 
Domain 
Direction 
Transfer 
I 
Meet (A) 
I n 
I u 
OUT[ENTRY] 
= 0 
n 
o u ~ [ B ]  
= 
f B (IN [B]) 
function 
Boundary 
(c) Postponable Expressions 
Sets of expressions 
Forwards 
f~ (2) 
= 
earliest[B] = anticipated[B] 
.in - 
available[B] 
.in 
latest[B] = (earliest[B] 
U postponable[B] 
.in) n 
(e-useg 
( ~ S , ~ U ~ ~ [ B ]  
(earliest 
[S] 
U 
postponable[~] 
.in))) 
(d) Used Expressions 
Sets of expressions 
Backwards 
~ B ( x >  
= 
(earliest[B] 
U x )  - 
e- use^ 
OUT[ENTRY] 
= 
0 
Equations 
Initialization 
Figure 9.34: Four data-flow passes in partial-redundancy elimination 
(e-useB U x )  - 
latest[B]) 
I N [ E X I T ]  = 0 
OUT[B] 
= fi3 
( I N  [B]) 
INPI 
= 
Ap,pred(B) 
OUT[PI 
OUT[B] 
= U 
IN[B] 
= 
~ B ( o u T [ B ] )  
OUTPI 
= /\S,succ(B) I N S ]  
IN[B] 
= 0 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Completing the Square 
Anticipated expressions (also called "very busy expressions" elsewhere) is a 
type of data-flow analysis we have not seen previously. While we have seen 
backwards-flowing frameworks such as live-variable analysis (Sect. 9.2.5), 
and we have seen frameworks where the meet is intersection such as avail- 
able expressions (Sect. 9.2.6), this is the first example of a useful analysis 
that has both properties. Almost all analyses we use can be placed in one 
of four groups, depending on whether they flow forwards or backwards, 
and depending on whether they use union or intersection for the meet. 
Notice also that the union analyses always involve asking about whether 
there exists a path along which something is true, while the intersection 
analyses ask whether something is true along all paths. 
1. Either 
(a) Available, or 
(b) In the set of anticipated expressions upon entry (i.e., it could be 
made available if we chose to compute it here), 
and 
2. Not killed in the block. 
The data-flow equations for available expressions are shown in Fig 9.34(b). 
To avoid confusing the meaning of IN, we refer to the result of an earlier analysis 
by appending "[B] 
.in" to the name of the earlier analysis. 
With the earliest placement strategy, the set of expressions placed at block 
B, i.e., earliest[B], is defined as the set of anticipated expressions that are not 
yet available. That is, 
Example 9.31 : 
The expression b + 
c in the flow graph in Figure 9.35 is not 
anticipated at the entry of block B3 but is anticipated at the entry of block 
B4. It is, however, not necessary to compute the expression b + 
c in block B4, 
because the expression is already available due to block B2. 
Example 9.32 : Shown with dark shadows in Fig. 9.33(a) are the blocks for 
which expression b + c is not available; they are B1, B2, 
B3, and B5. The 
early-placement positions are represented by the lightly shaded boxes with dark 
shadows, and are thus blocks B3 and B5. Note, for instance, that b + c is 
considered available on entry to B4, 
because there is a path B1 + 
Bz -+ B3 --+ 
B4 along which b + 
c is anticipated at least once - 
at B3 in this case - 
and 
since the beginning of B3, neither b nor c was recomputed. 
9.5. PARTIAL-RED UNDANCY ELIMINATION 
Figure 9.35: Flow graph for Example 9.31 illustrating the use of availability 
Postponable Expressions 
The third step postpones the computation of expressions as much as possible 
while preserving the original program semantics and minimizing redundancy. 
Example 9.33 illustrates the importance of this step. 
Example 9.33 : 
In the flow graph shown in Figure 9.36, the expression b + 
c 
is computed twice along the path B1 -+ B5 -+ B6 -+ B7. The expression b + 
c 
is anticipated even at the beginning of block B1. If we compute the expression 
as soon as it is anticipated, we would have computed the expression b + 
c in B1. 
The result would have to be saved from the beginning, through the execution 
of the loop comprising blocks Bq and B3, until it is used in block B7. Instead 
we can delay the computation of expression b + 
c until the beginning of B5 and 
until the flow of control is about to transition from B4 to B7. 
Formally, an expression x + 
y is postponable to a program point p if an early 
placement of x + 
y is encountered along every path from the entry node to p, 
and there is no subsequent use of x i
-
 
y after the last such placement. 
Example 9.34: Let us again consider expression b + 
c in Fig. 9.33. The two 
earliest points for b + 
c are B3 and B5; 
note that these are the two blocks that 
are both lightly and darkly shaded in Fig. 9.33(a), indicating that b + 
c is both 
anticipated and not available for these blocks, and only these blocks. We cannot 
postpone b + 
c from B5 to B6, 
because b + 
c is used in B5. We can postpone it 
from B3 to B4, however. 
But we cannot postpone b + 
c from B4 to B7. The reason is that, although 
b + 
c is not used in B4, 
placing its computation at B7 instead would lead to a 
CHAPTER 9. MACHINEINDEPENDENT OPTIMIZATIONS 
Figure 9.36: Flow graph for Example 9.33 to illustrate the need for postponing 
an expression 
redundant computation of b + 
c along the path B5 -+ B6 -+ B7. AS we shall 
see, B4 is one of the latest places we can compute b + 
c. 
The data-flow equations for the postponable-expressions problem are shown 
in Fig 9.34(c). The analysis is a forward pass. We cannot "postpone" an 
expression to the entry of the program, so OUT[ENTRY] = 0. An expression 
is postponable to the exit of block B if it is not used in the block, and either 
it is postponable to the entry of B or it is in earliest[B]. An expression is 
not postponable to the entry of a block unless all its predecessors include the 
expression in their postponable sets at their exits. Thus, the meet operator is 
set intersection, and the interior points must be initialized to the top element 
of the semilattice - 
the universal set. 
Roughly speaking, an expression is placed at the frontier where an expression 
transitions from being postponable to not being postponable. More specifically, 
an expression e may be placed at the beginning of a block B only if the expres- 
sion is in B's earliest or postponable set upon entry. In addition, B is in the 
postponement frontier of e if one of the following holds: 
1. e is not in postponable[B].out. In other words, e is in e- use^. 
2. e cannot be postponed to one of its successors. In other words, there 
exists a successor of B such that e is not in the earliest or postponable set 
upon entry to that successor. 
Expression e can be placed at the front of block B in either of the above 
scenarios because of the new blocks introduced by the preprocessing step in the 
algorithm. 
9.5. PARTIA 
L-RED 
UNDANCY ELIMINATION 
653 
Example 9.35 : 
Fig. 9.33(b) shows the result of the analysis. The light-shaded 
boxes represent the blocks whose earliest set includes b + 
c. The dark shadows 
indicate those that include b + 
c in their postponable set. The latest placements 
of the expressions are thus the entries of blocks B4 and B5, since 
1. b + 
c is in the postponable set of B4 but not B7, 
and 
2. B5's earliest set includes b + 
c and it uses b + 
c. 
The expression is stored into the temporary variable t in blocks B4 and B5, and 
t is used in place of b + 
c everywhere else, as shown in the figure. 
Used Expressions 
Finally, a backward pass is used to determine if the temporary variables in- 
troduced are used beyond the block they are in. We say that an expression is 
used at point p if there exists a path leading from p that uses the expression 
before the value is reevaluated. This analysis is essentially liveness analysis (for 
expressions, rather than for variables). 
The data-flow equations for the used expressions problem are shown in 
Fig 9.34(d). The analysis is a backward pass. A used expression at the exit 
of a block B is a used expression on entry only if it is not in the latest set. 
A block generates, as new uses, the set of expressions in e- use^. At the exit 
of the program, none of the expressions are used. Since we are interested in 
finding expressions that are used by any subsequent path, the meet operator is 
set union. Thus, the interior points must be initialized with the top element of 
the semilattice - 
the empty set. 
Putting it All Together 
All the steps of the algorithm are summarized in Algorithm 9.36. 
Algorithm 9.36 : 
Lazy code motion. 
INPUT: A flow graph for which e- use^ and e-killB have been computed for 
each block B. 
OUTPUT: A modified flow graph satisfying the four lazy code motion conditions 
in Section 9.5.3. 
METHOD: 
1. Insert an empty block along all edges entering a block with more than 
one predecessor. 
2. Find anticipated[B] 
.in for all blocks B ,  
as defined in Fig. 9.34(a). 
3. Find available[B] 
.in for all blocks B as defined in Fig. 9.34(b). 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
4. Compute the earliest placements for all blocks B: 
earliest[B] = 
anticipated[B].in 
- 
available[B].in 
5. Find postponable[B] 
.in for all blocks B as defined in Fig. 9.34(c). 
6. Compute the latest placements for all blocks B: 
Note that 1 
denotes complementation with respect to the set of all ex- 
pressions computed by the program. 
7. Find used[B].out for all blocks B, as defined in Fig. 9.34(d). 
8. For each expression, say x+ y, computed by the program, do the following: 
(a) Create a new temporary, say t, for x + 
y. 
(b) For all blocks B such that x + 
y is in latest 
[B] 
n used[B] 
.out, add 
t = x+y at the beginning of B. 
(c) For all blocks B such that x + 
y is in 
replace every original x + 
y by t. 
Summary 
Partial-redundancy elimination finds many different forms of redundant opera- 
tions in one unified algorithm. This algorithm illustrates how multiple data-flow 
problems can be used to find optimal expression placement. 
1. The placement constraints are provided by the anticipated-expressions 
analysis, which is a backwards data-flow analysis with a set-intersection 
meet operator, as it determines if expressions are used subsequent 
to each 
program point on all paths. 
2. The earliest placement of an expression is given by program points where 
the expression is anticipated but is not available. Available expressions 
are found with a forwards data-flow analysis with a set-intersection meet 
operator that computes if an expression has been anticipated before each 
program point along all paths. 
9.6. LOOPSINFLOW GRAPHS 
655 
3. The latest placement of an expression is given by program points where 
an expression can no longer be postponed. Expressions are postponable 
at a program point if for all paths reaching the program point, no use of 
the expression has been encountered. Postponable expressions are found 
with a forwards data-flow analysis with a set-intersection meet operator. 
4. Temporary assignments are eliminated unless they are used by some path 
subsequently. We find used expressions with a backwards data-flow anal- 
ysis, this time with a set-union meet operator. 
9.5.6 Exercises for Section 9.5 
Exercise 9.5.1 : 
For the flow graph in Fig. 9.37: 
a) Compute anticipated for the beginning and end of each block. 
b) Compute available for the beginning and end of each block. 
c) Compute earliest for each block. 
d) Compute postponable for the beginning and end of each block. 
e) Compute used for the beginning and end of each block. 
f) Compute latest for each block. 
g) Introduce temporary variable t; 
show where it is computed and where it 
is used. 
Exercise 9.5.2 : 
Repeat Exercise 9.5.1 for the flow graph of Fig. 9.10 (see the 
exercises to Section 9.1). You may limit your analysis to the expressions a + 
b, 
c - 
a, and b * d. 
!! Exercise 9.5.3 
: 
The concepts discussed in this section can also be applied to 
eliminate partially dead code. A definition of a variable is partially dead if the 
variable is live on some paths and not others. We can optimize the program 
execution by only performing the definition along paths where the variable 
is live. Unlike partial-redundancy elimination, where expressions are moved 
before the original, the new definitions are placed after the original. Develop 
an algorithm to move partially dead code, so expressions are evaluated only 
where they will eventually be used. 
9.6 Loops in Flow Graphs 
In our discussion so far, loops have not been handled differently; they have been 
treated just like any other kind of control flow. However, loops are important 
because programs spend most of their time executing them, and optimizations 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
ENTRY 
Y 
+ 
EXIT 
Figure 9.37: Flow graph for Exercise 9.5.1 
that improve the performance of loops can have a significant impact. Thus, it 
is essential that we identify loops and treat them specially. 
Loops also affect the running time of program analyses. If a program does 
not contain any loops, we can obtain the answers to data-flow problems by 
making just one pass through the program. For example, a forward data-flow 
problem can be solved by visiting all the nodes once, in topological order. 
In this section, we introduce the following concepts: dominators, dept 
h-first 
ordering, back edges, graph depth, and reducibility. Each of these is needed 
for our subsequent discussions on finding loops and the speed of convergence of 
iterative data-flow analysis. 
9.6.1 Dominators 
We say node d of a flow graph dominates node n, written d dona n, if every path 
from the entry node of the flow graph to n goes through d. Note that under 
this definition, every node dominates itself. 
Example 9.37 : 
Consider the flow graph of Fig. 9.38, with entry node 1. The 
entry node dominates every node (this statement is true for every flow graph). 
Node 2 dominates only itself, since control can reach any other node along a path 
that begins with 1 
-+ 3. Node 3 dominates all but 1 
and 2. Node 4 dominates 
9.6. LOOPS IN FLOW GRAPHS 
657 
all but 1, 2 and 3, since all paths from 1 
must begin with 1 
--+ 2 -+ 3 --+ 4 or 
1 
--+ 3 --+ 4. Nodes 5 and 6 dominate only themselves, since flow of control can 
skip around either by going through the other. Finally, 7 dominates 7, 8, 9, 
and 10; 8 dominates 8, 9, and 10; 9 and 10 dominate only themselves. 
Figure 9.38: A flow graph 
A useful way of presenting dominator information is in a tree, called the 
dominator tree, in which the entry node is the root, and each node d dominates 
only its descendants in the tree. For example, Fig. 9.39 shows the dominator 
tree for the flow graph of Fig. 9.38. 
Figure 9.39: Dominator tree for flow graph of Fig. 9.38 
The existence of dominator trees follows from a property of dominators: 
each node n has a unique immediate dominator m that is the last dominator 
of n on any path from the entry node to n. In terms of the dom relation, the 
658 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
immediate dominator m has that property that if d # n and d dom n, then 
d dom m. 
We shall give a simple algorithm for computing the dominators of every 
node n in a flow graph, based on the principle that if pl , 
pz, . . 
. , 
pk are all the 
predecessors of n, and d f 
n, then d dom n if and only if d dom pi for each i. 
This problem can be formulated as a forward data-flow analysis. The data-flow 
values are sets of basic blocks. A node's set of dominators, other than itself, is 
the intersection of the dominators of all its predecessors; thus the meet operator 
is set intersection. The transfer function for block B simply adds B itself to the 
set of input nodes. The boundary condition is that the ENTRY node dominates 
itself. Finally, the initialization of the interior nodes is the universal set, that 
is, the set of all nodes. 
Algorit 
hm 9.38 : 
Finding dominators. 
INPUT: A flow graph G with set of nodes N ,  set of edges E and entry node 
ENTRY. 
OUTPUT: D(n), the set of nodes that dominate node n, for all nodes n in N .  
METHOD: Find the solution to the data-flow problem whose parameters are 
shown in Fig. 9.40. The basic blocks are the nodes. D(n) = 
 OUT[^] for all n in 
N. 
Finding dominators using this data-flow algorithm is efficient. Nodes in the 
graph need to be visited only a few times, as we shall see in Section 9.6.7. 
Domain 
Boundary 
I OUT[ENTRY] = 
{ENTRY} 
Dominators 
The power set of N 
Direction 
Transfer function 
Initialization 
OUT[B] 
= 
N 
Forwards 
fB(x) = 
x U {B) 
Figure 9.40: A data-flow algorithfn for computing dominators 
Example 9.39 : Let us return to the flow graph of Fig. 9.38, and suppose 
the for-loop of lines (4) through (6) in Fig. 9.23 visits the nodes in numerical 
order. Let D(n) be the set of nodes in  OUT[^]. Since 1 
is the entry node, 
D(l) was assigned {I) at line (1). Node 2 has only 1 for a predecessor, so 
9.6. LOOPS IN FLOW GRAPHS 
659 
Properties of the dorn Relation 
A key observation about dominators is that if we take any acyclic path 
from the entry to node n, then all the dominators of n appear along this 
path, and moreover, they must appear in the same order along any such 
path. To see why, suppose there were one acyclic path PI to n along which 
dominators a and b appeared in that order and another path P2 to n, along 
which b preceded a. Then we could follow PI to a and P2 to n, thereby 
avoiding b altogether. Thus, b would not really dominate a. 
This reasoning allows us to prove that dorn is transitive: if a dorn b 
and b dorn c, then a dorn c. Also, dorn is antisymmetric: it is never possible 
that both a dorn b and b dorn a hold, if a # b. Moreover, if a and b are 
two dominators of n, then either a dorn b or b dorn a must hold. Finally, it 
follows that each node n except the entry must have a unique immediate 
dominator - 
the dominator that appears closest to n along any acyclic 
path from the entry to n. 
D(2) = (2) U D(1). Thus, D(2) is set to (1, 2). Then node 3, with predecessors 
1, 
2, 4, and 8, is considered. Since all the interior nodes are initialized with the 
universal set N, 
D(3) = 13) U ({1) n {I, 
2) n {I, 
2,. . . ,101 n {I, 
2,. . . , 
lo}) = {1,3} 
The remaining calculations are shown in Fig. 9.41. Since these values do not 
change in the second iteration through the outer loop of lines (3) through (6) 
in Fig. 9.23(a), they are the final answers to the dominator problem. 
Figure 9.41: Completion of the dominator calculation for Example 9.39 
660 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
9.6.2 
Depth-First Ordering 
As introduced in Section 2.3.4, a depth-first search of a graph visits all the 
nodes in the graph once, by starting at the entry node and visiting the nodes 
as far away from the entry node as quickly as possible. The route of the search 
in a depth-first search forms a depth-first spanning tree (DFST). Recall from 
Section 2.3.4 that a preorder traversal visits a node before visiting any of its 
children, which it then visits recursively in left-to-right order. Also, a postorder 
traversal visits a node's children, recursively in left-to-right order, before visiting 
the node itself. 
There is one more variant ordering that is important for flow-graph analysis: 
a depth-first ordering is the reverse of a postorder traversal. That is, in a depth- 
first ordering, we visit a node, then traverse its rightmost child, the child to its 
left, and so on. However, before we build the tree for the flow graph, we have 
choices as to which successor of a node becomes the rightmost child in the tree, 
which node becomes the next child, and so on. Before we give the algorithm 
for depth-first ordering, let us consider an example. 
Example 9.40 : One possible depth-first presentation of the flow graph in 
Fig. 9.38 is illustrated in Fig. 9.42. Solid edges form the tree; dashed edges are 
the other edges of the flow graph. A depth-first traversal of the tree is given 
by: 1 
--+ 3 --+ 4 -+ 6 --+ 7- 8 --+ 10, then back to 8, then to 9. We go back to 8 
once more, retreating to 7, 6, and 4, and then forward to 5. We retreat from 5 
back to 4, then back to 3 and 1. From I we go to 2, then retreat from 2, back 
to 1, 
and we have traversed the entire tree. 
The preorder sequence for the traversal is thus 
The postorder sequence for the traversal of the tree in Fig. 9.42 is 
The depth-first ordering, which is the reverse of the postorder sequence, is 
We now give an algorithm that finds a depth-first spanning tree and a depth- 
first ordering of a graph. It is this algorithm that finds the DFST in Fig. 9.42 
from Fig. 9.38. 
Algorithm 9.41 : 
Depth-first spanning tree and depth-first ordering. 
INPUT: A flow graph G. 
OUTPUT: A DFST T of G and an ordering of the nodes of G. 
9.6. LOOPS IN FLOW GRAPHS 
Figure 9.42: A depth-first presentation of the flow graph in Fig. 9.38 
METHOD: We use the recursive procedure search(n) of Fig. 9.43. The algo- 
rithm initializes all nodes of G to "~nvisited,~~ 
then calls search(no), 
where no 
is the entry. When it calls search(n), 
it first marks n "visited" to avoid adding 
n to the tree twice. It uses c to count from the number of nodes of G down to 
1, assigning depth-first numbers dfn[n] to nodes n as we go. The set of edges 
T forms the depth-first spanning tree for G. 
Example 9.42: For the flow graph in Fig. 9.42, Algorithm 9.41 sets c to 10 
and begins the search by calling search(1). The rest of the execution sequence 
is shownin Fig. 9.44. 
9.6.3 Edges in a Depth-First Spanning Tree 
When we construct a DFST for a flow graph, the edges of the flow graph fall 
into three categories. 
1. There are edges, called advancing edges, that go from a node m to a proper 
descendant of m in the tree. All edges in the DFST itself are advancing 
edges. There are no other advancing edges in Fig. 9.42, but, for example, 
if 4 -+ 
8 were an edge, it would be in this category. 
2. There are edges that go from a node m to an ancestor of m in the tree 
(possibly to m itself). These edges we shall term retreating edges. For 
example, 4 + 
3, 7 + 
4, 10 + 
7 and 9 + 1 are the retreating edges in 
Fig. 9.42. 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
void search(n) { 
mark n "visited" ; 
for (each successor s of n) 
if (s is "unvisited") { 
add edge n -+ s to T ;  
search(s) 
; 
main() { 
T  = 0; /* 
set of edges */ 
for (each node n of G) 
mark n "unvisited" ; 
c = 
number of nodes of G; 
search(no) 
; 
} 
Figure 9.43: Depth-first search algorithm 
3. There are edges m -+ 
n such that neither m nor n is an ancestor of the 
other in the DFST. Edges 2 -+ 3 and 5 -+ 7 are the only such examples 
in Fig. 9.42. We call these edges cross edges. An important property of 
cross edges is that if we draw the DFST so children of a node are drawn 
from left to right in the order in which they were added to the tree, then 
all cross edges travel from right to left. 
It should be noted that m -+ 
n is a retreating edge if and only if dfn[m] 2 
dfn[n]. To see why, note that if m is a descendant of n in the DFST, then 
search(m) terminates before search(n), so dfn[m] 2 dfn[n]. Conversely, if 
dfn[m] 2 dfn[n], then search(m) terminates before search(n), or m = n. But 
search(n) must have begun before search(m) if there is an edge m -+ n, or else 
the fact that n is a successor of m would have made n a descendant of m in the 
DFST. Thus the time search(m) is active is a subinterval of the time search(n) 
is active, from which it follows that n is an ancestor of m in the DFST. 
9.6.4 Back Edges and Reducibility 
A back edge is an edge a -+ b whose head b dominates its tail a. For any 
flow graph, every back edge is retreating, but not every retreating edge is a 
back edge. A flow graph is said to be reducible if all its retreating edges in 
any depth-first spanning tree are also back edges. In other words, if a graph 
is reducible, then all the DFST's have the same set of retreating edges, and 
9.6. LOOPS IN FLOW GRAPHS 
Call search(1) 
Call search(3) 
Call search(4) 
Call search(6) 
Call search(7) 
Call search(8) 
Call search(l0) 
Return to search(8) 
Call search(9) 
Return to search(8) 
Return to search(7) 
Return to search(6) 
Return to search(4) 
Call search(5) 
Return to search(4) 
Return to search(3) 
Return to search(1) 
Call search(2) 
Return to search(1) 
Node 1 has two successors. Suppose s = 
3 is consid- 
ered first; add edge 1 -+ 3 to T .  
Add edge 3 -+ 
4 to T. 
Node 4 has two successors, 4 and 6. Suppose s = 
6 is 
considered first; add edge 4 -+ 6 to T .  
Add 6 -+ 7 to T .  
Node 7 has two successors, 4 and 8. But 4 is already 
marked "visited" by search(4), 
so do nothing when 
s = 
4. For s = 
8, add edge 7 -+ 8 to T .  
Node 8 has two successors, 9 and 10. Suppose s = 10 
is considered first; add edge 8 -+ 10. 
10 has a successor, 7 ,  but 7 is already marked 
"visited." 
Thus, search(l0) completes by setting 
dfn[lO] 
= 10 and c = 
9. 
Set s = 
9 and add edge 8 -+ 9 to T .  
The only successor of 9, node 1, is already "visited," 
so set dfn[9] 
= 
9 and c = 
8. 
The last successor of 8, node 3, is "visited," so do 
nothing for s = 3. At this point, all successors of 8 
have been considered, so set dfn[8] 
= 
8 and c = 7. 
All of 7's successors have been considered, so set 
dfn[7] 
= 
7 and c = 
6. 
Similarly, 6's successors have been considered, so set 
dfn[6] 
= 
6 and c = 5. 
Successor 3 of 4 has been "visited," but 5 has not, so 
add 4 -+ 5 to the tree. 
Successor 7 of 5 has been "visited," thus set dfn[5] 
= 
5 
and c = 
4. 
All successors of 4 have been considered, set dfn[4] 
= 
4 
and c = 
3. 
Set dfn[3] 
= 
3 and c = 2. 
2 has not been visited yet, so add 1 -+ 2 to T .  
Set dfn[2] 
= 
2, c = 1. 
Set dfn[l] 
= 1 and c = 
0. 
Figure 9.44: Execution of Algorithm 9.41 on the flow graph in Fig. 9.43 
664 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Why Are Back Edges Retreating Edges? 
Suppose a -+ b is a back edge; i.e., its head dominates its tail. The 
sequence of calls of the function search in Fig. 9.43 that lead to node a 
must be a path in the flow graph. This path must, of course, include any 
dominator of a. It follows that a call to search(b) must be open when 
search(a) is called. Therefore b is already in the tree when a is added to 
the tree, and a is added as a descendant of b. Therefore, a -+ 
b must be a 
retreating edge. 
those are exactly the back edges in the graph. If the graph is nonreducible 
(not reducible), however, all the back edges are retreating edges in any DFST, 
but each DFST may have additional retreating edges that are not back edges. 
These retreating edges may be different from one DFST to another. Thus, if 
we remove all the back edges of a flow graph and the remaining graph is cyclic, 
then the graph is nonreducible, and conversely. 
Flow graphs that occur in practice are almost always reducible. Exclusive 
use of structured flow-of-control statements such as if-t 
hen-else, while-do, con- 
tinue, and break statements produces programs whose flow graphs are always 
reducible. Even programs written using goto statements often turn out to be 
reducible, as the programmer logically thinks in terms of loops and branches. 
Example 9.43 : 
The flow graph of Fig. 9.38 is reducible. The retreating edges 
in the graph are all back edges; that is, their heads dominate their respective 
tails. 
Example 9.44 : 
Consider the flow graph of Fig. 9.45, whose initial node is 1. 
Node 1 
dominates nodes 2 and 3, but 2 does not dominate 3, nor vice-versa. 
Thus, this flow graph has no back edges, since no head of any edge dominates its 
tail. There are two possible depth-first spanning trees, depending on whether 
we choose to call search(2) or search(3) first, from search(1). In the first case, 
edge 3 -+ 2 is a retreating edge but not a back edge; in the second case, 2 -+ 3 
is the retreating-but-not-back edge. Intuitively, the reason this flow graph is 
not reducible is that the cycle 2-3 can be entered at two different places, nodes 
2and3. 
Figure 9.45: The canonical nonreducible flow graph 
9.6. LOOPS IN FLOW GRAPHS 
9.6.5 
Depth of a Flow Graph 
Given a depth-first spanning tree for the graph, the depth is the largest number 
of retreating edges on any cycle-free path. We can prove the depth is never 
greater than what one would intuitively call the depth of loop nesting in the 
flow graph. If a flow graph is reducible, we may replace "retreating" by "back" 
in the definition of "depth," since the retreating edges in any DFST are exactly 
the back edges. The notion of depth then becomes independent of the DFST 
actually chosen, and we may truly speak of the "depth of a flow graph," rather 
than the depth of a flow graph in connection with one of its depth-first spanning 
trees. 
Example 9.45 : 
In Fig. 9.42, the depth is 3, since there is a path 
with three retreating edges, but no cycle-free path with four or more retreating 
edges. It is a coincidence that the "deepest" path here has only retreating 
edges; in general we may have a mixture of retreating, advancing, and cross 
edges in a deepest path. 
9.6.6 
Natural Loops 
Loops can be specified in a source program in many different ways: they can be 
written as for-loops, while-loops, or repeat-loops; they can even be defined using 
labels and goto statements. From a program-analysis point of view, it does not 
matter how the loops appear in the source code. What matters is whether they 
have the properties that enable easy optimization. In particular, we care about 
whether a loop has a single-entry node; if it does, compiler analyses can assume 
certain initial conditions to hold at the beginning of each iteration through the 
loop. This opportunity motivates the need for the definition of a "natural loop." 
A natural loop is defined by two essential properties. 
1. It must have a single-entry node, called the header. This entry node 
dominates all nodes in the loop, or it would not be the sole entry to the 
loop. 
2. There must be a back edge that enters the loop header. Otherwise, it is 
not possible for the flow of control to return to the header directly from 
the "loop" ; 
i.e., there really is no loop. 
Given a back edge n + 
d, we define the natural loop of the edge to be d 
plus the set of nodes that can reach n without going through d. Node d is the 
header of the loop. 
Algorithm 9.46 : 
Constructing the natural loop of a back edge. 
INPUT: A flow graph G and a back edge n + 
d. 
666 
CHAPTER 9. M
A
 CHINE-INDEPENDENT OPTIMIZATIONS 
OUTPUT: The set loop consisting of all nodes in the natural loop of n --+ d. 
METHOD: Let loop be {n, d}. Mark d as "visited," so that the search does not 
reach beyond d. Perform a depth-first search on the reverse control-flow graph 
starting with node n. Insert all the nodes visited in this search into loop. This 
procedure finds all the nodes that reach n without going through d. 
C1 
Example 9.47 
: In Fig. 9.38, there are five back edges, those whose heads 
dominate their tails: 10 -+ 
7, 7 --+ 4, 4 --+ 3, 8 --+ 3 and 9 --+ 1. Note that 
these are exactly the edges that one would think of as forming loops in the flow 
graph. 
Back edge 10 -+ 7 has natural loop {7,8,10}, since 8 and 10 are the only 
nodes that can reach 10 without going through 7. Back edge 7 -+ 4 has a 
natural loop consisting of {4,5,6,7,8,10} and therefore contains the loop of 
10 -+ 7. We thus assume the latter is an inner loop contained inside the former. 
The natural loops of back edges 4 -+ 
3 and 8 -+ 3 have the same header, 
node 3, and they also happen to have the same set of nodes: {3,4,5,6,7,8,10}. 
We shall therefore combine these two loops as one. This loop contains the two 
smaller loops discovered earlier. 
Finally, the edge 9 --+ 1 
has as its natural loop the entire flow graph, and 
therefore is the outermost loop. In this example, the four loops are nested 
within one another. It is typical, however, to have two loops, neither of which 
is a subset of the other. 
In reducible flow graphs, since all retreating edges are back edges, we can 
associate a natural loop with each retreating edge. That statement does not 
hold for nonreducible graphs. For instance, the nonreducible flow graph in 
Fig. 9.45 has a cycle consisting of nodes 2 and 3. Neither of the edges in the 
cycle is a back edge, so this cycle does not fit the definition of a natural loop. 
We do not identify the cycle as a natural loop, and it is not optimized as such. 
This situation is acceptable, because our loop analyses can be made simpler by 
assuming that all loops have single-entry nodes, and nonreducible programs are 
rare in practice anyway. 
By considering only natural loops as "loops," we have the useful property 
that unless two loops have the same header, they are either disjoint or one is 
nested within the other. Thus, we have a natural notion of innermost loops: 
loops that contain no other loops. 
When two natural loops have the same header, as in Fig. 9.46, it is hard to 
tell which is the inner loop. Thus, we shall assume that when two natural loops 
have the same header, and neither is properly contained within the other, they 
are combined and treated as a single loop. 
Example 9.48: The natural loops of the back edges 3 --+ 1 and 4 -+ 1 
in 
Fig. 9.46 are {I, 
2,3} and {I, 
2,4}, respectively. We shall combine them into a 
single loop, {1,2,3,4). 
However, were there another back edge 2 -+ 1 in Fig. 9.46, its natural 
loop would be {I, 
21, a third loop with header 1. This set of nodes is properly 
9.6. LOOPS IN FLOW GRAPHS 
Figure 9.46: Two loops with the same header 
contained within {I, 
2,3,4}, so it would not be combined with the other natural 
loops, but rather treated as an inner loop, nested within. 
9.6.7 Speed of Convergence of Iterative Data-Flow 
Algorit 
hrns 
We are now ready to discuss the speed of convergence of iterative algorithms. 
As discussed in Section 9.3.3, the maximum number of iterations the algorithm 
may take is the product of the height of the lattice and the number of nodes 
in the flow graph. For many data-flow analyses, it is possible to order the 
evaluation such that the algorithm converges in a much smaller number of 
iterations. The property of interest is whether all events of significance at a 
node will be propagated to that node along some acyclic path. Among the 
data-flow analyses discussed so far, reaching definitions, available expressions 
and live variables have this property, but constant propagation does not. More 
specifically: 
If a definition d is in IN[B], 
then there is some acyclic path from the block 
containing d to B such that d is in the IN'S and OUT'S all along that path. 
If an expression x + 
y is not available at the entrance to block B, then 
there is some acyclic path that demonstrates that either the path is from 
the entry node and includes no statement that kills or generates x + 
y, or 
the path is from a block that kills x + 
y and along the path there is no 
subsequent generation of x + 
y. 
If x is live on exit from block B, then there is an acyclic path from B to 
a use of x, along which there are no definitions of x. 
We should check that in each of these cases, paths with cycles add nothing. For 
example, if a use of x is reached from the end of block B along a path with a 
cycle, we can eliminate that cycle to find a shorter path along which the use of 
x is still reached from B. 
In contrast, constant propagation does not have this property. Consider a 
simple program that has one loop containing a basic block with statements 
668 
CHAPTER 9. MA 
CHINE-INDEPENDENT OPTIMIZATIONS 
The first time the basic block is visited, c is found to have constant value 1, but 
both a and b are undefined. Visiting the basic block the second time, we find 
that b and c have constant values 1. It takes three visits of the basic block for 
the constant value 1 
assigned to c to reach a. 
If all useful information propagates along acyclic paths, we have an opportu- 
nity to tailor the order in which we visit nodes in iterative data-flow algorithms, 
so that after relatively few passes through the nodes we can be sure information 
has passed along all the acyclic paths. 
Recall from Section 9.6.3 that if a -+ b is an edge, then the depth-first 
number of b is less than that of a only when the edge is a retreating edge. For 
forward data-flow problems, it is desirable to visit the nodes according to the 
depth-first ordering. Specifically, we modify the algorithm in Fig. 9.23(a) by 
replacing line (4), which visits the basic blocks in the flow graph with 
for (each block B other than ENTRY, in depth-first order) { 
Example 9.49 : 
Suppose we have a path along which a definition d propagates, 
such as 
where integers represent the depth-first numbers of the blocks along the path. 
Then the first time through the loop of lines (4) through (6) in the algorithm in 
Fig. 9.23(a), d will propagate from  OUT[^] to  IN[^] to  OUT[^], and so on, up to 
 OUT[^^]. It will not reach 1~[16] 
on that round, because as 16 precedes 35, we 
had already computed 1~[16] 
by the time d was put in  OUT[^^]. However, the 
next time we run through the loop of lines (4) through (6), when we compute 
1~[16], 
d will be included because it is in  OUT[^^]. Definition d will also propa- 
gate to  OUT[^^], 1~[23], 
and so on, up to  OUT[^^], where it must wait because 
 IN[^] was already computed on this round. On the third pass, d travels to  IN[^], 
 OUT[^], 1~[10], 
 OUT[^^], and 1~[17], 
so after three passes we establish that d 
reaches block 17. 
It should not be hard to extract the general principle from this example. If 
we use depth-first order in Fig. 9.23(a), then the number of passes needed to 
propagate any reaching definition along any acyclic path is no more than one 
greater than the number of edges along that path that go from a higher num- 
bered block to a lower numbered block. Those edges are exactly the retreating 
edges, so the number of passes needed is one plus the depth. Of course Algo- 
rithm 9.11 does not detect the fact that all definitions have reached wherever 
they can reach, until one more pass has yielded no changes. Therefore, the 
upper bound on the number of passes taken by that algorithm with depth-first 
9.6. LOOPSINFLOW GRAPHS 
669 
A Reason for Nonreducible Flow Graphs 
There is one place where we cannot generally expect a flow graph to be 
reducible. If we reverse the edges of a program flow graph, as we did 
in Algorithm 9.46 to find natural loops, then we may not get a reducible 
flow graph. The intuitive reason is that, while typical programs have loops 
with single entries, those loops sometimes have several exits, which become 
entries when we reverse the edges. 
block ordering is actually two plus the depth. A study1' has shown that typical 
flow graphs have an average depth around 2.75. Thus, the algorithm converges 
very quickly. 
In the case of backward-flow problems, like live variables, we visit the nodes 
in the reverse of the depth-first order. Thus, we may propagate a use of a 
variable in block 17 backwards along the path 
in one pass to  IN[^], where we must wait for the next pass in order to reach 
 OUT[^^]. On the second pass it reaches 1~[16], 
and on the third pass it goes 
from  OUT[^^] to  OUT[^]. 
In general, one-plus-the-depth passes suffice to carry the use of a variable 
backward, along any acyclic path. However, we must choose the reverse of 
depth-first order to visit the nodes in a pass, because then, uses propagate 
along any decreasing sequence in a single pass. 
The bound described so far is an upper bound on all problems where cyclic 
paths add no information to the analysis. In special problems such as domi- 
nators, the algorithm converges even faster. In the case where the input flow 
graph is reducible, the correct set of dominators for each node is obtained in 
the first iteration of a data-flow algorithm that visits the nodes in depth-first 
ordering. If we do not know that the input is reducible ahead of time, it takes 
an extra iteration to determine that convergence has occurred. 
9.6.8 
Exercises for Section 9.6 
Exercise 9.6.1: For the flow graph of Fig. 9.10 (see the exercises for Sec- 
tion 9.1): 
i. Compute the dominator relation. 
ii. Find the immediate dominator of each node. 
'OD. 
E. Knuth, "An empirical study of FORTRAN programs," Software - 
Practice and 
Experience 1:2 (1971), pp. 105-133. 
670 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
iii. Construct the dominator tree. 
iu. Find one depth-first ordering for the flow graph. 
u. Indicate the advancing, retreating, cross, and tree edges for your answer 
to iu. 
ui. Is the flow graph reducible? 
uii. Compute the depth of the flow graph. 
uiii. Find the natural loops of the flow graph. 
Exercise 9.6.2 : 
Repeat Exercise 9.6.1 on the following flow graphs: 
a) Fig. 9.3. 
b) Fig. 8.9. 
c) Your flow graph from Exercise 8.4.1. 
d) Your flow graph from Exercise 8.4.2. 
! 
Exercise 9.6.3 : 
Prove the following about the dom relation: 
a) If a d o m  b and b dorn c, then a dorn c (transitivity). 
b) It is never possible that both a d o m  b and b d o m  a hold, if a # b (anti- 
symmetry) 
. 
c) If a and b are two dominators of n, then either a dorn b or b d o m  a must 
hold. 
d) Each node n except the entry has a unique immediate dominator - 
the 
dominator that appears closest to n along any acyclic path from the entry 
to n. 
! 
Exercise 9.6.4 : 
Figure 9.42 is one depth-first presentation of the flow graph of 
Fig. 9.38. How many other depth-first presentations of this flow graph are there? 
Remember, order of children matters in distinguishing depth-first presentations. 
!! Exercise 9.6.5 : 
Prove that a flow graph is reducible if and only if when we 
remove all the back edges (those whose heads dominate their tails), the resulting 
flow graph is acyclic. 
! Exercise 9.6.6: A complete flow graph on n nodes has arcs i -+ j between 
any two nodes i and j (in both directions). For what values of n is this graph 
reducible? 
! 
Exercise 9.6.7 : 
A complete, acyclic flow graph on n nodes 1,2, 
. 
. 
. , 
n has arcs 
i -+ j for all nodes i and j such that i < j. Node 1 
is the entry. 
9.6. LOOPS IN FLOW GRAPHS 
a) For what values of n is this graph reducible? 
b) Does your answer to (a) change if you add self-loops i --+ i for all nodes 
i? 
! Exercise 9.6.8 : The natural loop of a back edge n + 
h was defined to be h 
plus the set of nodes that can reach n without going through h. Show that h 
dominates all the nodes in the natural loop of n +- 
h. 
!! Exercise 9.6.9 : 
We claimed that the flow graph of Fig. 9.45 is nonreducible. 
If the arcs were replaced by paths of disjoint nodes (except for the endpoints, 
of course), then the flow graph would still be nonreducible. In fact, node 1 
need not be the entry; it can be any node reachable from the entry along a 
path whose intermediate nodes are not part of any of the four explicitly shown 
paths. Prove the converse: that every nonreducible flow graph has a subgraph 
like Fig. 9.45, but with arcs possibly replaced by node-disjoint paths and node 1 
being any node reachable from the entry by a path that is node-disjoint from 
the four other paths. 
! 
! 
Exercise 9.6.10 : Show that every depth-first presentation for every nonre- 
ducible flow graph has a retreating edge that is not a back edge. 
!! Exercise 9.6.11 : 
Show that if the following condition 
f (a) /
I
 
g(a) /
I
 
a 5 f (s(a)) 
holds for all functions f and g, and value a, then the general iterative algorithm, 
Algorithm 9.25, with iteration following a depth-first ordering, converges within 
2-plus-the-depth passes. 
! 
Exercise 9.6.12 : Find a nonreducible flow graph with two different DFST's 
that have different depths. 
! 
Exercise 9.6.13 
: 
Prove the following: 
a) If a definition d is in IN[B], 
then there is some acyclic path from the block 
containing d to B such that d is in the IN'S and OUT'S all along that path. 
b) If an expression x + 
y is not available at the entrance to block B, then 
there is some acyclic path that demonstrates that fact; either the path 
is from the entry node and includes no statement that kills or generates 
x + 
y, or the path is from a block that kills x + 
y and along the path there 
is no subsequent generation of x + 
y. 
c) If x is live on exit from block B, then there is an acyclic path from B to 
a use of x, along which there are no definitions of x. 
672 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
9.7 Region-Based Analysis 
The iterative data-flow analysis algorithm we have discussed so far is just one 
approach to solving data-flow problems. Here we discuss another approach 
called region-based analysis. Recall that in the iterative-analysis approach, we 
create transfer functions for basic blocks, then find the fixedpoint solution by 
repeated passes over the blocks. Instead of creating transfer functions just for 
individual blocks, a region-based analysis finds transfer functions that summa- 
rize the execution of progressively larger regions of the program. Ultimately, 
transfer funchions for entire procedures are constructed and then applied, to get 
the desired data-flow values directly. 
While a data-flow framework using an iterative algorithm is specified by 
a semilattice of data-flow values and a family of transfer functions closed un- 
der composition, region-based analysis requires more elements. A region-based 
framework includes both a semilattice of data-flow values and a semilattice 
of transfer functions that must possess a meet operator, a composition oper- 
ator, and a closure operator. We shall see what all these elements entail in 
Section 9.7.4. 
A region-based analysis is particularly useful for data-flow problems where 
paths that have cycles may change the data-flow values. The closure operator 
allows the effect of a loop to be summarized more effectively than does iterative 
analysis. The technique is also useful for interprocedural analysis, where trans- 
fer functions associated with a procedure call may be treated like the transfer 
functions associated with basic blocks. 
For simplicity, we shall consider only forward data-flow problems in this 
section. We first illustrate how region-based analysis works by using the familiar 
example of reaching definitions. In Section 9.8 we show a more compelling use 
of this technique, when we study the analysis of induction variables. 
9.7.1 Regions 
In region-based analysis, a program is viewed as a hierarchy of regions, which 
are (roughly) portions of a flow graph that have only one point of entry. We 
should find this concept of viewing code as a hierarchy of regions intuitive, 
because a block-structured procedure is naturally organized as a hierarchy of 
regions. Each statement in a block-structured program is a region, as control 
flow can only enter at the beginning of a statement. Each level of statement 
nesting corresponds to a level in the region hierarchy. 
Formally, a region of a flow graph is a collection of nodes N and edges E 
such that 
1. There is a header h in N that dominates all the nodes in N. 
2. If some node m can reach a node n in N without going through h, then 
m is also in N. 
9.7. REGION-BASED ANALYSIS 
673 
3. E is the set of all the control flow edges between nodes nl and n2 in N, 
except (possibly) for some that enter h. 
Example 9.50: Clearly a natural loop is a region, but a region does not 
necessarily have a back edge and need not contain any cycles. For example, in 
Fig. 9.47, nodes B1 and B2, 
together with the edge B1 -+ B2, 
form a region; so 
do nodes B1, B2, 
and B3 with edges B1 -+ B2, 
B2 -+ B3, and B1 -+ 
B3. 
However, the subgraph with nodes B2 and B3 with edge B2 + 
BS does not 
form a region, because control may enter the subgraph at both nodes B2 and 
B3. More precisely, neither B2 nor Bg dominates the other, so condition (1) 
for 
a region is violated. Even if we picked, say, B2 to be the "header," we would 
violate condition (2), since we can reach B3 from B1 without going through B2, 
and B1 is not in the "region." 
Figure 9.47: Examples of regions 
9.7.2 Region Hierarchies for Reducible Flow Graphs 
In what follows, we shall assume the flow graph is reducible. If occasionally we 
must deal with nonreducible flow graphs, then we can use a technique called 
"node splitting" that will be discussed in Section 9.7.6. 
To construct a hierarchy of regions, we identify the natural loops. Recall 
from Section 9.6.6 that in a reducible flow graph, any two natural loops are 
either disjoint or one is nested within the other. The process of "parsing" a 
reducible flow graph into its hierarchy of loops begins with every block as a 
region by itself. We call these regions leaf regions. Then, we order the natural 
loops from the inside out, i.e., starting with the innermost loops. To process a 
loop, we replace the entire loop by a node in two steps: 
1. First, the body of the loop L (all nodes and edges except the back edges to 
the header) is replaced by a node representing a region R. Edges to the 
header of L now enter the node for R. An edge from any exit of loop L is 
replaced by an edge from R to the same destination. However, if the edge 
is a back edge, then it becomes a loop on R. We call R a body region. 
674 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
2. Next, we construct a region R' that represents the entire natural loop L. 
We call R' a loop region. The only difference between R and R' is that 
the latter includes the back edges to the header of loop L. Put another 
way, when R' replaces R in the flow graph, all we have to do is remove 
the edge from R to itself. 
We proceed this way, reducing larger and larger loops to single nodes, first with 
a looping edge and then without. Since loops of a reducible flow graph are 
nested or disjoint, the loop region's node can represent all the nodes of the 
natural loop in the series of flow graphs that are constructed by this reduction 
process. 
Eventually, all natural loops are reduced to single nodes. At that point, 
the flow graph may be reduced to a single node, or there may be several nodes 
remaining, with no loops; i.e., the reduced flow graph is an acyclic graph of 
more than one node. In the former case we are done constructing the region 
hierarchy, while in the latter case, we construct one more body region for the 
entire flow graph. 
Example 9.5 
1 : 
Consider the control flow graph in Fig. 9.48(a). There is one 
back edge in this flow graph, which leads from B4 to B2. The hierarchy of 
regions is shown in Fig. 9.48(b); the edges shown are the edges in the region 
flow graphs. There are altogether 8 regions: 
1. Regions R1, . 
. . , 
R5 are leaf regions representing blocks B1 through B5, 
respectively. Every block is also an exit block in its region. 
2. Body region Re represents the body of the only loop in the flow graph; it 
consists of regions R2, 
R3, and R4 and three interregion edges: B2 -+ B3, 
B2 -+ B4, and B3 -+ B4. It has two exit blocks, B3 and B4, since they 
both have outgoing edges not contained in the region. Figure 9.49(a) 
shows the flow graph with R6 reduced to a single node. Notice that 
although the edges R3 -+ R5 and R4 -+ R5 have both been replaced by 
edge Rs --+ R5, 
it is important to remember that the latter edge represents 
the two former edges, since we shall have to propagate transfer functions 
across this edge eventually, and we need to know that what comes out of 
both blocks B3 and B4 will reach the header of R5. 
3. Loop region R7 represents the entire natural loop. It includes one subre- 
gion, R6, 
and one back edge B4 + 
B2. It has also two exit nodes, again 
BS and B4. Figure 9.49(b) shows the flow graph after the entire natural 
loop is reduced to R7. 
4. Finally, body region Rs is the top region. It includes three regions, R1, 
R7, R5 and three interregion edges, B1 -+ 
B2, B3 -+ B5, and B4 -+ B5. 
When we reduce the flow graph to R8, 
it becomes a single node. Since 
there are no back edges to its header, B1, there is no need for a final step 
reducing this body region to a loop region. 
9.7. REGION-BASED ANALYSIS 
r*-i 
B ,  (EXIT) 
Figure 9.48: (a) An example flow graph for the reaching definitions problem 
and (b) Its region hierarchy 
676 
CHAPTER 9. MACHIRE-INDEPENDENT OPTIMIZATIONS 
(a) After reducing to 
(b) After reducing to 
a body region 
a loop region 
Figure 9.49: Steps in the reduction of the flow graph of Fig. 9.47 to a single 
region 
To summarize the process of decomposing reducible flow graphs hierarchi- 
cally, we offer the following algorithm. 
Algorithm 9.52 : Constructing a bottom-up order of regions of a reducible 
flow graph. 
INPUT: A reducible flow graph G. 
OUTPUT: A list of regions of G that can be used in region-based data-flow 
problems. 
METHOD: 
1. Begin the list with all the leaf regions consisting of single blocks of G, in 
any order. 
2. Repeatedly choose a natural loop L such that if there are any natural 
loops contained within L, then these loops have had their body and loop 
regions added to the list already. Add first the region consisting of the 
body of L (i.e., L without the back edges to the header of L), and then 
the loop region of L. 
3. If the entire flow graph is not itself a natural loop, add at the end of the 
list the region consisting of the entire flow graph. 
9.7.3 Overview of a Region-Based Analysis 
For each region R, and for each subregion R' within R, we compute a transfer 
function fR,INIRl) 
that summarizes the effect of executing all possible paths 
9.7. REGION-BASED ANALYSIS 
677 
Where "Reducibless 
Comes From 
We now see why reducible flow graphs were given that name. While we 
shall not prove this fact, the definition of "reducible flow graph7' 
used in 
this book, involving the back edges of the graph, is equivalent to several 
definitions in which we mechanically reduce the flow graph to a single 
node. The process of collapsing natural loops described in Section 9.7.2 
is one of them. Another interesting definition is that the reducible flow 
graphs are all and only those graphs that can be reduced to a single node 
by the following two transformations: 
TI: 
Remove an edge from a node to itself. 
T2: 
If node n has a single predecessor m, and n is not the entry of the 
flow graph, combine rn and n. 
leading from the entry of R to the entry of R', while staying within R. We say 
that a block B within R is an exit block of region R if it has an outgoing edge to 
some block outside R. We also compute a transfer function for each exit block 
B of R, denoted fR,OUTIBI, 
that summarizes the effect of executing all possible 
paths within R, leading from the entry of R to the exit of B. 
We then proceed up the region hierarchy, computing transfer functions for 
progressively larger regions. We begin with regions that are single blocks, where 
fB,INIBl 
is just the identity function and fB,OUTIB] 
is the transfer function for 
the block B itself. As we move up the hierarchy, 
If R is a body region, then the edges belonging to R form an acyclic 
graph on the subregions of R. We may proceed to compute the transfer 
functions in a topological order of the subregions. 
If R is a loop region, then we only need to account for the effect of the 
back edges to the header of R. 
Eventually, we reach the top of the hierarchy and compute the transfer 
functions for region R,, that is the entire flow graph. How we perform each of 
these computations will be seen in Algorithm 9.53. 
The next step is to compute the data-flow values at the entry and exit of 
each block. We process the regions in the reverse order, starting with region 
R,, and working our way down the hierarchy. For each region, we compute the 
data-flow values at the entry. For region R,, we apply fR,,INIRl 
(IN[ENTRY]) 
to get the data-flow values at the entry of the subregions R in R,,. We repeat 
until we reach the basic blocks at the leaves of the region hierarchy. 
678 
CHAPTER 9. MACHPNE-INDEPENDENT OPTIMIZATIONS 
9.7.4 Necessary Assumptions About Transfer Functions 
In order for region-based analysis to work, we need to make certain assumptions 
about properties of the set of transfer functions in the framework. Specifically, 
we need three primitive operations on transfer functions: composition, meet 
and closure; only the first is required for data-flow frameworks that use the 
iterative algorithm. 
Composition 
The transfer function of a sequence of nodes can be derived by composing the 
functions representing the individual nodes. Let fl and f2 be transfer functions 
of nodes nl and n2. The effect of executing nl followed by n2 is represented 
by f2 0 
fl. Function composition has been discussed in Section 9.2.2, and an 
example using reaching definitions was shown in Section 9.2.4. To review, let 
geni and killi be the gen and kill sets for fi. Then: 
Thus, the gen and kill sets for f2 0 
fl are gen2 U (genl - 
kill2) 
and killl U kill2, 
respectively. The same idea works for any transfer function of the gen-kill form. 
Other transfer functions may also be closed, but we have to consider each case 
separately. 
Meet 
Here, the transfer functions themselves are values of a semilattice with a meet 
operator Af . The meet of two transfer functions fl and f2, 
fl Af f2, is defined 
by (fl Af f2)(x) = fl 
(x) A fi 
(x), where A is the meet operator for data-flow 
values. The meet operator on transfer functions is used to combine the effect 
of alternative paths of execution with the same end points. Where it is not am- 
biguous, from now on, we shall refer to the meet operator of transfer functions 
also as A. For the reaching-definitions framework, we have 
That is, the gen and kill sets for fl A f2 are gent U gen2 and killl n kill2, 
respectively. Again, the same argument applies to any set of gen-kill transfer 
functions. 
9.7. REGION-BASED ANALYSIS 
Closure 
If f represents the transfer function of a cycle, then f 
represents the effect of 
going around the cycle n times. In the case where the number of iterations is 
not known, we have to assume that the loop may be executed 0 or more times. 
We represent the transfer function of such a loop by f *, the closure of f ,  which 
is defined by 
Note that f 0  must be the identity transfer function, since it represents the 
effect of going zero times around the loop, i.e., starting at the entry and not 
moving. If we let I represent the identity transfer furiction, then we can write 
Suppose the transfer function f in a reaching definitions framework has a 
gen set and a kill set. Then, 
f2(4 
= 
f ( f  
( X I )  
= (gen 
U ((gen 
U ( x  
- 
kill)) - 
kill) 
= 
gen U ( x  
- 
kill) 
f3(0 
= f ( f 2 ( x ) )  
= 
gen U ( x  
- 
kill) 
and so on: any fn(x) is gen U ( x  
- 
kill). That is, going around a loop doesn't 
affect the transfer function, if it is of the gen-kill form. Thus, 
f*(x) = 
I A  
f l ( x )  
A  
f 2 ( x )  
A ... 
= 
x u 
(gen 
U ( x  
- 
kill)) 
= 
gen U x 
That is, the gen and kill sets for f *  
are gen and 0, respectively. Intuitively, 
since we might not go around a loop at all, anything in x will reach the entry 
to the loop. In all subsequent iterations, the reaching definitions include those 
in the gen set. 
680 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
9.7.5 
An Algorithm for Region-Based Analysis 
The following algorithm solves a forward data-flow-analysis problem on a re- 
ducible flow graph, according to some framework that satisfies the assumptions 
of Section 9.7.4. Recall that fR,INIR/J 
and fR,OUTIBJ 
refer to transfer functions 
that transform data-flow values at the entry to region R into the correct value 
at the entry of subregion R' and the exit of the exit block B, respectively. 
Algorithm 9.53 : 
Region-based analysis. 
INPUT: A data-flow framework with the properties outlined in Section 9.7.4 
and a reducible flow graph G. 
OUTPUT: Data-flow values IN[B] 
for each block B of G. 
METHOD: 
1. Use Algorithm 9.52 to construct the bottom-up sequence of regions of G, 
say R1, R2,. 
. 
. , 
R,, where R, is the topmost region. 
2. Perform the bottom-up analysis to compute the transfer functions sum- 
marizing the effect of executing a region. For each region R1, R2, 
. 
. . , 
R,, 
in the bottom-up order, do the following: 
(a) If R is a leaf region corresponding to block B, let fR,INIBl 
= I, 
and 
fR,OUTIBl 
= 
fB, the transfer function associated with block B. 
(b) If R is a body region, perform the computation of Fig. 9.50(a). 
(c) If R is a loop region, perform the computation of Fig. 9.50(b). 
3. Perform the top-down pass to find the data-flow values at the beginning 
of each region. 
(a) IN[R,] = 
IN[ENTRY]. 
(b) For each region R in {Rl, . . 
. 
R,-l), in the top-down order, compute 
INIR] 
= f ~ '  
,IN[R] 
(IN[R']) 
7 
where R' is the immediate enclosing region of R. 
Let us first look at the details of how the bottom-up analysis works. In 
line (1) of Fig. 9.50(a) we visit the subregions of a body region, in some topolog- 
ical order. Line (2) computes the transfer function representing all the possible 
paths from the header of R to the header of S; 
then in lines (3) and (4) we com- 
pute the transfer functions representing all the possible paths from the header 
of R to the exits of R - 
that is, to the exits of all blocks that have successors 
outside S. Notice that all the predecessors B' in R must be in regions that 
precede S in the topological order constructed at line (1). Thus, fR,OUTIBll 
will have been computed already, in line (4) of a previous iteration through the 
outer loop. 
9.7. REGION-BASED ANALYSIS 
681 
For loop regions, we perform the steps of lines (1) 
through (4) in Fig. 9.50(b) 
Line (2) computes the effect of going around the loop body region S zero or 
more times. Lines (3) and (4) compute the effect at the exits of the loop after 
one or more iterations. 
In the top-down pass of the algorithm, step 3(a) first assigns the boundary 
condition to the input of the top-most region. Then if R is immediately con- 
tained in R', we can simply apply the transfer function fRt,INIRl 
to the data-flow 
value IN[R'] to compute IN[R]. 
1) for (each subregion S immediately contained in R, in 
topological order) { 
2) 
~R,IN[SI 
= Apredecessors B in R of the header of S ~R,OUT[BI 
; 
/* if S is the header of region R, then fR,IN(SI is the 
meet over nothing, which is the identity function */ 
3 
for (each exit block B in S) 
(a) Constructing transfer functions for a body region R 
1) let S be the body region immediately nested within R; that is, 
S is R without back edges from R to the header of R; 
2, 
~R,IN[SI 
= (Apredecessors B in R of the header of S 
~S,OUT[BI) 
* ; 
3) 
for (each exit block B in R) 
(b) Constructing transfer functions for a loop region R' 
Figure 9.50: Details of region-based data-flow computations 
Example 9.54 : 
Let us apply Algorithm 9.53 to find reaching definitions in the 
flow graph in Fig. 9.48(a). Step 1 
constructs the bottom-up order in which the 
regions are visited; this order will be the numerical order of their subscripts, 
Rl,R2,... ,Rn. 
The values of the gen and kill sets for the five blocks are summarized below: 
Remember the simplified rules for gen- 
kill transfer functions, from Section 
9.7.4: 
682 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
To take the meet of transfer functions, take the union of the gen's and 
the intersection of the kill's. 
To compose transfer functions, take the union of both the gen's and the 
kill's. However, as an exception, an expression that is generated by the 
first function, not generated by the second, but killed by the second is not 
in the gen of the result. 
To take the closure of a transfer function, retain its gen and replace the 
kill by 0. 
The first five regions R1, 
. 
. 
. , 
R5 are blocks Bl , 
. . 
. , 
B5, respectively. For 
1 < i 5 5, fRi,INIBil is the identity function, and f R i , o u ~ [ ~ i l  
is the transfer 
function for block Bi : 
Figure 9.51: Computing transfer functions for the flow graph in Fig. 9.48(a), 
using region-based analysis 
The rest of the transfer functions constructed in Step 2 of Algorithm 9.50 
are summarized in Fig. 9.51. Region R6, consisting of regions Rp 
, 
Rg 
, 
and R*, 
9.7. REGION-BASED ANALYSIS 
683 
represents the loop body and thus does not include the back edge B4 + 
B2. The 
order of processing these regions will be the only topological order: Rz 
, 
R3, 
R4. 
First, R2 has no predecessors within R6; 
remember that the edge B4 -+ B2 goes 
outside R6. Thus, fR6,1N[B21 is the identity function," and fR6,0UT[B21 
is the 
transfer function for block B2 itself. 
The header of region B3 has one predecessor within R6, namely Rz. The 
transfer function to its entry is simply the transfer function to the exit of Bz, 
fR6,0UT[B2], 
which has already been computed. We compose this function 
with the transfer function of B3 within its own region to compute the transfer 
function to the exit of B3. 
Last, for the transfer function to the entry of R4, 
we must compute 
because both B2 and B3 are predecessors of B4, 
the header of R4. This transfer 
function is composed with the transfer function fR470UT[Brl 
to get the desired 
function fR670UT[B41. Notice, for example, that d3 is not killed in this transfer 
function, because the path B2 -+ 
B4 does not redefine variable a. 
Now, consider loop region R7. It contains only one subregion R6 which 
represents its loop body. Since there is only one back edge, B4 + 
B2, to the 
header of R6, 
the transfer function representing the execution of the loop body 
0 or more times is just f~670UT[B,1: 
the gen set is {d4,d5,d6) 
and the kill 
set is 0. There are two exits out of region R7, blocks B3 and B4. Thus, this 
transfer function is composed with each of the transfer functions of R6 to get 
the corresponding transfer functions of R7. Notice, for instance, how d6 is in 
the gen set for fR7,B3 because of paths like B2 + 
B4 + 
B2 -+ B3, or even 
B2 -+ BS + 
B4 3 B 2  
-+ 
B3. 
Finally, consider R8, the entire flow graph. Its subregions are R1, R7, and 
R5, which we shall consider in that topological order. As before, the transfer 
function fR8,1N[BlI 
is simply the identity function, and the transfer function 
~ R ~ , o u T [ B ~ ]  
is just fR1 
,OUT[B~] 
which in turn is f
~
l
 
The header of R7, which is B2, 
has only one predecessor, B1, so the transfer 
function to its entry is simply the transfer function out of B1 in region Rg. 
We compose f R s 7 ~ u ~ [ B 1 1  
with the transfer functions to the exits of B3 and B4 
within R7 
to obtain their corresponding transfer functions within Rg 
. Lastly, we 
consider R5. Its header, B5, 
has two predecessors within Rg, namely B3 and B4. 
Therefore, we compute fRs ,OUT[B~] 
A 
,OUT[B~] get fR8 
, I N [ B ~ ]  
- Since the 
transfer function of block B5 is the identity function, fR8,0UT[B51 
= 
fR8,1N[B51. 
Step 3 computes the actual reaching definitions from the transfer functions. 
In step 3(a), IN[R~] 
= 0 since there are no reaching definitions at the beginning 
of the program. Figure 9.52 shows how step 3(b) computes the rest of the 
data-flow values. The step starts with the subregions of R8. Since the transfer 
function from the start of Rg to the start of each of its subregion has been 
"strictly speaking, we mean f R 6 , ~ ~ [ R 2 1 ,  
but when a region like R
2
 is a single block, it is 
often clearer if we use the block name rather than the region name in this context. 
684 
CHAPTER 9. MACHTNE-INDEPENDENT OPTIMIZATIONS 
computed, a single application of the transfer function finds the data-flow value 
at the start each subregion. We repeat the steps until we get the data-flow 
values of the leaf regions, which are simply the individual basic blocks. Note 
that the data-flow values shown in Figure 9.52 are exactly what we would get 
had we applied iterative data-flow analysis to the same flow graph, as must be 
the case, of course. 
Figure 9.52: Final steps of region-based flow analysis 
9.7.6 Handling Nonreducible Flow Graphs 
If nonreducible flow graphs are expected to be common for the programs to be 
processed by a compiler or other program-processing software, then we recom- 
mend using an iterative rather than a hierarchy-based approach to data-flow 
analysis. However, if we need only to be prepared for the occasional nonre- 
ducible flow graph, then the following "node-splitting " technique is adequate. 
Construct regions from natural loops to the extent possible. If the flow graph 
is nonreducible, we shall find that the resulting graph of regions has cycles, but 
no back edges, so we cannot parse the graph any further. A typical situation 
is suggested in Fig. 9.53(a), which has the same structure as the nonreducible 
flow graph of Fig. 9.45, but the nodes in Fig. 9.53 may actually be complex 
regions, as suggested by the smaller nodes within. 
We pick some region R that has more than one predecessor and is not the 
header of the entire flow graph. If R has k predecessors, make k copies of the 
entire flow graph R, and connect each predecessor of R's header to a different 
copy of R. Remember that only the header of a region could possibly have a 
predecessor outside that region. It turns out, although we shall not prove it, 
that such node splitting results in a reduction by at least one in the number of 
regions, after new back edges are identified and their regions constructed. The 
resulting graph may still not be reducible, but by alternating a splitting phase 
with a phase where new natural loops are identified and collapsed to regions, 
we eventually are left with a single region; i.e., the flow graph has been reduced. 
9.7. REGION-BASED ANALYSIS 
Figure 9.53: Duplicating a region to make a nonreducible flow graph become 
reducible 
Example 9.55 : The splitting shown in Fig. 9.53(b) has turned the edge 
RZb 
-+ R3 into a back edge, since R3 now dominates R2b. These two regions 
may thus be combined into one. The resulting three regions - 
R1, R2, and 
the new region - 
form an acyclic graph, and therefore may be combined into 
a single body region. We thus have reduced the entire flow graph to a single 
region. In general, additional splits may be necessary, and in the worst case, 
the total number of basic blocks could become exponential in the number of 
blocks in the original flow graph. 
We must also think about how the result of the data-flow analysis on the 
split flow graph relates to the answer we desire for the original flow graph. 
There are two approaches we might consider. 
1. Splitting regions may be beneficial for the optimization process, and we 
can simply revise the flow graph to have copies of certain blocks. Since 
each duplicated block is entered along only a subset of the paths that 
reached the original, the data-flow values at these duplicated blocks will 
tend to contain more specific information than was available at the orig- 
inal. For instance, fewer definitions may reach each of the duplicated 
blocks that reach the original block. 
2. If we wish to retain the original flow graph, with no splitting, then after 
analyzing the split flow graph, we look at each split block B, and its 
corresponding set of blocks B1, 
B2, 
. . . , 
Bk. We may compute IN[B] = 
IN[B~] 
A IN [B2] 
A . 
. 
A IN [Bk], and similarly for the OUT'S. 
686 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
9.7.7 Exercises for Section 9.7 
Exercise 9.7.1 : 
For the flow graph of Fig. 9.10 (see the exercises for Section 
9.1): 
i. Find all the possible regions. You may, however, omit from the list the 
regions consisting of a single node and no edges. 
ii. Give the set of nested regions constructed by Algorithm 9.52. 
iii. Give a TI-T2 
reduction of the flow graph as described in the box on "Where 
'Reducible' Comes From" in Section 9.7.2. 
Exercise 9.7.2 : 
Repeat Exercise 9.7.1 on the following flow graphs: 
a) Fig. 9.3. 
b) Fig. 8.9. 
c) Your flow graph from Exercise 8.4.1. 
d) Your flow graph from Exercise 8.4.2. 
Exercise 9.7.3 : 
Prove that every natural loop is a region. 
!! Exercise 9.7.4: Show that a flow graph is reducible if and only it can be 
transformed to a single node using: 
a) The operations Tl and T2 described in the box in Section 9.7.2. 
b) The region definition introduced in Section 9.7.2. 
! 
Exercise 9.7.5 
: Show that when you apply node splitting to a nonreducible 
flow graph, and then perform TI-T2 reduction on the resulting split graph, you 
wind up with strictly fewer nodes than you started with. 
! 
Exercise 9.7.6 
: What happens if you apply node-splitting and TI-T2 reduc- 
tion alternately, to reduce a complete directed graph of n nodes? 
9.8 
Symbolic Analysis 
We shall use symbolic analysis in this section to illustrate the use of region- 
based analysis. In this analysis, we track the values of variables in programs 
symbolically as expressions of input variables and other variables, which we 
call reference variables. Expressing variables in terms of the same set of ref- 
erence variables draws out their relationships. Symbolic analysis can be used 
for a range of purposes such as optimization, parallelization, and analyses for 
program underst 
anding. 
9.8. SYMBOLIC ANALYSIS 
Figure 9.54: An example program motivating symbolic analysis 
Example 9.56 : Consider the simple example program in Fig. 9.54. Here, we 
use x as the sole reference variable. Symbolic analysis will find that y has the 
value x - 
1 
and x 
has the value x - 
2 after their respective assignment statements 
in lines (2) and (3). This information is useful, for example, in determining that 
the two assignments in lines (4) and (5) write to different memory locations and 
can thus be executed in parallel. Furthermore, we can tell that the condition 
z > x is never true, thus allowing the optimizer to remove the conditional 
statement in lines (6) and (7) all together. 
9.8.1 Affine Expressions of Reference Variables 
Since we cannot create succinct and closed-form symbolic expressions for all 
values computed, we choose an abstract domain and approximate the compu- 
tations with the most precise expressions within the domain. We have already 
seen an example of this strategy before: constant propagation. In constant 
propagation, our abstract domain consists of the constants, an UNDEF symbol 
if we have not yet determined if the value is a constant, and a special NAC 
symbol that is used whenever a variable is found not to be a constant. 
The symbolic analysis we present here expresses values as aJgine expressions 
of reference variables whenever possible. An expression is affine with respect to 
variables vl, uz, . 
. . , 
u
,
 if it can be expressed as co + 
clul + . 
- + 
cnun, where 
CO, CI, . . 
. , 
Cn are constants. Such expressions are informally known as linear 
expressions. Strictly speaking, an affine expression is linear only if c
o
 is zero. 
We are interested in affine expressions because they are often used to index 
arrays in loops-such 
information is useful for optimizations and parallelization. 
Much more will be said about this topic in Chapter 11. 
Induction Variables 
Instead of using program variables as reference variables, an affine expression 
can also be written in terms of the count of iterations through the loop. Vari- 
ables whose values can be expressed as cli + 
co, where i is the count of iterations 
through the closest enclosing loop, are known as induction variables. 
Example 9.57 : 
Consider the code fragment 
688 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
f o r  (m = 10; 
m < 20; 
m++) 
( x = m*3; A
[
x
l
 = 0; 
) 
Suppose we introduce for the loop a variable, say i, representing the number of 
iterations executed. The value i is 0 in the first iteration of the loop, 1 
in the 
second, and so on. We can express variable m as an affine 
expression of i, 
namely 
m = i + 10. Variable x, which is 3m, takes on values 30,33, . 
. . ,57 during 
successive iterations of the loop. Thus, x has the affine expression x = 
30 + 
3i. 
We conclude that both m and x are induction variables of this loop. 
Expressing variables as affine expressions of loop indexes makes the series 
of values being computed explicit and enables several transformations. The 
series of values taken on by an induction variable can be computed with addi- 
tions rather than multiplications. This transformation is known as "strength 
reduction" and was introduced in Sections 8.7 and 9.1. For instance, we can 
eliminate the multiplication x=m*3 
from the loop of Example 9.57 by rewriting 
the loop as 
x = 27; 
f o r  (m = 10; 
m < 20; 
m++) 
( x = x+3; 
A
[
x
]
 = 0; 
) 
In addition, notice that the locations assigned 0 in that loop, &A + 
30, &A 
+ 
33, . . . , 
&A+ 57, are also affine expressions of the loop index. In fact, this series 
of integers is the only one that needs to be computed; we need neither m nor 
x. The code above can be replaced simply by: 
f o r  (x = &A+30; 
x <= &A+57; 
x = x+3) 
*x = 0; 
Besides speeding up the computation, symbolic analysis is also useful for 
parallelization. When the array indexes in a loop are affine expressions of loop 
indexes, we can reason about relations of data accessed across the iterations. 
For example, we can tell that the locations written are different in each itera- 
tion and therefore all the iterations in the loop can be executed in parallel on 
different processors. Such information is used in Chapters 10 and 11 to extract 
parallelism from sequential programs. 
Other Reference Variables 
If a variable is not a linear function of the reference variables already chosen, 
we have the option of treating its value as reference for future operations. For 
example, consider the code fragment: 
9.8. SYMBOLIC ANALYSIS 
689 
While the value held by a after the function call cannot itself be expressed 
as a linear function of any reference variables, it can be used as reference for 
subsequent statements. For example, using a as a reference variable, we can 
discover that c is one larger than b at the end of the program. 
1) a = 0; 
2) 
f o r  (f = l o o ;  f  ( 2 0 0 ;  f++) { 
3) 
a = a + l ;  
4) 
b  
= 10 * a; 
5 
> 
c  
= 0; 
6) 
f o r  (g = 10; g  < 20; g++) { 
7) 
d = b + c ;  
8) 
c = c + l ;  
} 
1 
Figure 9.55: Source code for Example 9.58 
Exarnple 9.58 : Our running example for this section is based on the source 
code shown in Fig. 9.55. The inner and outer loops are easy to understand, 
since f and g are not modified except as required by the for-loops. It is thus 
possible to replace f and g by reference variables i and j that count the number 
of iterations of the outer and inner loops, respectively. That is, we can let 
f = i + 99 and g = j + 9, and substitute for f and g throughout. When 
translating to intermediate code, we can take advantage of the fact that each 
loop iterates at least once, and so postpone the test for i 5 100 and j 5 10 to 
the ends of the loops. Figure 9.56 shows the flow graph for the code of Fig. 9.55, 
after introducing i and j and treating the for-loops as if they were repeat-loops. 
It turns out that a, b, c, and d are all induction variables. The sequences of 
values assigned to the variables in each line of the code are shown in Figure 9.57. 
As we shall see, it is possible to discover the affine expressions for these variables, 
in terms of the reference variables i and j. That is, at line (4) a = 
i, at line (7) 
d = l O i + j - 1 ,  andat line (8), c =  
j. 
9.8.2 Data-Flow Problem Formulation 
This analysis finds affine expressions of reference variables introduced (1) to 
count the number of iterations executed in each loop, and (2) to hold values 
at the entry of regions where necessary. This analysis also finds induction 
variables, loop invariants, as well as constants, as degenerate affine expressions. 
Note that this analysis cannot find all constants because it only tracks affine 
expressions of reference variables. 
690 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Figure 9.56: Flow graph and its region hierarchy for Example 9.58 
Data-Flow Values: Symbolic Maps 
The domain of data-flow values for this analysis is symbolic maps, which are 
functions that map each variable in the program to a value. The value is either 
an affine function of reference values, or the special symbol NAA to represent 
a non-affine expression. If there is only one variable, the bottom value of the 
semilattice is a map that sends the variable to NAA. The semilattice for n 
variables is simply the product of the individual semilattices. We use mNAA 
to 
denote the bottom of the semilattice which maps all variables to NAA. We can 
define the symbolic map that sends all variables to an unknown value to be the 
top data-flow value, as we did for constant propagation. However, we do not 
need top values in region-based analysis. 
Example 9.59 : The symbolic maps associated with each block for the code 
in Example 9.58 are shown in Figure 9.58. We shall see later how these maps 
are discovered; they are the result of doing region-based data-flow analysis on 
the flow graph of Fig. 9.56. 
9.8. SYMBOLIC ANALYSIS 
Figure 9.57: Sequence of values seen in program points in Example 9.58. 
NAA 
I NAA 
i = 100 
j = I , . . .  , l o  
100 
1000 
1000.. . ,1009 
1...10 
NAA 
1Oi 
1 
O
i
 
1Oi 
1 
O
i
 
1Oi - 
10 
1 
5 i 5 100 
j = 1 
,... 
,10 
i 
1Oi 
lOi,. . 
. ,10i + 
9 
1...10 
NAA 
0 
j - 
1 
i = 2  
j = 1 
,... ,10 
2 
20 
20,. . 
. ,29 
I , . . .  , l o  
- 
NAA 
NAA 
NAA 
NAA 
NAA 
lOi + 
j - 
1 
IOi + 
j - 
1 
l O i + j  - 
11 
i = l  
j = I , . . .  , I 0  
10 
' 10,. 
. 
. ,19 
I , . . .  , I 0  
line 
3 
7 
8 
Figure 9.58: Symbolic maps of the program in Example 9.58. 
var 
2
a
1
 
b 
d 
c 
The symbolic map associated with the entry of the program is m
,
,
,
.
 
At 
the exit of B1, 
the value of a is set to 0. Upon entry to block B2 
, a has value 
0 in the first iteration and increments by one in each subsequent iteration of 
the outer loop. Thus a has value i - 
1 at the entry of the ith iteration and 
value i at the end. The symbolic map at the entry of B2 maps variables b, c, d 
to NAA, because the variables have unknown values on entry to the inner loop. 
Their values depend on the number of iterations of the outer loop, so far. The 
symbolic map on exit from B2 reflects the assignment statements to a, b, and 
c in that block. The rest of the symbolic maps can be deduced in a similar 
manner. Once we have established the validity of the maps in Fig. 9.58, we can 
replace each of the assignments to a, b, c, and d in Fig. 9.55 by the appropriate 
affine expressions. That is, we can replace Fig. 9.55 by the code in Fig. 9.59. 
Transfer Function of a St 
at 
ement 
The transfer functions in this data-flow problem send symbolic maps to sym- 
bolic maps. To compute the transfer function of an assignment statement, we 
interpret the semantics of the statement and determine if the assigned vari- 
able can be expressed as an affine expression of the values on the right of the 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
1) a = 0; 
2) 
f o r  (i = 1; i <= 100; i++) { 
3) 
a = i; 
4) 
b = lO*i; 
5 
> 
c = 0; 
6) 
f o r  (j = 1; j <= 10; j++) { 
7 )  
d = 10*i + j -1; 
8) 
c = j; 
Figure 9.59: The code of Fig. 9.55 with assignments replaced by affine expres- 
sions of the reference variables i and j 
Cautions Regarding Transfer Functions on Value 
Maps 
A subtlety in the way we define the transfer functions on symbolic maps 
is that we have options regarding how the effects of a computation are 
expressed. When m is the map for the input of a transfer function, m(x) 
is really just "whatever value variable x happens to have on entry". We 
try very hard to express the result of the transfer function as an affine 
expression of values that are described by the input map. 
You should observe the proper interpretation of expressions like 
f (m) 
(x), where f is a transfer function, m a map, and x a variable. As 
is conventional in mathematics, we apply functions from the left, meaning 
that we first compute f (m), which is a map. Since a map is a function, 
we may then apply it to variable x to produce a value. 
assignment. The values of all other variables remain unchanged. 
The transfer function of statement s, denoted f,, is defined as follows: 
1. If s is not an assignment statement, then f, is the identity function. 
2 .  If s is an assignment statement to variable x, then 
m 
( 4  
for all variables u # x 
co + 
cl m(y) + 
c2m 
(x) if x is assigned c
o
 + 
CI y + 
C ~ Z ,  
= i 
(el = 
0, or m(y) # NAA) , and 
(c, = 
0, or m(x) # NAA) 
NAA 
otherwise. 
9.8. SYMBOLIC ANALYSIS 
693 
The expression co + 
clm(y) + 
c2m(z) 
is intended to represent all the possible 
forms of expressions involving arbitrary variables y and z that may appear on 
the right side of an assignment to x and that give x a value that is an afine 
transformation on prior values of variables. These expressions are: co, co + 
y, 
c
o
 - 
y, y + 
x, 
x - 
y, cl * y, and y/(l/cl). Note that in many cases, one or more 
of CO, c1, and c2 are 0. 
Example 9.60 : If the assignment is x=y+z, 
then co = 0 and cl = c2 = 1. If 
the assignment is x=y/5, then c
o
 = ca = 0, and cl = 115. 
Composition of Transfer Functions 
To compute fi 0 f17 where fl and f2 are defined in terms of input map m, 
we substitute the value of m(vi) in the definition of f2 with the definition of 
fl 
(m) 
(Vi). w e  replace all operations on NAA values with NAA. That is, 
1. If f2(m) 
(v) = NAA, 
then (f2 0 fl) 
(m) 
(v) = 
NAA. 
2. If f2(m)(v) = 
c
o
 + xi 
cim(vi), 
then 
if fl (m) 
(Vi) = 
NAA for some i # 0, 
Ci # 0 
= { 
c 
(m) 
( v )  otherwise 
Example 9.61 : 
The transfer functions of the blocks in Example 9.58 can be 
computed by composing the transfer functions of their constituent statements. 
These transfer functions are defined in Fig. 9.60. 
Figure 9.60: Transfer Functions of Example 9.58 
Solution to Data-Flow Problem 
We use the notation IN;,j[B3] 
and OUTi,j[B3] 
to refer to the input and output 
data-flow values of block B3 in iteration j of the inner loop and iteration i of 
the outer loop. For the other blocks, we use I N ~ [ B ~ ]  
and O U T ~ [ B ~ ]  
to refer 
to these values in the ith iteration of the outer loop. Also, we can see that 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
OUT[B~] 
= f~ 
(IN[B~]), 
for all Bk 
OUT[B~] 
2 1Nl[B2] 
ou~i[B2] > 1~i,l[B3], 
l < i < l O  
ou~i,j-l[B3] 2 1Nij[B3], 
1 
< i  < 100, 2 < 
j < 10 
OUT~,IO[&] 2 I N ~ [ B ~ ] ,  2 < i 5 100 
OUTi-l[B4] > INI[BZ], 
1 < i < 100 
Figure 9.61: Constraints satisfied on each iteration of the nested loops 
the symbolic maps shown in Fig. 9.58 satisfy the constraints imposed by the 
transfer functions, listed in Fig. 9.61. 
The first constraint says that the output map of a basic block is obtained 
by applying the block's transfer function to the input map. The rest of the 
constraints say that the output map of a basic block must be greater than or 
equal to the input map of a successor block in the execution. 
Note that our iterative data-flow algorithm cannot produce the above solu- 
tion because it lacks the concept of expressing data-flow values in terms of the 
number of iterations executed. Region-based analysis can be used to find such 
solutions, as we shall see in the next section. 
9.8.3 Region-Based Symbolic Analysis 
We can extend the region-based analysis described in Section 9.7 to find ex- 
pressions of variables in the ith iteration of a loop. A region-based symbolic 
analysis has a bottom-up pass and a top-down pass, like other region-based al- 
gorithms. The bottom-up pass summarizes the effect of a region with a transfer 
function that sends a symbolic map at the entry to an output symbolic map at 
the exit. In the top-down pass, values of symbolic maps are propagated down 
to the inner regions. 
The difference lies in how we handle loops. In Section 9.7, the effect of a loop 
is summarized with a closure operator. Given a loop with body f, 
its closure 
f 
* is defined as an infinite meet of all possible numbers of applications of f .  
However, to find induction variables, we need to determine if a value of a variable 
is an affine function of the number of iterations executed so far. The symbolic 
map must be parameterized by the number of the iteration being executed. 
Furthermore, whenever we know the total number of iterations executed in a 
loop, we can use that number to find the values of induction variables after the 
loop. For instance, in Example 9.58, we claimed that a has the value of i after 
executing the ith iteration. Since the loop has 100 iterations, the value of a 
must be 100 at the end of the loop. 
In what follows, we first define the primitive operators: meet and composi- 
tion of transfer functions for symbolic analysis. Then show how we use them 
to perform region-based analysis of induction variables. 
9.8. SYMBOLIC ANALYSIS 
Meet of Transfer Functions 
When computing the meet of two functions, the value of a variable is NAA unless 
the two functions map the variable to the same value and the value is not NAA. 
Thus, 
f l  (m>(v> 
if fl (m)(v) 
= 
f2(m)(v) 
f 
f 
= { NAA 
otherwise 
Parameterized Function Composit 
ions 
To express a variable as an affine function of a loop index, we need to compute 
the effect of composing a function some given number of times. If the effect of 
one iteration is summarized by transfer function f ,  then the effect of executing 
i iterations, for some i > 0, is denoted fi. Note that when i = 
0, f i  = 
f 0  = 
I ,  
the identify function. 
Variables in the program are divided into three categories: 
1. I f  f (m)(x) 
= 
m(x) 
+ 
c, where c is a constant, then fi(m)(x) 
= 
m(x) 
+ 
ci 
for every value of i > 0. We say that x is a basic induction variable of the 
loop whose body is represented by the transfer function f .  
2. If f(m)(x) 
= m(x), 
then fi(m)(x) 
= m(x) 
for all i > 0. The variable x 
is not modified and it remains unchanged at the end of any number of 
iterations through the loop with transfer function f. We say that x is a 
sgmbolic constant in the loop. 
3. If f (m) 
(x) 
= 
co + 
clm(xl) 
+ 
. . . 
+ 
cnm(xn), 
where each x
k
 is either a basic 
induction variable or a symbolic constant, then for i > 0, 
We say that x is also an induction variable, though not a basic one. Note 
that the formula above does not apply if i = 0. 
4. In all other cases, f "m) 
(x) 
= 
NAA. 
To find the effect of executing a fixed number of iterations, we simply replace 
i 
above by that number. In the case where the number of iterations is unknown, 
the value at the start of the last iteration is given by f * . In this case, the only 
variables whose values can still be expressed in the affine form are the loop- 
invariant variables. 
m 
(v) 
if f (m) 
(v) 
= 
m(v) 
f * m v  
= { A 
otherwise 
Example 9.62 : 
For the innermost loop in Example 9.58, the effect of executing 
i iterations, i > 0, is summarized by fh3. From the definition of fs3, 
we see 
that a and b are symbolic constants, c is a basic induction variable as it is 
696 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
incremented by one every iteration. d is an induction variable because it is an 
affine function the symbolic constant b and basic induction variable c. Thus, 
If we could not tell how many times the loop of block B3 iterated, then we 
could not use f and would have to use f * to express the conditions at the end 
of the loop. In this case, we would have 
A Region-Based Algorithm 
Algorithm 9.63 : 
Region-based symbolic analysis. 
INPUT: A reducible flow graph G. 
OUTPUT: Symbolic maps IN[B] 
for each block B of G. 
METHOD: We make the following modifications to Algorithm 9.53. 
1. We change hob we construct the transfer function for a loop region. In 
the original algorithm we use the fR,IN[sl transfer function to map the 
symbolic map at the entry of loop region R to a symbolic map at the 
entry of loop body S after executing an unknown number of iterations. It 
is defined to be the closure of the transfer function representing all paths 
leading back to the entry of the loop, as shown in Fig. 9.50(b). Here we 
define fR,i,INISl to represent the effect of execution from the start of the 
loop region to the entry of the ith iteration. Thus, 
i- 1 
~ R , ~ , I N [ s ]  
= ( 
A 
fs,OUT[B]) 
predecessors B in R of the header of S 
2. If the number of iterations of a region is known, the summary of the region 
is computed by replacing i with the actual count. 
3. In the top-down pass, we compute fR,i,INIB] to find the symbolic map 
associated with the entry of the ith iteration of a loop. 
9.8. SYMBOLIC ANALYSIS 
697 
4. In the case where the input value of a variable m(v) 
is used on the right- 
hand-side of a symbolic map in region R, and m(v) 
= 
NAA upon entry to 
thqregion, we introduce a new reference variable t, add assignment t = v 
to the beginning of region R, and all references of m(v) 
are replaced by t. 
If we did not introduce a reference variable at this point, the NAA value 
held by v would penetrate into inner loops. 
Figure 9.62: Transfer function relations in the bottom-up pass for Example 9.58. 
Exarnple 9.64 : 
For Example 9.58, we show how the transfer functions for the 
program are computed in the bottom-up pass in Fig. 9.62. Region R5 is the 
inner loop, with body B5. The transfer function representing the path from the 
entry of region R5 to the beginning of the jth iteration, j 2 1, is f&'. 
The 
transfer function representing the path to the end of the jth iteration, j 2 1, 
is f i 3  
. 
Region Rs consists of blocks B2 and B4, 
with loop region R5 in the middle. 
The transfer functions from the entry of B2 and R5 can be computed in the 
same way as in the original algorithm. Transfer function fR6,0UT[B31 represents 
the composition of block Ba and the entire execution of the inner loop, since 
f~~ is the identity function. Since the inner loop is known to iterate 10 times, 
we can replace j by 10 to summarize the effect of the inner loop precisely. The 
698 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Figure 9.63: Transfer functions computed in the bottom-up pass for Exam- 
ple 8.58 
f 
fR5,j,IN[Bs] 
f
~
~
,
 
~,ouT[B~] 
~ R ~ , I N [ B ~ ]  
f
~
e
 
, I N [ R ~ ]  
~ R ~ , o u T [ B ~ ~  
~ R ~ , ~ , I N [ R B ]  
~ R ~ , ~ , o u T [ B ~ ]  
~ R ~ , I N [ B ~ ~  
fR8 ,IN[R?I 
fR8 ,OUT[B~] 
rest of the transfer functions can be computed in a similar manner. The actual 
transfer functions computed are shown in Fig. 9.63. 
The symbolic map at the entry of the program is simply r
n
,
,
,
.
 
We use the 
top-down pass to compute the symbolic map to the entry to successively nested 
regions until we find all the symbolic maps for every basic block. We start by 
computing the data-flow values for block B1 in region R8: 
Descending down to regions R7 and Rs, 
we get 
f (m) 
(a) 
m(a) 
m(a) 
m(a) 
m(a) 
+ 
1 
m(a) 
+ 
m(a) 
f - 
l 
m(a) 
+ 
m(a) 
0 
loo 
Finally, in region Rs, 
we get 
Not surprisingly, these equations produce the results we showed in Fig. 9.58. 
f (4 
(b) 
db) 
4) 
4 4  
lOm(a) 
+ 
10 
+ lo 
NAA 
+ lei 
m 
(b) 
m(b) 
1000 
Example 9.58 shows a simple program where every variable used in the 
symbolic map has an affine expression. We use Example 9.65 to illustrate why 
and how we introduce reference variables in Algorithm 9.63. 
f 
(m) 
(c> 
m(c) 
+ 
j - 
1 
m(c)+j 
(c) 
0 
lo 
N 
A 
A 
lo 
(4 
m(c) 
1
0
 
f 
(m) 
(4 
NAA 
m(b)+m(c)+ 
j - 1  
(4 
m(d) 
10m(a) 
+ 
9 
NAA 
lOm(a)+ 
1
O
i
 + 
9 
m(d) 
m(d) 
1009 
9.8. SYMBOLIC ANALYSIS 
1) f o r  ( i  = 1 ;  i < n; i++) 
( 
2 
) 
a = i n p u t 0 ;  
3 
f o r  ( j  = 1 ;  j < 10; j++) ( 
4) 
a = a - I ;  
5 
> 
b = j + a ;  
6 
) 
a = a +  
1; 
3 
1 
(a) A loop where a fluctuates. 
f o r  ( i  = 1 ;  i < n; i++) 
( 
(b) A reference variable t makes b an induction variable. 
Figure 9.64: The need to introduce reference variables 
Example 9.65 : Consider the simple example in Fig. 9.64(a). Let f j  be the 
transfer function summarizing the effect of executing j iterations of the inner 
loop. Even though the value of a may fluctuate during the execution of the 
loop, we see that b is an induction variable based on the value of a on entry of 
the loop; that is, f j  (m) 
(b) = 
m(a) - 
1 
+ 
j 
. Because a is assigned an input value, 
the symbolic map upon entry to the inner loop maps a to NAA. We introduce 
a new reference variable t to save the value of a upon entry, and perform the 
substitutions as in Fig. 9.64(b). 
9.8.4 Exercises for Section 9.8 
Exercise 9.8.1 
: 
For the flow graph of Fig. 9.10 (see the exercises for Section 
9.1), give the transfer functions for 
a) Block B2. 
b) Block B4 
c) Block B5. 
700 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
Exercise 9.8.2 : Consider the inner loop of Fig. 9.10, consisting of blocks B3 
and B4. If i represents the number of times around the loop, and f is the 
transfer function for the loop body (i.e., excluding the edge from B4 to B3) 
from the entry of the loop (i.e., the beginning of B3) to the exit from B4, 
then 
what is fV 
Remember that f takes as argument a map m, and m assigns a 
value to each of variables a, b, d, and e. We denote these values m(a), and so 
on, although we do not know their values. 
! Exercise 9.8.3 : 
Now consider the outer loop of Fig. 9.10, consisting of blocks 
B2, 
B3, 
B4, 
and B5. Let g be the transfer function for the loop body, from the 
entry of the loop at B2 
to its exit at B5. Let i measure the number of iterations 
of the inner loop of B3 and B4 (which count of iterations we cannot know), and 
let j measure the number of iterations of the outer loop (which we also cannot 
know). What is gj? 
9.9 Summary of Chapter 9 
+ Global Common Subexpressions: An important optimization is finding 
computations of the same expression in two different basic blocks. If one 
precedes the other, we can store the result the first time it is computed 
and use the stored result on subsequent occurrences. 
O Copy Propagation: A copy statement, u = u, assigns one variable u to 
another, u. In some circumstances, we can replace all uses of u by u, thus 
eliminating both the assignment and u. 
O Code Motion: Another optimization is to move a computation outside the 
loop in which it appears. This change is only correct if the computation 
produces the same value each time around the loop. 
+ Induction Variables: Many loops have induction variables, variables that 
take on a linear sequence of values each time around the loop. Some of 
these are used only to count iterations, and they often can be eliminated, 
thus reducing the time it takes to go around the loop. 
+ Data-Flow Analysis: A data-flow analysis schema defines a value at each 
point in the program. Statements of the program have associated transfer 
functions that relate the value before the statement to the value after. 
Statements with more than one predecessor must have their value defined 
by combining the values at the predecessors, using a meet (or confluence) 
operator. 
+ Data-Flow Analysis on Basic Blocks: Because the propagation of data- 
flow values within a block is usually quite simple, data-flow equations 
are generally set up to have two variables for each block, called IN and 
OUT, that represent the data-flow values at the beginning and end of the 
9,9. SUMMARY OF CHAPTER 9 
70 1 
block, respectively. The transfer functions for the statements in a block 
are composed to get the transfer function for the block as a whole. 
+ Reaching Definitions: The reaching-definitions data-flow framework has 
values that are sets of statements in the program that define values for 
one or more variables. The transfer function for a block kills definitions 
of variables that are definitely redefined in the block and adds ("gener- 
ates") in those definitions of variables that occur within the block. The 
confluence operator is union, since definitions reach a point if they reach 
any predecessor of that point. 
+ Live Variables: Another important data-flow framework computes the 
variables that are live (will be used before redefinition) at each point. 
The framework is similar to reaching definitions, except that the transfer 
function runs backward. A variable is live at the beginning of a block if 
it is either used before definition in the block or is live at the end of the 
block and not redefined in the block. 
+ Available Expressions: To discover global common subexpressions, we 
determine the available expressions at each point - 
expressions that have 
been computed and neither of the expression's arguments were redefined 
after the last computation. The data-flow framework is similar to reaching 
definitions, but the confluence operator is intersection rather than union. 
+ Abstraction of Data-Flow Problems: Common data-flow problems, such 
as those already mentioned, can be expressed in a common mathematical 
structure. The values are members of a semilattice, whose meet is the 
confluence operator. Transfer functions map lattice elements to lattice 
elements. The set of allowed transfer functions must be closed under 
composition and include the identity function. 
+ Monotone Frameworks: A semilattice has a 5 relation defined by a 5 b 
if and only if a A b = a. Monotone frameworks have the property that 
each transfer function preserves the 5 relationship; that is, a 5 b implies 
f (a) 5 f (b), 
for all lattice elements a and b and transfer function f .  
+ Distributive Frameworks: These frameworks satisfy the condition that 
f ( a ~  
b) = f (a) 
A f (b), for all lattice elements a and b and transfer function 
f . It can be shown that the distributive condition implies the monotone 
condition. 
+ Iterative Solution to Abstract Frameworks: All monotone data-flow frame- 
works can be solved by an iterative algorithm, in which the IN and 
OUT values for each block are initialized appropriately (depending on 
the framework), and new values for these variables are repeatedly com- 
puted by applying the transfer and confluence operations. This solution 
is always safe (optimizations that it suggests will not change what the 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
program does), but the solution is certain to be the best possible only if 
the framework is distributive. 
+ The Constant Propagation Framework: While the basic frameworks such 
as reaching definitions are distributive, there are interesting monotone- 
but-not-distributive frameworks as well. One involves propagating con- 
stants by using a semilattice whose elements are mappings from the pro- 
gram variables to constants, plus two special values that represent "no 
information" and "definitely not a constant 
." 
+ Partial-Redundancy Elimination: Many useful optimizations, such as code 
motion and global common-subexpression elimination, can be generalized 
to a single problem called partial-redundancy elimination. Expressions 
that are needed, but are available along only some of the paths to a 
point, are computed only along the paths where they are not available. 
The correct application of this idea requires the solution to a sequence of 
four different data-flow problems plus other operations. 
+ Dominators: A node in a flow graph dominates another if every path to 
the latter must go through the former. A proper dominator is a dominator 
other than the node itself. Each node except the entry node has an imme- 
diate dominator - 
that one of its proper dominators that is dominated 
by all the other proper dominators. 
+ Depth-First Ordering o
f
 Flow Graphs: If we perform a depth-first search 
of a flow graph, starting at its entry, we produce a depth-first spanning 
tree. The depth-first order of the nodes is the reverse of a postorder 
traversal of this tree. 
+ Classification of Edges: When we construct a depth-first spanning tree, 
all the edges of the flow graph can be divided into three groups: advanc- 
ing edges (those that go from ancestor to proper descendant), retreating 
edges (those from descendant to ancestor) and cross edges (others). An 
important property is that all the cross edges go from right to left in the 
tree. Another important property is that of these edges, only the retreat- 
ing edges have a head lower than its tail in the depth-first order (reverse 
postorder). 
+ Back Edges: A back edge is one whose head dominates its tail. Every 
back edge is a retreating edge, regardless of which depth-first spanning 
tree for its flow graph is chosen. 
+ Reducible Flow Graphs: If every retreating edge is a back edge, regardless 
of which depth-first spanning tree is chosen, then the flow graph is said to 
be reducible. The vast majority of flow graphs are reducible; those whose 
only control-flow statements are the usual loop-forming and branching 
statements are certainly reducible. 
9.10. REFERENCES FOR CHAPTER 9 
703 
+ Natural Loops: A natural loop is a set of nodes with a header node that 
dominates all the nodes in the set and has at least one back edge entering 
that node. Given any back edge, we can construct its natural loop by 
taking the head of the edge plus all nodes that can reach the tail of the 
edge without going through the head. Two natural loops with different 
headers are either disjoint or one is completely contained in the other; 
this fact lets us talk about a hierarchy of nested loops, as long as "loops" 
are taken to be natural loops. 
+ Depth-First Order Makes the Iterative Algorithm Eficient: The iterative 
algorithm requires few passes, as long as propagation of information along 
acyclic paths is sufficient; i.e., cycles add nothing. If we visit nodes in 
depth-first order, any data-flow framework that propagates information 
forward, e.g., reaching definitions, will converge in no more than 2 plus 
the largest number of retreating edges on any acyclic path. The same 
holds for backward-propagating frameworks, like live variables, if we visit 
in the reverse of depth-first order (i.e., in postorder). 
+ Regions: Regions are sets of nodes and edges with a header h that domi- 
nates all nodes in the region. The predecessors of any node other than h 
in the region must also be in the region. The edges of the region are all 
that go between nodes of the region, with the possible exception of some 
or all that enter the header. 
+ Regions and Reducible Flow Graphs: Reducible flow graphs can be parsed 
into a hierarchy of regions. These regions are either loop regions, which 
include all the edges into the header, or body regions that have no edges 
into the header. 
+ Region-Based Data-Flow Analysis: An alternative to the iterative ap- 
proach to data-flow analysis is to work up and down the region hierarchy, 
computing transfer functions from the header of each region to each node 
in that region. 
+ Region-Based Induction Variable Detection: An important application of 
region-based analysis is in a data-flow framework that tries to compute 
formulas for each variable in a loop region whose value is an affine (linear) 
function of the number of times around the loop. 
9.10 References for Chapter 9 
Two early compilers that did extensive code optimization are Alpha [7] and 
Fortran H [16]. The fundamental treatise on techniques for loop optimization 
(e.g., code motion) is [I], 
although earlier versions of some of these ideas appear 
in [8]. An informally distributed book [4] was influential in disseminating code- 
optimization ideas. 
704 
CHAPTER 9. MACHINE-INDEPENDENT OPTIMIZATIONS 
The first description of the iterative algorithm for data-flow analysis is from 
the unpublished technical report of Vyssotsky and Wegner [20]. The scientific 
study of data-flow analysis is said to begin with a pair of papers by Allen [2] 
and Cocke [3]. 
The lattice-theoretic abstraction described here is based on the work of Kil- 
dall [13]. These frameworks assumed distributivity, which many frameworks do 
not satisfy. After a number of such frameworks came to light, the monotonicity 
condition was embedded in the model by [5] and [Ill. 
Partial-redundancy elimination was pioneered by [17]. The lazy-code-mo- 
tion algorithm described in this chapter is based on [14] 
Dominators were first used in the compiler described in [13]. However, the 
idea dates back to [18]. 
The notion of reducible flow graphs comes from [2]. The structure of these 
flow graphs, as presented here, is from [9] and [lo]. [12] and [15] first connected 
reducibility of flow graphs to the common nested control-flow structures, which 
explains why this class of flow graphs is so common. 
The definition of reducibility by TI-T2 reduction, as used in region-based 
analysis, is from [19]. The region-based approach was first used in a compiler 
described in [2 
11. 
The static single-assignment (SSA) form of intermediate representation in- 
troduced in Section 6.1 incorporates both data flow and control flow into its 
representation. SSA facilitates the implementation of many optimizing trans- 
formations from a common framework [6]. 
1. Allen, F. E., "Program optimization," Annual Review in Automatic Pro- 
gramming 5 (1969), pp. 239-307. 
2. Allen, F. E., "Control flow analysis," ACM Sigplan Notices 5:7 (1970), 
pp. 1-19. 
3. Cocke, J., "Global common subexpression elimination," A CM SIGPLAN 
Notices 5:7 (1970), pp. 20-24. 
4. Cocke, J. and J. T. Schwartz, Programming Languages and Their Com- 
pilers: Preliminary Notes, Courant Institute of Mathematical Sciences, 
New York Univ., New York, 1970. 
5. Cousot, P. and R. Cousot, "Abstract interpretation: a unified lattice 
model for static analysis of programs by construction or approximation of 
fixpoints," Fourth ACM Symposium on Principles of Programming Lan- 
guages (1977), pp. 238-252. 
6. Cytron, R., J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck, 
"Efficiently computing static single assignment form and the control de- 
pendence graph," A CM Transactions on Programming Languages and 
Systems 13:4 (1991), pp. 451-490. 
9.10. REFERENCES FOR CHAPTER 9 
705 
7. Ershov, A. P., "Alpha - 
an automatic programming system of high effi- 
ciency," J. ACM 13:l (1966), pp. 17-24. 
8. Gear, C. W., "High speed compilation of efficient object code," Comm. 
ACM 8:8 (1965), pp. 483-488. 
9. Hecht, M. S. and J. D. Ullman, "Flow graph reducibility," SIAM J. Com- 
puting 1 (1972), pp. 188-202. 
10. Hecht, M. S. and J. D. Ullman, "Characterizations of reducible flow 
graphs," J. ACM 21 (1974), pp. 367-375. 
11. Kam, J. B. and J. D. Ullman, "Monotone data flow analysis frameworks," 
Acta Inforrnatica 7:3 (1977), pp. 305-318. 
12. Kasami, T., W. W. Peterson, and N. Tokura, "On the capabilities of while, 
repeat, and exit statements," Comm. ACM 16:8 (1973), pp. 503-512. 
13. Kildall, G., "A unified approach to global program optimization," A 
CM 
Symposium on Principles of Programming Languages (1973), pp. 194-206. 
14. Knoop, J., "Lazy code motion," Proc. ACM SIGPLAN 1992 conference 
on Programming Language Design and Implementation, pp. 224-234. 
15. Kosaraju, S. R., "Analysis of structured programs," J. Computer and 
System Sciences 9:3 (1974), pp. 232-255. 
16. Lowry, E. S. and C. W. Medlock, "Object code optimization," Comm. 
ACM 12:l (1969), pp. 13-22. 
17. Morel, E. and C. Renvoise, "Global optimization by suppression of partial 
redundancies," Comm. ACM 22 (1979), pp. 96-103. 
18. Prosser, R. T., "Application of boolean matrices to the analysis of flow 
diagrams," AFIPS Eastern Joint Computer Conference (1959), Spartan 
Books, Baltimore MD, pp. 133-138. 
19. Ullman, J. D., "Fast algorithms for the elimination of common subexpres- 
sions," Acta Inforrnatica 2 (1973), pp. 191-213. 
20. Vyssotsky, V. and P. Wegner, "A graph theoretical Fortran source lan- 
guage analyzer," unpublished technical report, Bell Laboratories, Murray 
Hill NJ, 1963. 
21. Wulf, W. A., R. K. Johnson, C. B. Weinstock, S. 0. Hobbs, and C. M. 
Geschke, The Design of an Optimizing Compiler, Elsevier, New York, 
1975. 
Chapter 10 
Instruct 
ion-Level 
Parallelism 
Every modern high-performance processor can execute several operations in a 
single clock cycle. The "billion-dollar question" is how fast can a program be 
run on a processor with instruction-level parallelism? The answer depends on: 
1. The potential parallelism in the program. 
2. The available parallelism on the processor. 
3. Our ability to extract parallelism from the original sequential program. 
4. Our ability to find the best parallel schedule given scheduling constraints. 
If all the operations in a program are highly dependent upon one another, 
then no amount of hardware or parallelization techniques can make the program 
run fast in parallel. There has been a lot of research on understanding the 
limits of parallelization. Typical nonnumeric applications have many inherent 
dependences. For example, these programs have many data-dependent branches 
that make it hard even to predict which instructions are to be executed, let alone 
decide which operations can be executed in parallel. Therefore, work in this area 
has focused on relaxing the scheduling constraints, including the introduction 
of new architectural features, rather than the scheduling techniques themselves. 
Numeric applications, such as scientific computing and signal processing, 
tend to have more parallelism. These applications deal with large aggregate 
data structures; operations on distinct elements of the structure are often inde- 
pendent of one another and can be executed in parallel. Additional hardware 
resources can take advantage of such parallelism and are provided in high- 
performance, general-purpose machines and digital signal processors. These 
programs tend to have simple control structures and regular data-access pat- 
terns, and static techniques have been developed to extract the available paral- 
lelism from these programs. Code scheduling for such applications is interesting 
CHAPTER 10. INSTRUCTION-LE 
VEL PARALLELISM 
and significant, as they offer a large number of independent operations to be 
mapped onto a large number of resources. 
Both parallelism extraction and scheduling for parallel execution can be 
performed either statically in software, or dynamically in hardware. In fact, 
even machines with hardware scheduling can be aided by software scheduling. 
This chapter starts by explaining the fundamental issues in using instruction- 
level parallelism, which is the same regardless of whether the parallelism is 
managed by software or hardware. We then motivate the basic data-dependence 
analyses needed for the extraction of parallelism. These analyses are useful for 
many optimizations other than instruction-level parallelism as we shall see in 
Chapter 11. 
Finally, we present the basic ideas in code scheduling. We describe a tech- 
nique for scheduling basic blocks, a method for handling highly data-dependent 
control flow found in general-purpose programs, and finally a technique called 
software pipelining that is used primarily for scheduling numeric programs. 
0 . 1  Processor Architectures 
When we think of instruction-level parallelism, we usually imagine a processor 
issuing several operations in a single clock cycle. In fact, it is possible for 
a machine to issue just one operation per clock1 and yet achieve instruction- 
level parallelism using the concept of pipelining. In the following, we shall first 
explain pipelining then discuss multiple-instruction issue. 
10.1.1 Instruction Pipelines and Branch Delays 
Practically every processor, be it a high-performance supercomputer or a stan- 
dard machine, uses an instruction pipeline. With an instruction pipeline, a 
new instruction can be fetched every clock while preceding instructions are still 
going through the pipeline. Shown in Fig. 10.1 is a simple 5-stage instruction 
pipeline: it first fetches the instruction (IF), decodes it (ID), executes the op- 
eration (EX), accesses the memory (MEM), and writes back the result (WB). 
The figure shows how instructions i, 
i + 
1, i + 
2, i + 
3, and i + 
4 can execute at 
the same time. Each row corresponds to a clock tick, and each column in the 
figure specifies the stage each instruction occupies at each clock tick. 
If the result from an instruction is available by the time the succeeding in- 
struction needs the data, the processor can issue an instruction every clock. 
Branch instructions are especially problematic because until they are fetched, 
decoded and executed, the processor does not know which instruction will ex- 
ecute next. Many processors speculatively fetch and decode the immediately 
succeeding instructions in case a branch is not taken. When a branch is found 
to be taken, the instruction pipeline is emptied and the branch target is fetched. 
l
~
e
 
shall refer to a clock "tick" or clock cycle simply as a "clock," when the intent is 
clear. 
10.1. PROCESSOR ARCHITECTURES 
1. IF 
2. 
ID 
3. 
EX 
4. 
MEM 
5. 
WB 
6. 
7. 
8. 
9. 
IF 
ID 
IF 
EX 
ID 
IF 
MEM 
EX 
ID 
IF 
WB 
MEM 
EX 
ID 
WB 
MEM 
EX 
WB 
MEM 
WB 
Figure 10.1: Five consecutive instructions in a 5-stage instruction pipeline 
Thus, taken branches introduce a delay in the fetch of the branch target and 
introduce "hiccups" in the instruction pipeline. Advanced processors use hard- 
ware to predict the outcomes of branches based on their execution history and 
to prefetch from the predicted target locations. Branch delays are nonetheless 
observed if branches are mispredicted. 
10.1.2 Pipelined Execution 
Some instructions take several clocks to execute. One common example is the 
memory-load operation. Even when a memory access hits in the cache, it usu- 
ally takes several clocks for the cache to return the data. We say that the 
execution of an instruction is pipelined if succeeding instructions not dependent 
on the result are allowed to proceed. Thus, even if a processor can issue only 
one operation per clock, several operations might be in their execution stages 
at the same time. If the deepest execution pipeline has n stages, potentially 
n operations can be '5n flight" at the same time. Note that not all instruc- 
tions are fully pipelined. While floating-point adds and multiplies often are 
fully pipelined, floating-point divides, being more complex and less frequently 
executed, often are not. 
Most general-purpose processors dynamically detect dependences between 
consecutive instructions and automatically stall the execution of instructions if 
their operands are not available. Some processors, especially those embedded 
in hand-held devices, leave the dependence checking to the software in order to 
keep the hardware simple and power consumption low. In this case, the compiler 
is responsible for inserting "no-op" instructions in the code if necessary to assure 
that the results are available when needed. 
710 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
10.1.3 Multiple Instruction Issue 
By issuing several operations per clock, processors can keep even more opera- 
tions in flight. The largest number of operations that can be executed simul- 
taneously can be computed by multiplying the instruction issue width by the 
average number of stages in the execution pipeline. 
Like pipelining, parallelism on multiple-issue machines can be managed ei- 
ther by software or hardware. Machines that rely on software to manage their 
parallelism are known as VLIW (Very-Long-Instruction-Word) machines, while 
those that manage their parallelism with hardware are known as superscalar 
machines. VLIW machines, as their name implies, have wider than normal 
instruction words that encode the operations to be issued in a single clock. 
The compiler decides which operations are to be issued in parallel and encodes 
the information in the machine code explicitly. Superscalar machines, on the 
other hand, have a regular instruction set with an ordinary sequential-execution 
semantics. Superscalar machines automatically detect dependences among in- 
structions and issue them as their operands become available. Some processors 
include both VLIW and superscalar functionality. 
Simple hardware schedulers execute instructions in the order in which they 
are fetched. If a scheduler comes across a dependent instruction, it and all 
instructions that follow must wait until the dependences are resolved (i.e., the 
needed results are available). Such machines obviously can benefit from having 
a static scheduler that places independent operations next to each other in the 
order of execution. 
More sophisticated schedulers can execute instructions "out of order." Op- 
erations are independently stalled and not allowed to execute until all the values 
they depend on have been produced. Even these schedulers benefit from static 
scheduling, because hardware schedulers have only a limited space in which to 
buffer operations that must be stalled. Static scheduling can place independent 
operations close together to allow better hardware utilization. More impor- 
tantly, regardless how sophisticated a dynamic scheduler is, it cannot execute 
instructions it has not fetched. When the processor has to take an unexpected 
branch, it can only find parallelism among the newly fetched instructions. The 
compiler can enhance the performance of the dynamic scheduler by ensuring 
that these newly fetched instructions can execute in parallel. 
10.2 Code-Scheduling Constraints 
Code scheduling is a form of program optimization that applies to the machine 
code that is produced by the code generator. Code scheduling is subject to 
three kinds of constraints: 
1. Control-dependence constraints. All the operations executed in the origi- 
nal program must be executed in the optimized one. 
CODE-SCHED ULING CONSTRAINTS 
2. Data-dependence constraints. The operations in the optimized program 
must produce the same results as the corresponding ones in the original 
program. 
3. Resource constraints. The schedule must not oversubscribe the resources 
on the machine. 
These scheduling constraints guarantee that the optimized program pro- 
duces the same results as the original. However, because code scheduling 
changes the order in which the operations execute, the state of the memory 
at any one point may not match any of the memory states in a sequential ex- 
ecution. This situation is a problem if a program's execution is interrupted 
by, for example, a thrown exception or a user-inserted breakpoint. Optimized 
programs are therefore harder to debug. Note that this problem is not specific 
to code scheduling but applies to all other optimizations, including partial- 
redundancy elimination (Section 9.5) and register allocation (Section 8.8). 
10.2.1 Data Dependence 
It is easy to see that if we change the execution order of two operations that do 
not touch any of the same variables, we cannot possibly affect their results. In 
fact, even if these two operations read the same variable, we can still permute 
their execution. Only if an operation writes to a variable read or written by 
another can changing their execution order alter their results. Such pairs of 
operations are said to share a data dependence, and their relative execution 
order must be preserved. There are three flavors of data dependence: 
1. True dependence: read after write. If a write is followed by a read of the 
same location, the read depends on the value written; such a dependence 
is known as a true dependence. 
Antidependence: write after read. If a read is followed by a write to the 
same location, we say that there is an antidependence from the read to 
the write. The write does not depend on the read per se, but if the write 
happens before the read, then the read operation will pick up the wrong 
value. Antidependence is a byprod~ict 
of imperative programming, where 
the same memory locations are used to store different values. It is not a 
"true" dependence and potentially can be eliminated by storing the values 
in different locations. 
3. Output dependence: write after write. Two writes to the same location 
share an output dependence. If the dependence is violated, the value of the 
memory location written will have the wrong value after both operations 
are performed. 
Antidependence and output dependences are referred to as storage-related de- 
pendences. These are not "true7' dependences and can be eliminated by using 
CHAPTER 10. INSTRUCTION-LE 
VEL PARALLELISM 
different locations to store different values. Note that data dependences apply 
to both memory accesses and register accesses. 
10.2.2 Finding Dependences Among Memory Accesses 
To check if two memory accesses share a data dependence, we only need to tell 
if they can refer to the same location; we do not need to know which location is 
being accessed. For example, we can tell that the two accesses *p and (*p)+4 
cannot refer to the same location, even though we may not know what p points 
to. Data dependence is generally undecidable at compile time. The compiler 
must assume that operations may refer to the same location unless it can prove 
otherwise. 
Example 10.1 : 
Given the code sequence 
unless the compiler knows that p cannot possibly point to a, it must conclude 
that the three operations need to execute serially. There is an output depen- 
dence flowing from statement (I) to statement (2), and there are two true 
dependences flowing from statements (I) 
and (2) to statement (3). 
Data-dependence analysis is highly sensitive to the programming language 
used in the program. For type-unsafe languages like C and C++, where a 
pointer can be cast to point to any kind of object, sophisticated analysis is 
necessary to prove independence between any pair of pointer-based memory ac- 
cesses. Even local or global scalar variables can be accessed indirectly unless we 
can prove that their addresses have not been stored anywhere by any instruc- 
tion in the program. In type-safe languages like Java, objects of different types 
are necessarily distinct from each other. Similarly, local primitive variables on 
the stack cannot be aliased with accesses through other names. 
A correct discovery of data dependences requires a number of different forms 
of analysis. We shall focus on the major questions that must be resolved if the 
compiler is to detect all the dependences that exist in a program, and how to 
use this information in code scheduling. Later chapters show how these analyses 
are performed. 
Array Data-Dependence Analysis 
Array data dependence is the problem of disambiguating between the values of 
indexes in array-element accesses. For example, the loop 
for ( i  = 0 ;  i < n; i++) 
A [2*il = A [2*i+1] ; 
10.2. CODE-SCHED 
ULING CONSTRAINTS 
copies odd elements in the array A to the even elements just preceding them. 
Because all the read and written locations in the loop are distinct from each 
other, there are no dependences between the accesses, and all the iterations in 
the loop can execute in parallel. Array data-dependence analysis, often referred 
to simply as data-dependence analysis, is very important for the optimization 
of numerical applications. This topic will be discussed in detail in Section 11.6. 
Pointer- 
Alias Analysis 
We say that two pointers are aliased if they can refer to the same object. 
Pointer-alias analysis is difficult because there are many potentially aliased 
pointers in a program, and they can each point to an unbounded number of 
dynamic objects over time. To get any precision, pointer-alias analysis must be 
applied across all the functions in a program. This topic is discussed starting 
in Section 12.4. 
Int 
erprocedural Analysis 
For languages that pass parameters by reference, interprocedural analysis is 
needed to determine if the same variable is passed as two or more different 
arguments. Such aliases can create dependences between seemingly distinct 
parameters. Similarly, global variables can be used as parameters and thus 
create dependences between parameter accesses and global variable accesses. 
Interprocedural analysis, discussed in Chapter 12, is necessary to determine 
these aliases. 
10.2.3 Tradeoff Between Register Usage and Parallelism 
In this chapter we shall assume that the machine-independent intermediate rep- 
resentation of the source program uses an unbounded number of pseudoregisters 
to represent variables that can be allocated to registers. These variables include 
scalar variables in the source program that cannot be referred to by any other 
names, as well as temporary variables that are generated by the compiler to 
hold the partial results in expressions. Unlike memory locations, registers are 
uniquely named. Thus precise data-dependence constraints can be generated 
for register accesses easily. 
The unbounded number of pseudoregisters used in the intermediate repre- 
sentation must eventually be mapped to the small number of physical registers 
available on the target machine. Mapping several pseudoregisters to the same 
physical register has the unfortunate side effect of creating artificial storage 
dependences that constrain instruction-level parallelism. Conversely, executing 
instructions in parallel creates the need for more storage to hold the values being 
computed simultaneously. Thus, the goal of minimizing the number of registers 
used conflicts directly with the goal of maximizing instruction-level parallelism. 
Examples 10.2 and 10.3 below illustrate this classic trade-off between storage 
and parallelism. 
CHAPTER 10. INSTRUCTION-LE VEL 
PARALLELISM 
Hardware Register Renaming 
Instruction-level parallelism was first used in computer architectures as a 
means to speed up ordinary sequential machine code. Compilers at the 
time were not aware of the instruction-level parallelism in the machine and 
were designed to optimize the use of registers. They deliberately reordered 
instructions to minimize the number of registers used, and as a result, also 
minimized the amount of parallelism available. Example 10.3 illustrates 
how minimizing register usage in the computation of expression trees also 
limits its parallelism. 
There was so little parallelism left in the sequential code that com- 
puter architects invented the concept of hardware register renaming to 
undo the effects of register optimization in compilers. Hardware register 
renaming dynamically changes the assignment of registers as the program 
runs. It interprets the machine code, stores values intended for the same 
register in different internal registers, and updates all their uses to refer 
to the right registers accordingly. 
Since the artificial register-dependence constraints were introduced 
by the compiler in the first place, they can be eliminated by using a 
register-allocation algorithm that is cognizant of instruction-level paral- 
lelism. Hardware register renaming is still useful in the case when a ma- 
chine's instruction set can only refer to a small number of registers. This 
capability allows an implementation of the architecture to map the small 
number of architectural registers in the code to a much larger number of 
internal registers dynamically. 
Example 10.2 : 
The code below copies the values of variables in locations a 
and c to variables in locations b and d, respectively, using pseudoregisters t1 
and t2. 
LD t l ,  a 
// tl = a 
ST b, tl 
// b 
= t1 
LD t 2 ,  c 
// t 2  = c 
S T d , t 2  
/ / d  = t 2  
I
f
 all the memory locations accessed are known to be distinct from each other, 
then the copies can proceed in parallel. However, if tl 
and t 2  
are assigned the 
same register so as to minimize the number of registers used, the copies are 
necessarily serialized. 
Example 10.3 
: Traditional register-allocation techniques aim to minimize 
the number of registers used when performing a computation. Consider the 
expression 
10.2. CODE-SCHED 
ULING CONSTRAINTS 
Figure 10.2: Expression tree in Example 10.3 
shown as a syntax tree in Fig. 10.2. It is possible to perform this computation 
using three registers, as illustrated by the machine code in Fig. 10.3. 
L
D
 rl, a 
// rl = a 
L
D
 r 2 ,  b 
// r 2  = b 
ADD r1, rl, r 2  // rl = r l + r 2  
L
D
 r 2 ,  c 
// r 2  = c 
ADD rl, rl, r 2  // rl = r l + r 2  
L
D
 r 2 ,  d 
// r 2  = d 
L
D
 r 3 ,  e 
// r 3  = e 
ADD r 2 ,  r 2 ,  r 3  // r 2  = r2+r3 
ADD r1, r1, r 2  // r1 = r l + r 2  
Figure 10.3: Machine code for expression of Fig. 10.2 
The reuse of registers, however, serializes the computation. The only oper- 
ations allowed to execute in parallel are the loads of the values in locations a 
and b, and the loads of the values in locations d and e. It thus takes a total of 
7 steps to complete the computation in parallel. 
Had we used different registers for every partial sum, the expression could 
be evaluated in 4 steps, which is the height of the expression tree in Fig. 10.2. 
The parallel computation is suggested by Fig. 10.4. 
Figure 10.4: Parallel evaluation of the expression of Fig. 10.2 
716 
CHAPTER 10. INSTRUCTION-LE 
VEL PARALLELISM 
10.2.4 Phase Ordering Between Register Allocation and 
Code Scheduling 
If registers are allocated before scheduling, the resulting code tends to have 
many storage dependences that limit code scheduling. On the other hand, if 
code is scheduled before register allocation, the schedule created may require 
so many registers that register spzllzng (storing the contents of a register in 
a memory location, so the register can be used for some other purpose) may 
negate the advantages of instruction-level parallelism. Should a compiler allo- 
cate registers first before it schedules the code? Or should it be the other way 
round? Or, do we need to address these two problems at the same time? 
To answer the questions above, we must consider the characteristics of the 
programs being compiled. Many nonnumeric applications do not have that 
much available parallelism. It suffices to dedicate a small number of registers 
for holding temporary results in expressions. We can first apply a coloring 
algorithm, as in Section 8.8.4, to allocate registers for all the nontemporary 
variables, then schedule the code, and finally assign registers to the temporary 
variables. 
This approach does not work for numeric applications where there are many 
more large expressions. We can use a hierarchical approach where code is op- 
timized inside out, starting with the innermost loops. Instructions are first 
scheduled assuming that every pseudoregister will be allocated its own physical 
register. Register allocation is applied after scheduling and spill code is added 
where necessary, and the code is then rescheduled. This process is repeated for 
the code in the outer loops. When several inner loops are considered together 
in a common outer loop, the same variable may have been assigned different 
registers. We can change the register assignment to avoid having to copy the 
values from one register to another. In Section 10.5, we shall discuss the in- 
teraction between register allocation and scheduling further in the context of a 
specific scheduling algorithm. 
10.2.5 Control Dependence 
Scheduling operations within a basic block is relatively easy because all the 
instructions are guaranteed to execute once control flow reaches the beginning 
of the block. Instructions in a basic block can be reordered arbitrarily, as long as 
all the data dependences are satisfied. Unfortunately, basic blocks, especially in 
nonnumeric programs, are typically very small; on average, there are only about 
five instructions in a basic block. In addition, operations in the same block are 
often highly related and thus have little parallelism. Exploiting parallelism 
across basic blocks is therefore crucial. 
An optimized program must execute all the operations in the original pro- 
gram. It can execute more instructions than the original, as long as the extra 
instructions do not change what the program does. Why would executing extra 
instructions speed up a program's execution? If we know that an instruction 
20.2. CODE-SCHEDULING CONSTRAINTS 
71 
7 
is likely to be executed, and an idle resource is available to perform the opera- 
tion "for free," we can execute the instruction speculatively. The program runs 
faster when the speculation turns out to be correct. 
An instruction il is said to be control-dependent on instruction iz if the 
outcome of i2 
determines whether il is to be executed. The notion of control 
dependence corresponds to the concept of nesting levels in block-structured 
programs. Specifically, in the if-else statement 
if (c) s l ;  e l s e  s2; 
sl 
and s2 are control dependent on c. Similarly, in the while-statement 
while (c) s ;  
the body s is control dependent on c. 
Example 10.4 : 
In the code fragment 
the statements b = a*a and d = a+c have no data dependence with any other 
part of the fragment. The statement b = a*a depends on the comparison a > t. 
The statement d = a+c, however, does not depend on the comparison and can 
be executed any time. Assuming that the multiplication a * a does not cause 
any side effects, it can be performed speculatively, as long as b is written only 
after a is found to be greater than t. 
10.2.6 Speculative Execution Support 
Memory loads are one type of instruction that can benefit greatly from specula- 
tive execution. Memory loads are quite common, of course. They have relatively 
long execution latencies, addresses used in the loads are commonly available in 
advance, and the result can be stored in a new temporary variable without 
destroying the value of any other variable. Unfortunately, memory loads can 
raise exceptions if their addresses are illegal, so speculatively accessing illegal 
addresses may cause a correct program to halt unexpectedly. Besides, mispre- 
dicted memory loads can cause extra cache misses and page faults, which are 
extremely costly. 
Example 10.5 : 
In the fragment 
if (p != null) 
q = *p; 
dereferencing p speculatively will cause this correct program to halt in error if 
p i s n u l l .  
El 
Many high-performance processors provide special features to support spec- 
ulative memory accesses. We mention the most important ones next. 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
Prefet 
ching 
The prefetch instruction was invented to bring data from memory to the cache 
before it is used. A prefetch instruction indicates to the processor that the 
program is likely to use a particular memory word in the near future. If the 
location specified is invalid or if accessing it causes a page fault, the processor 
can simply ignore the operation. Otherwise, the processor will bring the data 
from memory to the cache if it is not already there. 
Poison Bits 
Another architectural feature called poison bits was invented to allow specu- 
lative load of data from memory into the register file. Each register on the 
machine is augmented with a poison bit. If illegal memory is accessed or the 
accessed page is not in memory, the processor does not raise the exception im- 
mediately but instead just sets the poison bit of the destination register. An 
exception is raised only if the contents of the register with a marked poison bit 
are used. 
Predicated Execution 
Because branches are expensive, and mispredicted branches are even more so 
(see Section 10.1), predicated instructions were invented to reduce the number 
of branches in a program. A predicated instruction is like a normal instruction 
but has an extra predicate operand to guard its execution; the instruction is 
executed only if the predicate is found to be true. 
As an example, a conditional move instruction CMOVZ R
2
 ,R3, 
R 1  has the 
semantics that the contents of register R3 are moved to register R
2
 only if 
register R l  is zero. Code such as 
can be implemented with two machine instructions, assuming that a, b, c, and 
d are allocated to registers Rl, R2, R4, R5, respectively, as follows: 
ADD 
R3, R4, R
5
 
CMOVZ R2, R3, R l  
This conversion replaces a series of instructions sharing a control dependence 
with instructions sharing only data dependences. These instructions can then 
be combined with adjacent basic blocks to create a larger basic block. More 
importantly, with this code, the processor does not have a chance to mispredict, 
thus guaranteeing that the instruction pipeline will run smoothly. 
Predicated execution does come with a cost. Predicated instructions are 
fetched and decoded, even though they may not be executed in the end. Static 
schedulers must reserve all the resources needed for their execution and ensure 
10.2. CODE-SCHEDULING CONSTRAINTS 
Dynamically Scheduled Machines 
The instruction set of a statically scheduled machine explicitly defines what 
can execute in parallel. However, recall from Section 10.1.2 that some ma- 
chine architectures allow the decision to be made at run time about what 
can be executed in parallel. With dynamic scheduling, the same machine 
code can be run on different members of the same family (machines that 
implement the same instruction set) that have varying amounts of parallel- 
execution support. In fact, machine-code compatibility is one of the major 
advantages of dynamically scheduled machines. 
Static schedulers, implemented in the compiler by software, can help 
dynamic schedulers (implemented in the machine's hardware) better utilize 
machine resources. To build a static scheduler for a dynamically sched- 
uled machine, we can use almost the same scheduling algorithm as for 
statically scheduled machines except that no-op instructions left in the 
schedule need not be generated explicitly. The matter is discussed further 
in Section 10.4.7. 
that all the potential data dependences are satisfied. Predicated execution 
should not be used aggressively unless the machine has many more resources 
than can possibly be used otherwise. 
10.2.7 
A Basic Machine Model 
Many machines can be represented using the following simple model. A machine 
M = (R, 
T), 
consists of: 
1. A set of operation types T, such as loads, stores, arithmetic operations, 
and so on. 
2. A vector R = [rl, 
ra, . 
. .] 
representing hardware resources, where ri is the 
number of units available of the ith kind of resource. Examples of typical 
resource types include: memory access units, ALU's, and floating-point 
functional units. 
Each operation has a set of input operands, a set of output operands, and a 
resource requirement. Associated with each input operand is an input latency 
indicating when the input value must be available (relative to the start of the 
operation). Typical input operands have zero latency, meaning that the values 
are needed immediately, at the clock when the operation is issued. Similarly, 
associated with each output operand is an output latency, which indicates when 
the result is available, relative to the start of the operation. 
Resource usage for each machine operation type t is modeled by a two- 
dimensional resource-reservation table, RTt. The width of the table is the 
720 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
number of kinds of resources in the machine, and its length is the duration 
over which resources are used by the operation. Entry RTt[i, 
j] is the number 
of units of the jth resource used by an operation of type t, i clocks after it is 
issued. For notational simplicity, we assume RTt[i, 
j] = 
0 if i refers to a nonex- 
istent entry in the table (i.e., i is greater than the number of clocks it takes 
to execute the operation). Of course, for any t, i, and j, RTt [i, 
j] must be less 
than or equal to R[j] 
, 
the number of resources of type j that the machine has. 
Typical machine operations occupy only one unit of resource at the time 
an operation is issued. Some operations may use more than one functional 
unit. For example, a multiply-and-add operation may use a multiplier in the 
first clock and an adder in the second. Some operations, such as a divide, may 
need to occupy a resource for several clocks. Fully pipelined operations are 
those that can be issued every clock, even though their results are not available 
until some number of clocks later. We need not model the resources of every 
stage of a pipeline explicitly; one single unit to represent the first stage will do. 
Any operation occupying the first stage of a pipeline is guaranteed the right to 
proceed to subsequent stages in subsequent clocks. 
Figure 10.5: A sequence of assignments exhibiting data dependences 
10.2.8 Exercises for Section 10.2 
Exercise 1
0
.
2
.
1
 
: 
The assignments in Fig. 10.5 have certain dependences. For 
each of the following pairs of statements, classify the dependence as (i) true de- 
pendence, (ii) antidependence, (iii) output dependence, or (iv) no dependence 
(i.e., the instructions can appear in either order): 
a) Statements (I) 
and (4). 
b) Statements (3) and (5). 
c) Statements (1) 
and (6). 
d) Statements (3) and (6). 
e) Statements (4) and (6). 
Exercise 1
0
.
2
.
2
 
: 
Evaluate the expression ((u+v) 
+ 
(w + 
x)) + 
(y + 
t) 
exactly as 
parenthesized (i.e., do not use the commutative or associative laws to reorder the 
20.3. BASIC-BLOCK SCHEDULING 
72 1 
additions). Give register-level machine code to provide the maximum possible 
parallelism. 
Exercise 10.2.3 
: 
Repeat Exercise 10.2.2 for the following expressions: 
b) (u 
+ 
(v 
+ 
w)) + (x 
+ 
(y 
+ 
z ) ) .  
If instead of maximizing the parallelism, we minimized the number of registers, 
how many steps would the computation take? How many steps do we save by 
using maximal parallelism? 
Exercise 10.2.4 
: The expression of Exercise 10.2.2 can be executed by the 
sequence of instructions shown in Fig. 10.6. If we have as much parallelism as 
we need, how many steps are needed to execute the instructions? 
LD rl, u 
// rl = u 
LD r2, v 
// r2 = v 
ADD rl, rl, r2 
// rl = rl + r2 
LD r2, w 
// r2 = w 
LD r3, x 
// r3 = x 
ADD r2, r2, r3 
// r2 = r2 + r3 
ADD rl, rl, r2 
// rl = rl + r2 
LD r2, y 
// r2 = y 
LD r3, z 
// r3 = z 
ADD r2, r2, r3 
// r2 = r2 + r3 
ADD rl, rl, r2 
// rl = r1 + r2 
Figure 10.6: Minimal-register implementation of an arithmetic expression 
! 
Exercise 10.2.5 : Translate the code fragment discussed in Example 10.4, 
using the CMOVZ 
conditional copy instruction of Section 10.2.6. What are the 
data dependences in your machine code? 
10.3 Basic-Block Scheduling 
We are now ready to start talking about code-scheduling algorithms. We start 
with the easiest problem: scheduling operations in a basic block consisting of 
machine instructions. Solving this problem optimally is NP-complete. But in 
practice, a typical basic block has only a small number of highly constrained 
operations, so simple scheduling techniques suffice. We shall introduce a simple 
but highly effective algorithm, called list scheduling, for this problem. 
722 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
10.3.1 Data-Dependence Graphs 
We represent each basic block of machine instructions by a data-dependence 
graph, G = (N, E), 
having a set of nodes N representing the operations in the 
machine instructions in the block and a set of directed edges E representing 
the data-dependence constraints among the operations. The nodes and edges 
of G are constructed as follows: 
1. Each operation n in N has a resource-reservation table RT,, whose value 
is simply the resource-reservation table associated with the operation type 
of n. 
2. Each edge e in E is labeled with delay d, indicating that the destination 
node must be issued no earlier than d, clocks after the source node is 
issued. Suppose operation nl is followed by operation n2, and the same 
location is accessed by both, with latencies l1 and 1
2
 respectively. That 
is, the location's value is produced ll clocks after the first instruction 
begins, and the value is needed by the second instruction l2 clocks after 
that instruction begins (note ll = 1 
and 1
2
 = 
0 is typical). Then, there is 
an edge nl -+ nz in E labeled with delay ll - 
12. 
Example 10.6 : Consider a simple machine that can execute two operations 
every clock. The first must be either a branch operation or an ALU operation 
of the form: 
OP d s t ,  s r c l ,  src2 
The second must be a load or store operation of the form: 
LD dst , addr 
ST addr, s r c  
The load operation (LD) is fully pipelined and takes two clocks. However, 
a load can be followed immediately by a store ST that writes to the memory 
location read. All other operations complete in one clock. 
Shown in Fig. 10.7 is the dependence graph of an example of a basic block 
and its resources requirement. We might imagine that R 1  is a stack pointer, used 
to access data on the stack with offsets such as 0 or 12. The first instruction 
loads register R2, and the value loaded is not available until two clocks later. 
This observation explains the label 2 on the edges from the first instruction to 
the second and fifth instructions, each of which needs the value of R2. Similarly, 
there is a delay of 2 on the edge from the third instruction to the fourth; the 
value loaded into R3 is needed by the fourth instruction, and not available until 
two clocks after the third begins. 
Since we do not know how the values of R 1  and R7 relate, we have to consider 
the possibility that an address like 8 
(RI) is the same as the address 0 (R7). That 
10.3. BASIC-BLOCK SCHEDULING 
data 
dependences 
resource- 
reservation 
tables 
alu mem 
Figure 10.7: Data-dependence graph for Example 10.6 
is, the last instruction may be storing into the same address that the third 
instruction loads from. The machine model we are using allows us to store into 
a location one clock after we load from that location, even though the value to 
be loaded will not appear in a register until one clock later. This observation 
explains the label 1 
on the edge from the third instruction to the last. The 
same reasoning explains the edges and labels from the first instruction to the 
last. The other edges with label 1 are explained by a dependence or possible 
dependence conditioned on the value of R7. 
10.3.2 List Scheduling of Basic Blocks 
The simplest approach to scheduling basic blocks involves visiting each node of 
the data-dependence graph in "prioritized topological order." Since there can 
be no cycles in a data-dependence graph, there is always at least one topological 
order for the nodes. However, among the possible topological orders, some may 
be preferable to others. We discuss in Section 10.3.3 some of the strategies for 
724 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
Pictorial Resource-Reservation Tables 
It is frequently useful to visualize a resource-reservation table for an oper- 
ation by a grid of solid and open squares. Each column corresponds to one 
of the resources of the machine, and each row corresponds to one of the 
clocks during which the operation executes. Assuming that the operation 
never needs more than one unit of any one resource, we may represent 1's 
by solid squares, and 0's by open squares. In addition, if the operation 
is fully pipelined, then we only need to indicate the resources used at the 
first row, and the resource-reservation table becomes a single row. 
This representation is used, for instance, in Example 10.6. In Fig. 10.7 
we see resource-reservation tables as rows. The two addition operations 
require the "alu" resource, while the loads and stores require the "mem" 
resource. 
picking a topological order, but for the moment, we just assume that there is 
some algorithm for picking a preferred order. 
The list-scheduling algorithm we shall describe next visits the nodes in the 
chosen prioritized topological order. The nodes may or may not wind up being 
scheduled in the same order as they are visited. But the instructions are placed 
in the schedule as early as possible, so there is a tendency for instructions to 
be scheduled in approximately the order visited. 
In more detail, the algorithm computes the earliest time slot in which each 
node can be executed, according to its data-dependence constraints with the 
previously scheduled nodes. Next, the resources needed by the node are checked 
against a resource-reservation table that collects all the resources committed so 
far. The node is scheduled in the earliest time slot that has sufficient resources. 
Algorithm 10.7 
: 
List scheduling a basic block. 
INPUT: A machine-resource vector R = [rl 
, 
r2, . . 
. 
1, where ri is the number 
of units available of the ith kind of resource, and a data-dependence graph 
G = (N, 
E). Each operation n in N is labeled with its resource-reservation 
table RT,; each edge e = nl -+ n
2
 in E is labeled with de indicating that n
z
 
must execute no earlier than de clocks after nl. 
OUTPUT: A schedule S 
that maps the operations in N into time slots in which 
the operations can be initiated satisfying all the data and resources constraints. 
METHOD: Execute the program in Fig. 10.8. A discussion of what the "prior- 
itized topological order" might be follows in Section 10.3.3. 
10.3. BASIC-BLOCK SCHEDULING 
R T  = an empty reservation table; 
for (each n in N in prioritized topological order) { 
s = 
maxe=,-+n 
in E(S(P) + 
d e ) ;  
/* 
Find the earliest time this instruction could begin, 
given when its predecessors started. */ 
while (there exists i such that RT[s + 
i] + 
RTn[i] > R) 
s = s + l ;  
/* 
Delay the instruction further until the needed 
resources are available. */ 
S(n) = s; 
for (all i) 
RT 
[S + 
i] = 
RT 
[S + 
i] + 
RTn 
[i] 
Figure 10.8: A list scheduling algorithm 
10.3.3 Prioritized Topological Orders 
List scheduling does not backtrack; it schedules each node once and only once. 
It uses a heuristic priority function to choose among the nodes that are ready 
to be scheduled next. Here are some observations about possible prioritized 
orderings of the nodes: 
Without resource constraints, the shortest schedule is given by the critical 
path, the longest path through the data-dependence graph. A metric 
useful as a priority function is the height of the node, which is the length 
of a longest path in the graph originating from the node. 
On the other hand, if all operations are independent, then the length 
of the schedule is constrained by the resources available. The critical 
resource is the one with the largest ratio of uses to the number of units 
of that resource available. Operations using more critical resources may 
be given higher priority. 
Finally, we can use the source ordering to break ties between operations; 
the operation that shows up earlier in the source program should be sched- 
uled first. 
Example 10.8 
: 
For the data-dependence graph in Fig. 10.7, the critical path, 
including the time to execute the last instruction, is 6 clocks. That is, the 
critical path is the last five nodes, from the load of R 3  to the store of R7. The 
total of the delays on the edges along this path is 5, to which we add 1 
for the 
clock needed for the last instruction. 
Using the height as the priority function, Algorithm 10.7 finds an optimal 
schedule as shown in Fig. 10.9. Notice that we schedule the load of R 3  first, 
since it has the greatest height. The add of R 3  and R 4  has the resources to be 
CHAPTER 20. INSTRUCTION-LEVEL PARALLELISM 
schedule 
resource- 
reservation 
table 
ADD R3,R3,R4 
ADD R3,R3,R2 
alu mem 
LD R3,8(R1) 
LD R2,O(R1) 
ST 4(Rl),R2 
ST 12(Rl),R3 
ST O(R7),R7 
Figure 10.9: Result of applying list scheduling to the example in Fig. 10.7 
scheduled at the second clock, but the delay of 2 for a load forces us to wait 
until the third clock to schedule this add. That is, we cannot be sure that R3 
will have its needed value until the beginning of clock 3. 
1) 
LD Ri, a 
LD Ri, a 
LD Ri, a 
2) 
LD R2, b 
LD R2, b 
LD R2, b 
3) S
U
B
 R3, Rl, R2 S
U
B
 Ri, Ri, R2 S
U
B
 R3, Rl, R2 
4) ADD R2, Rl, R2 ADD R2, Ri, R2 ADD R4, R1, R2 
5) ST a, R3 
ST a, R1 
ST a, R3 
6) 
ST b, R2 
ST b, R2 
ST b, R4 
Figure 10.10: Machine code for Exercise 10.3.1 
10.3.4 Exercises for Section 10.3 
Exercise 10.3.1 
: 
For each of the code fragments of Fig. 10.10, draw the data- 
dependence graph. 
Exercise 10.3.2 : Assume a machine with one ALU resource (for the ADD 
and S
U
B
 operations) and one MEM resource (for the LD and ST operations). 
Assume that all operations require one clock, except for the LD, which requires 
two. However, as in Example 10.6, a ST on the same memory location can 
commence one clock after a LD on that location commences. Find a shortest 
schedule for each of the fragments in Fig. 10.10. 
10.4. GLOBAL CODE SCHEDULING 
Exercise 10.3.3 
: 
Repeat Exercise 10.3.2 assuming: 
i. The machine has one ALU resource and two MEM resources. 
ii. The machine has two ALU resources and one MEM resource. 
iii. The machine has two ALU resources and two MEM resources. 
1) LD R1, a 
2) 
ST b, R 1  
3) 
LD R2, c 
4) 
ST c ,  R 1  
5) 
LD Ri, d 
6
)
 ST d, R2 
7) 
S T a ,  R 1  
Figure 10.11: Machine code for Exercise 10.3.4 
Exercise 10.3.4 
: 
Assuming the machine model of Example 10.6 (as in Exer- 
cise 10.3.2): 
a) Draw the data dependence graph for the code of Fig. 10.11. 
b) What are all the critical paths in your graph from part (a)? 
! 
c) Assuming unlimited MEM resources, what are all the possible schedules 
for the seven instructions? 
10.4 Global Code Scheduling 
For a machine with a moderate amount of instruction-level parallelism, sched- 
ules created by compacting individual basic blocks tend to leave many resources 
idle. In order to make better use of machine resources, it is necessary to con- 
sider code-generation strategies that move instructions from one basic block 
to another. Strategies that consider more than one basic block at a time are 
referred to as global scheduling algorithms. To do global scheduling correctly, 
we must consider not only data dependences but also control dependences. We 
must ensure that 
1. All instructions in the original program are executed in the optimized 
program, and 
2. While the optimized program may execute extra instructions specula- 
tively, these instructions must not have any unwanted side effects. 
728 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
10.4.1 Primitive Code Motion 
Let us first study the issues involved in moving operations around by way of a 
simple example. 
Example 10.9: Suppose we have a machine that can execute any two oper- 
ations in a single clock. Every operation executes with a delay of one clock, 
except for the load operation, which has a latency of two clocks. For simplicity, 
we assume that all memory accesses in the example are valid and will hit in the 
cache. Figure 10.12(a) shows a simple flow graph with three basic blocks. The 
code is expanded into machine operations in Figure 10.12(b). All the instruc- 
tions in each basic block must execute serially because of data dependences; in 
fact, a no-op instruction has to be inserted in every basic block. 
Assume that the addresses of variables a, b, c, d, and e are distinct and that 
those addresses are stored in registers R 1  through R5, respectively. The com- 
putations from different basic blocks therefore share no data dependences. We 
observe that all the operations in block B3 are executed regardless of whether 
the branch is taken, and can therefore be executed in parallel with operations 
from block B1. We cannot move operations from B1 down to B3, because they 
are needed to determine the outcome of the branch. 
Operations in block B2 are control-dependent on the test in block B1. We 
can perform the load from B2 speculatively in block B1 for free and shave two 
clocks from the execution time whenever the branch is taken. 
Stores should not be performed speculatively because they overwrite the 
old value in a memory location. It is possible, however, to delay a store op- 
eration. We cannot simply place the store operation from block B2 in block 
B3, because it should only be executed if the flow of control passes through 
block B2. However, we can place the store operation in a duplicated copy of 
BS. Figure 10.12(c) shows such an optimized schedule. The optimized code 
executes in 4 clocks, which is the same as the time it takes to execute B3 alone. 
Example 10.9 shows that it is possible to move operations up and down 
an execution path. Every pair of basic blocks in this example has a different 
"dominance relation," and thus the considerations of when and how instructions 
can be moved between each pair are different. As discussed in Section 9.6.1, 
a block B is said to dominate block B' if every path from the entry of the 
control-flow graph to B' goes through B. Similarly, a block B postdominates 
block B' if every path from B' to the exit of the graph goes through B. When 
B dominates B' and B' postdominates B, we say that B and B' are control 
equivalent, meaning that one is executed when and only when the other is. For 
the example in Fig. 10.12, assuming B1 is the entry and B3 the exit, 
1. B1 and B3 
are control equivalent: B1 dominates B3 and B3 
postdominates 
B1, 
2. B1 dominates Bz but B2 does not postdominate B1, and 
10.4. GLOBAL CODE SCHEDULING 
(a) Source program 
(b) Locally scheduled machne code 
LD R6,O(R1), LD R8,O(R4) 
LD R7,O(R2) 
ADD R8,R8,R8, BEQZ R6,L 
4
:
 
.
I
 ST I
O(R5),R8 
ST O(R5),R8, ST O(R3),R7 I B3 ' 
(c) Globally scheduled machine code 
Figure 10.12: Flow graphs before and after global scheduling in Example 10.9 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
3. B2 does not dominate B3 but B3 postdominates B2. 
It is also possible for a pair of blocks along a path to share neither a dominance 
nor post 
dominance relation. 
10.4.2 Upward Code Motion 
We now examine carefully what it means to move an operation up a path. 
Suppose we wish to move an operation from block src up a control-flow path to 
block dst. We assume that such a move does not violate any data dependences 
and that it makes paths through dst and src run faster. If dst dominates src, 
and src postdominates dst, then the operation moved is executed once and only 
once, when it should. 
If src does not postdominate dst 
Then there exists a path that passes through dst that does not reach src. An 
extra operation would have been executed in this case. This code motion is 
illegal unless the operation moved has no unwanted side effects. If the moved 
operation executes "for free" (i.e., it uses only resources that otherwise would 
be idle), then this move has no cost. It is beneficial only if the control flow 
reaches src. 
If dst does not dominate src 
Then there exists a path that reaches src without first going through dst. We 
need to insert copies of the moved operation along such paths. We know how 
to achieve exactly that from our discussion of partial redundancy elimination 
in Section 9.5. We place copies of the operation along basic blocks that form a 
cut set separating the entry block from src. At each place where the operation 
is inserted, the following constraints must be satisfied: 
1. The operands of the operation must hold the same values as in the original, 
2. The result does not overwrite a value that is still needed, and 
3. It itself is not subsequently overwritten bef~re 
reaching src. 
These copies render the original instruction in src fully redundant, and it thus 
can be eliminated. 
We refer to the extra copies of the operation as compensation code. As dis- 
cussed in Section 9.5, basic blocks can be inserted along critical edges to create 
places for holding such copies. The compensation code can potentially make 
some paths run slower. Thus, this code motion improves program execution 
only if the optimized paths are executed more frequently than the nonopti- 
mized ones. 
10.4. GLOBAL CODE SCHEDULING 
10.4.3 Downward Code Motion 
Suppose we are interested in moving an operation from block src 
down a control- 
flow path to block dst. We can reason about such code motion in the same way 
as above. 
If src does not dominate dst 
Then there exists a path that reaches dst without first visiting src. Again, an 
extra operation will be executed in this case. Unfortunately, downward code 
motion is often applied to writes, which have the side effects of overwriting old 
values. We can get around this problem by replicating the basic blocks along 
the paths from src to dst, and placing the operation only in the new copy of 
dst. Another approach, if available, is to use predicated instructions. We guard 
the operation moved with the predicate that guards the src block. Note that 
the predicated instruction must be scheduled only in a block dominated by 
the computation of the predicate, because the predicate would not be available 
otherwise. 
If &st does not postdominate src 
As in the discussion above, compensation code needs to be inserted so that the 
operation moved is executed on all paths not visiting dst. This transformation 
is again analogous to partial redundancy elimination, except that the copies are 
placed below the src block in a cut set that separates src from the exit. 
Summary of Upward and Downward Code Motion 
From this discussion, we see that there is a range of possible global code mo- 
tions which vary in terms of benefit, cost, and implementation complexity. Fig- 
ure 10.13 
shows a summary of these various code motions; the lines correspond 
to the following four cases: 
Figure 10.13: Summary of code motions 
1 
2 
3 
4 
1. Moving instructions between control-equivalent blocks is simplest and 
most cost effective. No extra operations are ever executed and no com- 
pensation code is needed. 
up: src postdom dst 
down: src dom dst 
Yes 
no 
Yes 
no 
dst dom src 
dst postdom src 
Yes 
Yes 
no 
no 
speculation 
code dup. 
no 
Yes 
no 
Yes 
compensation 
code 
no 
no 
Yes 
Yes 
732 
CHAPTER 10. INSTRUCTION-LE VEL 
PARALLELISM 
2. Extra operations may be executed if the source does not postdominate 
(dominate) the destination in upward (downward) code motion. This 
code motion is beneficial if the extra operations can be executed for free, 
and the path passing through the source block is executed. 
3. Compensation code is needed if the destination does not dominate (post- 
dominate) the source in upward (downward) code motion. The paths with 
the compensation code may be slowed down, so it is important that the 
optimized paths are more frequently executed. 
4. The last case combines the disadvantages of the second and third case: 
extra operations may be executed and compensation code is needed. 
10.4.4 Updating Data Dependences 
As illustrated by Example 10.10 below, code motion can change the data- 
dependence relations between operations. Thus data dependences must be 
updated after each code movement. 
Example 10.10 : 
For the flow graph shown in Fig. 10.14, either assignment to 
x can be moved up to the top block, since all the dependences in the original 
program are preserved with this transformation. However, once we have moved 
one assignment up, we cannot move the other. More specifically, we see that 
variable x is not live on exit in the top block before the code motion, but it is 
live after the motion. If a variable is live at a program point, then we cannot 
move speculative definitions to the variable above that program point. 
Figure 10.14: Example illustrating the change in data dependences due to code 
motion. 
10.4.5 Global Scheduling Algorithms 
We saw in the last section that code motion can benefit some paths while 
hurting the performance of others. The good news is that instructions are not 
all created equal. In fact, it is well established that over 90% of a program's 
execution time is spent on less than 10% of the code. Thus, we should aim to 
10.4. GLOBAL CODE SCHEDULING 
make the frequently executed paths run faster while possibly making the less 
frequent paths run slower. 
There are a number of techniques a compiler can use to estimate execution 
frequencies. It is reasonable to assume that instructions in the innermost loops 
are executed more often than code in outer loops, and that branches that go 
backward are more likely to be taken than not taken. Also, branch statements 
found to guard program exits or exception-handling routines are unlikely to be 
taken. The best frequency estimates, however, come from dynamic profiling. In 
this technique, programs are instrumented to record the outcomes of conditional 
branches as they run. The programs are then run on representative inputs to 
determine how they are likely to behave in general. The results obtained from 
this technique have been found to be quite accurate. Such information can be 
fed back to the compiler to use in its optimizations. 
Region-Based Scheduling 
We now describe a straightforward global scheduler that supports the two eas- 
iest forms of code motion: 
1. Moving operations up to control-equivalent basic blocks, and 
2. Moving operations speculatively up one branch to a dominating predeces- 
sor. 
Recall from Section 9.7.1 that a region is a subset of a control-flow graph that 
can be reached only through one entry block. We may represent any procedure 
as a hierarchy of regions. The entire procedure constitutes the top-level region, 
nested in it are subregions representing the natural loops in the function. We 
assume that the control-flow graph is reducible. 
Algorithm 10.11 : 
Region-based scheduling. 
INPUT: A control-flow graph and a machine-resource description. 
OUTPUT: A schedule S mapping each instruction to a basic block and a time 
slot. 
METHOD: Execute the program in Fig. 10.15. Some shorthand terminology 
should be apparent: ControlEquiu(B) is the set of blocks that are control- 
equivalent to block B, and DorninatedSucc applied to a set of blocks is the set 
of blocks that are successors of at least one block in the set and are dominated 
by all. 
Code scheduling in Algorithm 10.11 proceeds from the innermost regions 
to the outermost. When scheduling a region, each nested subregion is treated 
as a black box; instructions are not allowed to move in or out of a subregion. 
They can, however, move around a subregion, provided their data and control 
dependences are satisfied. 
734 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
for (each region R in topological order, so that inner regions 
are processed before outer regions) { 
compute data dependences; 
for (each basic block B of R in prioritized topological order) { 
CandBEocks = ControEEquiv(B) U 
DominatedSucc( ControlEquiv(B)) 
; 
CandInsts = ready instructions in CandBlocks; 
for (t = 
0,1,. 
. . until all instructions from B are scheduled) { 
for (each instruction n in CandInsts in priority order) 
if (n has no resource conflicts at time t) 
{ 
S(n) = (B, 
t); 
update resource commitments; 
update data dependences; 
I 
update CandInsts; 
I 
I 
1 
Figure 10.15: A region-based global scheduling algorithm 
All control and dependence edges flowing back to the header of the region are 
ignored, so the resulting control-flow and data-dependence graphs are acyclic. 
The basic blocks in each region are visited in topological order. This ordering 
guarantees that a basic block is not scheduled until all the instructions it de- 
pends on have been scheduled. Instructions to be scheduled in a basic block B 
are drawn from all the blocks that are control-equivalent to B (including B), 
as well as their immediate successors that are dominated by B. 
A list-scheduling algorithm is used to create the schedule for each basic 
block. The algorithm keeps a list of candidate instructions, CandInsts, which 
contains all the instructions in the candidate blocks whose predecessors all have 
been scheduled. It creates the schedule clock-by-clock. For each clock, it checks 
each instruction from the CandInsts in priority order and schedules it in that 
clock if resources permit. Algorithm 10.11 
then updates CandInsts and repeats 
the process, until all instructions from B are scheduled. 
The priority order of instructions in CandInsts uses a priority function sim- 
ilar to that discussed in Section 10.3. We make one important modification, 
however. We give instructions from blocks that are control equivalent to B 
higher priority than those from the successor blocks. The reason is that in- 
structions in the latter category are only speculatively executed in block B. 
10.4. GLOBAL CODE SCHEDULING 
Loop Unrolling 
In region-based scheduling, the boundary of a loop iteration is a barrier to code 
motion. Operations from one iteration cannot overlap with those from another. 
One simple but highly effective technique to mitigate this problem is to unroll 
the loop a small number of times before code scheduling. A for-loop such as 
f o r  ( i  = 0; i < N ;  i++) ( 
S ( i )  ; 
can be written as in Fig. 10.16(a). Similarly, a repeat-loop such as 
repeat 
s ;  
u n t i l  C; 
can be written as in Fig. 10.16(b). Unrolling creates more instructions in the 
loop body, permitting global scheduling algorithms to find more parallelism. 
f o r  ( i  = 0; i+4 < N ;  i+=4) ( 
S ( i )  ; 
S ( i + l )  
; 
S 
(i+2) 
; 
S 
(i+3) ; 
> 
f o r  ( ; i < N; i++) ( 
S ( i )  ; 
> 
(a) Unrolling a for-loop. 
repeat ( 
s ;  
i f  (C) break; 
s ;  
i f  (C) break; 
s ;  
i f  (C) break; 
s ;  
3 u n t i l  C; 
(b) Unrolling a repeat-loop. 
Figure 10.16: Unrolled loops 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
Neighborhood Compiiction 
Algorithm 10.11 only supports the first two forms of code motion described in 
Section 10.4.1. Code motions that require the introduction of compensation 
code can sometimes be useful. One way to support such code motions is to 
follow the region-based scheduling with a simple pass. In this pass, we can 
examine each pair of basic blocks that are executed one after the other, and 
check if any operation can be moved up or down between them to improve 
the execution time of those blocks. If such a pair is found, we check if the 
instruction to be moved needs to be duplicated along other paths. The code 
motion is made if it results in an expected net gain. 
This simple extension can be quite effective in improving the performance of 
loops. For instance, it can move an operation at the beginning of one iteration 
to the end of the preceding iteration, while also moving the operation from the 
first iteration out of the loop. This optimization is particularly attractive for 
tight loops, which are loops that execute only a few instructions per iteration. 
However, the impact of this technique is limited by the fact that each code- 
motion decision is made locally and independently. 
10.4.6 Advanced Code Motion Techniques 
If our target machine is statically scheduled and has plenty of instruction-level 
parallelism, we may need a more aggressive algorithm. Here is a high-level 
description of further extensions: 
1. To facilitate the extensions below, we can add new basic blocks along 
control-flow edges originating from blocks with more than one predecessor. 
These basic blocks will be eliminated at the end of code scheduling if they 
are empty. A useful heuristic is to move instructions out of a basic block 
that is nearly empty, so that the block can be eliminated completely. 
2. In Algorithm 10.11, the code to be executed in each basic block is sched- 
uled once and for all as each block is visited. This simple approach suffices 
because the algorithm can only move operations up to dominating blocks. 
To allow motions that require the addition of compensation code, we take 
a slightly different approach. When we visit block L(, we only schedule 
instructions from B and all its control-equivalent blocks. We first try to 
place these instructions in predecessor blocks, which have already been 
visited and for which a partial schedule already exists. We try to find 
a destination block that would lead to an improvement on a frequently 
executed path and then place copies of the instruction on other paths to 
guarantee correctness. If the instructions cannot be moved up, they are 
scheduled in the current basic block as before. 
3. Implementing downward code motion is harder in an algorithm that visits 
basic blocks in topological order, since the target blocks have yet to be 
10.4. GLOBAL CODE SCHEDULING 
737 
scheduled. However, there are relatively fewer opportunities for such code 
motion anyway. We move all operations that 
(a) can be moved, and 
(b) cannot be executed for free in their native block. 
This simple strategy works well if the target machine is rich with many 
unused hardware resources. 
10.4.7 Interaction with Dynamic Schedulers 
A dynamic scheduler has the advantage that it can create new schedules ac- 
cording to the run-time conditions, without having to encode all these possible 
schedules ahead of time. If a target machine has a dynamic scheduler, the static 
scheduler's primary function is to ensure that instructions with high latency are 
fetched early so that the dynamic scheduler can issue them as early as possible. 
Cache misses are a class of unpredictable events that can make a big differ- 
ence to the performance of a program. If data-prefetch instructions are avail- 
able, the static scheduler can help the dynamic scheduler significantly by placing 
these prefetch instructions early enough that the data will be in the cache by 
the time they are needed. If prefetch instructions are not available, it is useful 
for a compiler to estimate which operations are likely to miss and try to issue 
them early. 
If dynamic scheduling is not available on the target machine, the static 
scheduler must be conservative and separate every data-dependent pair of op- 
erations by the minimum delay. If dynamic scheduling is available, however, the 
compiler only needs to place the data-dependent operations in the correct order 
to ensure program correctness. For best performance, the compiler should as- 
sign long delays to dependences that are likely to occur and short ones to those 
that are not likely. 
Branch misprediction is an important cause of loss in performance. Because 
of the long misprediction penalty, instructions on rarely executed paths can still 
have a significant effect on the total execution time. Higher priority should be 
given to such instructions to reduce the cost of misprediction. 
10.4.8 Exercises for Section 10.4 
Exercise 10.4.1 
: 
Show how to unroll the generic while-loop 
while (C) 
s ;  
! Exercise 10.4.2 
: 
Consider the code fragment: 
if (
x
 == 0 )  a = b; 
else a = c; 
d = a; 
CHAPTER 10. INSTRUCTION-LE VEL 
PARALLELISM 
Assume a machine that uses the delay model of Example 10.6 (loads take two 
clocks, all other instructions take one clock). Also assume that the machine 
can execute any two instructions at once. Find a shortest possible execution 
of this fragment. Do not forget to consider which register is best used for each 
of the copy steps. Also, remember to exploit the information given by register 
descriptors as was described in Section 8.6, to avoid unnecessary loads and 
stores. 
10.5 Software Pipelining 
As discussed in the introduction of this chapter, numerical applications tend 
to have much parallelism. In particular, they often have loops whose iterations 
are completely independent of one another. These loops, known as do-all loops, 
are particularly attractive from a parallelization perspective because their iter- 
ations can be executed in parallel to achieve a speed-up linear in the number 
of iterations in the loop. Do-all loops with many iterations have enough par- 
allelism to saturate all the resources on a processor. It is up to the scheduler 
to take full advantage of the available parallelism. This section describes an al- 
gorithm, known as software pipelining, that schedules an entire loop at a time, 
taking full advantage of the parallelism across iterations. 
10.5.1 Introduction 
We shall use the do-all loop in Example 10.12 throughout this section to explain 
software pipelining. We first show that scheduling across iterations is of great 
importance, because there is relatively little parallelism among operations in 
a single iteration. Next, we show that loop unrolling improves performance 
by overlapping the computation of unrolled iterations. However, the boundary 
of the unrolled loop still poses as a barrier to code motion, and unrolling still 
leaves a lot of performance "on the table." The technique of software pipelining, 
on the other hand, overlaps a number of consecutive iterations continually until 
it runs out of iterations. This technique allows software pipelining to produce 
highly efficient and compact code. 
Example 10.12 : 
Here is a typical do-all loop: 
f o r  (i 
= 0; i < n; i++) 
D [ i ]  
= A[i]*B[i] 
+ c; 
Iterations in the above loop write to different memory locations, which are 
themselves distinct from any of the locations read. Therefore, there are no 
memory dependences between the iterations, and all iterations can proceed in 
parallel. 
We adopt the following model as our target machine throughout this section. 
In this model 
10.5. SOFTWARE PIPELINING 
739 
The machine can issue in a single clock: one load, one store, one arithmetic 
operation, and one branch operation. 
The machine has a loop-back operation of the form 
which decrements register R and, unless the result is 0, branches to loca- 
tion L. 
Memory operations have an auto-increment addressing mode, denoted by 
++ after the register. The register is automatically incremented to point 
to the next consecutive address after each access. 
The arithmetic operations are fully pipelined; they can be initiated every 
clock but their results are not available until 2 clocks later. All other 
instructions have a single-clock latency. 
If iterations are scheduled one at a time, the best schedule we can get on 
our machine model is shown in Fig. 10.17. Some assumptions about the layout 
of the data also also indicated in that figure: registers R1, R2, and R 3  hold the 
addresses of the beginnings of arrays A, B, and D, register R 4  holds the constant 
c, and register RIO holds the value n - 
1, 
which has been computed outside the 
loop. The computation is mostly serial, taking a total of 7 clocks; only the 
loop-back instruction is overlapped with the last operation in the iteration. 
// 
R 1 ,  R 2 ,  R 3 = & A y  &By 
&D 
/ 
R 4  
= c 
// 
RIO 
= n-1 
L: 
LD 
R 5 ,  O(Rl++) 
LD 
R 6 ,  O(R2++) 
MUL R 7 ,  R 5 ,  R 6  
noP 
ADD R 8 ,  R 7 ,  R 4  
noP 
ST 
0 
(R3++) , R 8  
BL R 1 0 ,  L 
Figure 10.17: Locally scheduled code for Example 10.12 
In general, we get better hardware utilization by unrolling several iterations 
of a loop. However, doing so also increases the code size, which in turn can 
have a negative impact on overall performance. Thus, we have to compromise, 
picking a number of times to unroll a loop that gets most of the performance im- 
provement, yet doesn't expand the code too much. The next example illustrates 
the tradeoff. 
740 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
Example 10.13 : 
While hardly any parallelism can be found in each iteration 
of the loop in Example 10.12, there is plenty of parallelism across the iterations. 
Loop unrolling places several iterations of the loop in one large basic block, 
and a simple list-scheduling algorithm can be used to schedule the operations 
to execute in parallel. If we unroll the loop in our example four times and 
apply Algorithm 10.7 to the code, we can get the schedule shown in Fig. 10.18. 
(For simplicity, we ignore the details of register allocation for now). The loop 
executes in 13 clocks, or one iteration every 3.25 clocks. 
A loop unrolled k times takes at least 2k + 
5 clocks, achieving a throughput 
of one iteration every 2 + 
5/k clocks. Thus, the more iterations we unroll, the 
faster the loop runs. As n -+ oo, a fully unrolled loop can execute on average an 
iteration every two clocks. However, the more iterations we unroll, the larger 
the code gets. We certainly cannot afford to unroll all the iterations in a loop. 
Unrolling the loop 4 times produces code with 13 instructions, or 163% of the 
optimum; unrolling the loop 8 times produces code with 21 instructions, or 
131% of the optimum. Conversely, if we wish to operate at, say, only 110% of 
the optimum, we need to unroll the loop 25 times, which would result in code 
with55instructions. 
10.5.2 Software Pipelining of Loops 
Software pipelining provides a convenient way of getting optimal resource usage 
and compact code at the same time. Let us illustrate the idea with our running 
example. 
Example 10.14 : In Fig. 10.19 is the code from Example 10.12 unrolled five 
times. (Again we leave out the consideration of register usage.) Shown in row i 
are all the operations issued at clock i; 
shown in column j are all the operations 
from iteration j. Note that every iteration has the same schedule relative to its 
beginning, and also note that every iteration is initiated two clocks after the 
preceding one. It is easy to see that this schedule satisfies all the resource and 
dat 
a-dependence constraints. 
We observe that the operations executed at clocks 7 and 8 are the same 
as those executed at clocks 9 and 10. Clocks 7 and 8 execute operations from 
the first four iterations in the original program. Clocks 9 and 10 also execute 
operations from four iterations, this time from iterations 2 to 5. In fact, we 
can keep executing this same pair of multi-operation instructions to get the 
effect of retiring the oldest iteration and adding a new one, until we run out of 
iterations. 
Such dynamic behavior can be encoded succinctly with the code shown in 
Fig. 10.20, if we assume that the loop has at least 4 iterations. Each row in 
the figure corresponds to one machine instruction. Lines 7 and 8 form a 2-clock 
loop, which is executed n - 
3 times, where n is the number of iterations in the 
original loop. 
[7 
20.5. SOFTWARE PIPELINING 
L: LD 
LD 
LD 
MUL LD 
MUL LD 
ADD 
LD 
ADD 
LD 
ST 
MUL 
LD 
ST 
MUL 
ADD 
ADD 
ST 
ST 
BL (L) 
Figure 10.18: Unrolled code for Example 10.12 
Clock j = l  j = 2  j = 3  j = 4  j = 5  
1 LD 
2 
LD 
3 
MUL 
LD 
4 
LD 
5 
MUL 
LD 
6 
ADD 
LD 
7 
MUL 
LD 
8 
ST 
ADD 
LD 
9 
MUL 
LD 
10 
ST 
ADD 
LD 
11 
MUL 
12 
ST 
ADD 
13 
14 
ST 
ADD 
15 
16 
ST 
Figure 10.19: Five unrolled iterations of the code in Example 10.12 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
1) 
LD 
2) 
LD 
3) 
MUL LD 
4, 
LD 
5 
> 
MUL LD 
6) 
ADD 
LD 
7) 
L: 
MUL LD 
8) 
ST 
ADD 
LD 
BL (L) 
9) 
MUL 
10) 
ST 
ADD 
11) 
12) 
ST 
ADD 
13) 
14) 
ST 
Figure 10.20: Software-pipelined code for Example 10.12 
The technique described above is called software pipelining, because it is the 
software analog of a technique used for scheduling hardware pipelines. We can 
think of the schedule executed by each iteration in this example as an 8-stage 
pipeline. A new iteration can be started on the pipeline every 2 clocks. At 
the beginning, there is only one iteration in the pipeline. As the first iteration 
proceeds to stage three, the second iteration starts to execute in the first pipeline 
stage. 
By clock 7, the pipeline is fully filled with the first four iterations. In the 
steady state, four consecutive iterations are executing at the same time. A new 
iteration is started as the oldest iteration in the pipeline retires. When we run 
out of iterations, the pipeline drains, and all the iterations in the pipeline run 
to completion. The sequence of instructions used to fill the pipeline, lines 1 
through 6 in our example, is called the prolog; lines 7 and 8 are the steady state; 
and the sequence of instructions used to drain the pipeline, lines 9 through 14, 
is called the epilog. 
For this example, we know that the loop cannot be run at a rate faster 
than 2 clocks per iteration, since the machine can only issue one read every 
clock, and there are two reads in each iteration. The software-pipelined loop 
above executes in 2n + 6 clocks, where n is the number of iterations in the 
original loop. As n -+ 
ca, 
the throughput of the loop approaches the rate of 
one iteration every two clocks. Thus, software scheduling, unlike unrolling, can 
potentially encode the optimal schedule with a very compact code sequence. 
Note that the schedule adopted for each individual iteration is not the 
shortest possible. Comparison with the locally optimized schedule shown in 
Fig. 10.17 shows that a delay is introduced before the ADD 
operation. The delay 
is placed strategically so that the schedule can be initiated every two clocks 
without resource conflicts. Had we stuck with the locally compacted schedule, 
10.5. SOFTWARE PIPELINING 
743 
the initiation interval would have to be lengthened to 4 clocks to avoid resource 
conflicts, and the throughput rate would be halved. This example illustrates 
an important principle in pipeline scheduling: the schedule must be chosen 
carefully in order to optimize the throughput. A locally compacted schedule, 
while minimizing the time to complete an iteration, may result in suboptimal 
throughput when pipelined. 
10.5.3 Register Allocation and Code Generation 
Let us begin by discussing register allocation for the software-pipelined loop in 
Example 10.14. 
Example 10.15 : In Example 10.14, the result of the multiply operation in 
the first iteration is produced at clock 3 and used at clock 6. Between these 
clock cycles, a new result is generated by the multiply operation in the second 
iteration at clock 5; this value is used at clock 8. The results from these two 
iterations must be held in different registers to prevent them from interfering 
with each othet. Since interference occurs only between adjacent pairs of itera- 
tions, it can be avoided with the use of two registers, one for the odd iterations 
and one for the even iterations. Since the code for odd iterations is different 
from that for the even iterations, the size of the steady-state loop is doubled. 
This code can be used to execute any loop that has an odd number of iterations 
greater than or equal to 5. 
i f  (N >= 5) 
N2 = 3 + 2 * floor 
( (N-3) 
/2) ; 
e l s e  
N2 = 0; 
for ( i  = 0; i < N2; i++) 
D[i] 
= A[i]* 
B[i] + c; 
for ( i  = N2; i < N; i++) 
D[i] 
= A[i]* 
B[i] + c; 
Figure 10.21: Source-level unrolling of the loop from Example 10.12 
To handle loops that have fewer than 5 iterations and loops with an even 
number of iterations, we generate the code whose source-level equivalent is 
shown in Fig. 10.21. The first loop is pipelined, as seen in the machine-level 
equivalent of Fig. 10.22. The second loop of Fig. 10.21 need not be optimized, 
since it can iterate at most four times. 
10.5.4 Do-Across Loops 
Software pipelining can also be applied to loops whose iterations share data 
dependences. Such loops are known as do-across loops. 
744 
CHAPTER 10. INSTRUCTION-LE VEL 
PARALLELISM 
MUL R7,R5,R6 
MUL R9,R5,R6 
ADD R8,R7,R4 
MUL R7,R5,R6 
ADD R8 
,R9 
,R4 ST 0 
(R3++) 
,R8 
MUL R9,R5,R6 
ADD R8 
,R7 
,R4 ST 0 
(R3++) 
,R8 BL R1O 
,L 
MUL R7,R5,R6 
ADD R8 
,R9 
,R4 ST 0 
(R3++) 
,R8 
ADD R8 
,R7 
,R4 ST 0 
(R3++) 
,R8 
Figure 10.22: Code after software pipelining and register allocation in Exam- 
ple 10.15 
Example 10.16 : 
The code 
for (i = 0; i < n ;  
i++) ( 
sum = sum + A [ i l  ; 
B[i] 
= A C i ]  * b; 
> 
has a data dependence between consecutive iterations, because the previous 
value of sum 
is added to A[i] 
to create a new value of sum. 
It is possible to execute 
the summation in O(1og n) time if the machine can deliver sufficient parallelism, 
but for the sake of this discussion, we simply assume that all the sequential 
dependences must be obeyed, and that the additions must be performed in the 
original sequential order. Because our assumed machine model takes two clocks 
to complete an ADD, the loop cannot execute faster than one iteration every two 
clocks. Giving the machine more adders or multipliers will not make this loop 
run any faster. The throughput of do-across loops like this one is limited by 
the chain of dependences across iterations. 
The best locally compacted schedule for each iteration is shown in Fig. 
10.23 
(a), and the software-pipelined code is in Fig. 10.23(b). This software- 
pipelined loop starts an iteration every two clocks, and thus operates at the 
optimal rate. 
10.5. SOFT WARE PIPELINING 
// R l  = &A; R2 = &B 
// R 3  = sum 
// R 4  = b 
// RlO = n-l 
L: 
LD R 5 ,  O(R1++) 
MUL R 6 ,  R 5 ,  R 4  
ADD R 3 ,  R 3 ,  R 4  
S T  R 6 ,  O(R2++) 
BL R 1 0 ,  L 
(a) The best locally compacted schedule. 
// R 1  = &A; R2 = &B 
// R 3  = sum 
// R 4  = b 
// R 1 0  = n-2 
LD R 5 ,  O(Rl++) 
MUL R 6 ,  R5, R 4  
L: 
ADD R 3 ,  R 3 ,  R 4  
S T  R 6 ,  O(R2++) 
LD R5, O(Rl++) 
MUL R 6 ,  R 5 ,  R 4  
BL R l O ,  L 
ADD R 3 ,  R 3 ,  R4 
ST R 6 ,  O(R2++) 
(b) The software-pipelined version. 
Figure 10.23: Software-pipelining of a do-across loop 
10.5.5 
Goals and Constraints of Software Pipelining 
The primary goal of software pipelining is to maximize the throughput of a 
long-running loop. A secondary goal is to keep the size of the code generated 
reasonably small. In other words, the software-pipelined loop should have a 
small steady state of the pipeline. We can achieve a small steady state by 
requiring that the relative schedule of each iteration be the same, and that the 
iterations be initiated at a constant interval. Since the throughput of the loop is 
simply the inverse of the initiation interval, the objective of software pipelining 
is to minimize this interval. 
A software-pipeline schedule for a data-dependence graph G = (N, 
E) can 
be specified by 
1. An initiation interval T and 
2. A relative schedule S that specifies, for each operation, when that opera- 
tion is executed relative to the start of the iteration to which it belongs. 
746 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
Thus, an operation n in the ith iteration, counting from 0, is executed at clock 
i x T + 
S(n) 
. Like all the other scheduling problems, software pipelining has two 
kinds of constraints: resources and data dependences. We discuss each kind in 
detail below. 
Modular Resource Reservation 
Let a machine's resources be represented by R = [rl, 
r a ,  . 
. 
.I, where ri is the 
number of units of the ith kind of resource available. If an iteration of a loop 
requires ni units of resource i, then the average initiation interval of a pipelined 
loop is at least maxi(ni/ri) clock cycles. Software pipelining requires that the 
initiation intervals between any pair of iterations have a constant value. Thus, 
the initiation interval must have at least maxi bi/ril 
clocks. If maxi(ni/ri) is 
less than 1, it is useful to unroll the source code a small number of times. 
Example 10.17 : Let us return to our software-pipelined loop shown in Fig. 
10.20. Recall that the target machine can issue one load, one arithmetic op- 
eration, one store, and one loop-back branch per clock. Since the loop has 
two loads, two arithmetic operations, and one store operation, the minimum 
initiation interval based on resource constraints is 2 clocks. 
Iteration 1 
Ld Alu St 
Iteration 2 
Ld Alu St 
Iteration 3 
Ld Alu St 
Iteration 4 
Steady state 
Ld Alu St 
Ld Alu St 
Figure 10.24: Resource requirements of four consecutive iterations from the 
code in Example 10.13 
Figure 10.24 shows the resource requirements of four consecutive iterations 
across time. More resources are used as more iterations get initiated, culmi- 
10.5. SOFTWARE PIPELINING 
747 
nating in maximum resource commitment in the steady state. Let RT be the 
resource-reservation table representing the commitment of one iteration, and let 
RTs represent the commitment of the steady state. RTs combines the commit- 
ment from four consecutive iterations started T clocks apart. The commitment 
of row 0 in the table RTs corresponds to the sum of the resources committed in 
RT 
[0], 
RT 
[2], 
RT[4], 
and RT 
[6]. Similarly, the commitment of row 1 
in the ta- 
ble corresponds to the sum of the resources committed in RT[l], 
RT[3], RT[5], 
and RT[7]. That is, the resources committed in the ith row in the steady state 
are given by 
RTs [i] = 
RT 
[t]. 
{t I (t mod 2)=2) 
We refer to the resource-reservation table representing the steady state as the 
modular resource-reservation table of the pipelined loop. 
To check if the software-pipeline schedule has any resource conflicts, we can 
simply check the commitment of the modular resource-reservation table. Surely, 
if the commitment in the steady state can be satisfied, so can the commitments 
in the prolog and epilog, the portions of code before and after the steady-state 
loop. 
In general, given an initiation interval T and a resource-reservation table of 
an iteration RT, the pipelined schedule has no resource conflicts on a machine 
with resource vector R if and only if RTs[i] 5 R for all i = 
O , 1 , .  . . , 
T - 
1. 
Data-Dependence Constraints 
Data dependences in software pipelining are different from those we have en- 
countered so far because they can form cycles. An operation may depend on 
the result of the same operation from a previous iteration. It is no longer ade- 
quate to label a dependence edge by just the delay; we also need to distinguish 
between instances of the same operation in different iterations. We label a de- 
pendence edge nl -+ 
n
2
 with label (6, 
d) if operation n 2  in iteration i must be 
delayed by at least d clocks after the execution of operation nl in iteration i - 
6. 
Let S, 
a function from the nodes of the data-dependence graph to integers, be 
the software pipeline schedule, and let T be the initiation interval target. Then 
The iteration difference, 6, must be nonnegative. Moreover, given a cycle of 
data-dependence edges, at least one of the edges has a positive iteration differ- 
ence. 
Example 10.18 : Consider the following loop, and suppose we do not know 
the values of p and q: 
f o r  (i = 0 ;  i < n; i++) 
* 
(p++) = * 
(q++) + c ;  
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
We must assume that any pair of * 
(p++) and * 
(q++) accesses may refer to 
the same memory location. Thus, all the reads and writes must execute in 
the original sequential order. Assuming that the target machine has the same 
characteristics as that described in Example 10.13, the data-dependence edges 
for this code are as shown in Fig. 10.25. Note, however, that we ignore the 
loop-control instructions that would have to be present, either computing and 
testing i, 
or doing the test based on the value of R1 
or R2. 
Figure 10.25: Data-dependence graph for Example 10.18 
The iteration difference between related operations can be greater than one, 
as shown in the following example: 
for (i = 2; i < n; i++) 
A [ i ]  
= B[i] + A[i-21 
Here the value written in iteration i is used two iterations later. The dependence 
edge between the store of A[i] and the load of A[i - 
2
1
 thus has a difference of 
2 iterations. 
The presence of data-dependence cycles in a loop imposes yet another limit 
on its execution throughput. For example, the data-dependence cycle in Fig. 
10.25 imposes a delay of 4 clock ticks between load operations from consecutive 
iterations. That is, loops cannot execute at a rate faster than one iteration 
every 4 clocks. 
The initiation interval of a pipelined loop is no smaller than 
ma* 
["einc::] 
c a cycle in G Ce 
in , 
clocks. 
In summary, the initiation interval of each software-pipelined loop is bound- 
ed by the resource usage in each iteration. Namely, the initiation interval must 
be no smaller than the ratio of units needed of each resource and the units 
10.5. SOFTWARE PIPELINING 
749 
available on the machine. In addition, if the loops have data-dependence cycles, 
then the initiation interval is further constrained by the sum of the delays in 
the cycle divided by the sum of the iteration differences. The largest of these 
quantities defines a lower bound on the initiation interval. 
10.5.6 A Software-Pipelining Algorithm 
The goal of software pipelining is to find a schedule with the smallest possible 
initiation interval. The problem is NP-complete, and can be formulated as an 
integer-linear-programming problem. We have shown that if we know what the 
minimum initiation interval is, the scheduling algorithm can avoid resource con- 
flicts by using the modular resource-reservation table in placing each operation. 
But we do not know what the minimum initiation interval is until we can find 
a schedule. How do we resolve this circularity? 
We know that the initiation interval must be greater than the bound com- 
puted from a loop's resource requirement and dependence cycles as discussed 
above. If we can find a schedule meeting this bound, we have found the opti- 
mal schedule. If we fail to find such a schedule, we can try again with larger 
initiation intervals until a schedule is found. Note that if heuristics, rather than 
exhaustive search, are used, this process may not find the optimal schedule. 
Whether we can find a schedule near the lower bound depends on properties 
of the data-dependence graph and the architecture of the target machine. We 
can easily find the optimal schedule if the dependence graph is acyclic and 
if every machine instruction needs only one unit of one resource. It is also 
easy to find a schedule close to the lower bound if there are more hardware 
resources than can be used by graphs with dependence cycles. For such cases, 
it is advisable to start with the lower bound as the initial initiation-interval 
target, then keep increasing the target by just one clock with each scheduling 
attempt. Another possibility is to find the initiation interval using a binary 
search. We can use as an upper bound on the initiation interval the length of 
the schedule for one iteration produced by list scheduling. 
10.5.7 Scheduling Acyclic Data-Dependence Graphs 
For simplicity, we assume for now that the loop to be software pipelined contains 
only one basic block. This assumption will be relaxed in Section 10.5.11. 
Algorithm 10.19 : Software pipelining an acyclic dependence graph. 
INPUT: A machine-resource vector R = [rl 
, 
r2, . 
. . 
1, where ri is the number 
of units available of the ith kind of resource, and a data-dependence graph 
G = (N, 
E). Each operation n in N is labeled with its resource-reservation 
table RT,; each edge e = 
nl + 
n
2
 in E is labeled with (be, 
de) indicating that 
n2 must execute no earlier than de clocks after node nl from the Seth preceding 
iteration. 
OUTPUT: A software-pipelined schedule S 
and an initiation interval T .  
CHAPTER 10. INSTRUCTION-LE 
VEL PARALLELISM 
METHOD: Execute the program in Fig. 10.26. 
1
7
 
To = 
max 1 
En,i 
RTn (i, 
j )  1 
. 
7 
3 
rJ' 
for (T = 
To, 
To + 
1,. 
. 
. , 
until all nodes in N are scheduled) { 
RT = an empty reservation table with T rows; 
for (each n in N in prioritized topological order) { 
SO = 
maXe=p-+n 
in E (S(P) 
+ 
de) ; 
for ( s = s o , s o + l ,  ... 
, s o + T - 1 )  
if (NodeScheduled 
(RT, 
T, 
n, 
s) break; 
if (n cannot be scheduled in RT) break; 
1 
I 
1 
NodeScheduled (RT, 
T, 
n, 
s) { 
RT' = 
RT; 
for (each row i in RTn) 
RTr[(s 
+ 
i) 
mod T] = 
RTf[(s 
+ 
i) 
mod T] + 
RTn[i]; 
if (for all i, RT1(i) 
5 R) { 
RT = 
RT'; 
S(n) = 
s; 
return true; 
I- 
else return false; 
Figure 10.26: Software-pipelining algorithm for acyclic graphs 
Algorithm 10.19 software pipelines acyclic data-dependence graphs. The 
algorithm first finds a bound on the initiation interval, To, based on the re- 
source requirements of the operations in the graph. It then attempts to find 
a software-pipelined schedule starting with To as the target initiation interval. 
The algorithm repeats with increasingly larger initiation intervals if it fails to 
find a schedule. 
The algorithm uses a list-scheduling approach in each attempt. It uses a 
modular resource-reservation RT to keep track of the resource commitment in 
the steady state. Operations are scheduled in topological order so that the 
data dependences can always be satisfied by delaying operations. To schedule 
an operation, it first finds a lower bound so according to the data-dependence 
constraints. It then invokes NodeScheduled to check for possible resource con- 
flicts in the steady state. If there is a resource conflict, the algorithm tries to 
schedule the operation in the next clock. If the operation is found to conflict for 
10.5. SOFTWARE PIPELINING 
75 
1 
T consecutive clocks, because of the modular nature of resource-conflict detec- 
tion, further attempts are guaranteed to be futile. At that point, the algorithm 
considers the attempt a failure, and another initiation interval is tried. 
The heuristics of scheduling operations as soon as possible tends to minimize 
the length of the schedule for an iteration. Scheduling an instruction as early 
as possible, however, can lengthen the lifetimes of some variables. For example, 
loads of data tend to be scheduled early, sometimes long before they are used. 
One simple heuristic is to schedule the dependence graph backwards because 
there are usually more loads than stores. 
10.5.8 Scheduling Cyclic Dependence Graphs 
Dependence cycles complicate software pipelining significantly. When schedul- 
ing operations in an acyclic graph in topological order, data dependences with 
scheduled operations can impose only a lower bound on the placement of each 
operation. As a result, it is always possible to satisfy the data-dependence con- 
straints by delaying operations. The concept of "topological order" does not 
apply to cyclic graphs. In fact, given a pair of operations sharing a cycle, plac- 
ing one operation will impose both a lower and upper bound on the placement 
of the second. 
Let nl and n
2
 be two operations in a dependence cycle, S be a software- 
pipeline schedule, and T be the initiation interval for the schedule. A depen- 
dence edge nl + n
2
 with label ( S l ,  
d l )  imposes the following constraint on 
S(nl) 
and S(n2): 
(81 x T )  
+S(n2) 
- 
S(n1) 
2 
dl. 
Similarly, a dependence edge (nl, 
n2) 
with label (S2, 
d2) imposes constraint 
( 6 2  x T )  
+ 
S ( w )  
- 
S(n2) 
> 
d2. 
Thus, 
S(n1) 
+ 
dl - 
(61 x T )  
5 S(n2) 
2 
S(n1) 
- 
d2 + 
( 6 2  x T ) .  
A strongly connected component (SCC) in a graph is a set of nodes where 
every node in the component can be reached by every other node in the compo- 
nent. Scheduling one node in an SCC will bound the time of every other node 
in the component both from above and from below. Transitively, if there exists 
a path p leading from nl to n2, then 
S ( n 2 )  - 
S(nl) 
2 C (de - 
(6, x T ) )  
Observe that 
752 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
Around any cycle, the sum of the 6's must be positive. If it were 0 or 
negative, then it would say that an operation in the cycle either had to 
precede itself or be executed at the same clock for all iterations. 
The schedule of operations within an iteration is the same for all iterations; 
that requirement is essentially the meaning of a "software pipeline." As 
a result, the sum of the delays (second components of edge labels in a 
data-dependence graph) around a cycle is a lower bound on the initiation 
interval T. 
When we combine these two points, we see that for any feasible initiation inter- 
val T, the value of the right side of Equation (10.1) must be negative or zero. 
As a result, the strongest constraints on the placement of nodes is obtained 
from the simple paths - 
those paths that contain no cycles. 
Thus, for each feasible T, computing the transitive effect of data depen- 
dences on each pair of nodes is equivalent to finding the length of the longest 
simple path from the first node to the second. Moreover, since cycles cannot 
increase the length of a path, we can use a simple dynamic-programming al- 
gorithm to find the longest paths without the "simple-path" requirement, and 
be sure that the resulting lengths will also be the lengths of the longest simple 
paths (see Exercise 10.5.7). 
Figure 10.27: Dependence graph and resource requirement in Example 10.20 
Example 10.20 : 
Figure 10.27 shows a data-dependence graph with four nodes 
a, 
b, c, d. Attached to each node is its resource-reservation table; attached to 
each edge is its iteration difference and delay. Assume for this example that the 
target machine has one unit of each kind of resource. Since there are three uses 
of the first resource and two of the second, the initiation interval must be no less 
than 3 clocks. There are two SCC's in this graph: the first is a trivial component 
consisting of the node a alone, and the second consists of nodes b, c, and d. The 
longest cycle, b, c, d, b, has a total delay of 3 clocks connecting nodes that are 
1 
iteration apart. Thus, the lower bound on the initiation interval provided by 
data-dependence cycle constraints is also 3 clocks. 
10.5. SOFT 
WARE PIPELINING 
753 
Placing any of b, c, or d in a schedule constrains all the other nodes in the 
component. Let T be the initiation interval. Figure 10.28 shows the transitive 
dependences. Part (a) shows the delay and the iteration difference 6, for each 
edge. The delay is represented directly, but 6 is represented by "adding" to the 
delay the value -ST. 
Figure 10.28(b) shows the length of the longest simple path between two 
nodes, when such a path exists; its entries are the sums of the expressions given 
by Fig. 10.28(a), 
for each edge along the path. Then, in (c) and (d) 
, 
we see the 
expressions of (b) with the two relevant values of T ,  
that is, 3 and 4, substituted 
for T. The difference between the schedule of two nodes S(n2) 
- 
S(nl) must 
be no less than the value given in entry (nl , 
n2) 
in each of the tables (c) or (d) 
, 
depending on the value of T chosen. 
For instance, consider the entry in Fig. 10.28 for the longest (simple) path 
from c to b, which is 2 - 
T. The longest simple path from c to b is c + 
d + 
b. 
The total delay is 2 along this path, and the sum of the 6's is 1, 
representing the 
fact that the iteration number must increase by 1. Since T is the time by which 
each iteration follows the previous, the clock at which b must be scheduled is 
at least 2 - 
T clocks after the clock at which c is scheduled. Since T is at least 
3, we are really saying that b may be scheduled T - 
2 clocks before c, or later 
than that clock, but not earlier. 
Notice that considering nonsimple paths from c to b does not produce a 
stronger constraint. We can add to the path c -+ 
d -+ b any number of iterations 
of the cycle involving d and b. If we add k such cycles, we get a path length 
of 2 - 
T + 
k (3 - 
T) 
, since the total delay along the path is 3, and the sum of 
the 6's is 1. Since T 2 
3, this length can never exceed 2 - 
T ;  i.e., the strongest 
lower bound on the clock of b relative to the clock of c is 2 - 
T ,  the bound we 
get by considering the longest simple path. 
For example, from entries (b, 
c) and (c, 
b), we see that 
That is, 
Put equivalently, c must be scheduled one clock after b. If T = 
4, however, 
That is, c is scheduled one or two clocks after b. 
Given the all-points longest path information, we can easily compute the 
range where it is legal to place a node due to data dependences. We see that 
there is no slack in the case when T = 
3, and the slack increases as T increases. 
CHAPTER 10. INSTRUCTION-LE VEL PARALLELISM 
a
b
c
d
 
a
 
b
 
C 
d
 
a
b
c
d
 
(a) Original edges. 
(b) Longest simple paths. 
a
b
c
d
 
a
 
b
 
C 
d
 
a
b
c
d
 
(c) Longest simple paths (T=3). 
(d) Longest simple paths (T=4). 
Figure 10.28: Transitive dependences in Example 10.20 
Algorit hrn 10.2 
1 : 
Software pipelining. 
INPUT: A machine-resource vector R = [rl 
, 
r2, . . 
. 
1, where ri is the number 
of units available of the ith kind of resource, and a data-dependence graph 
G = (N, 
E). Each operation n in N is labeled with its resource-reservation 
table RT,; each edge e = 
nl -+ 
n2 in E is labeled with (S,, d,) indicating that 
n2 must execute no earlier than d, clocks after node nl from the Seth preceding 
iteration. 
OUTPUT: A software-pipelined schedule S 
and an initiation interval T. 
METHOD: Execute the program in Fig. 10.29. 
Algorithm 10.21 
has a high-level structure similar to that of Algorithm 10.19, 
which only handles acyclic graphs. The minimum initiation interval in this case 
is bounded not just by resource requirements, but also by the data-dependence 
cycles in the graph. The graph is scheduled one strongly connected component 
at a time. By treating each strongly connected component as a unit, edges be- 
tween strongly connected components necessarily form an acyclic graph. While 
the top-level loop in Algorithm 10.19 schedules nodes in the graph in topological 
order, the top-level loop in Algorithm 10.21 schedules strongly connected com- 
ponents in topological order. As before, if the algorithm fails to schedule all the 
components, then a larger initiation interval is tried. Note that Algorithm 10.21 
behaves exactly like Algorithm 10.19 if given an acyclic data-dependence graph. 
Algorithm 10.21 computes two more sets of edges: E' is the set of all edges 
whose iteration difference is 0, E* 
is the all-points longest-path edges. That is, 
10.5. SOFTWARE PIPELINING 
. 
. 
To = 
ma. (my 1 
En,; 
RTn(i, 
I )  
1 , 
max 1". 
in c :r 
1). 
rj 
c acyclein G Ce 
,, , 
for (T = 
To, 
To + 
1,. 
. 
. or until all SCC's in G are scheduled) { 
R T  = an empty reservation table with T rows; 
E* 
= AllPairsLongestPath(G, T); 
for (each SCC C in G in prioritized topological order) { 
for (all n in C) 
SO 
(12) = 
maxe=p+n in E* 
,p scheduled (S(p) + 
de); 
first = some n such that so(n) 
is a minimum; 
so = so (first); 
for (s = 
so; s < so + T ;  s = 
s + 1) 
if (SccScheduled (RT, 
T, 
C, 
first, s)) 
break; 
if (C cannot be scheduled in RT) break; 
} 
1 
1 
SccScheduled(RT, 
T, 
c, 
first, s) { 
RT' = 
RT; 
if (not Nodescheduled (RT', T,first, s)) return false; 
for (each remaining n in c in prioritized 
topological order of edges in El) { 
S1 = 
maxe=nl+n in E* 
,nl in c,nl scheduled S(nl) + 
de - 
(6, X T )  
; 
Su = 
mine=n+nl in E*,nl 
in c,nl scheduled S(n') - 
de + 
(be x T); 
for (s = 
sl; 5 min(su,sl + T  - 
I); s = s +  1) 
if NodeScheduled(RT', T, 
n, 
s) break; 
if (n cannot be scheduled in RT') return false; 
} 
R T  = 
RT'; 
return true; 
} 
Figure 10.29: A software-pipelining algorithm for cyclic dependence graphs 
756 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
for each pair of nodes (p, 
n), there is an edge e in E* 
whose associated distance 
d, is the length of the longest simple path from p to n, 
provided that there is at 
least one path from p to n. E* 
is computed for each value of T, the initiation- 
interval target. It is also possible to perform this computation just once with 
a symbolic value of T and then substitute for T in each iteration, as we did in 
Example 10.20. 
Algorithm 10.21 uses backtracking. If it fails to schedule a SCC, it tries to 
reschedule the entire SCC a clock later. These scheduling attempts continue for 
up to T clocks. Backtracking is important because, as shown in Example 10.20, 
the placement of the first node in an SCC can fully dictate the schedule of all 
other nodes. If the schedule happens not to fit with the schedule created thus 
far, the attempt fails. 
To schedule a SCC, the algorithm determines the earliest time each node 
in the component can be scheduled satisfying the transitive data dependences 
in E*. It then picks the one with the earliest start time as the first node 
to schedule. The algorithm then invokes SccScheduled to try to schedule the 
component at the earliest start time. The algorithm makes at most T attempts 
with successively greater start times. If it fails, then the algorithm tries another 
initiation interval. 
The SccScheduled 
algorithm resembles Algorithm 10.19, but has three major 
differences. 
1. The goal of SccScheduled 
is to schedule the strongly connected component 
at the given time slot s. If the first node of the strongly connected com- 
ponent cannot be scheduled at s, SccScheduled returns false. The main 
function can invoke SccScheduled again with a later time slot if that is 
desired. 
2. The nodes in the strongly connected component are scheduled in topolog- 
ical order, based on the edges in El. Because the iteration differences on 
all the edges in El are 0, these edges do not cross any iteration boundaries 
and cannot form cycles. (Edges that cross iteration boundaries are known 
as loop carried). Only loop-carried dependences place upper bounds on 
where operations can be scheduled. So, this scheduling order, along with 
the strategy of scheduling each operation as early as possible, maximizes 
the ranges in which subsequent nodes can be scheduled. 
3. For strongly connected components, dependences impose both a lower and 
upper bound on the range in which a node can be scheduled. SccSched- 
uled computes these ranges and uses them to further limit the scheduling 
attempts. 
Example 10.22 : 
Let us apply Algorithm 10.21 
to the cyclic data-dependence 
graph in Example 10.20. The algorithm first computes that the bound on the 
initiation interval for this example is 3 clocks. We note that it is not possible 
to meet this lower bound. When the initiation interval T is 3, the transitive 
10.5. SOFTWARE PIPELINING 
757 
dependences in Fig. 10.28 dictate that S ( d )  - 
S ( b )  = 2. Scheduling nodes b 
and d two clocks apart will produce a conflict in a modular resource-reservation 
table of length 3. 
Figure 10.30: Behavior of Algorithm 10.21 on Example 10.20 
Figure 10.30 shows how Algorithm 10.21 
behaves with this example. It first 
tries to find a schedule with a 3-clock initiation interval. The attempt starts by 
scheduling nodes a and b as early as possible. However, once node b is placed in 
clock 2, node c can only be placed at clock 3, which conflicts with the resource 
usage of node a. That is, a and c both need the first resource at clocks that 
have a remainder of 0 modulo 3. 
The algorithm backtracks and tries to schedule the strongly connected com- 
ponent {b, c, d) a clock later. This time node b is scheduled at clock 3, and node 
c is scheduled successfully at clock 4. Node d, however, cannot be scheduled in 
758 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
clock 5. That is, both b and d need the second resource at clocks that have a 
remainder of 0 modulo 3. Note that it is just a coincidence that the two con- 
flicts discovered so far are at clocks with a remainder of 0 modulo 3; the conflict 
might have occurred at clocks with remainder 1 
or 2 in another example. 
The algorithm repeats by delaying the start of the SCC {b, c, 
d} by one 
more clock. But, as discussed earlier, this SCC can never be scheduled with an 
initiation interval of 3 clocks, so the attempt is bound to fail. At this point, 
the algorithm gives up and tries to find a schedule with an initiation interval 
of 4 clocks. The algorithm eventually finds the optimal schedule on its sixth 
attempt. 
1
0
.
5
.
9
 Improvements to the Pipelining Algorithms 
Algorithm 10.21 is a rather simple algorithm, although it has been found to 
work well on actual machine targets. The important elements in this algorithm 
are 
1. The use of a modular resource-reservation table to check for resource 
conflicts in the steady state. 
2. The need to compute the transitive dependence relations to find the legal 
range in which a node can be scheduled in the presence of dependence 
cycles. 
3. ~ a c k t r a c k i n ~  
is useful, and nodes on critical cycles (cycles that place the 
highest lower bound on the initiation interval T) must be rescheduled 
together because there is no slack between them. 
There are many ways to improve Algorithm 10.21. For instance, the al- 
gorithm takes a while to realize that a bclock initiation interval is infeasible 
for the simple Example 10.22. We can schedule the strongly connected com- 
ponents independently first to determine if the initiation interval is feasible for 
each component. 
We can also modify the order in which the nodes are scheduled. The order 
used in Algorithm 10.21 has a few disadvantages. First, because nontrivial 
SCC's are harder to schedule, it is desirable to schedule them first. Second, some 
of the registers may have unnecessarily long lifetimes. It is desirable to pull the 
definitions closer to the uses. One possibility is to start with scheduling strongly 
connected components with critical cycles first, then extend the schedule on 
both ends. 
10.5.10 Modular Variable Expansion 
A scalar variable is said to be privatixable in a loop if its live range falls within 
an iteration of the loop. In other words, a privatizable variable must not be live 
upon either entry or exit of any iteration. These variables are so named because 
10.5. SOFTWARE PIPELINING 
759 
Are There Alternatives to Heuristics? 
We can formulate the problem of simultaneously finding an optimal 
software pipeline schedule and register assignment as an integer-linear- 
programming problem. While many integer linear programs can be solved 
quickly, some of them can take an exorbitant amount of time. To use an 
integer-linear-programming solver in a compiler, we must be able to abort 
the procedure if it does not complete within some preset limit. 
Such an approach has been tried on a target machine (the SGI R8000) 
empirically, and it was found that the solver could find the optimal solution 
for a large percentage of the programs in the experiment within a reason- 
able amount of time. It turned out that the schedules produced using a 
heuristic approach were also close to optimal. The results suggest that, 
at least for that machine, it does not make sense to use the integer-linear- 
programming approach, especially from a software engineering perspec- 
tive. Because the integer-linear solver may not finish, it is still necessary 
to implement some kind of a heuristic scheduler in the compiler. Once 
such a heuristic scheduler is in place, there is little incentive to implement 
a scheduler based on integer programming techniques as well. 
different processors executing different iterations in a loop can have their own 
private copies and thus not interfere with one another. 
Variable expansion refers to the transformation of converting a privatizable 
scalar variable into an array and having the ith iteration of the loop read and 
write the ith element. This transformation eliminates the antidependence con- 
straints between reads in one iteration and writes in the subsequent iterations, 
as well as output dependences between writes from different iterations. If all 
loop-carried dependences can be eliminated, all the iterations in the loop can 
be executed in parallel. 
Eliminating loop-carried dependences, and thus eliminating cycles in the 
data-dependence graph, can greatly improve the effectiveness of software pipe- 
lining. As illustrated by Example 10.15, we need not expand a privatizable 
variable fully by the number of iterations in the loop. Only a small number of 
iterations can be executing at a time, and privatizable variables may simultane- 
ously be live in an even smaller number of iterations. The same storage can thus 
be reused to hold variables with nonoverlapping lifetimes. More specifically, if 
the lifetime of a register is 1 clocks, and the initiation interval is T, then only 
q = 
values can be live at any one point. We can allocate q registers to the 
variable, with the variable in the ith iteration using the (i mod q)th register. 
We refer to this transformation as modular variable expansion. 
Algorithm 10.23 : 
Software pipelining with modular variable expansion. 
INPUT: A data-dependence graph and a machine-resource description. 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
OUTPUT: Two loops, one software pipelined and one unpipelined. 
METHOD: 
1. Remove the loop-carried antidependences and output dependences asso- 
ciated with privatizable variables from the data-dependence graph. 
2. Software-pipeline the resulting dependence graph using Algorithm 10.21. 
Let T be the initiation interval for which a schedule is found, and L be 
the length of the schedule for one iteration. 
3. From the resulting schedule, compute q,, the minimum number of regis- 
ters needed by each privatizable variable v. Let Q = 
max, q,. 
4. Generate two loops: a software-pipelined loop and an unpipelined loop. 
The software-pipelined loop has 
copies of the iterations, placed T clocks apart. It has a prolog with 
instructions, a steady state with QT instructions, and an epilog of L - 
T 
instructions. Insert a loop-back instruction that branches from the bottom 
of the steady state to the top of the steady state. 
The number of registers assigned to privatizable variable v is 
q, 
if Q mod q, = 
0 
otherwise 
The variable v in iteration i uses the (i mod q:)th register assigned. 
Let n be the variable representing the number of iterations in the source 
loop. The software-pipelined loop is executed if 
The number of times the loop-back branch is taken is 
Thus, the number of source iterations executed by the software-pipelined 
loop is 
[$I---l+Qnl 
i f n 2  [ $ l + Q - I  
otherwise 
The number of iterations executed by the unpipelined loop is n
s
 = 
n - 
n 2 .  
10.5. SOFTWARE PIPELINING 
Example 10.24 : 
For the software-pipelined loop in Fig. 10.22, L = 
8, T = 
2, 
and Q = 
2. The software-pipelined loop has 7 copies of the iterations, with the 
prolog, steady state, and epilog having 6, 4, and 6 instructions, respectively. 
Let n be the number of iterations in the source loop. The software-pipelined 
loop is executed if n 2 
5, in which case the loop-back branch is taken 
times, and the software-pipelined loop is responsible for 
of the iterations in the source loop. 
Modular expansion increases the size of the steady state by a factor of 
Q. Despite this increase, the code generated by Algorithm 10.23 is still fairly 
compact. In the worst case, the software-pipelined loop would take three times 
as many instructions as that of the schedule for one iteration. Roughly, together 
with the extra loop generated to handle the left-over iterations, the total code 
size is about four times the original. This technique is usually applied to tight 
inner loops, so this increase is reasonable. 
Algorithm 10.23 minimizes code expansion at the expense of using more 
registers. We can reduce register usage by generating more code. We can use 
the minimum q, registers for each variable v if we use a steady state with 
instructions. Here, LCMu represents the operation of taking the least common 
multzple of all the qu 
's, as v ranges over all the privatizable variables (i.e., the 
smallest integer that is an integer multiple of all the q,'s). Unfortunately, the 
least common multiple can be quite large even for a few small qu7s. 
10.5.11 Conditional Statements 
If predicated instructions are available, we can convert control-dependent in- 
structions into predicated ones. Predicated instructions can be software-pipe- 
lined like any other operations. However, if there is a large amount of data- 
dependent control flow within the loop body, scheduling techniques described 
in Section 10.4 may be more appropriate. 
If a machine does not have predicated instructions, we can use the concept 
of hierarchical reduction, described below, to handle a small amount of data- 
dependent control flow. Like Algorithm 10.11, in hierarchical reduction the 
control constructs in the loop are scheduled inside-out, starting with the most 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
deeply nested structures. As each construct is scheduled, the entire construct is 
reduced to a single node representing all the scheduling constraints of its com- 
ponents with respect to the other parts of the program. This node can then be 
scheduled as if it were a simple node within the surrounding control construct. 
The scheduling process is complete when the entire program is reduced to a 
single node. 
In the case of a conditional statement with "then" and "else" branches, we 
schedule each of the branches independently. Then: 
1. The constraints of the entire conditional statement are conservatively 
taken to be the union of the constraints from both branches. 
2. Its resource usage is the maximum of the resources used in each branch. 
3. Its precedence constraints are the union of those in each branch, obtained 
by pretending that both branches are executed. 
This node can then be scheduled like any other node. Two sets of code, cor- 
responding to the two branches, are generated. Any code scheduled in parallel 
with the conditional statement is duplicated in both branches. If multiple con- 
ditional statements are overlapped, separate code must be generated for each 
combination of branches executed in parallel. 
10.5.12 Hardware Support for Software Pipelining 
Specialized hardware support has been proposed for minimizing the size of 
software-pipelined code. The rotating register file in the Itanium architecture is 
one such example. A rotating register file has a base register, which is added to 
the register number specified in the code to derive the actual register accessed. 
We can get different iterations in a loop to use different registers simply by 
changing the contents of the base register at the boundary of each iteration. 
The Itanium architecture also has extensive predicated instruction support. Not 
only can predication be used to convert control dependence to data dependence 
but it also can be used to avoid generating the prologs and epilogs. The body 
of a software-pipelined loop contains a superset of the instructiolns issued in the 
prolog and epilog. We can simply generate the code for the steady state and 
use predication appropriately to suppress the extra operations to get the effects 
of having a prolog and an epilog. 
While Itanium's hardware support improves the density of software-pipe- 
lined code, we must also realize that the support is not cheap. Since software 
pipelining is a technique intended for tight innermost loops, pipelined loops tend 
to be small anyway. Specialized support for software pipelining is warranted 
principally for machines that are intended to execute many software-pipelined 
loops and in situations where it is very important to minimize code size. 
10.5. SOFTWARE PIPELINING 
1) L: LD 
R 1 ,  a ( R 9 )  
2) 
ST b ( R 9 )  , 
R 1  
3 
LD R 2 ,  c ( R 9 )  
4, 
ADD R 3 ,  R 1 ,  R 2  
5) 
ST c ( R 9 ) ,  R 3  
6) 
SUB R 4 ,  R 1 ,  R 2  
7 )  
ST b (R9) , 
R 4  
8) 
BL R 9 ,  L 
Figure 10.31: Machine code for Exercise 10.5.2 
10.5.13 Exercises for Section 10.5 
Exercise 10.5.1 
: In Example 10.20 we showed how to establish the bounds 
on the relative clocks at which b and c are scheduled. Compute the bounds for 
each of five other pairs of nodes (i) for general T (ii) 
for T = 
3 (iii) for T = 
4. 
Exercise 10.5.2 
: 
In Fig. 10.31 
is the body of a loop. Addresses such as a 
(R9) 
are intended to be memory locations, where a is a constant, and R 9  is the register 
that counts iterations through the loop. You may assume that each iteration 
of the loop accesses different locations, because R 9  has a different value. Using 
the machine model of Example 10.12, schedule the loop of Fig. 10.31 in the 
following ways: 
a) Keeping each iteration as tight as possible (i.e., only introduce one nop af- 
ter each arithmetic operation), unroll the loop twice. Schedule the second 
iteration to commence at the earliest possible moment without violat- 
ing the constraint that the machine can only do one load, one store, one 
arithmetic operation, and one branch at any clock. 
b) Repeat part (a), but unroll the loop three times. Again, start each itera- 
tion as soon as you can, subject to the machine constraints. 
! 
c) Construct fully pipelined code subject to the machine constraints. In this 
part, you can introduce extra nop's if needed, but you must start a new 
iteration every two clock ticks. 
Exercise 10.5.3 
: A certain loop requires 5 loads, 7 stores, and 8 arithmetic 
operations. What is the minimum initiation interval for a software pipelining 
of this loop on a machine that executes each operation in one clock tick, and 
has resources enough to do, in one clock tick: 
a) 3 loads, 4 stores, and 5 arithmetic operations. 
b) 3 loads, 3 stores, and 3 arithmetic operations. 
764 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
! Exercise 1
0
.
5
.
4
 
: Using the machine model of Example 10.12, Find the min- 
imum initiation interval and a uniform schedule for the iterations, for the fol- 
lowing loop: 
f o r  (i 
= 1; i < n; i + + )  ( 
A[i] = B[i-11 + 1; 
B [ i l  = A[i-11 
+ 2; 
> 
Remember that the counting of iterations is handled by auto-increment of reg- 
isters, and no operations are needed solely for the counting associated with the 
for-loop. 
! 
Exercise 1
0
.
5
.
5
 
: 
Prove that Algorithm 10.19, in the special case where every 
operation requires only one unit of one resource, can always find a software- 
pipeline schedule meeting the lower bound. 
! Exercise 1
0
.
5
.
6
 
: 
Suppose we have a cyclic data-dependence graph with nodes 
a, b, c, and d. There are edges from a to b and from c to d with label (0,l) 
and there are edges from b to c and from d to a with label (1,l). There are no 
other edges. 
a) Draw the cyclic dependence graph. 
b) Compute the table of longest simple paths among the nodes. 
c) Show the lengths of the longest simple paths if the initiation interval T is 
2. 
d) Repeat (c) if T = 
3. 
e) For T = 
3, what are the constraints on the relative times that each of the 
instructions represented by a, b, c, and d may be scheduled? 
! Exercise 1
0
.
5
.
7
 
: Give an O(n3) algorithm to find the length of the longest 
simple path in an n-node graph, on the assumption that no cycle has a positive 
length. Hint: Adapt Floyd's algorithm for shortest paths (see, e.g., A. V. Aho 
and J. D. Ullman, Foundations of Computer Science, Computer Science Press, 
New York, 1992). 
!! Exercise 1
0
.
5
.
8
:
 
Suppose we have a machine with three instruction types, 
which we'll call A, B, and C. All instructions require one clock tick, and the 
machine can execute one instruction of each type at each clock. Suppose a loop 
consists of six instructions, two of each type. Then it is possible to execute 
the loop in a software pipeline with an initiation interval of two. However, 
some sequences of the six instructions require insertion of one delay, and some 
require insertion of two delays. Of the 90 possible sequences of two A's, two 
B's and two Cis, how many require no delay? How many require one delay? 
10.6. SUMMARY OF CHAPTER 10 
Hint: There is symmetry among the three instruction types so two sequences 
that can be transformed into one another by permuting the names A, B, and 
C must require the same number of delays. For example, ABBCAC must be 
the same as BCCABA. 
10.6 Summary of Chapter 10 
+ Architectural Issues: Optimized code scheduling takes advantage of fea- 
tures of modern computer architectures. Such machines often allow pipe- 
lined execution, where several instructions are in different stages of exe- 
cution at the same time. Some machines also allow several instructions 
to begin execution at the same time. 
+ Data Dependences: When scheduling instructions, we must be aware of 
the effect instructions have on each memory location and register. True 
data dependences occur when one instruction must read a location after 
another has written it. Antidependences occur when there is a write after 
a read, and output dependences occur when there are two writes to the 
same location. 
+ Eliminating Dependences: By using additional locations to store data, 
antidependences and output dependences can be eliminated. Only true 
dependences cannot be eliminated and must surely be respected when the 
code is scheduled. 
+ Data-Dependence Graphs for Basic Blocks: These graphs represent the 
timing constraints among the statements of a basic block. Nodes corre- 
spond to the statements. An edge from n to rn labeled d says that the 
instruction rn must start at least d clock cycles after instruction n starts. 
+ Prioritized Topological Orders: The data-dependence graph for a basic 
block is always acyclic, and there usually are many topological orders 
consistent with the graph. One of several heuristics can be used to select 
a preferred topological order for a given graph, e.g., choose nodes with 
the longest critical path first. 
+ List Scheduling: Given a prioritized topological order for a data-depend- 
ence graph, we may consider the nodes in that order. Schedule each node 
at the earliest clock cycle that is consistent with the timing constraints im- 
plied by the graph edges, the schedules of all previously scheduled nodes, 
and the resource constraints of the machine. 
+ Interblock Code Motion: Under some circumstances it is possible to move 
statements from the block in which they appear to a predecessor or suc- 
cessor block. The advantage is that there may be opportunities to execute 
instructions in parallel at the new location that do not exist at the orig- 
inal location. I
f
 there is not a dominance relation between the old and 
CHAPTER 10. INSTRUCTION-LEVEL PARALLELISM 
new locations, it may be necessary to insert compensation code along 
certain paths, in order to make sure that exactly the same sequence of 
instructions is executed, regardless of the flow of control. 
+ Do-All Loops: A do-all loop has no dependences across iterations, so any 
iterations may be executed in parallel. 
+ Software Pipelining of Do-All Loops: Software pipelining is a technique 
for exploiting the ability of a machine to execute several instructions at 
once. We schedule iterations of the loop to begin at small intervals, per- 
haps placing no-op instructions in the iterations to avoid conflicts between 
iterations for the machine's resources. The result is that the loop can be 
executed quickly, with a preamble, a coda, and (usually) a tiny inner loop. 
+ Do-Across Loops: Most loops have data dependences from each iteration 
to later iterations. These are called do-across loops. 
+ Data-Dependence Graphs for Do-Across Loops: To represent the depen- 
dences among instructions of a do-across loop requires that the edges be 
labeled by a pair of values: the required delay (as for graphs representing 
basic blocks) and the number of iterations that elapse between the two 
instructions that have a dependence. 
+ List Scheduling of Loops: To schedule a loop, we must choose the one 
schedule for all the iterations, and also choose the initiation interval at 
which successive iterations commence. The algorithm involves deriving 
the constraints on the relative schedules of the various instructions in the 
loop by finding the length of the longest acyclic paths between the two 
nodes. These lengths have the initiation interval as a parameter, and thus 
put a lower bound on the initiation interval. 
10.7 References for Chapter 10 
For a more in-depth discussion on processor architecture and design, we recom- 
mend Hennessy and Patterson [5]. 
The concept of data dependence was first discussed in Kuck, Muraoka, and 
Chen [6] and Lamport [8] in the context of compiling code for multiprocessors 
and vector machines. 
Instruction scheduling was first used in scheduling horizontal microcode 
([2, 3, 11, and 121). Fisher's work on microcode compaction led him to pro- 
pose the concept of a VLIW machine, where compilers directly can control the 
parallel execution of operations [3]. Gross and Hennessy [4] used instruction 
scheduling to handle the delayed branches in the first MIPS RISC instruction 
set. This chapter's algorithm is based on Bernstein and Rodeh's [I] 
more gen- 
eral treatment of scheduling of operations for machines with instruction-level 
parallelism. 
10.7. REFERENCES FOR CHAPTER 10 
767 
The basic idea behind software pipelining was first developed by Pate1 and 
Davidson [9] for scheduling hardware pipelines. Software pipelining was first 
used by Rau and Glaeser [lo] 
to compile for a machine with specialized hardware 
designed to support software pipelining. The algorithm described here is based 
on Lam [7], 
which assumes no specialized hardware support. 
I. Bernstein, D. and M. Rodeh, "Global instruction scheduling for super- 
scalar machines," Proc. ACM SIGPLAN 1991 Conference on Program- 
ming Language Design and Implementation, pp. 241-255. 
2. Dasgupta, S., "The organization of microprogram stores," Computing 
Surveys 11:l (1979), pp. 39-65. 
3. Fisher, J. A., "Trace scheduling: a technique for global microcode com- 
paction," IEEE Trans. on Computers C-30:7 (1981), pp. 478-490. 
4. Gross, T. R. and Hennessy, J. L., "Optimizing delayed branches," Proc. 
15th Annual Workshop on Microprogramming (1982), pp. 114-120. 
5. Hennessy, J. L. and D. A. Patterson, Computer Architecture: A Quanti- 
tative Approach, Third Edition, Morgan Kaufman, San Francisco, 2003. 
6. Kuck, D., Y. Muraoka, and S. Chen, "On the number of operations 
simultaneously executable in Fortran-like programs and their resulting 
speedup," IEEE Transactions on Computers C-21:12 (1972), pp. 1293- 
1310. 
7. Lam, M. S., "Software pipelining: an effective scheduling technique for 
VLIW machines," Proc. ACM SIGPLAN 1988 Conference on Program- 
ming Language Design and Implementation, pp. 318-328. 
8. Lamport, L., "The parallel execution of DO loops," Comm. ACM 17:2 
(1974), pp. 83-93. 
9. Patel, J. H. and E. S. Davidson, "Improving the throughput of a pipeline 
by insertion of delays," Proc. Third Annual Symposium on Computer Ar- 
chitecture (1976), pp. 159-164. 
10. Rau, B. R. and C. D. Glaeser, "Some scheduling techniques and an 
easily schedulable horizontal architecture for high performance scientific 
computing," Proc. 14th Annual Workshop on Microprogramming (1981), 
pp. 183-198. 
11. Tokoro, M., E. Tamura, and T. Takizuka, "Optimization of micropro- 
grams," IEEE Trans. on Computers C-30:7 (1981), pp. 491-504. 
12. Wood, G., "Global optimization of microprograms through modular con- 
trol constructs," Proc. 12th Annual Workshop in Microprogramming 
(1979), pp. 1-6. 
Chapter 11 
Optimizing for Parallelism 
and Locality 
This chapter shows how a compiler can enhance parallelism and locality in com- 
putationally intensive programs involving arrays to speed up target programs 
running on multiprocessor systems. Many scientific, engineering, and commer- 
cial applications have an insatiable need for computational cycles. Examples 
include weather prediction, protein-folding for designing drugs, fluid-dynamics 
for designing aeropropulsion systems, and quantum chromodynamics for study- 
ing the strong interactions in high-energy physics. 
One way to speed up a computation is to use parallelism. Unfortunately, it 
is not easy to develop software that can take advantage of parallel machines. 
Dividing the computation into units that can execute on different processors in 
parallel is already hard enough; yet that by itself does not guarantee a speedup. 
We must also minimize interprocessor communication, because communication 
overhead can easily make the parallel code run even slower than the sequential 
execution! 
Minimizing communication can be thought of as a special case of improving 
a program's data locality. In general, we say that a program has good data 
locality if a processor often accesses the same data it has used recently. Surely 
if a processor on a parallel machine has good locality, it does not need to com- 
municate with other processors frequently. Thus, parallelism and data locality 
need to be considered hand-in-hand. Data locality, by itself, is also important 
for the performance of individual processors. Modern processors have one or 
more level of caches in the memory hierarchy; a memory access can take tens of 
machine cycles whereas a cache hit would only take a few cycles. If a program 
does not have good data locality and misses in the cache often, its performance 
will suffer. 
Another reason why parallelism and locality are treated together in this same 
chapter is that they share the same theory. If we know how to optimize for data 
locality, we know where the parallelism is. You will see in this chapter that the 
770 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
program model we used for data-flow analysis in Chapter 9 is inadequate for 
parallelization and locality optimization. The reason is that work on data-flow 
analysis assumes we don't distinguish among the ways a given statement is 
reached, and in fact these Chapter 9 techniques take advantage of the fact that 
we don't distinguish among different executions of the same statement, e.g., in 
a loop. To parallelize a code, we need to reason about the dependences among 
different dynamic executions of the same statement to determine if they can be 
executed on different processors simultaneously. 
This chapter focuses on techniques for optimizing the class of numerical 
applications that use arrays as data structures and access them with simple 
regular patterns. More specifically, we study programs that have afine array 
accesses with respect to surrounding loop indexes. For example, if i and j are 
the index variables of surrounding loops, then Z[i][j] and Z[i][i + 
j] are affine 
accesses. A function of one or more variables, il , 
ia, 
. . 
. , 
in is afine if it can 
be expressed as a sum of a constant, plus constant multiples of the variables, 
i.e., co + 
clxl + 
~2x2 
+ . . + 
cnxn, where co, el,. 
. . , 
c, are constants. Affine 
functions are usually known as linear functions, although strictly speaking, 
linear functions do not have the co term. 
Here is a simple example of a loop in this domain: 
for (i 
= 0; i 
< 10; i++) 
( 
ZCil = 0; 
Because iterations of the loop write to different locations, different processors 
can execute different iterations concurrently. On the other hand, if there is 
another statement Z C j l  = I being executed, we need to worry about whether 
i could ever be the same as j, and if so, in which order do we execute those 
instances of the two statements that share a common value of the array index. 
Knowing which iterations can refer to the same memory location is impor- 
tant. This knowledge lets us specify the data dependences that must be honored 
when scheduling code for both uniprocessors and multiprocessors. Our objective 
is to find a schedule that honors all the data dependences such that operations 
that access the same location and cache lines are performed close together if 
possible, and on the same processor in the case of multiprocessors. 
The theory we present in this chapter is grounded in linear algebra and 
integer programming techniques. We model iterations in an n-deep loop nest 
as an n-dimensional polyhedron, whose boundaries are specified by the bounds 
of the loops in the code. Affine functions map each iteration to the array 
locations it accesses. We can use integer linear programming to determine if 
there exist two iterations that can refer to the same location. 
The set of code transformations we discuss here fall into two categories: 
afine partitioning and blocking. Affine partitioning splits up the polyhedra 
of iterations into components, to be executed either on different machines or 
one-by-one sequentially. On the other hand, blocking creates a hierarchy of 
iterations. Suppose we are given a loop that sweeps through an array row-by- 
11.2. BASIC CONCEPTS 
row. We may instead subdivide the array into blocks and visit all elements in a 
block before moving to the next. The resulting code will consist of outer loops 
traversing the blocks, and then inner loops to sweep the elements within each 
block. Linear algebra techniques are used to determine both the best affine 
partitions and the best blocking schemes. 
In the following, we first start with an overview of the concepts in parallel 
computation and locality optimization in Section 11.1. Then, Section 11.2 is 
an extended concrete example - 
matrix multiplication - 
that shows how loop 
transformations that reorder the computation inside a loop can improve both 
locality and the effectiveness of parallelization. 
Sections 11.3 
to Sections 11.6 present the preliminary information necessary 
for loop transformations. Section 11.3 shows how we model the individual 
iterations in a loop nest; Section 11.4 shows how we model array index functions 
that map each loop iteration to the array locations accessed by the iteration; 
Section 11.5 shows how to determine which iterations in a loop refer to the same 
array location or the same cache line using standard linear algebra algorithms; 
and Section 11.6 shows how to find all the data dependences among array 
references in a program. 
The rest of the chapter applies these preliminaries in coming up with the 
optimizations. Section 11.7 first looks at the simpler problem of finding par- 
allelism that requires no synchronization. To find the best affine partitioning, 
we simply find the solution to the constraint that operations that share a data 
dependence must be assigned to the same processor. 
Well, not too many programs can be parallelized without requiring any 
synchronization. Thus, in Sections 11.8 through 11.9.9, we consider the general 
case of finding parallelism that requires synchronization. We introduce the 
concept of pipelining, show how to find the affine partitioning that maximizes 
the degree of pipelining allowed by a program. We show how to optimize for 
locality in Section 11.10. Finally, we discuss how affine transforms are useful 
for optimizing for other forms of parallelism. 
11.1 Basic Concepts 
This section introduces the basic concepts related to parallelization and local- 
ity optimization. If operations can be executed in parallel, they also can be 
reordered for other goals such as locality. Conversely, if data dependences in 
a program dictate that instructions in a program must execute serially, there 
is obviously no parallelism, nor is there any opportunity to reorder instruc- 
tions to improve locality. Thus parallelization analysis also finds the available 
opportunities for code motion to improve data locality. 
To minimize communication in parallel code, we group together all related 
operations and assign them to the same processor. The resulting code must 
therefore have data locality. One crude approach to getting good data locality 
on a uniprocessor is to have the processor execute the code assigned to each 
772 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
processor in succession. 
In this introduction, we start by presenting an overview of parallel computer 
architectures. We then show the basic concepts in parallelization, the kind of 
transformations that can make a big difference, as well as the concepts useful 
for parallelization. We then discuss how similar considerations can be used to 
optimize locality. Finally, we introduce informally the mat 
hematical concepts 
used in this chapter. 
11.1.1 Multiprocessors 
The most popular parallel machine architecture is the symmetric multiproces- 
sor (SMP). High-performance personal computers often have two processors, 
and many server machines have four, eight, and some even tens of processors. 
Moreover, as it has become feasible for several high-performance processors to 
fit on a single chip, multiprocessors have become even more widely used. 
Processors on a symmetric multiprocessor share the same address space. To 
communicate, a processor can simply write to a memory location, which is then 
read by any other processor. Symmetric multiprocessors are so named because 
all processors can access all of the memory in the system with a uniform access 
time. Fig. 11.1 shows the high-level architecture of a multiprocessor. The 
processors may have their own first-level, second-level, and in some cases, even 
a third-level cache. The highest-level caches are connected to physical memory 
through typically a shared bus. 
Cache 
Cache 
First-level 
Cache 
1 
Cache 
Bus 
Memory 
Figure 11.1 
: The symmetric multi-processor architecture 
Symmetric multiprocessors use a coherent cache protocol to hide the presence 
of caches from the programmer. Under such a protocol, several processors are 
11.1. BASIC CONCEPTS 
allowed to keep copies of the same cache line1 at the same time, provided that 
they are only reading the data. When a processor wishes to write to a cache 
line, copies from all other caches are removed. When a processor requests data 
not found in its cache, the request goes out on the shared bus, and the data 
will be fetched either from memory or from the cache of another processor. 
The time taken for one processor to communicate with another is about 
twice the cost of a memory access. The data, in units of cache lines, must 
first be written from the first processor's cache to memory, and then fetched 
from the memory to the cache of the second processor. You may think that 
interprocessor communication is relatively cheap, since it is only about twice as 
slow as a memory access. However, you must remember that memory accesses 
are very expensive when compared to cache hits-they 
can be a hundred times 
slower. This analysis brings home the similarity between efficient parallelization 
and locality analysis. For a processor to perform well, either on its own or in 
the context of a multiprocessor, it must find most of the data it operates on in 
its cache. 
In the early 2000's, the design of symmetric multiprocessors no longer scaled 
beyond tens of processors, because the shared bus, or any other kind of inter- 
connect for that matter, could not operate at speed with the increasing number 
of processors. To make processor designs scalable, architects introduced yet an- 
other level in the memory hierarchy. Instead of having memory that is equally 
far away for each processor, they distributed the memories so that each pro- 
cessor could access its local memory quickly as shown in Fig. 11.2. Remote 
memories thus constituted the next level of the memory hierarchy; they are 
collectively bigger but also take longer to access. Analogous to the principle in 
memory-hierarchy design that fast stores are necessarily small, machines that 
support fast interprocessor communication necessarily have a small number of 
processors. 
There are two variants of a parallel machine with distributed memories: 
NUMA (nonuniform memory access) machines and message-passing machines. 
NUMA architectures provide a shared address space to the software, allowing 
processors to communicate by reading and writing shared memory. On message- 
passing machines, however, processors have disjoint address spaces, and proces- 
sors communicate by sending messages to each other. Note that even though it 
is simpler to write code for shared memory machines, the software must have 
good locality for either type of machine to perform well. 
1 
1.1.2 Parallelism in Applications 
We use two high-level metrics to estimate how well a parallel application will 
perform: parallelism coverage which is the percentage of the computation that 
runs in parallel, granularity of parallelism, which is the amount of computation 
that each processor can execute without synchronizing or communicating with 
others. One particularly attractive target of parallelization is loops: a loop may 
'You may wish to review the discussion of caches and cache lines in Section 7.4. 
774 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Cache 
Cache 
Cache 
I 
Bus or other Interconnect 
Processor 
u 
Cache 
Cache 
Memory 
Q 
Figure 11.2: Distributed memory machines 
have many iterations, and if they are independent of each other, we have found 
a great source of parallelism. 
Amdahl's Law 
The significance of parallelism coverage 
is succinctly captured by Amdahl's Law. 
Amdahl's Law states that, if f is the fraction of the code parallelized, and if 
the parallelized version runs on a p-processor machine with no communication 
or parallelization overhead, the speedup is 
For example, if half of the computation remains sequential, the computation can 
only double in speed, regardless of how many processors we use. The speedup 
achievable is a factor of 1.6 if we have 4 processors. Even if the parallelism 
coverage is 90%, we get at most a factor of 3 speed up on 4 processors, and a 
factor of 10 on an unlimited number of processors. 
Granularity of Parallelism 
It is ideal if the entire computation of an application can be partitioned into 
many independent coarse-grain tasks because we can simply assign the differ- 
ent tasks to different processors. One such example is the SET1 (Search for 
Extra-Terrestrial Intelligence) project, which is an experiment that uses home 
computers connected over the Internet to analyze different portions of radio 
telescope data in parallel. Each unit of work, requiring only a small amount 
11.1. BASIC CONCEPTS 
of input and generating a small amount of output, can be performed indepen- 
dently of all others. As a result, such a computation runs well on machines over 
the Internet, which has relatively high communication latency (delay) and low 
bandwidth. 
Most applications require more communication and interaction between pro- 
cessors, yet still allow coarse-grained parallelism. Consider, for example, the 
web server responsible for serving a large number of mostly independent re- 
quests out of a common database. We can run the application on a multi- 
processor, with a thread implementing the database and a number of other 
threads servicing user requests. Other examples include drug design or airfoil 
simulation, where the results of many different parameters can be evaluated 
independently. Sometimes the evaluation of even just one set of parameters in 
a simulation takes so long that it is desirable to speed it up with paralleliza- 
tion. As the granularity of available parallelism in an application decreases, 
better interprocessor communication support and more programming effort are 
needed. 
Many long-running scientific and engineering applications, with their simple 
control structures and large data sets, can be more readily parallelized at a finer 
grain than the applications mentioned above. Thus, this chapter is devoted pri- 
marily to techniques that apply to numerical applications, and in particular, to 
programs that spend most of their time manipulating data in multidimensional 
arrays. We shall examine this class of programs next. 
11.1.3 Loop-Level Parallelism 
Loops are the main target for parallelization, especially in applications using 
arrays. Long running applications tend to have large arrays, which lead to 
loops that have many iterations, one for each element in the array. It is not 
uncommon to find loops whose iterations are independent of one another. We 
can divide the large number of iterations of such loops among the processors. 
If the amount of work performed in each iteration is roughly the same, simply 
dividing the iterations evenly across processors will achieve maximum paral- 
lelism. Example 11.1 
is an extremely simple example showing how we can take 
advantage of loop-level parallelism. 
Example 11.1 : 
The loop 
for (i = 0; i < n; i++) ( 
Z 
[il = X 
[i] - Y 
[i] ; 
Z 
Cil = Z 
[i] * Z [i] ; 
1 
computes the square of diferences between elements in vectors X and Y and 
stores it into 2. 
The loop is parallelizable because each iteration accesses a 
different set of data. We can execute the loop on a computer with M processors 
by giving each processor an unique ID p = 0,1,. 
. 
. , 
M - 
1 and having each 
processor execute the same code: 
776 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Task-Level Parallelism 
It is possible to find parallelism outside of iterations in a loop. For example, 
we can assign two different function invocations, or two independent loops, 
to two processors. This form of parallelism is known as task parallelism. 
The task level is not as attractive a source of parallelism as is the loop 
level. The reason is that the number of independent tasks is a constant 
for each program and does not scale with the size of the data, as does the 
number of iterations of a typical loop. Moreover, the tasks generally are 
not of equal size, so it is hard to keep all the processors busy all the time. 
b = ceil(n/M) ; 
for (i = b*p; i < min(n,b*(p+l)) ; i++) ( 
Z[il = X[il - YCil; 
Z 
[i] = Z 
[il * Z 
[il ; 
3 
We divide the iterations in the loop evenly among the processors; the pth 
processor is given the pth swath of iterations to execute. Note that the number 
of iterations may not be divisible by M, so we assure that the last processor 
does not execute past the bound of the original laop by introducing a minimum 
operation. 
1
7
 
The parallel code shown in Example 11.1 is an SPMD (Single Program 
Multiple Data) program. The same code is executed by all processors, but it 
is parameterized by an identifier unique to each processor, so different proces- 
sors can take different actions. Typically one processor, known as the master, 
executes all the serial part of the computation. The master processor, upon 
reaching a parallelized section of the code, wakes up all the slave processors. 
All the processors execute the parallelized regions of the code. At the end 
of each parallelized region of code, all the processors participate in a barrier 
synchronization. Any operation executed before a processor enters a synchro- 
nization barrier is guaranteed to be completed before any other processors are 
allowed to leave the barrier and execute operations that come after the barrier. 
If we parallelize only little loops like those in Example 11.1, then the re- 
sulting code is likely to have low parallelism coverage and relatively fine-grain 
parallelism. We prefer to parallelize the outermost loops in a program, as that 
yields the coarsest granularity of parallelism. Consider, for example, the appli- 
cation of a two-dimensional FFT transformation that operates on an n x n data 
set. Such a program performs n FFT's on the rows of the data, then another 
n FFT's on the columns. It is preferable to assign each of the n independent 
FFT's to one processor each, rather than trying to use several processors to 
collaborate on one FFT. The code is easier to write, the parallelism coverage 
11.1. BASIC CONCEPTS 
for the algorithm is loo%, and the code has good data locality as it requires no 
communication at all while computing an FFT. 
Many applications do not have large outermost loops that are parallelizable. 
The execution time of these applications, however, is often dominated by time- 
consuming kernels, which may have hundreds of lines of code consisting of 
loops with different nesting levels. It is sometimes possible to take the kernel, 
reorganize its computation and partition it into mostly independent units by 
focusing on its locality. 
11.1.4 Data Locality 
There are two somewhat different notions of data locality that need to be con- 
sidered when parallelizing programs. Temporal locality occurs when the same 
data is used several times within a short time period. Spatial locality occurs 
when different data elements that are located near to each other are used within 
a short period of time. An important form of spatial locality occurs when all 
the elements that appear on one cache line are used together. The reason is 
that as soon as one element from a cache line is needed, all the elements in the 
same line are brought to the cache and will probably still be there if they are 
used soon. The effect of this spatial locality is that cache misses are minimized, 
with a resulting important speedup of the program. 
Kernels can often be written in many semantically equivalent ways but with 
widely varying data localities and performances. Example 11.2 shows an alter- 
native way of expressing the computation in Example 11.1. 
Example 11.2 : Like Example 11.1 the following also finds the squares of 
differences between elements in vectors X and Y. 
f o r  (i 
= 0; i < n; i + + )  
Z [il = X [i] - Y [i] 
; 
f o r  (i 
= 0; i < n; i + + )  
Z C i l  = Z C i ]  * Z [ i ]  ; 
The first loop finds the differences, the second finds the squares. Code like this 
appears often in real programs, because that is how we can optimize a program 
for vector machines, which are supercomputers which have instructions that 
perform simple arithmetic operations on vectors at a time. We see that the 
bodies of the two loops here are fused as one in Example 11.1. 
Given that the two programs perform the same computation, which performs 
better? The fused loop in Example 11.1 has better performance because it has 
better data locality. Each difference is squared immediately, as soon as it is 
produced; in fact, we can hold the difference in a register, square it, and write 
the result just once into the memory location Z[i]. In contrast, the code in 
Example 11.1 fetches Z[i] 
once, and writes it twice. Moreover, if the size of 
the array is larger than the cache, Z[i] 
needs to be refetched from memory the 
second time it is used in this example. Thus, this code can run significantly 
slower. 
778 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
f o r  ( j  = 0; j < n; j++) 
f o r  ( i  = 0; i < n; i++) 
Z [ i , j l  = 0; 
(a) Zeroing an array column-by-column. 
f o r  ( i  = 0; i < n; i++) 
f o r  ( j  = 0; j < n ;  j++) 
Z [ i , j l  = 0; 
(b) Zeroing an array row-by-row. 
b = c e i l  
(n/M) 
; 
f o r  ( i  = b*p; i < min(n,b*(p+l)); i++) 
f o r  ( j  = 0; j < n; j++) 
z [ i , j l  = 0; 
(c) Zeroing an array row-by-row in parallel. 
Figure 11.3: Sequential and parallel code for zeroing an array 
Example 11.3 
: Suppose we want to set array 2, 
stored in row-major order 
(recall Section 6.4.3), to all zeros. Fig. 11.3(a) and (b) sweeps through the 
array column-by-column and row-by-row, respectively. We can transpose the 
loops in Fig. 11.3(a) to arrive at Fig. 11.3(b). In terms of spatial locality, it is 
preferable to zero out the array row-by-row since all the words in a cache line 
are zeroed consecutively. In the column-by-column approach, even though each 
cache line is reused by consecutive iterations of the outer loop, cache lines will 
be thrown out before reuse if the size of a colum is greater than the size of the 
cache. For best performance, we parallelize the outer loop of Fig. 11.3(b) in a 
manner similar to that used in Example 11.1. 
The two examples above illustrate several important characteristics associ- 
ated with numeric applications operating on arrays: 
Array code often has many parallelizable loops. 
When loops have parallelism, their iterations can be executed in arbitrary 
order; they can be reordered to improve data locality drastically. 
As we create large units of parallel computation that are independent of 
each other, executing these serially tends to produce good data locality. 
11 
.I 
.5 
Introduction to Affine Transform Theory 
Writing correct and efficient sequential programs is difficult; writing parallel 
programs that are correct and efficient is even harder. The level of difficulty 
11.2. BASIC CONCEPTS 
increases as the granularity of parallelism exploited decreases. As we see above, 
programmers must pay attention to data locality to get high performance. Fur- 
thermore, the task of taking an existing sequential program and parallelizing it 
is extremely hard. It is hard to catch all the dependences in the program, es- 
pecially if it is not a program with which we are familiar. Debugging a parallel 
program is harder yet, because errors can be nondeterministic. 
Ideally, a parallelizing compiler automatically translates ordinary sequential 
programs into efficient parallel programs and optimizes the locality of these 
programs. Unfortunately, compilers without high-level knowledge about the 
application, can only preserve the semantics of the original algorithm, which 
may not be amenable to parallelization. Furthermore, programmers may have 
made arbitrary choices that limit the program's parallelism. 
Successes in parallelization and locality optimizations have been demon- 
strated for Fortran numeric applications that operate on arrays with affine 
accesses. Without pointers and pointer arithmetic, Fortran is easier to ana- 
lyze. Note that not all applications have affine accesses; most notably, many 
numeric applications operate on sparse matrices whose elements are accessed 
indirectly through another array. This chapter focuses on the parallelization 
and optimizations of kernels, consisting of mostly tens of lines. 
As illustrated by Examples 11.2 and 11.3, parallelization and locality op- 
timization require that we reason about the different instances of a loop and 
their relations with each other. This situation is very different from data-flow 
analysis, where we combine information associated with all instances together. 
For the problem of optimizing loops with array accesses, we use three kinds 
of spaces. Each space can be thought of as points on a grid of one or more 
dimensions. 
1. The iteration space is the set of the dynamic execution instances in a 
computation, that is, the set of combinations of values taken on by the 
loop indexes. 
2. The data space is the set of array elements accessed. 
3. The processor space is the set of processors in the system. Normally, 
these processors are assigned integer numbers or vectors of integers to 
distinguish among them. 
Given as input are a sequential order in which the iterations are executed and 
affine array-access functions (e.g., X [ i ,  
j + I]) that specify which instances in 
the iteration space access which elements in the data space. 
The output of the optimization, again represented as affine functions, defines 
what each processor does and when. To specify what each processor does, 
we use an affine function to assign instances in the original iteration space to 
processors. To specify when, we use an affine function to map instances in the 
iteration space to a new ordering. The schedule is derived by analyzing the 
array-access functions for data dependences and reuse patterns. 
780 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
The following example will illustrate the three spaces - 
iteration, data, 
and processor. It will also introduce informally the important concepts and 
issues that need to be addressed in using these spaces to parallelize code. The 
concepts each will be covered in detail in later sections. 
Example 11.4 
: 
Figure 11.4 illustrates the different spaces and their relations 
used in the following program: 
f l o a t  Z [I001 ; 
f o r  ( i  = 0; i < 10; i++) 
Z [i+IOl = Z [i] ; 
The three spaces and the mappings among them are as follows: 
Region of data accessed 
Data space 
Iteration space 
Affine array 
index functions 
Affine partitioning 
Figure 11.4: Iteration, data, and processor space for Example 11.4 
1. Iteration Space: The iteration space is the set of iterations, whose ID'S 
are given by the values held by the loop index variables. A d-deep loop 
nest(i.e., d nested loops) has d index variables, and is thus modeled by 
>
.
 
a d-dimensional space. The space of iterations is bounded by the lower 
and upper bounds of the loop indexes. The loop of this example defines a 
one-dimensional space of 10 iterations, labeled by the loop index values: 
i = O , l ,  ... 
,9. 
2. Data Space: The data space is given directly by the array declarations. 
In this example, elements in the array are indexed by a = 0,1, . . 
. ,99. 
Even though all arrays are linearized in a program's address space, we 
treat n-dimensional arrays as n-dimensional spaces, and assume that the 
individual indexes stay within their bounds. In this example, the array is 
one-dimensional anyway. 
11.1. BASIC CONCEPTS 
78 
1 
3. Processor Space: We pretend that there are an unbounded number of 
virtual processors in the system as our initial parallelization target. The 
processors are organized in a multidimensional space, one dimension for 
each loop in the nest we wish to parallelize. After parallelization, if we 
have fewer physical processors than virtual processors, we divide the vir- 
tual processors into even blocks, and assign a block each to a processor. 
In this example, we need only ten processors, one for each iteration of 
the loop. We assume in Fig. 11.4 that processors are organized in a one- 
dimensional space and numbered 0,1,. 
. . ,9, 
with loop iteration i assigned 
to processor i. If there were, say, only five processors, we could assign it- 
erations 0 and 1 to processor 0, iterations 2 and 3 to processor 1, and 
so on. Since iterations are independent, it doesn't matter how we do the 
assignment, as long as each of the five processors gets two iterations. 
4. Avgine Array-Index Function: Each array access in the code specifies a 
mapping from an iteration in the iteration space to an array element in 
the data space. The access function is affine if it involves multiplying the 
loop index variables by constants and adding constants. Both the array 
index functions i + 
10, and i are affine. From the access function, we can 
tell the dimension of the data accessed. In this case, since each index 
function has one loop variable, the space of accessed array elements is one 
dimensional. 
5. Avgine Partitioning: We parallelize a loop by using an affine function to 
assign iterations in an iteration space to processors in the processor space. 
In our example, we simply assign iteration i to processor i. We can also 
specify a new execution order with affine functions. If we wish to execute 
the loop above sequentially, but in reverse, we can specify the ordering 
function succinctly with an affine expression 10 - 
i. Thus, iteration 9 is 
the 1st iteration to execute and so on. 
6. Region of Data Accessed: To find the best affine partitioning, it useful to 
know the region of data accessed by an iteration. We can get the region of 
data accessed by combining the iteration space information with the array 
index function. In this case, the array access Z[i + 
101 touches the region 
{a 1 10 5 a < 20) and the access Z[i] touches the region {a10 < a < 10). 
7. Data Dependence: To determine if the loop is parallelizable, we ask if there 
is a data dependence that crosses the boundary of each iteration. For this 
example, we first consider the dependences of the write accesses in the 
loop. Since the access function Z[i + 
101 maps different iterations to differ- 
ent array locations, there are no dependences regarding the order in which 
the various iterations write values to the array. Is there a dependence be- 
tween the read and write accesses? Since only Z[10], 
Z[1 
ll, 
. 
. . ,2[19] 
are 
written (by the access Z[i + lo]), and only Z[O], 
Z[1], . . 
. ,Z[9] are read 
(by the access Z[i]) 
, 
there can be no dependencies regarding the relative 
order of a read and a write. Therefore, this loop is parallelizable. That 
782 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
is, each iteration of the loop is independent of all other iterations, and we 
can execute the iterations in parallel, or in any order we choose. Notice, 
however, that if we made a small change, say by increasing the upper 
limit on loop index i to 10 or more, then there would be dependencies, 
as some elements of array Z 
would be written on one iteration and then 
read 10 iterations later. In that case, the loop could not be parallelized 
completely, and we would have to think carefully about how iterations 
were partitioned among processors and how we ordered iterations. 
Formulating the problem in terms of multidimensional spaces and affine 
mappings between these spaces lets us use standard mathematical techniques 
to solve the parallelization and locality optimization problem generally. For 
example, the region of data accessed can be found by the elimination of variables 
using the Fourier-Motzkin elimination algorithm. Data dependence is shown to 
be equivalent to the problem of integer linear programming. Finally, finding 
the affine partitioning corresponds to solving a set of linear constraints. Don't 
worry if you are not familiar with these concepts, as they will be explained 
starting in Section 11.3. 
11.2 Matrix Multiply: An In-Depth Example 
We shall introduce many of the techniques used by parallel compilers in an ex- 
tended example. In this section, we explore the familiar matrix-multiplication 
algorithm to show that it is nontrivial to optimize even a simple and easily 
parallelizable program. We shall see how rewriting the code can improve data 
locality; that is, processors are able to do their work with far less communica- 
tion (with global memory or with other processors, depending on the architec- 
ture) than if the straightforward program is chosen. We shall also discuss how 
cognizance of the existence of cache lines that hold several consecutive data ele- 
ments can improve the running time of programs such as matrix multiplication. 
11.2.1 The Matrix-Multiplication Algorithm 
In Fig. 11.5 we see a typical matrix-multiplication program.2 It takes two n x n 
matrices, X and Y, and produces their product in a third n x n matrix 2. 
Recall that Zij - 
the element of matrix Z 
in row i and column j - 
must 
become C
F
=
,
 x i k y k i .  
The code of Fig. 11.5 
generates n2 results, each of which is an inner product 
- 
- 
between one row and one column of the two matrix operands. Clearly, the 
2 ~ n  
pseudocode programs in this chapter, we shall generally use C syntax, but to make 
multidimensional array accesses - 
the central issue for most of the chapter - 
easier to read, 
we shall use Fortran-style array references, that is, Z[i, 
j] instead of Z [ i ] [ j ] .  
11.2. MATRIX MULTIPLY: AN IN-DEPTH EXAMPLE 
f o r  ( i  = 0; i < n; i++) 
f o r  ( j  = 0; j < n; j++) ( 
Z [ i , j l  = 0.0; 
f o r  (k = 0; k  < n; k++) 
Z [ i , j l  = Z [ i , j l  + ~ [ i , k l * Y [ k , j l ;  
3 
Figure 11.5: The basic matrix-multiplication algorithm 
calculations of each of the elements of Z are independent and can be executed 
in parallel. 
The larger n is, the more times the algorithm touches each element. That is, 
there are 3n2 locations among the three matrices, but the algorithm performs 
n3 operations, each of which multiplies an element of X by an element of Y 
and adds the product to an element of Z. Thus, the algorithm is computation- 
intensive and memory accesses should not, in principle, constitute a bottleneck. 
Serial Execution of the Matrix Multiplication 
Let us first consider how this program behaves when run sequentially on a 
uniprocessor. The innermost loop reads and writes the same element of Z, and 
uses a row of X and a column of Y. Z[i, 
j ]  can easily be stored in a register 
and requires no memory accesses. Assume, without loss of generality, that the 
matrix is laid out in row-major order, and that c is the number of array elements 
in a cache line. 
Figure 11.6: The data access pattern in matrix multiply 
Figure 11.6 suggests the access pattern as we execute one iteration of the 
outer loop of Fig. 11.5. In particular, the picture shows the first iteration, with 
i = 0. Each time we move from one element of the first row of X to the next, 
784 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
we visit each element in a single column of Y. We see in Fig. 11.6 the assumed 
organization of the matrices into cache lines. That is, each small rectangle 
represents a cache line holding four array elements (i.e., c = 4 and n = 12 in 
the picture). 
Accessing X puts little burden on the cache. One row of X is spread among 
only n/c cache lines. Assuming these all fit in the cache, only n/c cache misses 
occur for a fixed value of index i, and the total number of misses for all of X is 
n2/c, the minimum possible (we assume n is divisible by c, for convenience). 
However, while using one row of X ,  the matrix-multiplication algorithm 
accesses all the elements of Y, column by column. That is, when j = 0, the 
inner loop brings to the cache the entire first column of Y. Notice that the 
elements of that column are stored among n different cache lines. If the cache 
is big enough (or n small enough) to hold n cache lines, and no other uses of 
the cache force some of these cache lines to be expelled, then the column for 
j = 
0 will still be in the cache when we need the second column of Y. In that 
case, there will not be another n cache misses reading Y, until j = 
c, at which 
time we need to bring into the cache an entirely different set of cache lines for 
Y. Thus, to complete the first iteration of the outer loop (with i = 
0) requires 
between n2/c 
and n2 
cache misses, depending on whether columns of cache lines 
can survive from one iteration of the second loop to the next. 
Moreover, as we complete the outer loop, for i = 1,2, and so on, we may 
have many additional cache misses as we read Y, or none at all. If the cache is 
big enough that all n2/c cache lines holding Y can reside together in the cache, 
then we need no more cache misses. The total number of cache misses is thus 
2n2/c, half for X and half for Y. However, if the cache can hold one column of 
Y but not all of Y, then we need to bring all of Y into cache again, each time 
we perform an iteration of the outer loop. That is, the number of cache misses 
is n2/c + 
n3/c; the first term is for X and the second is for Y. Worst, if we 
cannot even hold one column of Y in the cache, then we have n2 cache misses 
per iteration of the outer loop and a total of n2/c + 
n3 cache misses. 
Row- 
by-Row Parallelizat 
ion 
Now, let us consider how we could use some number of processors, say p proces- 
sors, to speed up the execution of Fig. 11.5. An obvious approach to parallelizing 
matrix multiplication is to assign different rows of Z to different processors. A 
processor is responsible for n/p consecutive rows (we assume n is divisible by 
p, for convenience). With this division of labor, each processor needs to access 
nlp rows of matrices X and 2, 
but the entire Y matrix. One processor will 
compute n2/p elements of 2, 
performing n3/p multiply-and-add operations to 
do so. 
While the computation time thus decreases in proportion to p, the commu- 
nication cost actually rises in proportion to p. That is, each of p processors 
has to read n2lp 
elements of X, but all n2 elements of Y. The total number 
of cache lines that must be delivered to the caches of the p processors is at last 
11.2. MATRIX MULTIPLY: AN IN-DEPTH EXAMPLE 
n2/c + 
p n 2 / ~ ;  
the two terms are for delivering X and copies of Y, respectively. 
As p approaches n, the computation time becomes O(n2) 
while the communi- 
cation cost is O(n3). That is, the bus on which data is moved between memory 
and the processors' caches becomes the bottleneck. Thus, with the proposed 
data layout, using a large number of processors to share the computation can 
actually slow down the computation, rather than speed it up. 
11.2.2 Optimizations 
The matrix-multiplication algorithm of Fig. 11.5 shows that even though an 
algorithm may reuse the same data, it may have poor data locality. A reuse 
of data results in a cache hit only if the reuse happens soon enough, before 
the data is displaced from the cache. In this case, n2 multiply-add operations 
separate the reuse of the same data element in matrix Y, so locality is poor. 
In fact, n operations separate the reuse of the same cache line in Y. In addi- 
tion, on a multiprocessor, reuse may result in a cache hit only if the data is 
reused by the same processor. When we considered a parallel implementation 
in Section 11.2.1, we saw that elements of Y had to be used by every processor. 
Thus, the reuse of Y is not turned into locality. 
Changing Data Layout 
One way to improve the locality of a program is to change the layout of its data 
structures. For example, storing Y in column-major order would have improved 
the reuse of cache lines for matrix Y. The applicability of this approach is 
limited, because the same matrix normally is used in different operations. If Y 
played the role of X in another matrix multiplication, then it would suffer from 
being stored in column-major order, since the first matrix in a multiplication 
is better stored in row-major order. 
Blocking 
It is sometimes possible to change the execution order of the instructions to 
improve data locality. The technique of interchanging loops, however, does not 
improve the matrix-multiplication routine. Suppose the routine were written 
to generate a column of matrix Z 
at a time, instead of a row at a time. That 
is, make the j-loop the outer loop and the i-loop the second loop. Assuming 
matrices are still stored in row-major order, matrix Y enjoys better spatial and 
temporal locality, but only at the expense of matrix X. 
Blocking is another way of reordering iterations in a loop that can greatly 
improve the locality of a program. Instead of computing the result a row or 
a column at a time, we divide the matrix up into submatrices, or blocks, as 
suggested by Fig. 11.7, and we order operations so an entire block is used over 
a short period of time. Typically, the blocks are squares with a side of length 
B. If B evenly divides n, then all the blocks are square. If B does not evenly 
786 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
divide n, then the blocks on the lower and right edges will have one or both 
sides of length less than B. 
-
n
 
b 
-
B
e
 
Figure 11.7: A matrix divided into blocks of side B 
Figure 11.8 shows a version of the basic matrix-multiplication algorithm 
where all three matrices have been blocked into squares of side B. As in 
Fig. 11.5, Z is assumed to have been initialized to all 0's. We assume that 
B divides n; if not, then we need to modify line (4) so the upper limit is 
min(ii + 
B, 
n), and similarly for lines (5) and (6). 
1 
f o r  ( i i  = 0; ii < n; ii = ii+B) 
2) 
f o r  ( j j  = 0; j j  < n; j j  = jj+B) 
3 
f o r  (kk = 0; kk < n; kk = kk+B) 
4) 
f o r  ( i  = i i ;  i < ii+B; i++) 
5 
1 
f o r  ( j  = j j ;  j < jj+B; j++) 
6 
f o r  (k = kk; k  < kk+B; k++) 
7 
Z [ i , j ]  
= Z [ i , j ]  + X[i,k]*Y[k,jl; 
Figure 11.8: Matrix multiplication with blocking 
The outer three loops, lines (I) 
through (3), 
use indexes ii, jj, and kk, which 
are always incremented by B, and therefore always mark the left or upper edge 
of some blocks. With fixed values of ii, j 
j, and kk, lines (4) through (7) enable 
the blocks with upper-left corners X[ii, 
kk] and Y[kk, 
jj] to make all possible 
contributions to the block with upper-left corner Z[ii, 
jj]. 
I
f
 we pick B properly, we can significantly decrease the number of cache 
misses, compared with the basic algorithm, when all of X, Y, or Z cannot fit 
in the cache. Choose B such that it is possible to fit one block from each of the 
matrices in the cache. Because of the order of the loops, we actually need each 
2 2.2. MATRIX MULTIPLY: AN IN-DEPTH EXAMPLE 
787 
Another View of Block-Based Matrix Multiplication 
We can imagine that the matrices X ,  Y, and Z 
of Fig. 11.8 are not n x n 
matrices of floating-point numbers, but rather (n/B) x (n/B) matrices 
whose elements are themselves B x B matrices of floating-point numbers. 
Lines (1) through (3) of Fig. 11.8 are then like the three loops of the 
basic algorithm in Fig. 11.5, but with n / B  as the size of the matrices, 
rather than n. We can then think of lines (4) through (7) of Fig. 11.8 
as implementing a single multiply-and-add operation of Fig. 11.5. Notice 
that in this operation, the single multiply step is a matrix-multiply step, 
and it uses the basic algorithm of Fig. 11.5 on the floating-point numbers 
that are elements of the two matrices involved. The matrix addition is 
element-wise addition of floating-point numbers. 
block of Z 
in cache only once, so (as in the analysis of the basic algorithm in 
Section 11.2.1) we shall not count the cache misses due to Z. 
To bring a block of X or Y to the cache takes B ~ / C  
cache misses; recall c 
is the number of elements in a cache line. However, with fixed blocks from X 
and Y, we perform B3 multiply-and-add operations in lines (4) through (7) of 
Fig. 11.8. Since the entire matrix-multiplication requires n3 multiply-and-add 
operations, the number of times we need to bring a pair of blocks to the cache 
is n3/B3. As we require 2 ~ ~ / c  
cache misses each time we do, the total number 
of cache misses is 2n3/Bc. 
It is interesting to compare this figure 2n3/Bc with the estimates given in 
Section 11.2.1. There, we said that if entire matrices can fit in the cache, then 
0(n2/c) cache misses suffice. However, in that case, we can pick B = n, i.e., 
make each matrix be a single block. We again get O(n2/c) as our estimate of 
cache misses. On the other hand, we observed that if entire matrices will not 
fit in cache, we require O(n3/c) cache misses, or even O(n3) cache misses. In 
that case, assuming that we can still pick a significantly large B (e.g., B could 
be 200, and we could still fit three blocks of 8-byte numbers in a one-megabyte 
cache), there is a great advantage to using blocking in matrix multiplication. 
The blocking technique can be reapplied for each level of the memory hi- 
erarchy. For example, we may wish to optimize register usage by holding the 
operands of a 2 x 2 matrix multiplication in registers. We choose successively 
bigger block sizes for the different levels of caches and physical memory. 
Similarly, we can distribute blocks between processors to minimize data traf- 
fic. Experiments showed that such optimizations can improve the performance 
of a uniprocessor by a factor of 3, and the speed up on a multiprocessor is close 
to linear with respect to the number of processors used. 
788 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
11.2.3 Cache Interference 
Unfortunately, there is somewhat more to the story of cache utilization. Most 
caches are not fully associative (see Section 7.4.2). In a direct-mapped cache, 
if n is a multiple of the cache size, then all the elements in the same row of 
an n x n array will be competing for the same cache location. In that case, 
bringing in the second element of a column will throw away the cache line of 
the first, even though the cache has the capacity to keep both of these lines at 
the same time. This situation is referred to as cache interference. 
There are various solutions to this problem. The first is to rearrange the 
data once and for all so that the data accessed is laid out in consecutive data 
locations. The second is to embed the n x n array in a larger m x n array where 
m is chosen to minimize the interference problem. Third, in some cases we can 
choose a block size that is guaranteed to avoid interference. 
11.2.4 Exercises for Section 11.2 
Exercise 11.2.1 : The block-based matrix-multiplication algorithm of Fig. 
11.8 does not have the initialization of the matrix Z 
to zero, as the code of 
Fig. 11.5 does. Add the steps that initialize Z to all zeros in Fig. 11.8. 
11.3 Iteration Spaces 
The motivation for this study is to exploit the techniques that, in simple settings 
like matrix multiplication as in Section 11.2, were quite straightforward. In the 
more general setting, the same techniques apply, but they are far less intuitive. 
But by applying some linear algebra, we can make everything work in the 
general setting. 
As discussed in Section 11.1.5, there are three kinds of spaces in our trans- 
formation model: iteration space, data space, and processor space. Here we 
start with the iteration space. The iteration space of a loop nest is defined to 
be all the combinations of loop-index values in the nest. 
Often, the iteration space is rectangular, as in the matrix-multiplication 
example of Fig. 11.5. There, each of the nested loops had a lower bound of 0 
and an upper bound of n - 
1. However, in more complicated, but still quite 
realistic, loop nests, the upper and/or lower bounds on one loop index can 
depend on the values of the indexes of the outer loops. We shall see an example 
shortly. 
11.3.1 Constructing Iteration Spaces from Loop Nests 
To begin, let us describe the sort of loop nests that can be handled by the 
techniques to be developed. Each loop has a single loop index, which we assume 
is incremented by 1 at each iteration. That assumption is without loss of 
generality, since if the incrementation is by integer c > 1, 
we can always replace 
11.3. ITERATION SPACES 
789 
uses of the index i by uses of ci + a  for some positive or negative constant a, and 
then increment i by 1 
in the loop. The bounds of the loop should be written as 
affine expressions of outer loop indices. 
Example 11.5 : 
Consider the loop 
f o r  ( i  = 2; i <= 100; i = i+3) 
Z [ i l  = 0; 
which increments i by 3 each time around the loop. The effect is to set to 0 each 
of the elements 2[2], 
Z[5], 
Z[8], . . . , 
Z[98]. We can get the same effect with: 
f o r  ( j  = 0; j <= 32; j++) 
Z[3*j+2] = 0; 
That is, we substitute 3 j  + 
2 for i. The lower limit i = 2 becomes j = 0 (just 
solve 3 j  + 
2 = 2 for j), 
and the upper limit i 5 100 becomes J
'
 5 32 (simplify 
3 j  
+ 
2 5 100 to get j 5 32.67 and round down because j has to be an integer). 
Typically, we shall use for-loops in loop nests. A while-loop or repeat-loop 
can be replaced by a for-loop if there is an index and upper and lower bounds 
for the index, as would be the case in something like the loop of Fig. 11.9(a). 
A for-loop like f o r  (i=O; i<lOO; i++) serves exactly the same purpose. 
However, some while- or repeat-loops have no obvious limit. For example, 
Fig. 11.9(b) 
may or may not terminate, but there is no way to tell what condition 
on i in the unseen body of the loop causes the loop to break. Figure 11.9(c) 
is another problem case. Variable n might be a parameter of a function, for 
example. We know the loop iterates n times, but we don't know what n is at 
compile time, and in fact we may expect that different executions of the loop 
will execute different numbers of times. In cases like (b) and (c), we must treat 
the upper limit on i as infinity. 
A d-deep loop nest can be modeled by a d-dimensional space. The dimen- 
sions are ordered, with the kth dimension representing the kth nested loop, 
counting from the outermost loop, inward. A point (xl, 
xa, 
. 
. . , 
xd) in this 
space represents values for all the loop indexes; the outermost loop index has 
value X I ,  the second loop index has value 2 2 ,  and so on. The innermost loop 
index has value xd. 
But not all points in this space represent combinations of indexes that ac- 
tually occur during execution of the loop nest. As an affine function of outer 
loop indices, each lower and upper loop bound defines an inequality dividing 
the iteration space into two half spaces: those that are iterations in the loop 
(the positive half space), and those that are not (the negative half space). The 
conjunction (logical AND) of all the linear equalities represents the intersection 
of the positive half spaces, which defines a convex polyhedron, which we call the 
iteration space for the loop nest. A convex polyhedron has the property that if 
790 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
i = 0; 
while (i<100) ( 
<some statements not involving i> 
i = i+1; 
> 
(a) A while-loop with obvious limits. 
i = 0; 
while (1) ( 
(some statements> 
i = i+1; 
1 
(b) It is unclear when or if this loop terminates. 
i = 0; 
while (i<n) ( 
(some statements not involving i or n> 
i = i+1; 
1 
(c) We don't know the value of n, so we don't 
know when this loop terminates. 
Figure 11.9: Some while-loops 
two points are in the polyhedron, all points on the line between them are also in 
the polyhedron. All the iterations in the loop are represented by the points with 
integer coordinates found within the polyhedron described by the loop-bound 
inequalities. And conversely, all integer points within the polyhedron represent 
iterations of the loop nest at some time. 
Figure 11.10: A 2-dimensional loop nest 
Example 11.6 : 
Consider the 2-dimensional loop nest in Fig. 11.10. We can 
model this two-deep loop nest by the 2-dimensional polyhedron shown in Fig. 
11.11. The two axes represent the values of the loop indexes i and j. Index i 
can take on any integral value between 0 and 5; index j can take on any integral 
value such that i 5 j 5 7. 
11.3. ITERATION SPACES 
Figure 11.11: The iteration space of Example 11.6 
Iteration Spaces and Array-Accesses 
In the code of Fig. 11.10, the iteration space is also the portion of the array 
A that the code accesses. That sort of access, where the array indexes are 
also loop indexes in some order, is very common. However, we should not 
confuse the space of iterations, whose dimensions are loop indexes, with 
the data space. If we had used in Fig. 11.10 an array access like A[2*i, 
i+ 
j] 
instead of A[i, 
j], 
the difference would have been apparent. 
11.3.2 Execution Order for Loop Nests 
A sequential execution of a loop nest sweeps through iterations in its iteration 
space in an ascending lexicographic order. A vector i = [io, 
il , 
. 
. . , 
in] is lexi- 
cographically less than another vector i' = [ih, 
ii, . . . , 
ii,], written i 4 it, 
if and 
only if there exists an m < min(n, 
n') such that [io, 
il , 
. . . , 
i,] 
= [ib, 
ii 
, 
. . 
. , 
ik] 
and im+l < ik,,. 
Note that m = 0 is possible, and in fact common. 
Example 11.7 
: With i as the outer loop, the iterations in the loop nest in 
Example 11.6 are executed in the order shown in Fig. 11.12. 
11.3.3 Matrix Formulation of Inequalities 
The iterations in a d-deep loop can be represented mathematically as 
{iin .Zd I B i + b  2 0 )  
(1 
1.1) 
792 CHAPTER 11. OPTIMIZING FOR PARALLELISMAND LOCALITY 
Figure 11.12: Iteration order for loop nest of Fig. 11.10 
Here, 
1. 2, 
as is conventional in mathematics, represents the set of integers - 
positive, negative, and zero, 
2. B is a d x d integer matrix, 
3. b is an integer vector of length d, and 
4. 0 is a vector of d 0's. 
Example 1 
1.8 : 
We can write the inequalities of Example 11.6 as in Fig. 11.13. 
That is, the range of i is described by i > 0 and i 5 5; the range of j 
is 
described by j > i and j 5 7. We need to put each of these inequalities in 
the form ui + 
vj 
+ w 
> 0. Then, [u, 
v] 
becomes a row of the matrix B in the 
inequality (1 
1. 
I), 
and w becomes the corresponding component of the vector 
b. For instance, i 2 
0 is of this form, with u = 1, v 
= 0, and w = 0. This 
inequality is represented by the first row of B and top element of b in Fig. 11.13. 
Figure 11.13: Matrix-vector multiplication and a vector inequality represents 
the inequalities defining an iteration space 
As another example, the inequality i 5 5 is equivalent to (- l)i 
+ 
(0) 
j 
+5 2 
0, 
and is represented by the second row of B and b in Fig. 11.13. Also, j > i 
becomes (-l)i + 
(1)j 
+ 
0 2 
0 and is represented by the third row. Finally, j 
5 7 
becomes (0)i + 
(-l)j + 
7 2 
0 and is the last row of the matrix and vector. 
11 
-3. ITERATION SPACES 
793 
Manipulating Inequalities 
To convert inequalities, as in Example 11.8, we can perform transforma- 
tions much as we do for equalities, e.g., adding or subtracting from both 
sides, or multiplying both sides by a constant. The only special rule we 
must remember is that when we multiply both sides by a negative number, 
we have to reverse the direction of the inequality. Thus, i 5 5, multiplied 
by -1, becomes -i 2 
-5. Adding 5 to both sides, gives -i + 
5 2 
0, which 
is essentially the second row of Fig. 11.13. 
11.3.4 Incorporating Symbolic Constants 
Sometimes, we need to optimize a loop nest that involves certain variables that 
are loop-invariant for all the loops in the nest. We call such variables symbolic 
constants, but to describe the boundaries of an iteration space we need to treat 
them as variables and create an entry for them in the vector of loop indexes, 
i.e., the vector i in the general formulation of inequalities (11.1). 
Example 11.9 : 
Consider the simple loop: 
for ( i =  
0; i < = n ;  
i++) ( 
This loop defines a one-dimensional iteration space, with index i, bounded by 
i 2 
0 and i 5 n. Since n is a symbolic constant, we need to include it as a 
variable, giving us a vector of loop indexes [i, 
n]. In matrix-vector form, this 
iteration space is defined by 
Notice that, although the vector of array indexes has two dimensions, only the 
first of these, representing i, is part of the output - 
the set of points lying with 
the iteration space. 
11.3.5 Controlling the Order of Execution 
The linear inequalities extracted from the lower and upper bounds of a loop 
body define a set of iterations over a convex polyhedron. As such, the represen- 
tation assumes no execution ordering between iterations within the iteration 
space. The original program imposes one sequential order on the iterations, 
which is the lexicographic order with respect to the loop index variables ordered 
from the outermost to the innermost. However, the iterations in the space can 
be executed in any order as long as their data dependences are honored (i.e., 
794 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
the order in which writes and reads of any array element are performed by the 
various assignment statements inside the loop nest do not change). 
The problem of how we choose an ordering that honors the data dependences 
and optimizes for data locality and parallelism is hard and is dealt with later 
starting from Section 11.7. Here we assume that a legal and desirable ordering 
is given, and show how to generate code that enforce the ordering. Let us start 
by showing an alternative ordering for Example 11.6. 
Example 11.10 : 
There are no dependences between iterations i~ the program 
in Example 11.6. We can therefore execute the iterations in arbitrary order, 
sequentially or concurrently. Since iteration [i, 
j] accesses element Z[j, 
i] in 
the code, the original program visits the array in the order of Fig. 11.14(a). 
To improve spatial locality, we prefer to visit contiguous words in the array 
consecutively, as in Fig. 11.14(b). 
This access pattern is obtained if we execute the iterations in the order 
shown in Fig. 11.14(c). That is, instead of sweeping the iteration space in 
Fig. 11.11 horizontally, we sweep the iteration space vertically, so j becomes 
the index of the outer loop. The code that executes the iterations in the above 
order is 
f o r  ( j  = 0; j <= 7; j++) 
f o r  (i 
= 0; i <= min(5,j); i++) 
Z[j,i] = 0; 
Given a convex polyhedron and an ordering of the index variables, how do 
we generate the loop bounds that sweep through the space in lexicographic 
order of the variables? In the example above, the constraint i 5 j shows up as 
a lower bound for index j in the inner loop in the original program, but as an 
upper bound for index i, again in the inner loop, in the transformed program. 
The bounds of the outermost loop, expressed as linear combinations of sym- 
bolic constants and constants, define the range of all the possible values it can 
take on. The bounds for inner loop variables are expressed as linear combi- 
nations of outer loop index variables, symbolic constants and constants. They 
define the range the variable can take on for each combination of values in outer 
loop variables. 
Projection 
Geometrically speaking, we can find the loop bounds of the outer loop index 
in a two-deep loop nest by projecting the convex polyhedron representing the 
iteration space onto the outer dimension of the space. The projection of a 
polyhedron on a lower-dimensional space is intuitively the shadow cast by the 
object onto that space. The projection of the two-dimensional iteration space in 
Fig. 11.11 
onto the i axis is the vertical line from 0 to 5; and the projection onto 
11.3. ITERATION SPACES 
(a) Original access order 
(b) Preferred order of access. 
(c) Preferred order of iterations. 
Figure 11.14: Reordering the accesses and iterations for a loop nest 
796 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
the j axis is the horizontal line from 0 to 7. When we project a 3-dimensional 
object along the x 
axis onto a 2-dimensional x and y plane, we eliminate variable 
x, 
losing the height of the individual points and simply record the 2-dimensional 
footprint of the object in the x-y plane. 
Loop bound generation is only one of the many uses of projection. Projection 
can be defined formally as follows. Let S be an n-dimensional polyhedron. 
The projection of S onto the first m of its dimensions is the set of points 
(xl, 
22,. . 
. , 
xm) 
such that for some x,+l, xm+2,. 
. 
. , 
x,, vector [XI, $2,. . . , 
x,] 
is in S. We can compute projection using Fourier-Motxkin elimination, as 
follows: 
Algorit 
hrn 11.11 : 
Fourier-Motzkin elimination. 
INPUT: A polyhedron S with variables XI, x2, . . . , 
x,. 
That is, S is a set of 
linear constraints involving the variables xi. One given variable x, 
is specified 
to be the variable to be eliminated. 
OUTPUT: A polyhedron St 
with variables 21,. . . , 
xm-1, x,+l,. . . ,a, (i.e., all 
the variables of S except for x,) 
that is the projection of S onto dimensions 
other than the mth. 
METHOD: Let C be all the constraints in S involving x,. 
Do the following: 
1. For every pair of a lower bound and an upper bound on x, 
in C, such as 
create the new constraint 
Note that cl and cz are integers, but L and U may be expressions with 
variables other than x,. 
2. If integers cl and c2 have a common factor, divide both sides by that 
factor. 
3. If the new constraint is not satisfiable, then there is no solution to S; 
i.e., 
the polyhedra S and St 
are both empty spaces. 
4. St is the set of constraints S - 
C, plus all the constraints generated in 
step 2. 
Note, incidentally, that if x, 
has u
 
lower bounds and v upper bounds, elimi- 
nating x, 
produces up to u
v
 inequalities, but no more. 
The constraints added in step (1) of Algorithm 11.11 correspond to the im- 
plications of constraints C on the remaining variables in the system. Therefore, 
there is a solution in St if and only if there exists at least one corresponding 
11.3. ITERATION SPACES 
797 
solution in S. Given a solution in S' the range of the corresponding x
,
 can 
be found by replacing all variables but x
,
 in the constraints C by their actual 
values. 
Example 11.12 : 
Consider the inequalities defining the iteration space in Fig. 
11.11. Suppose we wish to use Fourier-Motzkin elimination to project the two- 
dimensional space away from the i dimension and onto the j dimension. There 
is one lower bound on i: 0 < i and two upper bounds: i 5 j and i 5 5. This 
generates two constraints: 0 < j and 0 < 5. The latter is trivially true and 
can be ignored. The former gives the lower bound on j, and the original upper 
bound j 5 7 gives the upper bound. 
Loop-Bounds Generation 
Now that we have defined Fourier-Motzkin elimination, the algorithm to gen- 
erate the loop bounds to iterate over a convex polyhedron (Algorithm 11.13) is 
straightforward. We compute the loop bounds in order, from the innermost to 
the outer loops. All the inequalities involving the innermost loop index vari- 
ables are written as the variable's lower or upper bounds. We then project 
away the dimension representing the innermost loop and obtain a polyhedron 
with one fewer dimension. We repeat until the bounds for all the loop index 
variables are found. 
Algorithm 11 
.I3 
: 
Computing bounds for a given order of variables. 
INPUT: A convex polyhedron S over variables ul, . 
. . , 
u, 
OUTPUT: A set of lower bounds Li and upper bounds Ui for each Ui, expressed 
only in terms of the vj's, for j < i. 
METHOD: The algorithm is described in Fig. 11.15. 
1
7
 
Example 11.14 : 
We apply Algorithm 11.13 to generate the loop bounds that 
sweep the the iteration space of Fig. 11.11 vertically. The variables are ordered 
j, i. The algorithm generates these bounds: 
We need to satisfy all the constraints, thus the bound on i is min(5, j). There 
are no redundancies in this example. 
798 CHAPTER 21. OPTIMIZING FOR PARALLELISM AND LOCALITY 
S
,
 = S; /* 
Use Algorithm 11.11 to find the bounds */ 
for ( i = n ; i > l ; i - - )  { 
L,, = all the lower bounds on ui in Si; 
U,, = all the upper bounds on ui in Si; 
= Constraints returned by applying Algorithm 11.11 
to eliminate ui from the constraints Si; 
1 
/* 
Remove redundancies */ 
st 
= 0; 
for ( i = l ; i < n ; i + + )  { 
Remove any bounds in L,, and U,, implied by St; 
Add the remaining constraints of L,, and U,; on U i  to S'; 
} 
Figure 11.15: Code to express variable bounds with respect to a given variable 
ordering 
Figure 11.16: Diagonalwise ordering of the iteration space of Fig. 11.11 
11.3.6 Changing Axes 
Note that sweeping the iteration space horizontally and vertically, as discussed 
above, are just two of the most common ways of visiting the iteration space. 
There are many other possibilities; for example, we can sweep the iteration space 
in Example 11.6 diagonal by diagonal, as discussed below in Example 11.15. 
Example 11.15 : 
We can sweep the iteration space shown in Fig. 11.11 diag- 
onally using the order shown in Fig. 11.16. The difference between the coordi- 
nates j and i  in each diagonal is a constant, starting with 0  
and ending with 
7. Thus, we define a new variable k  = j - 
i  and sweep through the iteration 
space in lexicographic order with respect to k  
and j .  Substituting i = 
j - 
k in 
the inequalities we get: 
O <  j - k  
5 5  
j - k <  
j 
5 7  
11.3. ITERATION SPACES 
799 
To create the loop bounds for the order described above, we can apply Algo- 
rithm 11.13 to the above set of inequalities with variable ordering k, j. 
From these inequalities, we generate the following code, replacing i by j - 
k in 
array accesses. 
f o r  (k = 0; k <= 7; 
k++) 
f o r  (j = k; j <= min(5+ky7); j++) 
Z[j,j-k] = 0; 
In general, we can change the axes of a polyhedron by creating new loop 
index variables that represent affine combinations of the original variables, and 
defining an ordering on those variables. The hard problem lies in choosing the 
right axes to satisfy the data dependences while achieving the parallelism and 
locality objectives. We discuss this problem starting with Section 11.7. What 
we have established here is that once the axes are chosen, it is straightforward 
to generate the desired code, as shown in Example 11.15. 
There are many other iteration-traversal orders not handled by this tech- 
nique. For example, we may wish to visit all the odd rows in an iteration space 
before we visit the even rows. Or, we may want to start with the iterations in 
the middle of the iteration space and progress to the fringes. For applications 
that have affine access functions, however, the techniques described here cover 
most of the desirable iteration orderings. 
11.3.7 Exercises for Section 11.3 
Exercise 11.3.1 
: 
Convert each of the following loops to a form where the loop 
indexes are each incremented by 1: 
C) f o r  (i=50; i>=lO; 
i--) X [ i ]  
= 0;. 
Exercise 11.3.2 
: 
Draw or describe the iteration spaces for each of the follow- 
ing loop nests: 
a) The loop nest of Fig. 11.17(a) 
b) The loop nest of Fig. 11.17(b). 
800 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
f o r  ( i  = 1 ;  i < 30; i++) 
f o r  ( j  = i+2; j < 40-i; j++) 
X [ i , j ]  
= 0; 
(a) Loop nest for Exercise 11.3.2(a). 
f o r  ( i  = 10; i <= 1000; i++) 
f o r  ( j  = i; j < i+lO; j++) 
XCi,jl = 0; 
(b) Loop nest for Exercise 11.3.2(b). 
f o r  (i = 0; i < 100; i++) 
f o r  ( j  = 0; j < l00+i; j++) 
f o r  (k = i + j ;  k < 100-i-j; k++) 
X[i,j,kl = 0; 
(c) Loop nest for Exercise 11.3.2(c). 
Figure 11.17: Loop nests for Exercise 11.3.2 
c) The loop nest of Fig. 11.17(c). 
Exercise 11.3.3 
: Write the constraints implied by each of the loop nests of 
Fig. 11.17 in the form of (11.1). That is, give the values of the vectors i and b 
and the matrix B. 
Exercise 11.3.4 : 
Reverse each of the loop-nesting orders for the nests of Fig. 
11.17. 
Exercise 11.3.5 : 
Use the Fourier-Motzkin elimination algorithm to eliminate 
i from each of the sets of constraints obtained in Exercise 11.3.3. 
Exercise 11.3.6 : 
Use the Fourier-Motzkin elimination algorithm to eliminate 
j from each of the sets of constraints obtained in Exercise 11.3.3. 
Exercise 11.3.7 : 
For each of the loop nests in Fig. 11.17, rewrite the code so 
the axis i is replaced by the major diagonal, i.e., the direction of the axis is 
characterized by i = 
j. The new axis should correspond to the outermost loop. 
Exercise 11.3.8 : 
Repeat Exercise 11.3.7 for the following changes of axes: 
a) Replace i by i + j ;  
i.e., the direction of the axis is the lines for which i + 
j 
is a constant. The new axis corresponds to the outermost loop. 
b) Replace j by i - 
2j. The new axis corresponds to the outermost loop. 
11.4. AFFINE ARRAY INDEXES 
80 
1 
! Exercise 11.3.9 
: 
Let A, B, and C be integer constants in the following loop, 
with C > 1 
and B > A: 
Rewrite the loop so the incrementation of the loop variable is 1 
and the initial- 
ization is to 0, that is, to be of the form 
f o r  ( j  = 0; j <= D; j++) 
Z[E*j + F1 = 0; 
for integers D, E, and F. Express D, E, and F in terms of A, B, and C. 
Exercise 11.3.10 
: 
For a generic two-loop nest 
f o r  ( i  = 0; i <= A; i++) 
f o r ( j  = B*i+C; j <= D*i+E; j++) 
with A through E integer constants, write the constraints that define the loop 
nest's iteration space in matrix-vector form, i.e., in the form Bi + 
b = 0. 
Exercise 11.3.11 
: Repeat Exercise 11.3.10 for a generic two-loop nest with 
symbolic integer constants m and n as in 
f o r  ( i  = 0; i <= m; i++) 
f o r ( j  = A*i+B; j <= C*i+n; j++) 
As before, A, B, and C stand for specific integer constants. Only i, j, r
n
,
 
and 
n should be mentioned in the vector of unknowns. Also, remember that only i 
and j are output variables for the expression. 
11.4 Affine Array Indexes 
The focus of this chapter is on the class of affine array accesses, where each array 
index is expressed as affine expressions of loop indexes and symbolic constants. 
Affine functions provide a succinct mapping from the iteration space to the data 
space, making it easy to determine which iterations map to the same data or 
same cache line. 
Just as the affine upper and lower bounds of a loop can be represented as a 
matrix-vector calculation, we can do the same for affine access functions. Once 
placed in the matrix-vector form, we can apply standard linear algebra to find 
pertinent information such as the dimensions of the data accessed, and which 
iterations refer to the same data. 
802 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
11.4.1 Affine Accesses 
We say that an array access in a loop is afine if 
1. The bounds of the loop are expressed as affine expressions of the sur- 
rounding loop variables and symbolic constants, and 
2. The index for each dimension of the array is also an affine expression of 
surrounding loop variables and symbolic constants. 
Example 11.16 : 
Suppose i and j are loop index variables bounded by affine 
expressions. Some examples of affine array accesses are Z[i], Z[i + 
j + 
1
1
,
 Z[O], 
Z[i,i], 
and 2[2*i+173*j- 101. If n is a symbolic constant for aloop nest, then 
2[3 
* n, 
n - 
j] is another example of an affine array access. However, Z[i * j] 
and Z[n * j] are not affine accesses. 
Each affine array access can be described by two matrices and two vectors. 
The first matrix-vector pair is the B and b that describe the iteration space for 
the access, as in the inequality of Equation (11.1). The second pair, which we 
usually refer to as F and f, represent the function(s) of the loop-index variables 
that produce the array index(es) used in the various dimensions of the array 
access. 
Formally, we represent an array access in a loop nest that uses a vector of 
index variables i by the four-tuple F 
= (F, 
f, 
B, 
b); 
it maps a vector i within 
the bounds 
to the array element location 
Example 11.17 
: 
In Fig. 11.18 are some common array accesses, expressed in 
matrix notation. The two loop indexes are i and j, and these form the vector 
i. Also, X ,  Y, and Z are arrays with 1, 
2, and 3 dimensions, respectively. 
The first access, A[i - 
1
1
,
 is represented by a 1 
x 2 matrix F and a vector f 
of length 1. Notice that when we perform the matrix-vector multiplication and 
add in the vector f, we are left with a single function, i - 
1, 
which is exactly the 
formula for the access to the one-dimensional array X. Also notice the third 
access, Y [j, 
j + 
1
1
,
 which, after matrix-vector multiplication and addition, yields 
a pair of functions, (j, 
j + 1). These are the indexes of the two dimensions of 
the array access. 
Finally, let us observe the fourth access Y [I, 
21. This access is a constant, 
and unsurprisingly the matrix F is all 0's. Thus, the vector of loop indexes, i, 
does not appear in the access function. 
11.4. AFFINE ARRAY INDEXES 
Figure 11.18: Some array accesses and their matrix-vector representations 
11.4.2 Affine and Nonaffine Accesses in Practice 
There are certain common data access patterns found in numerical programs 
that fail to be affine. Programs involving sparse matrices are one important 
example. One popular representation for sparse matrices is to store only the 
nonzero elements in a vector, and auxiliary index arrays are used to mark 
where a row starts and which columns contain nonzeros. Indirect array accesses 
are used in accessing such data. An access of this type, such as X[Y[i]], is a 
nonaffine access to the array X. If the sparsity is regular, as in banded matrices 
having nonzeros only around the diagonal, then dense arrays can be used to 
represent the subregions with nonzero elements. In that case, accesses may be 
affine. 
Another common example of nonaffine accesses is linearized arrays. Pro- 
grammers sometimes use a linear array to store a logically multidimensional 
object. One reason why this is the case is that the dimensions of the array 
may not be known at compile time. An access that would normally look like 
Z[i, 
j] would be expressed as Z[i * n + 
j ] ,  which is a quadratic function. We 
can convert the linear access into a multidimensional access if every access can 
804 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
be decomposed into separate dimensions with the guarantee that none of the 
components exceeds its bound. Finally, we note that induction-variable analy- 
ses can be used to convert some nonaffine accesses into affine ones, as shown in 
Example 11.18. 
Example 11.18 : 
We can rewrite the code 
j = n; 
f o r  ( i  = 0; i <= n ;  i++) ( 
ZCjl = 0; 
j = j+2; 
3 
j = n; 
f o r  ( i  = 0; i <= n; i++) ( 
ZCn+2*i] = 0; 
3 
to make the access to matrix Z affine. 
I7 
11.4.3 Exercises for Section 11.4 
Exercise 11.4.1 : For each of the following array accesses, give the vector f 
and the matrix F that describe them. Assume that the vector of indexes i is 
i, 
j, . 
. 
. 
, 
and that all loop indexes have affine limits. 
11.5 Data Reuse 
From array access functions we derive two kinds of information useful for locality 
optimization and parallelization: 
1. Data reuse: for locality optimization, we wish to identify sets of iterations 
that access the same data or the same cache line. 
2. Data dependence: for correctness of parallelization and locality loop trans- 
formations, we wish to identify all the data dependences in the code. Re- 
call that two (not necessarily distinct) accesses have a data dependence if 
instances of the accesses may refer to the same memory location, and at 
least one of them is a write. 
11.5. DATA REUSE 
805 
In many cases, whenever we identify iterations that reuse the same data, there 
are data dependences between them. 
Whenever there is a data dependence, obviously the same data is reused. 
For example, in matrix multiplication, the same element in the output array 
is written O(n) 
times. The write operations must be executed in the original 
execution ordeq3 there is reuse because we can allocate the same element to a 
register. 
However, not all reuse can be exploited in locality optimizations; here is an 
example illustrating this issue. 
Example 11.19 : 
Consider the following loop: 
f o r  (i 
= 0 ;  i < n; i++) 
Z[7i+3] 
= Z[3i+5] ; 
We observe that the loop writes to a different location at each iteration, so there 
are no reuses or dependences on the different write operations. The loop, how- 
ever, reads locations 5,8,11,14,17,. 
. . 
, and writes locations 3,10,17,24,. 
. . 
. 
The read and write iterations access the same elements 17, 38, and 59 and so 
on. That is, the integers of the form 17 + 21j for j = 0,1,2,. 
. 
. are all those 
integers that can be written both as 7il = 3 and as 3i2 + 
5 ,  for some integers 
il and i2. However, this reuse occurs rarely, and cannot be exploited easily if 
at all. 
Data dependence is different from reuse analysis in that one of the accesses 
sharing a data dependence must be a write access. More importantly, data de- 
pendence needs to be both correct and precise. It needs to find all dependences 
for correctness, and it should not find spurious dependences because they can 
cause unnecessary serialization. 
With data reuse, we only need to find where most of the exploitable reuses 
are. This problem is much simpler, so we take up this topic here in this section 
and tackle data dependences in the next. We simplify reuse analysis by ignoring 
loop bounds, because they seldom change the shape of the reuse. Much of the 
reuse exploitable by affine partitioning resides among instances of the same 
array accesses, and accesses that share the same coeficient matrix (what we 
have typically called F in the affine index function). As shown above, access 
patterns like 7i + 
3 and 3i + 
5 have no reuse of interest. 
11.5.1 Types of Reuse 
We first start with Example 11.20 
to illustrate the different kinds of data reuses. 
In the following, we need to distinguish between the access as an instruction in 
3 ~ h e r e  
is a subtle point here. Because of the commutativity of addition, we would get the 
same answer to the sum regardless of the order in which we performed the sum. However, 
this case is very special. In general, it is far too complex for the compiler to determine what 
computation is being performed by a sequence of arithmetic steps followed by writes, and we 
cannot rely on there being any algebraic rules that will help us reorder the steps safely. 
806 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
a program, e.g., x = Z  Ci 
, 
j I , 
from the execution of this instruction many times, 
as we execute the loop nest. For emphasis, we may refer to the statement itself 
as a static access, while the various iterations of the statement as we execute 
its loop nest are called dynamic accesses. 
Reuses can be classified as self versus group. If iterations reusing the same 
data come from the same static access, we refer to the reuse as self reuse; if 
they come from different accesses, we refer to it as group reuse. The reuse is 
temporal if the same exact location is referenced; it is spatial if the same cache 
line is referenced. 
Example 11.20 : 
Consider the following loop nest: 
float Z  [n] ; 
f o r  (i = 0 ;  i < n; i++) 
f o r  ( j  = 0 ;  j < n; j++) 
Z [ j + l ]  
= ( Z [ j l  + Z [ j + l l  + Z [ j + 2 1 ) / 3 ;  
Accesses Z[j], Z[j + I], and Z[j + 
2
1
 each have self-spatial reuse because con- 
secutive iterations of the same access refer to contiguous array elements. Pre- 
sumably contiguous elements are very likely to reside on the same cache line. 
In addition, they all have self-temporal reuse, since the exact elements are used 
over and over again in each iteration in the outer loop. In addition, they all have 
the same coefficient matrix, and thus have group reuse. There is group reuse, 
both temporal and spatial, between the different accesses. Although there are 
4n2 accesses in this code, if the reuse can be exploited, we only need to bring 
in about n/c cache lines into the cache, where c is the number of words in a 
cache line. We drop a factor of n due to self-spatial reuse, a factor of c to due 
to spatial locality, and finally a factor of 4 due to group reuse. 
In the following, we show how we can use linear algebra to extract the reuse 
information from affine array accesses. We are interested in not just finding 
how much potential savings there are, but also which iterations are reusing the 
data so that we can try to move them close together to exploit the reuse. 
11.5.2 SelfReuse 
There can be substantial savings in memory accesses by exploiting self reuse. If 
the data referenced by a static access has k dimensions and the access is nested 
in a loop d deep, for some d > k, then the same data can be reused nd-k 
times, 
where n is the number of iterations in each loop. For example, if a 3-deep loop 
nest accesses one column of an array, then there is a potential savings factor 
of n2 accesses. It turns out that the dimensionality of an access corresponds 
to the concept of the rank of the coefficient matrix in the access, and we can 
find which iterations refer to the same location by finding the null space of the 
matrix, as explained below. 
11.5. DATA REUSE 
Rank of a 
Matrix 
The rank of a matrix F is the largest number of columns (or equivalently, rows) 
of F that are linearly independent. A set of vectors is linearly independent if 
none of the vectors can be written as a linear combination of finitely many other 
vectors in the set. 
Example 11.2 
1 : 
Consider the matrix 
Notice that the second row is the sum of the first and third rows, while the 
fourth row is the third row minus twice the first row. However, the first and 
third rows are linearly independent; neither is a multiple of the other. Thus, 
the rank of the matrix is 2. 
We could also draw this conclusion by examining the columns. The third 
column is twice the second column minus the first column. On the other hand, 
any two columns are linearly independent. Again, we conclude that the rank is 
2. 0 
Example 11.22 : Let us look at the array accesses in Fig. 11.18. The first 
access, X[i - 
I], has dimension 1, because the rank of the matrix [I 0] is 1. 
That is, the one row is linearly independent, as is the first column. 
The second access, Y 
[i, 
j] 
, has dimension 2. The reason is that the matrix 
has two independent rows (and therefore two independent columns, of course). 
The third access, Y [j, 
j + 1
1
,
 is of dimension 1, 
because the matrix 
has rank 1. Note that the two rows are identical, so only one is linearly in- 
dependent. Equivalently, the first column is 0 times the second column, so 
the columns are not independent. Intuitively, in a large, square array Y, the 
only elements accessed lie along a one-dimensional line, just above the main 
diagonal. 
The fourth access, Y[1,2] has dimension 0, because a matrix of all 0's has 
rank 0. Note that for such a matrix, we cannot find a linear sum of even one 
row that is nonzero. Finally, the last access, Z[i, 
i, 
2 * i + 
j], 
has dimension 2. 
Note that in the mati-ix for this access 
808 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
the last two rows are linearly independent; neither is a multiple of the other. 
However, the first row is a linear "sum" of the other two rows, with both 
coefficients 0. 
Null Space of a Matrix 
A reference in a d-deep loop nest with rank r accesses O(nr) data elements 
in O(nd) 
iterations, so on average, O(nd-') iterations must refer to the same 
array element. Which iterations access the same data? Suppose an access in 
this loop nest is represented by matrix-vector combination F and f. Let i and 
if be two iterations that refer to the same array element. Then Fi + 
f = 
Fif 
+ 
f. 
Rearranging terms, we get 
There is a well-known concept from linear algebra that characterizes when 
i and if satisfy the above equation. The set of all solutions to the equation 
Fv = 0 is called the null space of F. Thus, two iterations refer to the same 
array element if the difference of their loop-index vectors belongs to the null 
space of matrix F. 
It is easy to see that the null vector, v = 0, always satisfies Fv = 0. That 
is, two iterations surely refer to the same array element if their difference is 
0; in other words, if they are really the same iteration. Also, the null space is 
truly a vector space. That is, if Fvl = 0 and Fv2 = 0, then F(vl + 
vz) 
= 0 
and F(cvl) = 
0. 
If the matrix F is fully ranked, that is, its rank is d, then the null space of F 
consists of only the null vector. In that case, iterations in a loop nest all refer 
to different data. In general, the dimension of the null space, also known as the 
nullity, is d - 
r. If d > r, then for each element there is a (d - 
r)-dimensional 
space of iterations that access that element. 
The null space can be represented by its basis vectors. A k-dimensional null 
space is represented by k independent vectors; any vector that can be expressed 
as a linear combination of the basis vectors belongs to the null space. 
Example 11.23 
: 
Let us reconsider the matrix of Example 11.21: 
We determined in that example that the rank of the matrix is 2; thus the nullity 
is 3 - 
2 = 1. To find a basis for the null space, which in this case must be a 
single nonzero vector of length 3, we may suppose a vector in the null space to 
810 CHAPTER 2 1 .  OPTIMIZING FOR PARALLELISM AND LOCALITY 
Figure 11.19: Rank and nullity of affine accesses 
ACCESS 
x 
13-11 
YCi, jl 
YCj, j+ll 
YC1,21 
i
*
i
+
j
 
contiguous in C and X[i, 
j] and X[i + 
1, 
j] are contiguous in Fortran. Without 
loss of generality, in the rest of the chapter, we shall adopt the C (row-major) 
array layout. 
As a first approximation, we consider two array elements to share the same 
cache line if and only if they share the same row in a two-dimensional array. 
More generally, in an array of d dimensions, we take array elements to share 
a cache line if they differ only in the last dimension. Since for a typical array 
and cache, many array elements can fit in one cache line, there is significant 
speedup to be had by accessing an entire row in order, even though, strictly 
speaking, we occasionally have to wait to load a new cache line. 
The trick to discovering and taking advantage of self-spatial reuse is to drop 
the last row from the coefficient matrix F. If the resulting truncated matrix has 
rank that is less than the depth of the loop nest, then we can assure spatial 
locality by making sure that the innermost loop varies only the last coordinate 
of the array. 
Example 11.25 : 
Consider the last access, Z[l,i, 
2 * 
i + 
j], 
in Fig. 11.19. If we 
AFFINE 
EXPRESSION 
[ l  0 1 [ ; ] + [ - 1 1  
[ : ; ] [ ; ] + [ : I  
[:::][;]+[:I 
[::][;]+[:I 
0 0 
[: 
;][:]+[:] 
RANK 
1 
O 
2 
NULL- 
ITY 
1 
0 
1 
o 
BASIS 
OF 
NULL SPACE 
[
:
I
 
[ b I 
[:],[:I 
11.5. DATA REUSE 
delete the last row, we are left with the truncated matrix 
The rank of this matrix is evidently 1, 
and since the loop nest has depth 2, there 
is the opportunity for spatial reuse. In this case, since j is the inner-loop index, 
the inner loop visits contiguous elements of the array Z 
stored in row-major 
order. Making i the inner-loop index will not yield spatial locality, since as i 
changes, both the second and third dimensions change. 
The general rule for determining whether there is self-spatial reuse is as 
follows. As always, we assume that the loop indexes correspond to columns of 
the coefficient matrix in order, with the outermost loop first, and the innermost 
loop last. Then in order for there to be spatial reuse, the vector [O, 0, . . 
. ,0,1] 
must be in the null space of the truncated matrix. The reason is that if this 
vector is in the null space, then when we fix all loop indexes but the innermost 
one, we know that all dynamic accesses during one run through the inner loop 
vary in only the last array index. If the array is stored in row-major order, then 
these elements are all near one another, perhaps in the same cache line. 
Example 11.26 : 
Note that [0, 
I] (transposed as a column vector) is in the null 
space of the truncated matrix of Example 11.25. Thus, as mentioned there, we 
expect that with j as the inner-loop index, there will be spatial locality. On 
the other hand, if we reverse the order of the loops, so i is the inner loop, then 
the coefficient matrix becomes 
Now, [O,1] is not in the null space of this matrix. Rather, the null space is 
generated by the basis vector [I, 
01. Thus, as we suggested in Example 11.25, 
we do not expect spatial locality if i is the inner loop. 
We should observe, however, that the test for [O, 0, . . 
. ,0,1] being in the 
null space is not quite sufficient to assure spatial locality. For instance, suppose 
the access were not Z[l,i, 
2 * i + 
j] but Z[l,i,2 
* i + 
50 * j]. Then, only every 
fiftieth element of Z 
would be accessed during one run of the inner loop, and 
we would not reuse a cache line unless it were long enough to hold more than 
50 elements. 
11.5.4 Group Reuse 
We compute group reuse only among accesses in a loop sharing the same coef- 
ficient matrix. Given two dynamic accesses Fil + 
fi and Fi2 + 
f2, 
reuse of the 
same data requires that 
812 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Suppose v is one solution to this equation. Then if w is any vector in the null 
space of F1, 
w + 
v is also a solution, and in fact those are all the solutions to 
the equation. 
Example 11.27 
: 
The following 2-deep loop nest 
f o r  (i 
= 1; i <= n; i++) 
f o r  ( j  = 1; j <= n; j++) 
Z [ i a j ]  = Z [ i - l a j ] ;  
has two array accesses, Z[i, 
j] and Z[i - 
1, 
j]. Observe that these two accesses 
are both characterized by the coefficient matrix 
like the second access, Y [i, 
j] in Fig. 11.19. This matrix has rank 2, so there is 
no self-temporal reuse. 
However, each access exhibits self-spatial reuse. As described in Section 
11.5.3, when we delete the bottom row of the matrix, we are left with only the 
top row, [I, 
01, which has rank 1. Since [O,1] is in the null space of this truncated 
matrix, we expect spatial reuse. As each incrementation of inner-loop index j 
increases the second array index by one, we in fact do access adjacent array 
elements, and will make maximum use of each cache line. 
Although there is no self-temporal reuse for either access, observe that the 
two references Z[i, 
j] and Z[i- 1, 
j] access almost the same set of array elements. 
That is, there is group-temporal reuse because the data read by access Z[i - 
1, 
j] 
is the same as the data written by access Z[i, 
j], 
except for the case i = 1. This 
simple pattern applies to the entire iteration space and can be exploited to 
improve data locality in the code. Formally, discounting the loop bounds, the 
two accesses Z[i, 
j] and Z[i - 
1, 
f 
refer to the same location in iterations (il, 
jl) 
and (i2, 
j2), respectively, provided 
Rewriting the terms, we get 
That is, jl = 
j2 and i2 = 
il + 
1. 
Notice that the reuse occurs along the i-axis of the iteration space. That is, 
the iteration (i2, 
j2) occurs n iterations (of the inner loop) after the iteration 
11.5. DATA REUSE 
813 
(il , 
jl). Thus, many iterations are executed before the data written is reused. 
This data may or may not still be in the cache. If the cache manages to hold 
two consecutive rows of matrix 2, 
then access Z[i - 
1, 
j] does not miss in the 
cache, and the total number of cache misses for the entire loop nest is n2/c, 
where c is the number of elements per cache line. Otherwise, there will be twice 
as many misses, since both static accesses require a new cache line for each c 
dynamic accesses. 
Example 11.28 : 
Suppose there are two accesses 
A[i,j,i+ j] and A [ i + l , j - l , i +  j] 
in a 3-deep loop nest, with indexes i, j, and k, from the outer to the inner loop. 
Then two accesses il = [il, 
jl , 
kl] and i2 = [i2, 
j2, 
k2] reuse the same element 
whenever 
One solution to this equation for a vector v = [il - 
i2, 
jl - 
j2, 
kl - 
k2] is 
v = [I, 
-1,0]; that is, il = 
i2 + 
1, jl = 
j2 
- 
1, 
and kl = 
k2.4 However, the null 
space of the matrix 
is generated by the basis vector [O,0, 
11; that is, the third loop index, k, can be 
arbitrary. Thus, v, the solution to the above equation, is any vector [I, 
-1, m] 
for some m. Put another way, a dynamic access to A[i, 
j, i + 
j], 
in a loop nest 
with indexes i, 
j, and k, is reused not only by other dynamic accesses A[i, 
j, 
i+ j] 
with the same values of i and j and a different value of k, but also by dynamic 
accesses A[i + 
1, 
j - 
1, 
i + 
j] with loop index values i + 1, 
j - 
1, 
and any value 
ofk. 
Although we shall not do so here, we can reason about group-spatial reuse 
analogously. As per the discussion of self-spatial reuse, we simply drop the last 
dimension from consideration. 
The extent of reuse is different for the different categories of reuse. Self- 
temporal reuse gives the most benefit: a reference with a k-dimensional null 
space reuses the same data O(nk) 
times. The extent of self-spatial reuse is 
limited by the length of the cache line. Finally, the extent of group reuse is 
limited by the number of references in a group sharing the reuse. 
4 ~ t  
is interesting to observe that, although there is a solution in this case, there would be 
no solution if we changed one of the third components from i + 
j to i + 
j + 1. That is, in 
the example as given, both accesses touch those array elements that lie in the 2-dimensional 
subspace S defined by "the third component is the sum of the first two components." If we 
changed i + 
j to i + 
j + 
1, none of the elements touched by the second access would lie in S, 
and there would be no reuse at all. 
814 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
11.5.5 Exercises for Section 11.5 
Exercise 11.5.1 : Compute the ranks of each of the matrices in Fig. 11.20. 
Give both a maximal set of linearly dependent columns and a maximal set of 
linearly dependent rows. 
Figure 11.20: Compute the ranks and null spaces of these matrices 
Exercise 11.5.2 
: 
Find a basis for the null space of each matrix in Fig. 11.20. 
Exercise 11.5.3 : 
Assume that the iteration space has dimensions (variables) 
i, j, and k. For each of the accesses below, describe the subspaces that refer to 
the following single elements of the array: 
! 
Exercise 11.5.4: Suppose array A is stored in row-major order and accessed 
inside the following loop nest: 
f o r  (i 
= 0; i < 100; i++) 
f o r  ( j  = 0; j < 100; j++) 
f o r  (k = 0; k < 100; k++) 
<some access t o  A> 
Indicate for each of the following accesses whether it is possible to rewrite the 
loops so that the access to A exhibits self-spatial reuse; that is, entire cache 
lines are used consecutively. Show how to rewrite the loops, if so. Note: the 
rewriting of the loops may involve both reordering and introduction of new 
loop indexes. However, you may not change the layout of the array, e.g., by 
changing it to column-major order. Also note: in general, reordering of loop 
indexes may be legal or illegal, depending on criteria we develop in the next 
section. However, in this case, where the effect of each access is simply to set 
an array element to 0, you do not have to worry about the effect of reordering 
loops as far as the semantics of the program is concerned. 
11.6. ARRAY DATA-DEPENDENCE ANALYSIS 
Exercise 11.5.5 
: 
In Section 11.5.3 we commented that we get spatial locality 
if the innermost loop varies only as the last coordinate of an array access. 
However, that assertion depended on our assumption that the array was stored 
in row-major order. What condition would assure spatial locality if the array 
were stored in column-major order? 
! Exercise 11.5.6 : 
In Example 11.28 
we observed that the the existence of reuse 
between two similar accesses depended heavily on the particular expressions for 
the coordinates of the array. Generalize our observation there to determine 
for which functions f (i, 
j) there is reuse between the accesses A[i, 
j, i + 
j ]  and 
A[i -k 1,j 
- 
1, 
f (i,j)]. 
! 
Exercise 11.5.7 : 
In Example 11.27 
we suggested that there will be more cache 
misses than necessary if rows of the matrix Z are so long that they do not fit 
in the cache. If that is the case, how could you rewrite the loop nest in order 
to guarantee group-spatial reuse? 
11.6 Array Data-Dependence Analysis 
Parallelization or locality optimizations frequently reorder the operations ex- 
ecuted in the original program. As with all optimizations, operations can be 
reordered only if the reordering does not change the program's output. Since we 
cannot, in general, understand deeply what a program does, code optimization 
generally adopts a simpler, conservative test for when we can be sure that the 
program output is not affected: we check that the operations on any memory 
location are done in the same order in the original and modified programs. In 
the present study, we focus on array accesses, so the array elements are the 
memory locations of concern. 
Two accesses, whether read or write, are clearly independent (can be re- 
ordered) if they refer to two different locations. In addition, read operations 
do not change the memory state and therefore are also independent. Following 
Section 11.5, we say that two accesses are data dependent if they refer to the 
same memory location and at least one of them is a write operation. To be sure 
that the modified program does the same as the original, the relative execu- 
tion ordering between every pair of data-dependent operations in the original 
program must be preserved in the new program. 
Recall from Section 10.2.1 
that there are three flavors of data dependence: 
1. True dependence, where a write is followed by a read of the same location. 
816 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
2. Antidependence, where a read is followed by a write to the same location. 
3. Output dependence, which is two writes to the same location. 
In the discussion above, data dependence is defined for dynamic accesses. 
We say that a static access in a program depends on another as long as there 
exists a dynamic instance of the first access that depends on some instance of 
the ~ e c o n d . ~  
It is easy to see how data dependence can be used in parallelization. For 
example, if no data dependences are found in the accesses of a loop, we can 
easily assign each iteration to a different processor. Section 11.7 discusses how 
we use this information systematically in parallelization. 
11.6.1 Definition of Data Dependence of Array Accesses 
Let us consider two static accesses to the same array in possibly different loops. 
The first is represented by access function and bounds F = (F, 
f, 
B, 
b) and is 
in a d-deep loop nest; the second is represented by F
'
 
= (F', 
f' 
, 
B', b') and is 
in a dl-deep loop nest. These accesses are data dependent if 
1. At least one of them is a write reference and 
2. There exist vectors i in zd 
and it in zd' 
such that 
(a) Bi > 0, 
(b) B'i' > 0, and 
(c) Fi + 
f = 
F'i' + 
f'. 
Since a static access normally embodies many dynamic accesses, it is also 
meaningful to ask if its dynamic accesses may refer to the same memory loca- 
tion. To search for dependencies between instances of the same static access, we 
assume 3' = F' and augment the definition above with the additional constraint 
that i # i' to rule out the trivial solution. 
Example 11.29 : 
Consider the following 1-deep loop nest: 
This loop has two accesses: Z[i - 
1
1
 and Z[i]; the first is a read reference and 
the second a write. To find all the data dependences in this program, we need 
to check if the write reference shares a dependence with itself and with the read 
reference: 
5~ecall 
the difference between static and dynamic accesses. A static access is an array 
reference at a particular location in a program, while a dynamic access is one execution of 
that reference. 
11.6. ARRAY DATA-DEPENDENCE ANALYSIS 
817 
1. Data dependence between Z[i - 
1
1
 and Z[i]. Except for the first iteration, 
each iteration reads the value written in the previous iteration. Mathe- 
matically, we know that there is a dependence because there exist integers 
i and i' such that 
1 
5 i 5 10, 1 
5 i' < 10, and i - 
1 
= 
i'. 
There are nine solutions to the above system of constraints: (i = 2, i' = l), 
(i = 
3,i' = 
2), and so forth. 
2. Data dependence between Z[i] and itself. It is easy to see that different 
iterations in the loop write to different locations; that is, there are no 
data dependencies among the instances of the write reference Z[i]. Math- 
ematically, we know that there does not exist a dependence because there 
do not exist integers i and i' satisfying 
1 
5 i 5 10, 1 5 i' 5 10, i = 
it, 
and i # it. 
Notice that the third condition, i = it, 
comes from the requirement that 
Z[i] and Z[il] are the same memory location The contradictory fourth 
condition, i # it, comes from the requirement that the dependence be 
nontrivial - 
between different dynamic accesses. 
It is not necessary to consider data dependences between the read reference 
Z[i - 
1
1
 and itself because any two read accesses are independent. 
11.6.2 Integer Linear Programming 
Data dependence requires finding whether there exist integers that satisfy a 
system consisting of equalities and inequalities. The equalities are derived from 
the matrices and vectors representing the accesses; the inequalities are derived 
from the loop bounds. Equalities can be expressed as inequalities: an equality 
x = 
y can be replaced by two inequalities, x > y and y > x. 
Thus, data dependence may be phrased as a search for integer solutions that 
satisfy a set of linear inequalities, which is precisely the well-known problem 
of integer linear programming. Integer linear programming is an NP-complete 
problem. While no polynomial algorithm is known, heuristics have been de- 
veloped to solve linear programs involving many variables, and they can be 
quite fast in many cases. Unfortunately, such standard heuristics are inappro- 
priate for data dependence analysis, where the challenge is to solve many small 
and simple integer linear programs rather than large complicated integer linear 
programs. 
The data dependence analysis algorithm consists of three parts: 
818 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
1. Apply the GCD (Greatest Common Divisor) test, which checks if there is 
an integer solution to the equalities, using the theory of linear Diophan- 
tine equations. If there are no integer solutions, then there are no data 
dependences. Otherwise, we use the equalities to substitute for some of 
the variables thereby obtaining simpler inequalities. 
2. Use a set of simple heuristics to handle the large numbers of typical in- 
equalities. 
3. In the rare case where the heuristics do not work, we use a linear integer 
programming solver that uses a branch-and- 
bound approach based on 
Fourier-Motzkin elimination. 
11.6.3 The GCD Test 
The first subproblem is to check for the existence of integer solutions to the 
equalities. Equations with the stipulation that solutions must be integers are 
known as Diophantine equations. The following example shows how the issue 
of integer solutions arises; it also demonstrates that even though many of our 
examples involve a single loop nest at a time, the data dependence formulation 
applies to accesses in possibly different loops. 
Example 11.30 : 
Consider the following code fragment: 
f o r  ( i  = I ;  i < 10; i++) ( 
Z[2*il = . 
. 
.; 
3 
f o r  ( j  = 1 ;  j < 10; j++) C 
~ [ 2 * j + l ]  
= . 
. 
.; 
1 
The access 2 [ 2  
* i] only touches even elements of 2, 
while access 2 [ 2  
* j + 
1
1
 
touches only odd elements. Clearly, these two accesses share no data depen- 
dence regardless of the loop bounds. We can execute iterations in the second 
loop before the first, or interleave the iterations. This example is not as con- 
trived as it may look. An example where even and odd numbers are treated 
differently is an array of complex numbers, where the real and imaginary com- 
ponents are laid out side by side. 
To prove the absence of data dependences in this example, we reason as 
follows. Suppose there were integers i and j such that 2[2 
* i] and 2[2 
* j + 1
1
 
are the same array element. We get the Diophantine equation 
There are no integers i and j that can satisfy the above equation. The proof 
is that if i is an integer, then 2i is even. If j is an integer, then 2 j  is even, so 
2 j  + 
1 
is odd. No even number is also an odd number. Therefore, the equation 
11.6. ARRAY DATA-DEPENDENCE ANALYSIS 
819 
has no integer solutions, and thus there is no dependence between the read and 
write accesses. 
To describe when there is a solution to a linear Diophantine equation, we 
need the concept of the greatest common divisor (GCD) of two or more integers. 
The GCD of integers a1 
, 
a2, . . 
. , 
a,, denoted gcd(al, a2, . 
. 
. , 
a,,), is the largest 
integer that evenly divides all these integers. GCD7s 
can be computed efficiently 
by the well-known Euclidean algorithm (see the box on the subject). 
Example 11.31 : 
gcd(24,36,54) = 
6, because 2416, 3616, and 5416 each have 
remainder 0, yet any integer larger than 6 must leave a nonzero remainder when 
dividing at least one of 24,36, and 54. For instance, 12 divides 24 and 36 evenly, 
but not 54. 
The importance of the GCD is in the following theorem. 
Theorem 11.32 : 
The linear Diophantine equation 
has an integer solution for X I ,  2 2 ,  . 
. 
. , 
x, if and only if gcd(a1, az, . . 
. , 
a,) di- 
vides c. 
Example 11.33 
: 
We observed in Example 11.30 that the linear Diophantine 
equation 2i = 
2 j  + 1 
has no solution. We can write this equation as 
Now gcd(2, -2) = 
2, and 2 does not divide 1 
evenly. Thus, there is no solution. 
For another example, consider the equation 
Since gcd(24,36,54) = 
6, and 30/6 = 
5, there is a solution in integers for x, 
y, 
and x. One solution is x = -1, y = 0, and x = 1, 
but there are an infinity of 
other solutions. 
The first step to the data dependence problem is to use a standard method 
such as Gaussian elimination to solve the given equalities. Every time a linear 
equation is constructed, apply Theorem 11.32 to rule out, if possible, the ex- 
istence of an integer solution. If we can rule out such solutions, then answer 
Otherwise, we use the solution of the equations to reduce the number of 
variables in the inequalities. 
Example 11.34 : 
Consider the two equalities 
820 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
The Euclidean Algorithm 
The Euclidean algorithm for finding gcd(a, 
b) works as follows. First, as- 
sume that a and b are positive integers, and a > b. Note that the GCD of 
negative numbers, or the GCD of a negative and a positive number is the 
same as the GCD of their absolute values, so we can assume all integers 
are positive. 
If a = b, then gcd(a, 
b) = a. If a > b, let c be the remainder of alb. 
If c = 0, then b evenly divides a, so gcd(a, 
b) = b. Otherwise, compute 
gcd(b, 
c); this result will also be gcd(a, b). 
To compute gcd(al, 
a2 , 
. 
. . , 
an), for n > 2, use the Euclidean 
algorithm to compute gcd(al,az) = c. 
Then recursively compute 
gcd(c, 
as, 
ad,. . 
. an). 
Looking at each equality by itself, it appears there might be a solution. For 
the first equality, gcd(1, -2,l) = 1 divides 0, and for the second equality, 
gcd(3,2,1) = 1 divides 5. However, if we use the first equality to solve for 
z = 2y - 
x and substitute for x in the second equality, we get 2x + 
4y = 5. 
This Diophantine equation has no solution, since gcd(2,4) = 
2 does not divide 
5 evenly. 
11.6.4 Heuristics for Solving Integer Linear Programs 
The data dependence problem requires many simple integer linear programs be 
solved. We now discuss several techniques to handle simple inequalities and a 
technique to take advantage of the similarity found in data dependence analysis. 
Independent-Variables Test 
Many of the integer linear programs from data dependence consist of inequalities 
that involve only one unknown. The programs can be solved simply by testing 
if there are integers between the constant upper bounds and constant lower 
bounds independently. 
Example 11.35 : 
Consider the nested loop 
f o r  ( i  = 0; i <= 10; i++) 
f o r  ( j  = 0; j <= 10; j++) 
Z [ i ,  j] = Z[j+lO,i+91 ; 
11.6. ARRAY DATA-DEPENDENCE ANALYSIS 
821 
To find if there is a data dependence between Z[i, 
j] and Z[j 
+ 
10, 
i + 
91, we ask 
if there exist integers i, j, it, 
and j' such that 
The GCD test, applied to the two equalities above, will determine that there 
may be an integer solution. The integer solutions to the equalities are expressed 
by 
for any integers tl and tz. Substituting the variables tl and t2 
into the linear 
inequalities, we get 
Thus, combining the lower bounds from the last two inequalities with the upper 
bounds from the first two, we deduce 
Since the lower bound on t2 
is greater than its upper bound, there is no integer 
solution, and hence no data dependence. This example shows that even if there 
are equalities involving several variables, the GCD test may still create linear 
inequalities that involve one variable at a time. 
Acyclic Test 
Another simple heuristic is to find if there exists a variable that is bounded 
below or above by a constant. In certain circumstances, we can safely replace 
the variable by the constant; the simplified inequalities have a solution if and 
only if the original inequalities have a solution. Specifically, suppose every lower 
bound on vi is of the form 
while the upper bounds on vi are all of the form 
where cl , 
~ 1 ,  
. 
. 
. , 
C
i
 are all nonnegative. Then we can replace variable vi by 
its smallest possible integer value. If there is no such lower bound, we simply 
822 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
replace U i  with -m. Similarly, if every constraint involving ui can be expressed 
in the two forms above, but with the directions of the inequalities reversed, 
then we can replace variable ui with the largest possible integer value, or by m 
if there is no constant upper bound. This step can be repeated to simplify the 
inequalities and in some cases determine if there is a solution. 
Example 1 
1.36 : 
Consider the following inequalities: 
Variable ul is bounded from below by u2 and from above by u3. However, uz 
is bounded from below only by the constant 1, and u3 is bounded from above 
only by the constant 4. Thus, replacing uz by 1 
and us by 4 in the inequalities, 
we obtain 
which can now be solved easily with the independent-variables test. 
CI 
The Loop-Residue Test 
Let us now consider the case where every variable is bounded from below and 
above by other variables. It is commonly the case in data dependence analysis 
that constraints have the form ui 5 uj +c, which can be solved using a simplified 
version of the loop-residue test due to Shostack. A set of these constraints can be 
represented by a directed graph whose nodes are labeled with variables. There 
is an edge from ui to uj labeled c whenever there is a constraint ui 5 uj + 
c. 
We define the weight of a path to be the sum of the labels of all the edges 
along the path. Each path in the graph represents a combination of the con- 
straints in the system. That is, we can infer that u < u' + 
c whenever there 
exists a path from u to u' with weight c. A cycle in the graph with weight c 
represents the constraint u 5 u + 
c for each node u on the cycle. If we can 
find a negatively weighted cycle in the graph, then we can infer u < u, which is 
impossible. In this case, we can conclude that there is no solution and thus no 
dependence. 
We can also incorporate into the loop-residue test constraints of the form 
c 5 u and u < c for variable u and constant c. We introduce into the system of 
inequalities a new dummy variable uo, which is added to each constant upper 
and lower bound. Of course, uo must have value 0, but since the loop-residue 
test only looks for cycles, the actual values of the variables never becomes 
significant. To handle constant bounds, we replace 
11.6. ARRAY DATA-DEPENDENCE ANALYSIS 
Example 11.37 : 
Consider the inequalities 
The constant upper and lower bounds on vl 
become vo 5 vl 
- 
1 
and vl 5 vo 
+ 
10; 
the constant bounds on v2 and v3 are handled similarly. Then, converting the 
last constraint to vl 5 v3 - 
4, we can create the graph shown in Fig. 11.21. 
The cycle vl 
, 
v3, 
vo, vl has weight - 
1, so there is no solution to this set of 
inequalities. 
- 
1 
Figure 11.21: Graph for the constraints of Example 11.37 
Memoizat 
ion 
Often, similar data dependence problems are solved repeatedly, because simple 
access patterns are repeated throughout the program. One important technique 
to speed up data dependence processing is to use memoixation. Memoization 
tabulates the results to the problems as they are generated. The table of stored 
solutions is consulted as each problem is presented; the problem needs to be 
solved only if the result to the problem cannot be found in the table. 
1
1
.
6
.
5
 Solving General Integer Linear Programs 
We now describe a general approach to solving the integer linear programming 
problem. The problem is NP-complete; our algorithm uses a branch-and-bound 
approach that can take an exponential amount of time in the worst case. How- 
ever, it is rare that the heuristics of Section 11.6.4 cannot resolve the problem, 
and even if we do need to apply the algorithm of this section, it seldom needs 
to perform the branch-and-bound step. 
824 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
The approach is to first check for the existence of rational solutions to the 
inequalities. This problem is the classical linear-programming problem. If there 
is no rational solution to the inequalities, then the regions of data touched by the 
accesses in question do not overlap, and there surely is no data dependence. If 
there is a rational solution, we first try to prove that there is an integer solution, 
which is commonly the case. Failing that, we then split the polyhedron bounded 
by the inequalities into two smaller problems and recurse. 
Example 11.38 : 
Consider the following simple loop: 
for (i = 1; i < 10; i++) 
Z[i] = Z[i+lO] ; 
The elements touched by access Z[i] are Z[l] 
, 
. . 
. , 
Z[9], while the elements 
touched by Z[i + 101 are Z[11], 
. 
. . ,Z[19]. The ranges do not overlap and 
therefore there are no data dependences. More formally, we need to show that 
there are no two dynamic accesses i and it, with 1 5 i 5 9, 1 5 it 5 9, and 
i = 
it + 
10. If there were such integers i and it, 
then we could substitute it 
+ 
10 
for i and get the four constraints on it: 1 5 it 5 9 and 1 5 it 
+ 
10 5 9. However, 
it + 10 < 9 implies it 5 -1, which contradicts 1 < it. Thus, no such integers i 
and it exist. 
Algorithm 11.39 describes how to determine if an integer solution can be 
found for a set of linear inequalities based on the Fourier-Motzkin elimination 
algorithm. 
Algorithm 11.39 : 
Branch-and-bound solution to integer linear programming 
problems. 
INPUT: A convex polyhedron S, over variables vl, . 
. . , 
v,. 
OUTPUT: "yes" if S, has an integer solution, "no" otherwise. 
METHOD: The algorithm is shown in Fig. 11.22. 
Lines (1) through (3) attempt to find a rational solution to the inequalities. 
If there no rational solution, there is no integer solution. If a rational solution 
is found, this means that the inequalities define a nonempty polyhedron. It is 
relatively rare for such a polyhedron not to include any integer solutions - 
for 
that to happen, the polyhedron must be relatively thin along some dimension 
and fit between integer points. 
Thus, lines (4) through (9) try to check quickly if there is an integer solution. 
Each step of the Fourier-Motzkin elimination algorithm produces a polyhedron 
with one fewer dimension than the previous one. We consider the polyhedra in 
reverse order. We start with the polyhedron with one variable and assign to that 
variable an integer solution roughly in the middle of the range of possible values 
if possible. We then substitute the value for the variable in all other polyhedra, 
decreasing their unknown variables by one. We repeat the same process until 
11.6. ARRAY DATA-DEPENDENCE ANALYSIS 
apply Algorithm 11.13 to S, to project away variables 
vn, 
un-l, . 
. . , 
v1 in that order; 
let Si be the polyhedron after projecting away U i + l ,  for 
i = n - 1 , n - 2 ,  ... 
,O; 
if SO 
is false return "no" ; 
/* There is no rational solution if So, 
which involves 
only constants, has unsatisfiable constraints */ 
for (i = 1;i < n;i++) { 
if (Si 
does not include an integer value) break; 
pick ci, an integer in the middle of the range for ui in Si; 
modify Si by replacing vi by ci; 
} 
if (i == 
n + 
1) return "yes"; 
if (i == 1) 
return "no" ; 
let the lower and upper bounds on ui in Si be 
li and ui, respectively; 
recursively apply this algorithm to S
,
 U {vi < Lli] ) and 
s n  u {ui > [ ~ i l } ;  
if (either returns "yes") return "yes" else return "no" ; 
Figure 11.22: Finding an integer solution in inequalities 
we have processed all the polyhedra, in which case an integer solution is found, 
or we have found a variable for which there is no integer solution. 
If we cannot find an integer value for even the first variable, there is no 
integer solution (line 10). Otherwise, all we know is that there is no integer 
solution including the combination of specific integers we have picked so far, and 
the result is inconclusive. Lines (11) through (13) represent the branch-and- 
bound step. If variable vi is found to have a rational but not integer solution, 
we split the polyhedron into two with the first requiring that ui must be an 
integer smaller than the rational solution found, and the second requiring that 
ui must be an integer greater than the rational solution found. If neither has a 
solution, then there is no dependence. 
11.6.6 Summary 
We have shown that essential pieces of information that a compiler can glean 
from array references are equivalent to certain standard mathematical concepts. 
Given an access function F 
= (F, 
f ,  
B, 
b): 
I. The dimension of the data region accessed is given by the rank of the 
matrix F. The dimension of the space of accesses to the same location is 
given by the nullity of F. Iterations whose differences belong to the null 
space of F refer to the same array elements. 
826 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
2. Iterations that share self-temporal reuse of an access are separated by 
vectors in the null space of F. Self-spatial reuse can be computed similarly 
by asking when two iterations use the same row, rather than the same 
element. Two accesses Fil + fl and Fi2 + f2 share easily exploitable 
locality along the d direction, if d is the particular solution to the equation 
Fd = (fl - 
fi). In particular, if d is the direction corresponding to the 
innermost loop, i.e., the vector [O, 0, . 
. . ,0,1], 
then there is spatial locality 
if the array is stored in row-major form. 
3. The data dependence problem - 
whether two references can refer to the 
same location - 
is equivalent to integer linear programming. Two access 
functions share a data dependence if there are integer-valued vectors i 
and i' such that Bi 2 0, B'i' 2 0, and Fi + 
f = 
F'i' + 
f'. 
11.6.7 Exercises for Section 11.6 
Exercise 11.6.1 : 
Find the GCD's of the following sets of integers: 
Exercise 11.6.2 : 
For the following loop 
f o r  (i 
= 0; i < 10; i++) 
A [ i l  = A [ l O - i ]  ; 
indicate all the 
a) True dependences (write followed by read of the same location). 
b) Antidependences (read followed by write to the same location). 
c) Output dependences (write followed by another write to the same loca- 
t 
ion). 
! 
Exercise 11.6.3 
: 
In the box on the Euclidean algorithm, we made a number 
of assertions without proof. Prove each of the following: 
a) The Euclidean algorithm as stated always works. In particular, gcd(b, c) = 
gcd(a, b), where c is the nonzero remainder of alb. 
c) gcd(al , 
a2, . . 
. , 
a,) = 
gcd(gcd(al , 
a2), 
as, 
a4, . . . , 
a,) for n > 2. 
11.6. ARRAY DATA-DEPENDENCE ANALYSIS 
827 
d) The GCD is really a function on sets of integers; i.e., order doesn't matter. 
Show the commutative law for GCD: gcd(a, 
b) = 
gcd(b, 
a). Then, show the 
more difficult statement, the associative law for GCD: gcd(gcd(a, 
b), c) = 
gcd (a, 
gcd(b, c)) 
. Finally, show that together these laws imply that the 
GCD of a set of integers is the same, regardless of the order in which the 
GCD7s 
of pairs of integers are computed. 
e) If S and T are sets of integers, then gcd(S U T) = 
gcd(gcd(S), gcd(T)). 
! Exercise 11.6.4: Find another solution to the second Diophantine equation 
in Example 11.33. 
Exercise 11.6.5 
: 
Apply the independent-variables test in the following situa- 
tion. The loop nest is 
f o r  (i=O; i<lOO; 
i++) 
f o r  (j=O; j(100; j++) 
f o r  (k=O; 
kclOO; k++) 
and inside the nest is an assignment involving array accesses. Determine if there 
are any data dependences due to each of the following statements: 
d) A[i, j 
,k] = A[i+99,k+1OOy 
jl. 
Exercise 11.6.6 
: 
In the two constraints 
eliminate x by replacing it by a constant lower bound on y. 
Exercise 11.6.7 
: Apply the loop-residue test to the following set of con- 
straints: 
Exercise 11.6.8: Apply the loop-residue test to the following set of con- 
straints: 
828 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Exercise 11.6.9 : Apply the loop-residue test to the following set of con- 
straints: 
11.7 Finding Synchronization-Free Parallelism 
Having developed the theory of affine array accesses, their reuse of data, and 
the dependences among them, we shall now begin to apply this theory to paral- 
lelization and optimization of real programs. As discussed in Section 11.1.4, it 
is important that we find parallelism while minimizing communication among 
processors. Let us start by studying the problem of parallelizing an application 
without allowing any communication or synchronization between processors at 
all. This constraint may appear to be a purely academic exercise; how often 
can we find programs and routines that have such a form of parallelism? In 
fact, many such programs exist in real life, and the algorithm for solving this 
problem is useful in its own right. In addition, the concepts used to solve this 
problem can be extended to handle synchronization and communication. 
11.7.1 An Introductory Example 
Shown in Fig. 11.23 is an excerpt of a C translation (with Fortran-style array 
accesses retained for clarity) from a 5000-line Fortran multigrid algorithm to 
solve three-dimensional Euler equations. The program spends most its time 
in a small number of routines like the one shown in the figure. It is typical 
of many numerical programs. These often consist of numerous for-loops, with 
different nesting levels, and they have many array accesses, all of which are 
affine expressions of surrounding loop indexes. To keep the example short, we 
have elided lines from the original program with similar characteristics. 
The code of Fig. 11.23 operates on the scalar variable T and a number 
of different arrays with different dimensions. Let us first examine the use of 
variable T. Because each iteration in the loop uses the same variable T, we 
cannot execute the iterations in parallel. However, T is used only as a way 
to hold a common subexpression used twice in the same iteration. In the first 
two of the three loop nests in Fig. 11.23, each iteration of the innermost loop 
writes a value into T and uses the value immediately after twice, in the same 
iteration. We can eliminate the dependences by replacing each use of T by the 
right-hand-side expression in the previous assignment of T, without changing 
the semantics of the program. Or, we can replace the scalar T by an array. We 
then have each iteration (j, 
i) use its own array element T[j, 
i]. 
With this modification, the computation of an array element in each as- 
signment statement depends only on other array elements with the same values 
for the last two components ( j  and i, respectively). We can thus group all 
2 2.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
f o r  (j = 2; j <= jl; j++) 
f o r  (i = 2, i <= il, i++) ( 
AP[j ,il 
- 
- ..., 
T 
= 1.0/(1.0 + AP[j,i]); 
D[2,j,i] 
= T*AP[j,il; 
DW[l,2, 
j 
,i] = T*DW[l,2, 
j 
,il 
; 
1 
f o r  (k = 3; 
k <= kl-1; k++) 
f o r  (j = 2; j <= jl; j++) 
f o r  (i = 2; i <= il; i++) ( 
AM[j ,il 
= AP[j,il; 
APCj 
,il 
- 
- ...) 
T 
= ... 
AP[j,i] - AM[j,i]*D[k-l,j,i] ...; 
D[k,j,il 
=T*AP[j,i]; 
D W ~ l y k Y j y i ~  
= T*(DW[I,k,j,i] + DW[I,k-l,j,i]). . .; 
3 
.
a
.
 
f o r  (k = kl-I; k >= 2; 
k--) 
f o r  (j = 2; j <= jl; j++) 
f o r  (i = 2; i <= il; i++) 
DW[l,k,j,il = DW[l,k,j,il + ~[k,j,il*DW[l,k+~,j,il; 
Figure 11.23: Code excerpt of a multigrid algorithm 
operations that operate on the (j, 
i)th element of all arrays into one computa- 
tion unit, and execute them in the original sequential order. This modification 
produces (jl 
- 
1) x (il 
- 
1) units of computation that are all independent of 
one another. Notice that second and third nests in the source program involve 
a third loop, with index k. However, because there is no dependence between 
dynamic accesses with the same values for j and i, we can safely perform the 
loops on k inside the loops on j and i - 
that is, within a computation unit. 
Knowing that these computation units are independent enables a number 
of legal transforms on this code. For example, instead of executing the code as 
originally written, a uniprocessor can perform the same computation by execut- 
ing the units of independent operation one unit at a time. The resulting code, 
shown in Fig. 11.24, has improved temporal locality, because results produced 
are consumed immediately. 
The independent units of computation can also be assigned to different 
processors and executed in parallel, without requiring any synchronization or 
communication. Since there are (j 
1 
- 
1) x (il 
- 
1) independent units of com- 
putation, we can utilize at most (jl 
- 
1) x (il 
- 
1) processors. By organizing 
the processors as if they were in a 2-dimensional array, with ID'S (j, 
i), where 
2 5 j < jl and 2 5 i < il, 
the SPMD 
program to be executed by each 
processor is simply the body in the inner loop in Fig. 11.24. 
830 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
f o r  (j = 2; j <= jl; j++) 
f o r  (i = 2; i <= il; i++) ( 
APCj 
,il 
- 
- ...) 
TCj 
,il 
= 1.0/(1.0 + AP[j,i]); 
D[2, 
J 5j-1 
= TCj 
,il 
*AP[j ,i] 
; 
DW[Iy2, 
j 
,il 
= TCj 
,i]*DW[1,2, 
j 
,i]; 
f o r  (k = 3; k <= kl-I; k++) { 
AMCj 
,il 
= AP[j,i]; 
APIj 
,il 
- 
- ...; 
T[j ,il 
- 
- ...AP 
[j,il - AM[j,i]*~[k-l,j,i] ...; 
D[k, 
j 
,il 
= TCj 
,i]*AP[j ,i] 
; 
DW[l,k,j,il = T[j,il*(~~[l,k,j,i] 
+ DW[i,k-l,j,i]) ...; 
3 
... 
f o r  (k = kl-I; k >= 2; 
k--1 
DW[l,k,j,i] = DW[l,k,j,il + ~[k,j,il*DW[~,k+~,j,il; 
1 
Figure 11.24: Code of Fig. 11.23 transformed to carry outermost parallel loops 
The above example illustrates the basic approach to finding synchronization- 
free parallelism. We first split the computation into as many independent units 
as possible. This partitioning exposes the scheduling choices available. We 
then assign computation units to the processors, depending on the number of 
processors we have. Finally, we generate an SPMD program that is executed 
on each processor. 
11.7.2 Affine Space Partitions 
A loop nest is said to have k degrees of parallelism if it has, within the nest, k 
parallelizable loops - 
that is, loops such that there are no data dependencies 
between different iterations of the loops. For example, the code in Fig. 11.24 
has 2 degrees of parallelism. It is convenient to assign the operations in a com- 
putation with k degrees of parallelism to a processor array with k dimensions. 
We shall assume initially that each dimension of the processor array has 
as many processors as there are iterations of the corresponding loop. After 
all the independent computation units have been found, we shall map these 
"virtual" processors to the actual processors. In practice, each processor should 
be responsible for a fairly large number of iterations, because otherwise there 
is not enough work to amortize away the overhead of parallelization. 
We break down the program to be parallelized into elementary statements, 
such as 3-address statements. For each statement, we find an afine space 
partition that maps each dynamic instance of the statement, as identified by its 
loop indexes, to a processor ID. 
11.7. FINDING SYNCHRONIZATION-FREE 
PARALLELISM 
831 
Example 11.40 : 
As discussed above, the code of Fig. 11.23 has two degrees of 
parallelism. We view the processor array as a &dimensional space. Let (pl , 
pz) 
be the ID of a processor in the array. The parallelization scheme discussed in 
Section 11.7.1 can be described by simple affine partition functions. All the 
statements in the first loop nest have this same affine partition: 
All the statements in the second and third loop nests have the following same 
affine partition: 
The algorithm to find synchronization-free parallelism consists of three steps: 
1. Find, for each statement in the program, an affine partition that maxi- 
mizes the degree of parallelism. Note that we generally treat the state- 
ment, rather than the single access, as the unit of computation. The same 
affine partition must apply to each access in the statement. This grouping 
of accesses makes sense, since there is almost always dependence among 
accesses of the same statement anyway. 
2. Assign the resulting independent computation units among the processors, 
and choose an interleaving of the steps on each processor. This assignment 
is driven by locality considerations. 
3. Generate an SPMD program to be executed on each processor. 
We shall discuss next how to find the affine partition functions, how to gen- 
erate a sequential program that executes the partitions serially, and how to 
generate an SPMD program that executes each partition on a different pro- 
cessor. After we discuss how parallelism with synchronizations is handled in 
Sections 11.8 through 11.9.9, we return to Step 2 above in Section 11.10 and 
discuss the optimization of locality for uniprocessors and multiprocessors. 
11.7.3 Space-Partition Constraints 
To require no communication, each pair of operations that share a data depen- 
dence must be assigned to the same processor. We refer to these constraints as 
"space-partition constraints." Any mapping that satisfies these constraints cre- 
ates partitions that are independent of one another. Note that such constraints 
can be satisfied by putting all the operations in a single partition. Unfortu- 
nately, that "solution" does not yield any parallelism. Our goal is to create 
832 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
as many independent partitions as possible while satisfying the space-partition 
constraints; that is, operations are not placed on the same processor unless it 
is necessary. 
When we restrict ourselves to affine partitions, then instead of maximizing 
the number of independent units, we may maximize the degree (number of 
dimensions) of parallelism. It is sometimes possible to create more independent 
units if we can use piecewise affine partitions. A piecewise affine partition 
divides instances of a single access into different sets and allows a different 
affine partition for each set. However, we shall not consider such an option 
here. 
Formally, an affine partition of a program is synchronixation free if and only 
if for every two (not necessarily distinct) accesses sharing a dependence, Fl = 
(Fl, 
fl, Bl, 
bl) in statement sl nested in dl loops, and F2 
= (F2, 
f2, B2, b2) in 
statement 52 nested in d2 loops, the partitions (Cl, 
cl) and (C2, c2) for state- 
ments sl and s2, respectively, satisfy the following space-partition constraints: 
For all il in zdl 
and i2 in Zd2 
such that 
a) Blil + 
bl 2 
0, 
b) B2i2 + 
b2 2 
0, and 
c) Flil +fl = 
F2i2 +f2, 
it is the case that Clil + 
cl = 
C2i2 
+ 
c2. 
The goal of the parallelization algorithm is to find, for each statement, the 
partition with the highest rank that satisfies these constraints. 
Shown in Fig. 11.25 is a diagram illustrating the essence of the space- 
partition constraints. Suppose there are two static accesses in two loop nests 
with index vectors il and i2. These accesses are dependent in the sense that 
they access at least one array element in common, and at least one of them is a 
write. The figure shows particular dynamic accesses in the two loops that hap- 
pen to access the same array element, according to the affine access functions 
Fl 
il + 
fl and F2i2 
+ 
f2. Synchronization is necessary unless the affine partitions 
for the two static accesses, Cl 
il + 
cl and C2i2 
+ 
c2, assign the dynamic accesses 
to the same processor. 
If we choose an affine partition whose rank is the maximum of the ranks of 
all statements, we get the maximum possible parallelism. However, under this 
partitioning some processors may be idle at times, while other processors are 
executing statements whose affine partitions have a smaller rank. This situation 
may be acceptable if the time taken to execute those statements is relatively 
short. Otherwise, we can choose an affine partition whose rank is smaller than 
the maximum possible, as long as that rank is greater than 0. 
We show in Example 11.41 a small program designed to illustrate the power 
of the technique. Real applications are usually much simpler than this, but 
may have boundary conditions resembling some of the issues shown here. We 
shall use this example throughout this chapter to illustrate that programs with 
11.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
Loops 
Array 
Processor ID 
Figure 11.25: Space-partition constraints 
affine accesses have relatively simple space-partition constraints, that these con- 
straints can be solved using standard linear algebra techniques, and that the 
desired SPMD program can be generated mechanically from the affine parti- 
t 
ions. 
Example 11.41 : 
This example shows how we formulate the space-partition 
constraints for the program consisting of the small loop nest with two state- 
ments, s~ 
and s2, shown in Figure 11.26. 
f o r  ( i  = 1 ;  i <= 100; i++) 
f o r  ( j  = 1 ;  j <= 100; j++) ( 
X [ i , j l  
= X C i , j l  + YCi-1,jl; 
/* ( s l )  */ 
Y [ i , j l  = Y [ i , j l  + X[i,j-I]; 
/* (s2) */ 
3 
Figure 11.26: A loop nest exhibiting long chains of dependent operations 
We show the data dependences in the program in Figure 11.27. That is, each 
black dot represents an instance of statement sl, 
and each white dot represents 
an instance of statement s 2 .  The dot located at coordinates (i, 
j) represents the 
instance of the statement that is executed for those values of the loop indexes. 
Note, however, that the instance of s z  is located just below the instance of sl 
for the same (i, 
j) pair, so the vertical scale of j is greater than the horizontal 
scale of i. 
Notice that X[i, 
j] 
is written by sl 
(i, 
j), that is, by the instance of statement 
sl with index values i and j. It is later read by s2 
(i, 
j + I), so sl 
(i, 
j) must 
834 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Figure 11.27: Dependences of the code in Example 11.41 
precede s2 (i, 
j + I). This observation explains the vertical arrows from black 
dots to white dots. Similarly, Y[i, 
j] is written by s2(i, 
j )  and later read by 
sl 
(i + 1, 
j). Thus, s 2  
(i, 
j) must precede sl 
(i + 1, 
j), which explains the arrows 
from white dots to black. 
It is easy to see from this diagram that this code can be parallelized without 
synchronization by assigning each chain of dependent operations to the same 
processor. However, it is not easy to write the SPMD program that implements 
this mapping scheme. While the loops in the original program have 100 itera- 
tions each, there are 200 chains, with half originating and ending with statement 
sl and the other half originating and ending with ~ 2 .  
The lengths of the chains 
vary from 1 
to 100 iterations. 
Since there are two statements, we are seeking two affine partitions, one for 
each statement. We only need to express the space-partition constraints for 
one-dimensional affine partitions. These constraints will be used later by the 
solution method that tries to find all the independent one-dimensional affine 
partitions and combine them to get multidimensional affine partitions. We can 
thus represent the affine partition for each statement by a 1 
x 2 matrix and a 1 
x 1 
vector to translate the vector of indexes [i, 
j] into a single processor number. 
Let ([Cll 
C12], 
[cl]), 
([C21 
C22], 
[c2 
I), be the one-dimensional affine partitions for 
the statements sl and s2, respectively. 
We apply six data dependence tests: 
1. Write access X[i, 
j] and itself in statement sl 
, 
2. Write access X[i, 
f 
with read access X[i, 
f 
in statement sl 
, 
3. Write access X[i, 
j] in statement sl with read access X[i, 
j - 
1
1
 in state- 
ment 5-2, 
11.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
4. Write access Y 
[i, 
j] and itself in statement s2, 
5. Write access Y 
[i, 
j] with read access Y 
[i, 
j] in statement s2, 
6. Write access Y [i, 
j] in statement sz with read access Y [i- I, 
j] 
in statement 
We see that the dependence tests are all simple and highly repetitive. The only 
dependences present in this code occur in case (3) between instances of accesses 
X[i, 
j] and X[i, 
j - 
1
1
 and in case (6) between Y[i, 
j] and Y[i - 
1, 
j]. 
The space-partition constraints imposed by the data dependence between 
X[i, 
j] in statement sl and X[i, 
j - 
1
1
 in statement s2 can be expressed in the 
following terms: 
For all (i, 
j) and (i', 
j') such that 
we have 
That is, the first four conditions say that (i, 
j )  and (i', 
j') lie within the itera- 
tion space of the loop nest, and the last two conditions say that the dynamic 
accesses X[i, 
j] and X [i, 
j - 
I] touch the same array element. We can derive 
the space-partition constraint for accesses Y[i - 
I, 
j] in statement s2 and Y[i, 
j] 
in statement sl in a similar manner. 
11.7.4 Solving Space-Partition Constraints 
Once the space-partition constraints have been extracted, standard linear alge- 
bra techniques can be used to find the affine partitions satisfying the constraints. 
Let us first show how we find the solution to Example 11.41. 
Example 11.42 : 
We can find the affine partitions for Example 11.41 
with the 
following steps: 
1. Create the space-partition constraints shown in Example 11.41. We use 
the loop bounds in determining the data dependences, but they are not 
used in the rest of the algorithm otherwise. 
2. The unknown variables in the equalities are i, i', j ,  j', Cl1, C12, el, C21, 
Cz2, 
and c2. Reduce the number of unknowns by using the equalities due 
to the access functions: i = i' and j = 
j' - 
1. We do so using Gaussian 
elimination, which reduces the four variables to two: say tl = 
i = 
i', and 
t2 
= 
j + 1 
= 
j'. The equality for the partition becomes 
836 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
3. The equation above holds for all combinations of tl and t2. 
Thus, it must 
be that 
If we perform the same steps on the constraint between the accesses 
Y[i - 
1, 
j] and Y[i, 
j], 
we get 
Simplifying all the constraints together, we obtain the following relation- 
ships: 
4. Find all the independent solutions to the equations involving only un- 
knowns in the coefficient matrix, ignoring the unknowns in the constant 
vectors in this step. There is only one independent choice in the coef- 
ficient matrix, so the affine partitions we seek can have at most a rank 
of one. We keep the partition as simple as possible by setting Cll = 1. 
We cannot assign 0 to Cll because that will create a rank-0 coefficient 
matrix, which maps all iterations to the same processor. It then follows 
that Czl = 1, 
C22 = -1, C12 
= -1. 
5. Find the constant terms. We know that the difference between the con- 
stant terms, c
2
 - 
cl, must be -1. 
We get to pick the actual values, 
however. To keep the partitions simple, we pick c
2
 = 
0; thus cl = -1. 
Let p be the ID of the processor executing iteration (i, 
j). In terms of p, the 
affine partition is 
That is, the (i, 
j)th iteration of sl is assigned to the processor p = i - 
j - 
1, 
and the (i, 
j)th iteration of 2
3
2
 is assigned to processor p = 
i - 
j. 
Algorithm 11.43 : Finding a highest-ranked synchronization-free affine par- 
tition for a program. 
INPUT: A program with affine array accesses. 
11.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
OUTPUT: A partition. 
METHOD: Do the following: 
1. Find all data-dependent pairs of accesses in a program for each pair of 
data-dependent accesses, Fl = (Fl 
, 
fl , 
Bl 
, 
bl 
) in statement sl nested in 
dl loops and F2 = (Fa, 
4, 
B2, 
b2) 
in statement s2 nested in d2 loops. 
Let (Cl, 
cl) and (C2, 
c2) 
represent the (currently unknown) partitions of 
statements sl and s2, respectively. The space-partition constraints state 
that if 
then 
for all il and i2, 
within their respective loop bounds. We shall generalize 
the domain of iterations to include all il in Zdl and i2 in zd2; 
that is, the 
bounds are all assumed to be minus infinity to infinity. This assumption 
makes sense, since an affine partition cannot make use of the fact that an 
index variable can take on only a limited set of integer values. 
2. For each pair of dependent accesses, we reduce the number of unknowns 
in the index vectors. 
(a) Note that F i  + 
f is the same vector as 
That is, by adding an extra component 1 
at the bottom of column- 
vector i, we can make the column-vector f be an additional, last 
column of the matrix F. We may thus rewrite the equality of the 
access functions Flil + 
fi = 
F2i2 
+ 
f2 as 
(b) The above equations will in general have more than one solution. 
However, we may still use Gaussian elimination to solve the equations 
for the components of il and i2 as best we can. That is, eliminate as 
many variables as possible until we are left with only variables that 
cannot be eliminated. The resulting solution for il and i2 will have 
the form 
838 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
where U is an upper-triangular matrix and t is a vector of free vari- 
ables ranging over all integers. 
(c) We may use the same trick as in Step (2a) to rewrite the equality of 
the partitions. Substituting the vector (il, 
i2, 
1) 
with the result from 
Step (2b), we can write the constraints on the partitions as 
3. Drop the nonpartition variables. The equations above hold for all combi- 
nations of t if 
Rewrite these equations in the form Ax = 0, where x is a vector of all 
the unknown coefficients of the affine partitions. 
4. Find the rank of the affine partition and solve for the coefficient matrices. 
Since the rank of an affine partition is independent of the value of the 
constant terms in the partition, we eliminate all the unknowns that come 
from the constant vectors like cl or c2, thus replacing Ax = 0 by sim- 
plified constraints A'x' = 0. Find the solutions to A'x' = 0, expressing 
them as B, a set of basis vectors spanning the null space of A'. 
5. Find the constant terms. Derive one row of the desired affine partition 
from each basis vector in B, and derive the constant terms using Ax = 
0. 
Note that Step 3 ignores the constraints imposed by the loop bounds on 
variables t. The constraints are only stricter as a result, and the algorithm must 
therefore be safe. That is, we place constraints on the C's and c's assuming 
t is arbitrary. Conceivably, there would be other solutions for the C's and c's 
that are valid only because some values of t are impossible. Not searching for 
these other solutions may cause us to miss an optimization, but cannot cause 
the program to be changed to a program that does something different from 
what the original program does. 
11.7.5 A Simple Code-Generation Algorithm 
Algorithm 11.43 generates affine partitions that split computations into inde- 
pendent partitions. Partitions can be assigned arbitrarily to different proces- 
sors, since they are independent of one another. A processor may be assigned 
more than one partition and can interleave the execution of its partitions, as 
long as operations within each partition, which normally have data dependences, 
are executed sequentially. 
11.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
839 
It is relatively easy to generate a correct program given an affine partition. 
We first introduce Algorithm 11.45, a simple approach to generating code for a 
single processor that executes each of the independent partitions sequentially. 
Such code optimizes temporal locality, since array accesses that have several 
uses are very close in time. Moreover, the code easily can be turned into an 
SPMD program that executes each partition on a different processor. The code 
generated is, unfortunately, inefficient; we shall next discuss optimizations to 
make the code execute efficiently. 
The essential idea is as follows. We are given bounds for the index variables 
of a loop nest, and we have determined, in Algorithm 11.43, a partition for the 
accesses of a particular statement s. Suppose we wish to generate sequential 
code that performs the action of each processor sequentially. We create an 
outermost loop that iterates through the processor IDS. That is, each iteration 
of this loop performs the operations assigned to a particular processor ID. The 
original program is inserted as the loop body of this loop; in addition, a test 
is added to guard each operation in the code to ensure that each processor 
only executes the operations assigned to it. In this way, we guarantee that the 
processor executes all the instructions assigned to it, and does so in the original 
sequential order. 
Example 11.44: Let us generate code that executes the independent parti- 
tions in Example 11.41 sequentially. The original sequential program is from 
Fig. 11.26 is repeated here as Fig. 11.28. 
for ( i  = 1 ;  i <= 100; i++) 
f o r  ( j  = 1 ;  j <= 100; j++) C 
X[i,j] = X[i,j] + Y[i-1,jl; 
/* ( s l )  */ 
Y[i,jl = Y[i,jl + X[i,j-11; 
/* (s2) */ 
3 
Figure 11.28: Repeat of Fig. 11.26 
In Example 11.41, the affine partitioning algorithm found one degree of 
parallelism. Thus, the processor space can be represented by a single variable 
p. Recall also from that example that we selected an affine partition that, for 
all values of index variables i and j  with 1 < i 5 100 and 1 
< j 5 100, assigned 
1. Instance (i, 
j )  of statement s
l
 to processor p = 
i - 
j  - 
1, and 
2. Instance (i, 
j )  of statement 
to processor p = 
i - 
j. 
We can generate the code in three steps: 
1. For each statement, find all the processor IDS participating in the com- 
putation. We combine the constraints I < i < 100 and 1 5 j < 100 with 
one of the equations p = 
i - 
j  - 
1 or p = 
i - 
j, and project away i and j 
to get the new constraints 
840 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
(a) -100 5 p < 98 if we use the function p = i - 
j - 
1 that we get for 
statement sl, and 
(b) -99 5 p 5 99 if we use p = 
i - 
j from statement sz. 
2. Find all the processor IDS participating in any of the statements. When 
we take the union of these ranges, we get - 
100 5 p < 99; these bounds 
are sufficient to cover all instances of both statements sl and sz. 
Generate the code to iterate through the computations in each partition 
sequentially. The code, shown in Fig. 11.29, 
has an outer loop that iterates 
through all the partition IDS participating in the computation (line (I)). 
Each partition goes through the motion of generating the indexes of all 
the iterations in the original sequential program in lines (2) and (3) so 
that it can pick out the iterations the processor p is supposed to execute. 
The tests of lines (4) and (6) make sure that statements sl and sz are 
executed only when the processor p would execute them. 
The generated code, while correct, is extremely inefficient. First, even 
though each processor executes computation from at most 99 iterations, it gen- 
erates loop indexes for 100 x 100 iterations, an order of magnitude more than 
necessary. Second, each addition in the innermost loop is guarded by a test, 
creating another constant factor of overhead. These two kinds of inefficiencies 
are dealt with in Sections 11.7.6 and 11.7.7, respectively. 
I )  
f o r  (p = -100; p <= 99; p++) 
2 
) 
f o r  ( i  = 1 ;  i <= 100; i++) 
3 
) 
f o r  ( j  = I ;  j <= 100; j++) ( 
4) 
i f  (p == i-j-I) 
5 
X[i,jl = X[i,jl + YCi-1,jl; 
/* ( s l )  */ 
6 
if (p == i - j )  
7 
Y[i,jl = X[i,j-11 + YCi,jl; /* (s2) */ 
8 
> 
> 
Figure 11.29: A simple rewriting of Fig. 11.28 that iterates over processor space 
Although the code of Fig. 11.29 appears designed to execute on a unipro- 
cessor, we could take the inner loops, lines (2) through (8) 
, 
and execute them 
on 200 different processors, each of which had a different value for p, from -100 
to 99. Or, we could partition the responsibility for the inner loops among any 
number of processors less than 200, as long as we arranged that each processor 
knew what values of p it was responsible for and executed lines (2) through (8) 
for just those values of p. 
Algorithm 11.45: Generating code that executes partitions of a program 
sequentially. 
11.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
841 
INPUT: A program P with affine array accesses. Each statement s in the 
program has associated bounds of the form B,i + 
b, 2 
0, where i is the vector 
of loop indexes for the loop nest in which statement s appears. Also associated 
with statement s is a partition C,i+c, = 
p where p is an m-dimensional vector 
of variables representing a processor ID; m is the maximum, over all statements 
in program P, 
of the rank of the partition for that statement. 
OUTPUT: A program equivalent to P but iterating over the processor space 
rather than over loop indexes. 
METHOD: Do the following: 
1. For each statement, use Fourier-Motzkin elimination to project out all the 
loop index variables from the bounds. 
2. Use Algorithm 11.13 to determine bounds on the partition ID'S. 
3. Generate loops, one for each of the m dimensions of processor space. Let 
p = bl,pa,. . 
. ,p,,] be the vector of variables for these loops; that is, 
there is one variable for each dimension of the processor space. Each loop 
variable pi ranges over the union of the partition spaces for all statements 
in the program P. 
Note that the union of the partition spaces is not necessarily convex. To 
keep the algorithm simple, instead of enumerating only those partitions that 
have a nonempty computation to perform, set the lower bound of each p
i
 to 
the minimum of all the lower bounds imposed by all statements and the upper 
bound of each pi to the maximum of all the upper bounds imposed by all 
statements. Some values of p may thereby have no operations. 
The code to be executed by each partition is the original sequential pro- 
gram. However, every statement is guarded by a predicate so that only those 
operations belonging to the partition are executed. 
An example of Algorithm 11.45 will follow shortly. Bear in mind, however, 
that we are still far from the optimal code for typical examples. 
11.7.6 Eliminating Empty Iterations 
We now discuss the first of the two transformations necessary to generate ef- 
ficient SPMD code. The code executed by each processor cycles through all 
the iterations in the original program and picks out the operations that it is 
supposed to execute. If the code has k degrees of parallelism, the effect is that 
each processor performs k orders of magnitude more work. The purpose of the 
first transformation is to tighten the bounds of the loops to eliminate all the 
empty iterations. 
We begin by considering the statements in the program one at a time. A 
statement's iteration space to be executed by each partition is the original itera- 
tion space plus the constraint imposed by the affine partition. We can generate 
842 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
tight bounds for each statement by applying Algorithm 11.13 to the new iter- 
ation space; the new index vector is like the original sequential index vector, 
with processor ID'S added as outermost indexes. Recall that the algorithm will 
generate tight bounds for each index in terms of surrounding loop indexes. 
After finding the iteration spaces of the different statements, we combine 
them, loop by loop, making the bounds the union of those for each statement. 
Some loops end up having a single iteration, as illustrated by Example 11.46 
below, and we can simply eliminate the loop and simply set the loop index to 
the value for that iteration. 
Example 11.46 : For the loop of Fig. 11.30(a), Algorithm 11.43 will create 
the affine partition 
Algorithm 11.45 
will create the code of Fig. 11.30(b). Applying Algorithm 11.13 
to statement sl produces the bound: p 5 i 5 p, or simply i = p. Similarly, 
the algorithm determines j = p for statement s 2 .  Thus, we get the code of 
Fig. 11.30(c). Copy propagation of variables i and j will eliminate the unnec- 
essary test and produce the code of Fig. 11.30(d). 
We now return to Example 11.44 and illustrate the step to merge multiple 
iteration spaces from different statements together. 
Example 11.47: Let us now tighten the loop bounds of the code in Exam- 
ple 11.44. The iteration space executed by partition p for statement sl is defined 
by the following equalities and inequalities: 
Applying Algorithm 11.13 to the above creates the constraints shown in Fig. 
11.31(a). Algorithm 11.13 generates the constraint p + 2 5 i 5 100 + 
p + 1 
from i - 
p - 
1 
= 
j and 1 < j 5 100, and tightens the upper bound of p to 98. 
Likewise, the bounds for each of the variables for statement s
2
 are shown in 
Fig. 11.31(b). 
The iteration spaces for sl and sz in Fig. 11.31 are similar, but as ex- 
pected from Fig. 11.27, certain limits differ by 1 
between the two. The code in 
Fig. 11.32 executes over this union of iteration spaces. For example, for i use 
min(1, 
p + 
1) as the lower bound and max(100,100 + p  + 
1) as the upper bound. 
Note that the innermost loop has 2 iterations except that it has only one the 
first and last time it is executed. The overhead in generating loop indexes is 
thus reduced by an order of magnitude. Since the iteration space executed is 
larger than either that of sl and s2, 
conditionals are still necessary to select 
when these statements are executed. 
11.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
f o r  ( i = l ;  i<=N; i++) 
Y C i ]  = ZCil; /* ( s l )  */ 
f o r  (j=1; j<=N; j++) 
XCjl = Y C j l ;  /* (s2) */ 
(a) Initial code. 
f o r  (p=l; p<=N; p++) ( 
f o r  ( i = l ;  i<=N; i++) 
i f  (p == i )  
Y C i l  = Z C i l  ; /* ( s l )  */ 
f o r  ( j = l ;  j<=N; j++) 
i f  (p == j )  
X C j l  = YCjl ; /* (s2) */ 
1 
(b) Result of applying Algorithm 11.45. 
f o r  (p=l; p<=N; p++) ( 
i = p; 
i f  (p == i )  
Y Cil = Z C i l  ; /* (sl) */ 
j = p; 
i f  (p == j 
(c) After applying Algorithm 11.13. 
f o r  (p=1; p < = ~ ;  
p++) { 
YCpl = z Cpl ; /* ( s l )  */ 
x Cpl = y 
Cpl ; /* (s2) */ 
> 
(d) Final code. 
Figure 11.30: Code for Example 11.46 
844 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
(a) Bounds for statement sl 
. 
j :  i - p  5 j 
5 i - p  
1 5 j - 
< 100 
(b) Bounds for statement sa. 
Figure 11.31: Tighter bounds on p, i, and j for Fig. 11.29 
11.7.7 Eliminating Tests from Innermost Loops 
The second transformation is to remove conditional tests from the inner loops. 
As seen from the examples above, conditional tests remain if the iteration spaces 
of statements in the loop intersect but not completely. To avoid the need for 
conditional tests, we split the iteration space into subspaces, each of which 
executes the same set of statements. This optimization requires code to be 
duplicated and should only be used to remove conditionals in the inner loops. 
To split an iteration space to reduce tests in inner loops, we apply the 
following steps repeatedly until we remove all the tests in the inner loops: 
1. Select a loop that consists of statements with different bounds. 
2. Split the loop using a condition such that some statement is excluded 
from at least one of its components. We choose the condition from among 
the boundaries of the overlapping different polyhedra. If some statement 
has all its iterations in only one of the half planes of the condition, then 
such a condition is useful. 
3. Generate code for each of these iteration spaces separately. 
Example 11.48 : 
Let us remove the conditionals from the code of Fig. 11.32. 
Statements s
l
 and sz are mapped to the same set of partition ID'S except for 
11.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
f o r  (p = -100; p <= 99; 
p++) 
f o r  (i = rnax(l,p+l) ; i <= min(l00,101+p) ; i++) 
f o r  (j =max(l,i-p-I); j <=min(100,i-p); j++) C 
if (p == i-j-I) 
x[i,j] = X[i,j] + YCi-1,jl; /* ( ~ 1 )  
*/ 
if (p == i-j) 
Y[i,j] = X[i,j-I] + YCi,jl; /* ( ~ 2 )  
*/ 
3 
Figure 11.32: Code of Fig. 11.29 improved by tighter loop bounds 
the boundary partitions at either end. Thus, we separate the partition space 
into three subspaces: 
2. -99 5 p < 98, and 
The code for each subspace can then be specialized for the value(s) of p 
contained. Figure 11.33 shows the resulting code for each of the three iteration 
spaces. 
Notice that the first and third spaces do not need loops on i or j ,  because 
for the particular value of p that defines each space, these loops are degenerate; 
they have only one iteration. For example, in space (I), substituting p = -100 
in the loop bounds restricts i to 1, and subsequently j to 100. The assignments 
to p in spaces (1) and (3) are evidently dead code and can be eliminated. 
Next we split the loop with index i in space (2). Again, the first and last 
iterations of loop index i are different. Thus, we split the loop into three 
subspaces: 
a) max(1,p + 1) 5 i < 
p + 
2, where only sz is executed, 
b) max(1, 
p +  2) 5 i 5 min(100,lOO 
+p), where both sl and s2 are executed, 
and 
c) 101 + 
p < i 5 min(l0l + 
p, loo), where only sl is executed. 
The loop nest for space (2) in Fig. 11.33 
can thus be written as in Fig. 11.34(a). 
Figure 11.34(b) shows the optimized program. We have substituted Fig. 
11.34(a) for the loop nest in Fig. 11.33. We also propagated out assignments to 
p, i, and j into the array accesses. When optimizing at the intermediate-code 
level, some of these assignments will be identified as common subexpressions 
and re-extracted from the array-access code. 
846 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
/* space (1) */ 
p = -100; 
i = 1; 
j = 100; 
x [ i , j ]  = X[i,j] + ~ [ i - 1 , j I ;  /* ( ~ 1 )  
*/ 
/* space (2) */ 
f o r  (p = -99; p <= 98; p++) 
f o r  ( i  = max(l,p+l) ; i <= min(100,101+p) ; i++) 
f o r  ( j  = max(1,i-p-1); j <= min(100,i-p); j++) ( 
i f  (p == i-j-1) 
X[i,j] = X[i,j] + Y[i-1,jl; 
/* (s1) */ 
i f  (p == i - j )  
Y[i,j] = X[i,j-11 + ~ [ i , j ] ;  
/* (s2) */ 
> 
/* space (3) */ 
p = 99; 
i = 100; 
j = 1; 
Y[i,j] = X[i,j-11 + Y[i,jl ; /* ( ~ 2 )  
*/ 
Figure 11.33: Splitting the iteration space on the value of p 
11.7.8 
Source-Code Transforms 
We have seen how we can derive from simple affine partitions for each statement 
programs that are significantly different from the original source. It is not 
apparent from the examples seen so far how affine partitions correlate with 
changes at the source level. This section shows that we can reason about source 
code changes relatively easily by breaking down affine partitions into a series 
of primitive transforms. 
Seven Primitive Affine Transforms 
Every affine partition can be expressed as a series of primitive affine transforms, 
each of which corresponds to a simple change at the source level. There are 
seven kinds of primitive transforms: the first four primitives are illustrated in 
Fig. 11.35, the last three, also known as unimodular transforms, are illustrated 
in Fig. 11.36. 
The figure shows one example for each primitive: a source, an affine parti- 
tion, and the resulting code. We also draw the data dependences for the code 
before and after the transforms. From the data dependence diagrams, we see 
that each primitive corresponds to a simple geometric transform and induces a 
relatively simple code transform. The seven primitives are: 
11.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
847 
/* space (2) */ 
f o r  (p = -99; p <= 98; p++) ( 
/* space (2a) */ 
i f  (p >= 0) ( 
i = p+l; 
j = 1 ;  
YCi,jl = X[i,j-11 + YCi,jl; /* (s2) */ 
J 
/* space (2b) */ 
j = i-p-1; 
XCi,jl = X C i , j l  +YCi-1,jl; 
/* ( s l )  */ 
j = i-p; 
/* space (2c) */ 
i f  (p <= -1) ( 
(a) Splitting space (2) on the value of i. 
/* space ( I ) ;  p = -100 
*/ 
X C l ,  1001 = XC1,100l + Y CO, 1001 ; 
/* space (2) */ 
f o r  (p = -99; p <= 98; p++) ( 
i f  (p >= 0) 
YCp+l,ll = XCp+l,Ol + Y[p+l,ll; 
/* (s2) */ 
f o r  ( i  = max(l,p+2); i <= min(100,100+p); i++) ( 
X [i, 
i-p-11 
= X C i ,  i-p-11 + Y [i-1 i-p-11 ; 
/* ( s l )  */ 
Y [i, 
i-pl = X [i, 
i-p-I] + Y [i, 
i-p] ; 
/* ( ~ 2 )  
*/ 
> 
i f  (p <= -1) 
xC101+p, 1001 = XCIOl+p, 1001 + Y C101+p-1,1001; /* ( s l )  */ 
3 
/* space (3); p = 99 */ 
Yr10oy11 = xr100,01 + Y[100,1]; 
/* (s2) */ 
(b) Optimized code equivalent to Fig. 11.28. 
Figure 11.34: Code for Example 11.48 
848 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
f o r  (i=1; i<=N; i++) 
Y [i] 
= Z t i l  ; /*sl*/ 
f o r  (j=1; j<=N; j++) 
X[j] = Y [j] ; /*s2*/ 
Fusion 
s1 : p = 2  
S2 : p = j  
f o r  (p=l; p<=N; p++)C 
Y [pl = z Cpl ; 
x Cpl = y Cpl ; 
f o r  (p=l; p<=N; p++)( 
Y Cpl = Z Cpl ; 
x Cpl = Y CpI ; 
3 
S1 
S 2  
f o r  ( i = l ;  i<=N; i++) ( 
Y [ i ]  
= Z [ i ]  ; 
/*sl*/ 
X [ i ]  
= Y Ci-11; /*s2*/ 
f o r  (i=1; i<=N; i++) 
Y [ i ]  
= Z Cil ; /*sl*/ 
i f  (N>=l) X C
1
1
 =Y COI ; 
f o r  (p=i; p<=N-1; p++)( 
Y Cpl =z 
Cpl ; 
X [p+ll 
=Y Cpl ; 
-
,
 
Fission 
s1 : i = p  
f o r  ( j = l ;  j<=N; j++) 
~ [ j ]  
= YCjl ; / * ~ 2 * /  
Figure 11.35: Primitive affine transforms (I) 
f o r  ( i = l ;  i<=N; i++) 
y[2*i] = ~ [ 2 * i l ;  
/*sl*/ 
f o r  (j=1; j<=2N; j++) 
x[jl=YCjI ; 
/*''*I 
Scaling 
sl : p  
= 
2 * i  
f o r  (p=l; p<=2*N; p++)C 
i f  (p mod 2 == 0) 
Y [pl = Z Cpl ; 
x 
[pl = Y [pl ; 
1 
2 2.7. FINDING SYNCHRONIZATION-FREE 
PARALLELISM 
Figure 11.36: Primitive affine transforms (11) 
SOURCE 
CODE 
f o r  (i=O; i>=N; i++) 
Y [N-i] 
= Z [il ; 
/*sl*/ 
f o r  (j=O; j<=N; j++) 
X[j] = Y [j] ; 
/*s2*/ 
~1 
S 2  
f o r  ( i = l ;  i<=N; i++) 
f o r  (j=O; j<=M; j++) 
Z[i,jl = 
Z [ i - I ,  j l  ; 
l
x
s
x
r
 
f o r  ( i = l ;  i<=N+M-I; i++) 
f o r  (j=max(l,i+N) 
; 
j<=min(i,M); j++) 
Z[i,jl = 
ZCi-I, j-I] ; 
PARTITION 
Reversal 
s l : p = N - i  
(SZ : 
P = 
j )  
Permutation 
I] 
= [: 
:
I
 
[
;
I
 
Skewing 
TRANSFORMED 
CODE 
f o r  (p=O; p<=N; p++)( 
Y 
[pl = Z [N-pl ; 
X Cpl = Y 
Cpl ; 
1 
S1 
f o r  (p=O; p<=M; p++) 
f o r  (q=l; q<=N; i++) 
Z[q,pl = ZCq-Lpl 
IE+ 
f o r  (p=l; p<=N; p++) 
f o r  (q=l; q<=M; q++) 
Z [p,q-pl = 
Z[p-1,q-p-11 
850 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Unimodular Transforms 
A unimodular transform is represented by just a unimodular coefficient 
matrix and no constant vector. A unimodular matrix is a square matrix 
whose determinant is f 
1. The significance of a unimodular transform is 
that it maps an n-dimensional iteration space to another n-dimensional 
polyhedron, where there is a one-to-one correspondence between iterations 
of the two spaces. 
1. Fusion. The fusion transform is characterized by mapping multiple loop 
indexes in the original program to the same loop index. The new loop 
fuses statements from different loops. 
2. Fission. Fission is the inverse of fusion. It maps the same loop index 
for different statements to different loop indexes in the transformed code. 
This splits the original loop into multiple loops. 
3. Re-indexing. Re-indexing shifts the dynamic executions of a statement 
by a constant number of iterations. The affine transform has a constant 
term. 
4. Scaling. Consecutive iterations in the source program are spaced apart by 
a constant factor. The affine transform has a positive nonunit coefficient. 
5. Reversal. Execute iterations in a loop in reverse order. Reversal is char- 
acterized by having -1 as a coefficient. 
6. Permutation. Permute the inner and outer loops. The affine transform 
consists of permuted rows of the identity matrix. 
7. Skewing. Iterate through the iteration space in the loops at an angle. The 
affine transform is a unimodular matrix with 1's on the diagonal. 
A Geometric Interpretation of Parallelizat 
ion 
The affine transforms shown in all but the fission example are derived by apply- 
ing the synchronization-free affine partit 
ion algorithm to the respective source 
codes. (We shall discuss how fission can parallelize code with synchronization 
in the next section.) In each of the examples, the generated code has an (outer- 
most) parallelizable loop whose iterations can be assigned to different processors 
and no synchronization is necessary. 
These examples illustrate that there is a simple geometric interpret 
ation 
of how parallelization works. Dependence edges always point from an earlier 
instance to a later instance. So, dependences between separate statements 
not nested in any common loop follows the lexical order; dependences between 
11.7. FINDING SYNCHRONIZATION-FREE PARALLELISM 
851 
statements nested in the same loop follows the lexicographic order. Geometri- 
cally, dependences of a two-dimensional loop nest always point within the range 
[0°, 
180°), meaning that the angle of the dependence must be below 180°, but 
no less than 0'. 
The affine transforms change the ordering of iterations such that all the 
dependences are found only between operations nested within the same iteration 
of the outermost loop. In other words, there are no dependence edges at the 
boundaries of iterations in the outermost loop. We can parallelize simple source 
codes by drawing their dependences and finding such transforms geometrically. 
11.7.9 Exercises for Section 11.7 
Exercise 11.7.1 
: 
For the following loop 
f o r  ( i  = 2; i < 100; i++) 
A [i] 
= A [i-21 ; 
a) What is the largest number of processors that can be used effectively to 
execute this loop? 
b) Rewrite the code with processor p as a parameter. 
c) Set up and find one solution to the space-partition constraints for this 
loop. 
d) What is the affine partition of highest rank for this loop? 
Exercise 11.7.2 
: 
Repeat Exercise 11.7.1 for the loop nests in Fig. 11.37. 
Exercise 11.7.3 
: 
Rewrite the following code 
f o r  ( i  = 0; i < 100; i++) 
A [i] = 2*A [i] 
; 
f o r  ( j  = 0; j < 100; j++) 
A [ j ]  
= A [ j ]  + I; 
so it consists of a single loop. Rewrite the loop in terms of a processor number p 
so the code can be partitioned among 100 processors, with iteration p executed 
by processor p. 
Exercise 11.7.4 
: 
In the following code 
f o r  ( i  = 1 ;  i < 100; i++) 
f o r  ( j  = 1 ;  j < 100; j++) 
/* (s) */ 
A h ,  
jl = 
(A[i-l,j] + ~ [ i + l , j ]  
+ A[i,j-11 + A [ i , j + l l ) / 4 ;  
852 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
f o r  ( i  = 0; i <= 97; i++) 
A [i] 
= A [i+2] ; 
f o r  ( i  = 1 ;  i <= 100; i++) 
f o r  ( j  = 1 ;  j <= 100; j++) 
f o r  (k = 1 ;  k <= 100; k++) ( 
A[i,j,k] = A[i,j,k] + B[i-l,j,k]; 
B[i,j,k] = B[i,j,k] + C[i,j-1,kl; 
C[i,j,k] = C [ i , j  ,k] + A [ i , j  ,k-I]; 
3 
f o r  ( i  = 1 ;  i <= 100; i++) 
f o r  ( j  = 1; j <= 100; j++) 
f o r  (k = I ;  k <= 100; k++) ( 
A [ i ,  j ,kl = A C i ,  j ,kl + B[i-I, j ,kl ; 
B[i,j,kl = B[i,j,kl + A[i,j-l,kl; 
C[i,j,k] = C[i,j,kl + A[i,j,k-11 + B [ i , j , k l ;  
3 
Figure 11.37: Code for Exercise 11.7.2 
the only constraints are that the statement s that forms the body of the loop 
nest must execute iterations s(i - 
1, 
j )  and s(i, 
j - 
1) 
before executing iteration 
s(i, 
j). Verify that these are the only necessary constraints. Then rewrite the 
code so that the outer loop has index variable p, and on the pth iteration of the 
outer loop, all instances of s(i, 
j )  such that i + 
j = 
p are executed. 
Exercise 11.7.5 : 
Repeat Exercise 11.7.4, but arrange that on the pth iteration 
of the outer loop, instances of s such that i - 
j = 
p are executed. 
! 
Exercise 11.7.6 
: 
Combine the following loops 
f o r  ( i  = 0; i < 100; i++) 
A [il = B [il ; 
f o r  ( j  = 98; j >= 0; j = j-2) 
B[i] = i ;  
into a single loop, preserving all dependencies. 
11.8. SYNCHRONIZATION BETWEEN PARALLEL LOOPS 
Exercise 11.7.7 : 
Show that the matrix 
is unimodular. Describe the transformation it performs on a two-dimensional 
loop nest. 
Exercise 11.7.8 
: 
Repeat Exercise 11.7.7 
on the matrix 
11.8 Synchronization Between Parallel Loops 
Most programs have no parallelism if we do not allow processors to perform any 
synchronizations. But adding even a small constant number of synchronization 
operations to a program can expose more parallelism. We shall first discuss 
parallelism made possible by a constant number of synchronizations in this 
section and the general case, where we embed synchronization operations in 
loops, in the next. 
11.8.1 A Constant Number of Synchronizations 
Programs with no synchronization-free parallelism may contain a sequence of 
loops, some of which are parallelizable if they are considered independently. We 
can parallelize such loops by introducing synchronization barriers before and 
after their execution. Example 11.49 illustrates the point. 
for ( i  = 1; i < n; i++) 
f o r  ( j  = 0; j < n; j++) 
x[i,j] = f (XCi,jl + X[i-l,jl); 
for ( i  = 0; i < n; i++) 
for ( j  = I ;  j < n; j++) 
XCi, jl = g(X[i,jl + xCi,j-11); 
Figure 11.38: Two sequential loop nests 
Example 11.49 : 
In Fig. 11.38 is a program representative of an AD1 (Alter- 
nating Direction Implicit) integration algorithm. There is no synchronization- 
free parallelism. Dependences in the first loop nest require that each processor 
works on a column of array X; however, dependences in the second loop nest 
require that each processor works on a row of array X. For there to be no com- 
munication, the entire array has to reside on the same processor, hence there 
854 CHAPTER 21. OPTIMIZING FOR PARALLELISM AND LOCALITY 
is no parallelism. We observe, however, that both loops are independently 
parallelizable. 
One way to parallelize the code is to have different processors work on 
different columns of the array in the first loop, synchronize and wait for all 
processors to finish, and then operate on the individual rows. In this way, 
all the computation in the algorithm can be parallelized with the introduction 
of just one synchronization operation. However, we note that while only one 
synchronization is performed, this parallelization requires almost all the data 
in matrix X to be transferred between processors. It is possible to reduce the 
amount of communication by introducing more synchronizations, which we shall 
discuss in Section 11.9.9. 
CI 
It may appear that this approach is applicable only to programs consisting 
of a sequence of loop nests. However, we can create additional opportunities 
for the optimization through code transforms. We can apply loop fission to 
decompose loops in the original program into several smaller loops, which can 
then be parallelized individually by separating them with barriers. We illustrate 
this technique with Example 11.50. 
Example 11.50 : 
Consider the following loop: 
f o r  (i=l; 
i<=n; i + + )  ( 
X [ i ] = Y C i l  
+ Z C i l ;  
/* (sl) */ 
W[A[i]] 
= X C i ] ;  
/* ( ~ 2 )  
*/ 
3 
Without knowledge of the values in array A, we must assume that the access 
in statement s2 may write to any of the elements of W. Thus, the instances of 
s2 must be executed sequentially in the order they are executed in the original 
program. 
There is no synchronization-free parallelism, and Algorithm 11.43 will sim- 
ply assign all the computation to the same processor. However, at the least, 
instances of statement sl can be executed in parallel. We can parallelize part 
of this code by having different processors perform difference instances of state- 
ment s ~ .  
Then, in a separate sequential loop, one processor, say numbered 0, 
executes s 2 ,  as in the SPMD code shown in Fig. 11.39. 
11.8.2 
Program-Dependence Graphs 
To find all the parallelism made possible by a constant number of synchroniza- 
tions, we can apply fission to the original program greedily. Break up loops 
into as many separate loops as possible, and then parallelize each loop indepen- 
dently. 
To expose all the opportunities for loop fission, we use the abstraction of a 
program-dependence graph (PDG). A program dependence graph of a program 
11.8. SYNCHRONIZATION BETWEEN PARALLEL LOOPS 
XCpl =Y[pl + ZCpl; 
/* ( s l )  */ 
/* synchronization b a r r i e r  */ 
i f  (p == 0) 
f o r  ( i = l ;  i<=n; i++) 
W 
[A [i] 
1 = X 
[il ; /* (s2) */ 
Figure 11.39: SPMD code for the loop in Example 11.50, 
with p being a variable 
holding the processor ID 
is a graph whose nodes are the assignment statements of the program and whose 
edges capture the data dependences, and the directions of the data dependence, 
between statements. An edge from statement sl to statement s 2  exists whenever 
some dynamic instance of sl shares a data dependence with a Eater dynamic 
instance of s2. 
To construct the PDG for a program, we first find the data dependences 
between every pair of (not necessarily distinct) static accesses in every pair 
of (not necessarily distinct) statements. Suppose we determine that there is 
a dependence between access Fl in statement sl and access .F2 in statement 
s 2 .  Recall that an instance of a statement is specified by an index vector 
i = [il,i2,. 
. 
. ,
i
,
]
 
where ik is the loop index of the kth outermost loop in 
which the statement is embedded. 
1. If there exists a data-dependent pair of instances, il of sl and i2 of s 2 ,  
and il is executed before i2 in the original program, written il -
i
s
,
,
,
 i2, 
then there is an edge from sl to sa. 
2. Similarly, if there exists a data-dependent pair of instances, il of sl and 
i2 of s 2 ,  and i2 -
i
s
,
,
,
 il, then there is an edge from s 2  to sl. 
Note that it is possible for a data dependence between two statements sl and 
s2 to generate both an edge from sl to s
2
 and an edge from s 2  back to sl. 
In the special case where statements sl and s2 are not distinct, il -
i
s
,
,
,
 i2 if 
and only if il -i i2 (il is lexicographically less than iz). In the general case, sl 
and s 2  may be different statements, possibly belonging to different loop nests. 
Example 11.51 : 
For the program of Example 11.50, 
there are no dependences 
among the instances of statement sl 
. However, the ith instance of statement 
s2 must follow the ith instance of statement sl. Worse, since the reference 
W[A[i]] may write any element of array W, the ith instance of sz depends on 
all previous instances of s 2 .  That is, statement s 2  depends on itself. The PDG 
for the program of Example 11.50 is shown in Fig. 11.40. Note that there is 
one cycle in the graph, containing s 2  only. 
The program-dependence graph makes it easy to determine if we can split 
statements in a loop. Statements connected in a cycle in a PDG cannot be 
856 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Figure 11.40: Program-dependence graph for the program of Example 11.50 
split. If sl -+ s2 is a dependence between two statements in a cycle, then 
some instance of sl must execute before some instance of s2, and vice versa. 
Note that this mutual dependence occurs only if sl and sa are embedded in 
some common loop. Because of the mutual dependence, we cannot execute all 
instances of one statement before the other, and therefore loop fission is not 
allowed. On the other hand, if the dependence sl -+ s2 is unidirectional, we 
can split up the loop and execute all the instances of sl first, then those of s2. 
f o r  ( i  = 0; i < n; i++) { 
Z [i] 
= Z [il / W Cil ; 
/* (s1) */ 
f o r  ( j  = i ;  j < n ;  j++) { 
x [ i , j ]  = YCi,jl*YCi,jl; 
/* ( ~ 2 )  
*/ 
ZCjl = ZCjl + XCi,jl; 
/* (s3) */ 
3 
1 
(a) A program. 
(b) Its dependence graph. 
Figure 11.41: Program and dependence graph for Example 11.52. 
Example 11.52 : Figure 11.41 
(b) shows the program-dependence graph for 
the program of Fig. 11.41(a). Statements sl and ss belong to a cycle in the 
graph and therefore cannot be placed in separate loops. We can, however, split 
statement s2 out and execute all its instances before executing the rest of the 
computation, as in Fig. 11.42. The first loop is parallelizable, but the second 
is not. We can parallelize the first loop by placing barriers before and after its 
parallel execution. 
11.8. SYNCHRONIZATION BETWEEN PARALLEL LOOPS 
f o r  ( i  = 0; i < n; i++) 
f o r  ( j  = i ;  j < n; j++) 
X[i,j] =YCi,jl*YCi,jl; 
/* ( ~ 2 )  
*/ 
f o r  ( i  = 0; i < n; i++) C 
Z [i] = Z [il / W Cil ; 
/* ( s l )  */ 
f o r  ( j  = i ;  j < n; j++) 
ZCjl = ZCjl + XCi,jl; 
/* (s3) */ 
3 
Figure 11.42: Grouping strongly connected components of a loop nest 
11.8.3 Hierarchical Time 
While the relation +
,
,
,
,
 
can be very hard to compute in general, there is a 
family of programs to which the optimizations of this section are commonly 
applied, and for which there is a straightforward way to compute dependencies. 
Assume that the program is block structured, consisting of loops and simple 
arithmetic operations and no other control constructs. A statement in the 
program is either an assignment statement, a sequence of statements, or a loop 
construct whose body is a statement. The control structure thus represents a 
hierarchy. At the top of the hierarchy is the node representing the statement of 
the whole program. An assignment statement is a leaf node. If a statement is 
a sequence, then its children are the statements within the sequence, laid out 
from left to right according to their lexical order. If a statement is a loop, then 
its children are the components of the loop body, which is typically a sequence 
of one or more statements. 
so; 
L l :  f o r  ( i  = 0; . . 
.) { 
s1; 
L2: f o r  ( j  = 0; . 
. 
.) ( 
s2; 
s 3  
; 
1 
L3: f o r  (k = 0; ... ) 
s4 
; 
s 5  
; 
1 
Figure 11.43: A hierarchically structured program 
Example 11.53 : The hierarchical structure of the program in Fig. 11.43 is 
shown in Fig. 11.44. The hierarchical nature of the execution sequence is high- 
858 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
lighted in Fig. 11.45. The single instance of so precedes all other operations, 
because it is the first statement executed. Next, we execute all instructions 
from the first iteration of the outer loop before those in the second iteration 
and so forth. For all dynamic instances whose loop index i has value 0, the 
statements sl 
, 
L2, 
L3, and s5 are executed in lexical order. We can repeat the 
same argument to generate the rest of the execution order. 
Figure 11.44: Hierarchical structure of the program in Example 11.53. 
Figure 11.45: Execution order of the program in Example 11.53. 
We can resolve the ordering of two instances from two different statements in 
a hierarchical manner. If the statements share common loops, we compare the 
values of their common loop indexes, starting with the outermost loop. As soon 
as we find a difference between their index values, the difference determines the 
ordering. Only if the index values for the outer loops are the same do we need 
to compare the indexes of the next inner loop. This process is analogous to 
how we would compare time expressed in terms of hours, minutes and seconds. 
To compare two times, we first compare the hours, and only if they refer to 
11.8. SYNCHRONIZATION BETWEEN PARALLEL LOOPS 
the same hour would we compare the minutes and so forth. If the index values 
are the same for all common loops, then we resolve the order based on their 
relative lexical placement. Thus, the execution order for the simple nested-loop 
programs we have been discussing is often referred to as "hierarchical time." 
Let sl be a statement nested in a dl-deep loop, and s2 in a d2-deep loop, 
sharing d common (outer) loops; note d < dl and d 5 d2 certainly. Suppose 
i = [il , 
i2, 
. . . , 
idl] is an instance of sl and j = [jl 
, 
j2, 
. . 
. , 
jd2] 
is an instance of 
s2. 
i -is,,, j if and only if either 
2. [il, 
iz, 
. . 
. ,id] 
= [jl, 
j2,. 
. . , 
jd], 
and sl appears lexically before s2. 
The predicate [il, 
i2,. 
. . ,id] 
4 [jl, 
j2,. 
. . , 
jd] 
can be written as a disjunction 
of linear inequalities: 
A PDG edge from sl to s2 exists as long as the data-dependence condition 
and one of the disjunctive clauses can be made true simultaneously. Thus, 
we may need to solve up to d or d + 1 
linear integer programs, depending on 
whether sl appears lexically before s2, to determine the existence of one edge. 
11.8.4 The Parallelization Algorithm 
We now present a simple algorithm that first splits up the computation into as 
many different loops as possible, then parallelizes them independently. 
Algorithm 11.54 : Maximize the degree of parallelism allowed by O(1) syn- 
chronizations. 
INPUT: A program with array accesses. 
OUTPUT: SPMD code with a constant number of synchronization barriers. 
METHOD: 
1. Construct the program-dependence graph and partition the statements 
into strongly connected components (SCC7s). 
Recall from Section 10.5.8 
that a strongly connected component is a maximal subgraph of the orig- 
inal whose every node in the subgraph can reach every other node. 
2. Transform the code to execute SCC's in a topological order by applying 
fission if necessary. 
3. Apply Algorithm 11.43 to each SCC to find all of its synchronization-free 
parallelism. Barriers are inserted before and after each parallelized SCC. 
860 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
While Algorithm 11.54 finds all degrees of parallelism with O(1) 
synchro- 
nizations, it has a number of weaknesses. First, it may introduce unnecessary 
synchronizations. As a matter of fact, if we apply this algorithm to a program 
that can be parallelized without synchronization, the algorithm will parallelize 
each statement independently and introduce a synchronization barrier between 
the parallel loops executing each statement. Second, while there may only be a 
constant number of synchronizations, the parallelization scheme may transfer 
a lot of data among processors with each synchronization. In some cases, the 
cost of communication makes the parallelism too expensive, and we may even 
be better off executing the program sequentially on a uniprocessor. In the fol- 
lowing sections, we shall next take up ways to increase data locality, and thus 
reduce the amount of communication. 
11.8.5 Exercises for Section 11.8 
Exercise 11.8.1 
: 
Apply Algorithm 11.54 to the code of Fig. 11.46. 
f o r  (i=O; i(100; i++) 
A [ i ]  
= A [ i l  + X[il; 
/* ( s l )  */ 
f o r  (i=O; i(100; i++) 
f o r  (j=O; j(100; j++) 
B[i,j] = Y[i,jl + A [ i l  + ACjl; /* (s2) */ 
Figure 11.46: Code for Exercise 11.8.1 
Exercise 11 
3 . 2  
: 
Apply Algorithm 11.54 to the code of Fig. 11.47. 
f o r  (i=O; 
i<100; i++) 
A [ i ]  
= A [ i ]  + ~ [ i ]  
; /* ( s l )  */ 
f o r  (i=O; i(100; i++) ( 
B [i] 
= B [i] 
+ A [ i ]  ; /* (s2) */ 
f o r  (j=O; j(100; j++) 
CCj] = Y C j l  + BCjl; /* ( ~ 3 )  
*/ 
3 
Figure 11.47: Code for Exercise 11.8.2 
Exercise 11.8.3 : 
Apply Algorithm 11.54 to the code of Fig. 11.48. 
f o r  (i=O; i(100; i++) 
A [ i ]  
= A [ i ]  
+ X [ i l  ; /* ( ~ 1 )  
*/ 
f o r  (i=O; i<100; i++) C 
f o r  (j=O; j<lOO; j++) 
B[j] = A [ i ]  + YCjl; /* (s2) */ 
C[i] 
= B  
[i] 
+ Z [i] 
; /* (s3) */ 
f o r  (j=O; j(100; j++) 
D[i,j] = A [ i ]  + B[jl; /* (s4) */ 
Figure 11.48: Code for Exercise 11.8.3 
11.9 Pipelining 
In pipelining, a task is decomposed into a number of stages to be performed on 
different processors. For example, a task computed using a loop of n iterations 
can be structured as a pipeline of n stages. Each stage is assigned to a different 
processor; when one processor is finished with its stage, the results are passed 
as input to the next processor in the pipeline. 
In the following, we start by explaining the concept of pipelining in more 
detail. We then show a real-life numerical algorithm, known as successive over- 
relaxation, to illustrate the conditions under which pipelining can be applied, in 
Section 11.9.2. We then formally define the constraints that need to be solved 
in Section 11.9.6, and describe an algorithm for solving them in Section 11.9.7. 
Programs that have multiple independent solutions to the time-partition con- 
straints are known as having outermost fully permutable loops; such loops can 
be pipelined easily, as discussed in Section 11.9.8. 
11.9.1 What is Pipelining? 
Our initial attempts to parallelize loops partitioned the iterations of a loop nest 
so that two iterations that shared data were assigned to the same processor. 
Pipelining allows processors to share data, but generally does so only in a 
"local," way, with data passed from one processor to another that is adjacent 
in the processor space. Here is a simple example. 
Example 11.55 : 
Consider the loop: 
f o r  ( i  = 1 ;  i <= m; i++) 
f o r  ( j  = 1 ;  j  <= n; j++) 
X C i l  = X C i ]  + Y[i,j]; 
This code sums up the ith row of Y and adds it to the ith element of X. The 
inner loop, corresponding to the summation, must be performed sequentially 
862 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Figure 11.49: Pipelined execution of Example 11.55 with m = 
4 and n = 
3. 
Time 
1 
2 
3 
4 
5 
6 
because of the data dependence;6 however, the different summation tasks are 
independent. We can parallelize this code by having each processor perform 
a separate summation. Processor i accesses row i of Y and updates the ith 
element of X. 
Alternatively, we can structure the processors to execute the summation in a 
pipeline, and derive parallelism by overlapping the execution of the summations, 
as shown in Fig. 11.49. More specifically, each iteration of the inner loop can 
be treated as a stage of a pipeline: stage j takes an element of X generated 
in the previous stage, adds to it an element of Y, and passes the result to the 
next stage. Notice that in this case, each processor accesses a column, instead 
of a row, of Y. If Y is stored in column-major form, there is a gain in locality 
by partitioning according to columns, rather than by rows. 
We can initiate a new task as soon as the first processor is done with the first 
stage of the previous task. At the beginning, the pipeline is empty and only the 
first processor is executing the first stage. After it completes, the results are 
passed to the second processor, while the first processor starts on the second 
task, and so on. In this way, the pipeline gradually fills until all the processors 
are busy. When the first processor finishes with the last task, the pipeline starts 
to drain, with more and more processors becoming idle until the last processor 
finishes the last task. In the steady state, n tasks can be executed concurrently 
in a pipeline of n processors. 
1
7
 
It is interesting to contrast pipelining with simple parallelism, where differ- 
ent processors execute different tasks: 
Processors 
Pipelining can only be applied to nests of depth at least two. We can 
treat each iteration of the outer loop as a task and the iterations in the 
inner loop as stages of that task. 
1 
X[l]+=Y[l,l] 
X[2]+=Y[2,1] 
X[3]+=Y[3,1] 
X[4]+=Y[4,1] 
Tasks executed on a pipeline may share dependences. Information per- 
taining to the same stage of each task is held on the same processor; thus 
results generated by the ith stage of a task can be used by the ith stage 
6~emember 
that we do not take advantage of the assumed commutativity and associativity 
of addition. 
2 
X[l]+=Y[1,2] 
X[2]+=Y[2,2] 
X[3]+=Y[3,2] 
~[4]+=Y[4,2] 
3 
X[l]+=Y[1,3] 
X[2]+=Y[2,3] 
X[3]+=Y[3,3] 
X[4]+=Y[4,3] 
11.9. PIPELINING 
863 
of subsequent tasks with no communication cost. Similarly, each input 
data element used by a single stage of different tasks needs to reside only 
on one processor, as illustrated by Example 11.55. 
If the tasks are independent, then simple parallelization has better proces- 
sor utilization because processors can execute all at once without having 
to pay for the overhead of filling and draining the pipeline. However, 
as shown in Example 11.55, the pattern of data accesses in a pipelined 
scheme is different from that of simple parallelization. Pipelining may be 
preferable if it reduces communication. 
11 
3 . 2  Successive Over-Relaxation (SOR) 
: 
An Example 
Successive over-relaxation (SOR) is a technique for accelerating the conver- 
gence of relaxation methods for solving sets of simultaneous linear equations. 
A relatively simple template illustrating its data-access pattern is shown in 
Fig. 11.50(a). Here, the new value of an element in the array depends on the 
values of elements in its neighborhood. Such an operation is performed repeat- 
edly, until some convergence criterion is met. 
f o r  (i = 0; i <= m; i++) 
f o r  ( j  = 0; j <= n; j++) 
X [ j + l l  = 1/3 * ( X [ j l  + X [ j + l ]  
+ X [ j + 2 ] )  
(a) Original source. 
(b) Data dependences in the code. 
Figure 11.50: An example of successive over-relaxation (SOR) 
Sbown in Fig. 11.50(b) is a picture of the key data dependences. We do not 
show dependences that can be inferred by the dependences already included in 
the figure. For example, iteration [i, 
j] depends on iterations [i, 
j - 
1
1
,
 [i, 
j - 
2
1
 
and so om. It is clear from the dependences that there is no synchronization- 
free parallelism. Since the longest chain of dependences consists of O(m + 
n) 
edges, by introducing synchronization, we should be able to find one degree of 
parallelism and execute the O(mn) 
operations in O(m + 
n) unit time. 
864 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
In particular, we observe that iterations that lie along the 150" diagonals7 in 
Fig. 11.50(b) do not share any dependences. They only depend on the iterations 
that lie along diagonals closer to the origin. Therefore we can parallelize this 
code by executing iterations on each diagonal in order, starting at the origin 
and proceeding outwards. We refer to the iterations along each diagonal as a 
wavefront, and such a parallelization scheme as wavefronting. 
11.9.3 Fully Permutable Loops 
We first introduce the notion of full permutability, a concept useful for pipelining 
and other optimizations. Loops are fully permutable if they can be permuted 
arbitrarily without changing the semantics of the original program. Once loops 
are put in a fully permutable form, we can easily pipeline the code and apply 
transformations such as blocking to improve data locality. 
  he SOR code, as it it written in Fig. 11.50(a), is not fully permutable. 
As shown in Section 11.7.8, permuting two loops means that iterations in the 
original iteration space are executed column by column instead of row by row. 
For instance, the original computation in iteration [2,3] would execute before 
that of [1,4], 
violating the dependences shown in Fig. 11.50(b). 
We can, however, transform the code to make it fully permutable. Applying 
the affine transform 
to the code yields the code shown in Fig. 11.51(a). This transformed code 
is fully permutable, and its permuted version is shown in Fig. 11.51(c). We 
also show the iteration space and data dependences of these two programs in 
Fig. 11.51(b) and (d), respectively. From the figure, we can easily see that this 
ordering preserves the relative ordering between every data-dependent pair of 
accesses. 
When we permute loops, we change the set of operations executed in each 
iteration of the outermost loop drastically. The fact that we have this degree 
of freedom in scheduling means that there is a lot of slack in the ordering 
of operations in the program. Slack in scheduling means opportunities for 
parallelization. We show later in this section that if a loop has k outermost 
fully permutable loops, by introducing just 0(n) synchronizations, we can get 
O(k - 
1) degrees of parallelism (n is the number of iterations in a loop). 
11.9.4 Pipelining Fully Permutable Loops 
A loop with k outermost fully permutable loops can be structured as a pipeline 
with O(k - 
1) 
dimensions. In the SOR example, k = 
2, so we can structure the 
processors as a linear pipeline. 
7~.e., 
the sequences of points formed by repeatedly moving down 1 
and right 2. 
f o r  ( i  = 0; i <= m; i++) 
f o r  ( j  = i ;  j  <= i+n; j++) 
X[j-i+ll = 1/3 * (X[j-i] + X[j-i+l] + X[j-i+2]) 
(a) The code in Fig. 11.50 transformed by [: 
:
]
a
 
(b) Data dependences of the code in (a). 
f o r  ( j  = 0 ;  j  <= m+n; j++) 
f o r  ( i  = max(0,j); i <= rnin(rn,j), i++) 
XCj-i+1] = 1/3 * (XCj-i] + X[j-i+1] + x[j-i+2]) 
(c) A permutation of the loops in (a). 
I 
(d) Data dependences of the code in (b). 
Figure 11.51: Fully permutable version of the code Fig. 11.50 
866 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
We can pipeline the SOR code in two different ways, shown in Fig. 11.52(a) 
and Fig. 11.52(b), corresponding to the two possible permutations shown in 
Fig. 11.51 
(a) and (c), respectively. In each case, every column of the iteration 
space constitutes a task, and every row constitutes a stage. We assign stage i 
to processor i, thus each processor executes the inner loop of the code. Ignoring 
boundary conditions, a processor can execute iteration i only after processor 
p - 
1 
has executed iteration i - 
1. 
/* 0 <= p <= m */ 
f o r  ( j  = p; j <= p+n; j++) { 
i f  (p > 0) wait (p-I); 
X[j-p+ll = 1/3 * (X[j-p] + X[j-p+l] + X[j-p+2]); 
i f  (p < min (m,j)) signal (p+l); 
(a) Processors assigned to rows. 
/* 0 <= p <= m+n */ 
f o r  ( i  = max(0,p); i <= min(m,p); i++) ( 
i f  (p > max(0,i)) wait (p-I); 
X[p-i+l] = 1/3 * (X[p-i] + X[p-i+l] + ~[p-i+21) 
i f  (p < m+n) & (p > i )  signal (p+l); 
> 
(b) Processors assigned to columns. 
Figure 11.52: Two pipelining implementations of the code from Fig. 11.51 
Suppose every processor takes exactly the same amount of time to execute 
an iteration and synchronization happens instantaneously. Both these pipelined 
schemes would execute the same iterations in parallel; the only difference is 
that they have different processor assignments. All the iterations executed in 
parallel lie along the 135" diagonals in the iteration space in Fig. 11.51(b), 
which corresponds to the 150" diagonals in the iteration space of the original 
code; see Fig. 11.50(b). 
However, in practice, processors with caches do not always execute the same 
code in the same amount of time, and the time for synchronization also varies. 
Unlike the use of synchronization barriers which forces all processors to operate 
in lockstep, pipelining requires processors to synchronize and communicate with 
at most two other processors. Thus, pipeliriing has relaxed wavefronts, allowing 
some processors to surge ahead while others lag momentarily. This flexibility 
reduces the time processors spend waiting for other processors and improves 
parallel performance. 
The two pipelining schemes shown above are but two of the many ways 
in which the computation can be pipelined. As we said, once a loop is fully 
permutable, we have a lot of freedom in how we wish to parallelize the code. 
The first pipeline scheme maps iteration [i, 
j] to processor i; the second maps 
iteration [i, 
j ]  to processor j. We can create alternative pipelines by mapping 
iteration [i, 
j] to processor coi + 
cl j, provided co and cl are positive constants. 
Such a scheme would create pipelines with relaxed wavefronts between 90" and 
180°, both exclusive. 
11.9.5 General Theory 
The example just completed illustrates the following general theory underlying 
pipelining: if we can come up with at least two different outermost loops for a 
loop nest and satisfy all the dependences, then we can pipeline the computation. 
A loop with k outermost fully permutable loops has k - 
1 
degrees of pipelined 
parallelism. 
Loops that cannot be pipelined do not have alternative outermost loops. 
Example 11.56 shows one such instance. To honor all the dependences, each 
iteration in the outermost loop must execute precisely the computation found 
in the original code. However, such code may still contain parallelism in the 
inner loops, which can be exploited by introducing at least n synchronizations, 
where n is the number of iterations in the outermost loop. 
f o r  ( i  = 0; i < 100; i++) 1 
f o r  ( j  = 0; j < 100; j++) 
X[jl = XCjl + YCi,jl; 
/* ( s l )  */ 
Z [ i l  = X[A[i]]; 
/* (s2) */ 
Figure 11.53: A sequential outer loop (a) and its PDG (b) 
Example 11.56 : 
Figure 11.53 
is a more complex version of the problem we saw 
in Example 11.50. As shown in the program dependence graph in Fig. 11.53(b), 
statements sl and s2 belong to the same strongly connected component. Be- 
cause we do not know the contents of matrix A, we must assume that the 
access in statement sa may read from any of the elements of X. There is a true 
dependence from statement sl to statement s2 and an antidependence from 
868 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
statement s2 to statement sl. There is no opportunity for pipelining either, 
because all operations belonging to iteration i in the outer loop must precede 
those in iteration i + 
1. To find more parallelism, we repeat the parallelization 
process on the inner loop. The iterations in the second loop can be parallelized 
without synchronization. Thus, 200 barriers are needed, with one before and 
one after each execution of the inner loop. 
11.9.6 
Time-Partition Constraints 
We now focus on the problem of finding pipelined parallelism. Our goal is to 
turn a computation into a set of pipelinable tasks. To find pipelined parallelism, 
we do not solve directly for what is to be executed on each processor, like we 
did with loop parallelization. Instead, we ask the following fundamental ques- 
tion: What are all the possible execution sequences that honor the original data 
dependences in the loop? Obviously the original execution sequence satisfies all 
the data dependences. The question is if there are affine transformations that 
can create an alternative schedule, where iterations of the outermost loop exe- 
cute a different set of operations from the original, and yet all the dependences 
are satisfied. If we can find such transforms, we can pipeline the loop. The 
key point is that if there is freedom in scheduling operations, there is paral- 
lelism; details of how we derive pipelined parallelism from such transforms will 
be explained later. 
To find acceptable reorderings of the outer loop, we wish to find one- 
dimensional affine transforms, one for each statement, that map the original 
loop index values to an iteration number in the outermost loop. The trans- 
forms are legal if the assignment can satisfy all the data dependences in the 
program. The "time-partition constraints," shown below, simply say that if 
one operation is dependent upon the other, then the first must be assigned 
an iteration in the outermost loop no earlier than that of the second. If they 
are assigned in the same iteration, then it is understood that the first will be 
executed after than the second within the iteration. 
An affine-partition mapping of a program is a legal-time partition if and only 
if for every two (not necessarily distinct) accesses sharing a dependence, say 
in statement sl 
, 
which is nested in dl loops, and 
in statement s2 nested in d2 loops, the one-dimensional partition mappings 
(Cl 
, 
cl ) and (C2, 
c2) for statements sl and s2, respectively, satisfy the time- 
partition constraints: 
For all il in ,Zdl and i2 in z d 2  such that 
11.9. PIPELINING 
b) Blil + 
bl 2 
0, 
c) B2i2 + 
b2 2 
0, and 
it is the case that Clil + 
cl 5 C2i2 + 
c2. 
This constraint, illustrated in Fig. 11.54, looks remarkably similar to the 
space-partition constraints. It is a relaxation of the space-partition constraints, 
in that if two iterations refer to the same location, they do not necessarily have 
to be mapped to the same partition; we only require that the original relative 
execution order between the two iterations is preserved. That is, the constraints 
here have 5 where the space-partition constraints have =. 
0 
u u u u o  
Array 
1
7
 
q 
Time steps 
Figure 11.54: Time-Partition Constraints 
We know that there exists at least one solution to the time-partition con- 
straints. We can map operations in each iteration of the outermost loop back 
to the same iteration, and all the data dependences will be satisfied. This so- 
lution is the only solution to the time-partition constraints for programs that 
cannot be pipelined. On the other hand, if we can find several independent 
solutions to time-partition constraints, the program can be pipelined. Each 
independent solution corresponds to a loop in the outermost fully permutable 
nest. As you can expect, there is only one independent solution to the timing 
constraints extracted from the program in Example 11.56, where there is no 
pipelined parallelism, and that there are two independent solutions to the SOR 
code example. 
Example 11.57 : Let us consider Example 11.56, and in particular the data 
dependences of references to array X in statements sl and s 2 .  Because the 
870 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
access is not affine in statement s2, we approximate the access by modeling 
matrix X simply as a scalar variable in dependence analysis involving statement 
s2. Let (i, 
j) be the index value of a dynamic instance of sl and let it be the 
index value of a dynamic instance of s2. Let the computation mappings of 
statements sl, and s2 be ([GI, 
CI~], 
cl) and ([Czl], 
cz), respectively. 
Let us first consider the time-partition constraints imposed by dependences 
from statemeit sl to s2. Thus, i 5 it, the transformed (i,j)th iteration of sl 
must be no later than the transformed i'th iteration of s2; that is, 
Expanding, we get 
Since j can be arbitrarily large, independent of i and it, 
it must be that C12 = 
0. 
Thus, one possible solution to the constraints is 
Cll = C21 = 1 and C12 = 
cl = 
c2 = 
0. 
Similar arguments about the data dependence from sz to sl and s2 back to 
itself will yield a similar answer. In this particular solution, the ith iteration 
of the outer loop, which consists of the instance i of s2 and all instances (i, 
j) 
of sl, are all assigned to timestep i. Other legal choices of Cll, Czl, cl, and c2 
yield similar assignments, although there might be timesteps at which nothing 
happens. That is, all ways to schedule the outer loop require the iterations 
to execute in the same order as in the original code. This statement holds 
whether all 100 iterations are executed on the same processor, on 100 different 
processors, or anything in between. 
Example 11.58 : 
In the SOR code shown in Fig. 11.50(a), 
the write reference 
X [ j  + I] shares a dependence with itself and with the three read references in 
the code. We are seeking computation mapping ([Cl, 
C2], 
C )  for the assignment 
statement such that 
if there is a dependence from (i, 
j) to (it, 
j'). By definition, (i, 
j) 4 (it, 
j'); that 
is, either i < 
it or (i = 
it 
A j < 
j')). 
Let us consider three of the pairs of data dependences: 
1. True dependence from write access X[j 
+ 
l] 
to read access X [ j  
+ 
21. Since 
the instances must access the same location, j + 1 
= 
j' + 
2 or j = 
j' + 1. 
Substituting j = 
j' + 1 
into the timing constraints, we get 
Since j = j' + 1, j > j', the precedence constraints reduce to i < it. 
Therefore, 
2. Antidependence from read access X [ j  
+ 
2
1
 to write access X [j 
+ 
11. Here, 
j + 
2 = j' + 1, or j = j' - 
1. Substituting j = j' - 
1 
into the timing 
constraints, we get 
When i = 
i', we get 
When i < it, 
since C2 2 
0, we get 
3. Output dependence from write access X [ j  
+ 
I] back to itself. Here j = 
j'. 
The timing constraints reduce to 
Since only i < i' is relevant, we again get 
The rest of the dependences do not yield any new constraints. In total, 
there are three constraints: 
Here are two independent solutions to these constraints: 
The first solution preserves the execution order of the iterations in the outer- 
most loop. Both the original SOR code in Fig. 11.50(a) and the transformed 
code shown in Fig. 11.51(a) are examples of such an arrangement. The second 
solution places iterations lying along the 135" diagonals in the same outer loop. 
The code shown in Fig. 11.51 
(b) is an example of a code with that outermost 
loop composition. 
Notice that there are many other possible pairs of independent solutions. 
For example, 
would also be an independent solutions to the same constraints. We choose the 
simplest vectors to simplify code transformation. 
872 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
11.9.7 Solving Time-Partition Constraints by Farkas' 
Lemma 
Since time-partition constraints are similar to space-partition constraints, can 
we use a similar algorithm to solve them? Unfortunately, the slight difference 
between the two problems translates into a big technical difference between the 
two solution methods. Algorithm 11.43 simply solves for Cl 
, 
cl , 
C2, and c2, 
such that for all il in Zdl and i2 in Zd"f 
then 
The linear inequalities due to the loop bounds are only used in determining if 
two references share a data dependence, and are not used otherwise. 
To find solutions to the time-partition constraints, we cannot ignore the 
linear inequalities i 4 i'; ignoring them often would allow only the trivial so- 
lution of placing all iterations in the same partition. Thus, the algorithm to 
find solutions to the time-partition constraints must handle both equalities and 
inequalities. 
The general problem we wish to solve is: given a matrix A, find a vector 
c such that for all vectors x such that Ax 2 
0, it is the case that cTx 2 
0. 
In other words, we are seeking c such that the inner product of c and any 
coordinates in the polyhedron defined by the inequalities Ax 2 
0 always yields 
a nonnegative answer. 
This problem is addressed by Farkas' Lemma. Let A be an m x n matrix 
of reals, and let c be a real, nonzero n-vector. Farkas' lemma says that either 
the primal system of inequalities 
has a real-valued solution x, or the dual system 
has a real-valued solution y, but never both. 
The dual system can be handled by using Fourier-Motzkin elimination to 
project away the variables of y. For each c that has a solution in the dual 
system, the lemma guarantees that there are no solutions to the primal system. 
Put another way, we can prove the negation of the primal system, i.e., we can 
prove that cTx > 0 for all x such that Ax 2 
0, by finding a solution y to the 
dual system: 
= 
c and y > 0. 
Algorithm 11.59 : 
Finding a set of legal, maximally independent affine time- 
partition mappings for an outer sequential loop. 
About Farkas' Lemma 
The proof of the lemma can be found in many standard texts on linear 
programming. Farkas' Lemma, originally proved in 1901, is one of the 
theorems of the alternative. These theorems are all equivalent but, despite 
attempts over the years, a simple, intuitive proof for this lemma or any of 
its equivalents has not been found. 
INPUT: A loop nest with array accesses. 
OUTPUT: A maximal set of linearly independent time-partition mappings. 
METHOD: The following steps constitute the algorithm: 
1. Find all data-dependent pairs of accesses in a program. 
2. For each pair of data-dependent accesses, Fl = (Fl 
, 
fl , 
Bl , 
bl) in state- 
ment sl nested in dl loops and F2 
= (Fa, 
f2, 
B2, 
b2) 
in statement 5-2 nested 
in d2 loops, let (Cl 
, 
el) and (C2, 
c2) be the (unknown) time-partition 
mappings of statements sl and s 2 ,  respectively. Recall the time-partition 
constraints state that 
For all il in Zdl and i2 in zd2 
such that 
a) il 4
s
m
 
i2, 
b) Blil + 
bl 2 
0, 
c) B2i2 
+ 
b2 2 
0, and 
d) Flil + 
fl = 
F2i2 + 
f2, 
it is the case that Clil + 
cl 5 C2i2 
+ 
cz. 
Since il 4,,,, 
i 2  is a disjunctive union of a number of clauses, we can 
create a system of constraints for each clause and solve each of them 
separately, as follows: 
(a) Similarly to step (2a) in Algorithm 11.43, apply Gaussian elimination 
to the equations 
to reduce the vector 
to some vector of unknowns, x. 
874 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
(b) Let c be all the unknowns in the partition mappings. Express the 
linear inequality constraints due to the partition mappings as 
for some matrix D. 
(c) Express the precedence constraints on the loop index variables and 
the loop bounds as 
for some matrix A. 
(d) Apply Farkas' Lemma. Finding x to satisfy the two constraints above 
is equivalent to finding y  
such that 
=DTc 
and y 2  
0. 
Note that cTD here is cT in the statement of Farkas' Lemma, and 
we are using the negated form of the lemma. 
(e) In this form, apply Fourier-Motzkin elimination to project away the y 
variables, and express the constraints on the coefficients c as Ec > 0. 
(f) Let E'c' > 0 be the system without the constant terms. 
3. Find a maximal set of linearly independent solutions to E'c' 2 
0 using 
Algorithm B.l in Appendix B. The approach of that complex algorithm 
is to keep track of the current set of solutions for each of the statements, 
then incrementally look for more independent solutions by inserting con- 
straints that force the solution to be linearly independent for at least one 
statement. 
4. From each solution of c' found, derive one affine time-partition mapping. 
The constant terms are derived using Ec 2 0. 
Example 11.60 : 
The constraints for Example 11.57 can be written as 
Farkas' lemma says that these constraints are equivalent to 
Solving this system, we get 
Cll = Czl 2 
0 and C12 = 
ca - 
el = 
0. 
Notice that these constraints are satisfied by the particular solution we obtained 
in Example 11.57. 
11.9.8 Code Transformations 
If there exist k independent solutions to the time-partition constraints of a loop 
nest, then it is possible to transform the loop nest to have k outermost fully 
permutable loops, which can be transformed to create k- 1 degrees of pipelining, 
or to create k - 
1 
inner parallelizable loops. Furthermore, we can apply blocking 
to fully permutable loops to improve data locality of uniprocessors as well as 
reducing synchronization among processors in a parallel execution. 
Exploiting Fully Permutable Loops 
We can create a loop nest with k outermost fully permutable loops easily from 
k independent solutions to the time-partition constraints. We can do so by 
simply making the kth solution the kth row of the new transform. Once the 
affine transform is created, Algorithm 11.45 can be used to generate the code. 
Example 11.61 : 
The solutions found in Example 11.58 for our SOR example 
were 
Making the first solution the first row and the second solution the second row, 
we get the transform 
which yields the code in Fig. 11.51 
(a). 
Making the second solution the first row instead, we get the transform 
which yields the code in Fig. 11.51 
(c). 
876 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
It is easy to see that such transforms produce a legal sequential program. 
The first row partitions the entire iteration space according to the first solution. 
The timing constraints guarantee that such a decomposition does not violate 
any data dependences. Then, we partition the iterations in each of the outer- 
most loop according to the second solution. Again this must be legal because we 
are dealing with just subsets of the original iteration space. The same goes for 
the rest of the rows in the matrix. Since we can order the solutions arbitrarily, 
the loops are fully permutable. 
Exploiting Pipelining 
We can easily transform a loop with k outermost fully permutable loops into a 
code with k - 
1 
degrees of pipeline parallelism. 
Example 11.62 : Let us return to our SOR example. After the loops are 
transformed to be fully permutable, we know that iteration [il , 
i2] can be exe- 
cuted provided iterations [il, 
in 
- 
1
1
 and [il - 
1, 
i2] 
have been executed. We can 
guarantee this order in a pipeline as follows. We assign iteration il to processor 
pl. Each processor executes iterations in the inner loop in the original sequen- 
tial order, thus guaranteeing that iteration [il 
, 
i2] 
executes after [il 
, 
ia - 
I]. In 
addition, we require that processor p waits for the signal from processor p - 
1 
that it has executed iteration 
- 
1, 
i2] 
before it executes iteration b, 
i2]. 
This 
technique generates the pipelined code Fig. 11.52(a) and (b) from the fully 
permutable loops Fig. 1 
1.51 
(a) and (c) 
, respectively. 
In general, given k outermost fully permutable loops, the iteration with 
index values (il , 
. . 
. , 
ik) can be executed without violating data-dependence 
constraints, provided iterations 
have been executed. We can thus assign the partitions of the first k - 
1 
dimen- 
sions of the iteration space to O(nk-l) processors as follows. Each processor is 
responsible for one set of iterations whose indexes agree in the first k - 
1 
dimen- 
sions, and vary over all values of the kth index. Each processor executes the 
iterations in the kth loop sequentially. The processor corresponding to values 
[pl, 
p2,. . 
. , 
pr-l] for the first k - 
1 
loop indexes can execute iteration i in the 
kth loop as long as it receives a signal from processors 
[pl - 
1;p2,. 
. ,pk-l], . 
. 
1 blr 
- .  
- rpk-2,pk-1 - 
1
1
 
that they have executed their ith iteration in the kth loop. 
Wavefront 
ing 
It is also easy to generate k - 
1 
inner parallelizable loops from a loop with k 
outermost fully permutable loops. Although pipelining is preferable, we include 
this information here for completeness. 
11.9. PIPELINING 
We partition the computation of a loop with k outermost fully permutable 
loops using a new index variable it, 
where i' is defined to be some combination 
of all the indices in the k permutable loop nest. For example, it = 
il + . . 
. + 
.ik 
is one such combination. 
We create an outermost sequential loop that iterates through the it par- 
titions in increasing order; the computation nested within each partition is 
ordered as before. The first k - 
1 
loops within each partition are guaranteed 
to be parallelizable. Intuitively, if given a two-dimensional iteration space, this 
transform groups iterations along 135" diagonals as an execution of the outer- 
most loop. This strategy guarantees that iterations within each iteration of the 
outermost loop have no data dependence. 
Blocking 
A k-deep, fully permutable loop nest can be blocked in k-dimensions. Instead 
of assigning the iterations to processors based on the value of the outer or inner 
loop indexes, we can aggregate blocks of iterations into one unit. Blocking is 
useful for enhancing data locality as well as for minimizing the overhead of 
pipelining. 
Suppose we have a two-dimensional fully permutable loop nest, as in Fig. 
11.55(a), 
and we wish to break the computation into b x b blocks. The execution 
order of the blocked code is shown in Fig. 11.56, and the equivalent code is in 
Fig. 11.55(b). 
If we assign each block to one processor, then all the passing of data from one 
iteration to another that is within a block requires no interprocessor communi- 
cation. Alternatively, we can coarsen the granularity of pipelining by assigning 
a column of blocks to one processor. Notice that each processor synchronizes 
with its predecessors and successors only at block boundaries. Thus, another 
advantage of blocking is that programs only need to communicate data ac- 
cessed at the boundaries of the block with their neighbor blocks. Values that 
are interior to a block are managed by only one processor. 
Example 11.63 : 
We now use a real numerical algorithm - 
Cholesky decom- 
position - 
to illustrate how Algorithm 11.59 
handles single loop nests with only 
pipelining parallelism. The code, shown in Fig. 11.57, implements an O(n3) 
al- 
gorithm, operating on a 2-dimensional data array. The executed iteration space 
is a triangular pyramid, since j only iterates up to the value of the outer loop 
index i, and k only iterates to the value of j. The loop has four statements, all 
nested in different loops. 
Applying Algorithm 11.59 to this program finds three legitimate time di- 
mensions. It nests all the operations, some of which were originally nested in 
1- and 2-deep loop nests into a 3-dimensional, fully permutable loop nest. The 
code, together with the mappings, is shown in Fig. 11.58. 
The code-generation routine guards the execution of the operations with the 
original loop bounds to ensure that the new programs execute only operations 
878 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
f o r  (i=O; i<n; 
i++) 
f o r  (j=i; j<n; 
j++) ( 
<S> 
3 
(a) A simple loop nest. 
f o r  (ii = 0; ii<n; i+=b) 
f o r  (jj = 0; jjcn; jj+=b) 
f o r  (i = ii*b; i <= min(ii*b-1, n); i++) 
f o r  (j = ii*b; j <= min(jj*b-1, n); j++) ( 
<S> 
3 
(b) A blocked version of this loop nest. 
Figure 11.55: A 2-dimensional loop nest and its blocked version 
(a) Before. 
(b) After. 
Figure 11.56: Execution order before and after blocking a 2-deep loop nest. 
f o r  ( i  = 1 ;  i <= N; i++) ( 
f o r  ( j  = 1 ;  j <= i-1; j++) ( 
f o r  (k = 1 ;  k <= j-1; k++) 
x[i,jl = X [ i , j l  - X[i,k] * X[j,k]; 
X [ i , j l  = X C i , j l  / XCj,jl; 
3 
f o r  (
m
 = 1 ;  m <= i-1; m++) 
x[i,i] = X [ i , i l  - X [ i , m l  * X[i,m]; 
X [ i , i ]  
= s q r t ( X [ i , i ] ) ;  
3 
Figure 11.57: Cholesky decomposition 
f o r  ( i 2  = 1 ;  i 2  <= N; i2++) 
f o r  ( j 2  = 1 ;  j 2  <= i 2 ;  j2++) ( 
/* beginning of code f o r  processor ( i 2 , j 2 )  */ 
f o r  (k2 = 1 ;  k2 <= i 2 ;  k2++) ( 
// Mapping: i 2  = i, j 2  = j ,  k2 = k 
i f  ( j 2 < i 2  && k2<j2) 
~ [ i 2 ,  
j21 = X[i2,j21 - X[i2,k2] * X[j2,k2]; 
// Mapping: i 2  = i, j 2  = j ,  k2 = j 
i f  (j2==k2 && j2<i2) 
X [ i Z , j Z l  = X[i2,j21 / X[j2,j2]; 
// Mapping: i 2  = i, j2 = i, k2 = m 
i f  (i2==j2 && k2<i2) 
X[i2,i21 = X[i2,i2] - X[i2,k2] * X[i2,k2]; 
// Mapping: i 2  = i, j 2  = i, k2 = i 
i f  (i2==j2 && j2==k2) 
X [k2, k21 = s q r t  
(X [k2, k21) ; 
3 
/* ending of code f o r  processor ( i 2 , j 2 )  */ 
I 
Figure 11.58: Figure 11.57 written as a fully permutable loop nest 
880 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
that are in the original code. We can pipeline this code by mapping the 3- 
dimensional structure to a z-dimensional processor space. Iterations (i2, 
j2, k2) 
are assigned to the processor with ID (i2, 
j2). Each processor executes the 
innermost loop, the loop with the index k2. Before it executes the kth iteration, 
the processor waits for signals from the processors with ID'S (i2 - 
1, 
j2) and 
(i2, 
j2 - 
1). After it executes its iteration, it signals processors (i2 + 
1, 
j2) and 
(i2, 
j2 + 
1). 
11.9.9 Parallelism With Minimum Synchronization 
We have described three powerful parallelization algorithms in the last three 
sections: Algorithm 11.43 finds all parallelism requiring no synchronizations, 
Algorithm 11.54 finds all parallelism requiring only a constant number of syn- 
chronizations, and Algorithm 11.59 finds all the pipelinable parallelism requir- 
ing O(n) synchronizations where n is the number of iterations in the outermost 
loop. As a first approximation, our goal is to parallelize as much of the compu- 
tation as possible, while introducing as little synchronization as necessary. 
Algorithm 11.64, below, finds all the degrees of parallelism in a program, 
starting with the coarsest granularity of parallelism. In practice, to parallelize a 
code for a multiprocessor, we do not need to exploit all the levels of parallelism, 
just the outermost possible ones until all the computation is parallelized and 
all the processors are fully utilized. 
Algorithm 11.64 : Find all the degrees of parallelism in a program, with all 
the parallelism being as coarse-grained as possible. 
INPUT: A program to be parallelized. 
OUTPUT: A parallelized version of the same program. 
METHOD: Do the following: 
1. Find the maximum degree of parallelism requiring no synchronization: 
Apply Algorithm 11.43 to the program. 
2. Find the maximum degree of parallelism that requires O(1) synchroniza- 
tions: Apply Algorithm 11.54 to each of the space partitions found in 
step 1. (If no synchronization-free parallelism is found, the whole compu- 
tation is left in one partition). 
3. Find the maximum degree of parallelism that requires O(n) synchroniza- 
tions. Apply Algorithm 11.59 to each of the partitions found in step 2 
to find pipelined parallelism. Then apply Algorithm 11.54 to each of the 
partitions assigned to each processor, or the body of the sequential loop 
if no pipelining is found. 
4. Find the maximum degree of parallelism with successively greater degrees 
of synchronizations: Recursively apply Step 3 to computation belonging 
to each of the space partitions generated by the previous step. 
Example 11.65 : Let us now return to Example 11.56. No parallelism is 
found by Steps 1 and 2 of Algorithm 11.54; that is, we need more than a 
constant number of synchronizations to parallelize this code. In Step 3, applying 
Algorithm 11.59 determines that there is only one legal outer loop, which is the 
one in the original code of Fig. 11.53. So, the loop has no pipelined parallelism. 
In the second part of Step 3, we apply Algorithm 11.54 to parallelize the inner 
loop. We treat the code within a partition like a whole program, the only 
difference being that the partition number is treated like a symbolic constant. 
In this case the inner loop is found to be parallelizable and therefore the code 
can be parallelized with n synchronization barriers. 
Algorithm 11.64 finds all the parallelism in a program at each level of syn- 
chronization. The algorithm prefers parallelization schemes that have less syn- 
chronization, but less synchronization does not mean that the communication 
is minimized. Here we discuss two extensions to the algorithm to address its 
weaknesses. 
Considering Communication Cost 
Step 2 of Algorithm 11.64 parallelizes each strongly connected component in- 
dependently if no synchronization-free parallelism is found. However, it may 
be possible to parallelize a number of the components without synchronization 
and communication. One solution is to greedily find synchronization-free par- 
allelism among subsets of the program dependence graph that share the most 
data. 
If communication is necessary between strongly connected components, we 
note that some communication is more expensive than others. For example, 
the cost of transposing a matrix is significantly higher than just having to com- 
municate between neighboring processors. Suppose sl and s2 are statements in 
two separate strongly connected components accessing the same data in itera- 
tions il and i2, 
respectively. If we cannot find partition mappings (Cl, 
cl) and 
(C2, 
c2) for statements sl and s2, respectively, such that 
we instead try to satisfy the constraint 
where 6 is a small constant. 
Trading Communication for Synchronization 
Sometimes we would rather perform more synchronizations to minimize com- 
munication. Example 11.66 discusses one such example. Thus, if we cannot 
882 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
parallelize a code with just neighborhood communication among strongly con- 
nected components, we should attempt to pipeline the computation instead 
of parallelizing each component independently. As shown in Example 11.66, 
pipelining can be applied to a sequence of loops. 
Example 11.66 : For the AD1 integration algorithm in Example 11.49, we 
have shown that optimizing the first and second loop nests independently finds 
parallelism in each of the nests. However, such a scheme would require that 
the matrix be transposed between the loops, incurring O(n2) data traffic. If 
we use Algorithm 11.59 to find pipelined parallelism, we find that we can turn 
the entire program into a fully permutable loop nest, as in Fig. 11.59. We then 
can apply blocking to reduce the communication overhead. This scheme would 
incur O(n) synchronizations but would require much less communication. 
f o r  ( j  = 0; j  < n; j++) 
f o r  ( i  = 1 ;  i < n+l; i++) ( 
i f  ( i  < n) X[i,j] = f(X[i,j] + X[i-l,j]) 
i f  ( j  > 0) X C i - 1 , j l  = g(X[i-l,jl,XCi-1,j-11); 
> 
Figure 11.59: A fully permutable loop nest for the code of Example 11.49 
11.9.10 Exercises for Section 11.9 
Exercise 11.9.1 
: In Section 11.9.4, we discussed the possibility of using di- 
agonals other than the horizontal and vertical axes to pipeline the code of 
Fig. 11.51. Write code analogous to the loops of Fig. 11.52 for the diagonals: 
(a) 135" (b) 120". 
Exercise 11.9.2 
: Figure 11.55 
(b) can be simplified if b divides n evenly. 
Rewrite the code under that assumption. 
f o r  (i=O; i<lOO; i++) ( 
P[i,Ol = 1 ;  /* sl */ 
P[i,i] = I ;  /* s2 */ 
1 
f o r  (i=2; i<lOO; i++) 
f o r  ( j = l ;  j c i ;  j++) 
P[i,j] = P[i-l,j-I] 
+ P[i-1,jl; 
/* s 3  */ 
Figure 11.60: Computing Pascal's triangle 
11.9. PIPELINING 
883 
Exercise 11.9.3 
: 
In Fig. 11.60 is a program to compute the first 100 rows of 
Pascal's triangle. That is, P[i, 
j] will become the number of ways to choose j 
things out of i, for 0 5 j 5 i < 100. 
a) Rewrite the code as a single, fully permutable loop nest. 
b) Use 100 
processors in a pipeline to implement this code. Write the code for 
each processor p, in terms of p, and indicate the synchronization necessary. 
c) Rewrite the code using square blocks of 10 iterations on a side. Since the 
iterations form a triangle, there will be only 1 
+ 
2 + . 
. . 
+ 
10 = 
55 blocks. 
Show the code for a processor (pl , 
p2) assigned to the pl th block in the i 
direction and the p2 
th block in the j direction, in terms of pl and p2. 
f o r  (i=O; i<lOO; I++) ( 
A [ i ,  0,0] = Bl[i]; 
/* sl */ 
A[i,99,0] = B2[i]; 
/* s2 */ 
J 
f o r  ( j = l ;  j<99; j++) ( 
A[ 0 , j  
,01 = B3Ljl; /* s 3  */ 
A[99,j,01 = B4Cjl; /* s4 */ 
3 
f o r  (i=O; i<99; i++) 
f o r  (j=O; j<99; j++) 
f o r  (k=l; k(100; k++) 
A [ i ,  j ,k] = (4*A[i, j ,k-11 + A[i-1, j ,k-l] + 
A[i+l, j ,k-11 + A C i ,  j-1,k-11 
+ 
A[i,j+l,k-11; 
/* s5 */ 
Figure 11.61: Code for Exercise 11.9.4 
! 
Exercise 11.9.4 
: 
Repeat Exercise 11.9.2 for the code of Fig. 11.61. However, 
note that the iterations for this problem form a 3-dimensional cube of side 100. 
Thus, the blocks for part (c) should be 10 x 10 x 10, and there are 1000 of them. 
! Exercise 11.9.5 : Let us apply Algorithm 11.59 to a simple example of the 
time-partition constraints. In what follows, assume that the vector il is (il 
, 
jl), 
and vector i2 is (i2, 
jz); technically, both these vectors are transposed. The 
condition il +
,
,
,
,
 
i2 consists of the following disjunction: 
ii. il = 
i2 and jl < j2. 
The other equalities and inequalities are 
884 CHAPTER 21. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Finally, the time-partition inequality, with unknowns cl, dl, el, cz, d2, and ez, 
is 
a) Solve the time-partition constraints for case i - 
that is, where il < iz. In 
particular, eliminate as many of il, jl, i2, 
and j2 as you can, and set up 
the matrices D and A as in Algorithm 11.59. Then, apply Farkas' Lemma 
to the resulting matrix inequalities. 
b) Repeat part (a) for the case ii, where il = 
i2 and jl < 
j2. 
11.10 Locality Optimizations 
The performance of a processor, be it a part of a multiprocessor or not, is 
highly sensitive to its cache behavior. Misses in the cache can take tens of clock 
cycles, so high cache-miss rates can lead to poor processor performance. In the 
context of a multiprocessor with a common memory bus, contention on the bus 
can further add to the penalty of poor data locality. 
As we shall see, even if we just wish to improve the locality of uniprocessors, 
the affine-partitioning algorithm for parallelization is useful as a means of iden- 
tifying opportunities for loop transformations. In this section, we describe three 
techniques for improving data locality in uniprocessors and multiprocessors. 
1. We improve the temporal locality of computed results by trying to use the 
results as soon as they are generated. We do so by dividing a computation 
into independent partitions and executing all the dependent operations in 
each partition close together. 
2. Array contraction reduces the dimensions of an array and reduces the 
number of memory locations accessed. We can apply array contraction if 
only one location of the array is used at a given time. 
3. Besides improving temporal locality of computed results, we also need 
to optimize for the spatial locality of computed results, and for both the 
temporal and spatial locality of read-only data. Instead of executing each 
partition one after the other, we interleave a number of the partitions so 
that reuses among partitions occur close together. 
11.10. LOCALITY OPTIMIZATIONS 
11.10.1 Temporal Locality of Computed Data 
The affne-partitioning algorithm pulls all the dependent operations together; 
by executing these partitions serially we improve temporal locality of computed 
data. Let us return to the multigrid example discussed in Section 11.7.1. Ap- 
plying Algorithm 11.43 to parallelize the code in Fig 11.23 finds two degrees 
of parallelism. The code in Fig 11.24 contains two outer loops that iterate 
through the independent partitions serially. This transformed code has im- 
proved temporal locality, since computed results are used immediately in the 
same iteration. 
Thus, even if our goal is to optimize for sequential execution, it is profitable 
to use parallelization to find these related operations and place them together. 
The algorithm we use here is similar to that of Algorithm 11.64, which finds all 
the granularities of parallelism starting with the outermost loop. As discussed 
in Section 11.9.9, the algorithm parallelizes strongly connected components in- 
dividually, if we cannot find synchronization-free parallelism at each level. This 
parallelization tends to increase communication. Thus, we combine separately 
parallelized strongly connected components greedily, if they share reuse. 
11.10.2 Array Contraction 
The optimization of array contraction provides another illustration of the trade- 
off between storage and parallelism, which was first introduced in the context of 
instruction-level parallelism in Section 10.2.3. Just as using more registers al- 
lows for more instruction-level parallelism, using more memory allows for more 
loop-level parallelism. As shown in the multigrid example in Section 11.7.1, 
expanding a temporary scalar variable into an array allows different iterations 
to keep different instances of the temporary variables and to execute at the 
same time. Conversely, when we have a sequential execution that operates on 
one array element at a time serially, we can contract the array, replace it with 
a scalar, and have each iteration use the same location. 
In the transformed multigrid program shown in Fig. 11.24, each iteration of 
the inner loop produces and consumes a different element of AP, AM, T, and a 
row of D. If these arrays are not used outside of the code excerpt, the iterations 
can serially reuse the same data storage instead of putting the values in different 
elements and rows, respectively. Figure 11.62 shows the result of reducing the 
dimensionality of the arrays. This code runs faster than the original, because 
it reads and writes less data. Especially in the case when an array is reduced 
to a scalar variable, we can allocate the variable to a register and eliminate the 
need to access memory altogether. 
As less storage is used, less parallelism is available. Iterations in the trans- 
formed code in Fig. 11.62 now share data dependences and no longer can be 
executed in parallel. To parallelize the code on P processors, we can expand 
each of the scalar variables by a factor of P and have each processor access 
its own private copy. Thus, the amount by which the storage is expanded is 
886 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
f o r  (j = 2, j <= jl, j++) 
f o r  (i = 2, i <= il, i++) ( 
AP 
- 
- . 
. 
. 
, 
T 
= 1.0/(1.0 +AP); 
D 
[21 
= T*AP; 
DW[l,2, 
j 
,i] = T*DW[1,2, 
j 
,i] 
; 
f o r  (k=3, 
k <= kl-I, k++) ( 
AM 
= AP; 
AP 
- 
- . 
. 
. 
, 
T 
= ... 
AP -AM*D[k-I]...; 
D 
[kl 
= T*AP; 
. 
. 
. 
f o r  (k=kl-I, 
k>=2, 
k--) 
DW[l,k,j,i] = DW[i,k,j,i] +D[k]*DW[l,k+l,j,i]; 
3 
Figure 11.62: Code of Fig. 11.23 after partitioning (Fig. 11.24) and array con- 
traction 
directly correlated to the amount of parallelism exploited. 
There are three reasons it is common to find opportunities for array con- 
traction: 
Higher-level programming languages for scientific applications, such as 
Matlab and Fortran 90, support array-level operations. Each subexpres- 
sion of array operations produces a temporary array. Because the arrays 
can be large, every array operation such as a multiply or add would require 
reading and writing many memory locations, while requiring relatively few 
arithmetic operations. It is important that we reorder operations so that 
data is consumed as it is produced and that we contract these arrays into 
scalar variables. 
2. Supercomputers built in the 80's and 90's are all vector machines, so 
many scientific applications developed then have been optimized for such 
machines. Even though vectorizing compilers exist, many programmers 
still write their code to operate on vectors at a time. The multigrid code 
example of this chapter is an example of this style. 
3. Opportunities for contraction are also introduced by the compiler. As 
illustrated by variable T in the multigrid example, a compiler would ex- 
pand arrays to improve parallelization. We have to contract them when 
the space expansion is not necessary. 
Example 1 
It67 
: 
The array expression Z = 
W + 
X + 
Y translates to 
11.10. LOCALITY OPTIMIZATIONS 
Rewriting the code as 
f o r  ( i = O ;  i<n; i++) 
( T = W 
[i] + X [il ; Z [i] 
= T + Y [i] 3 
can speed it up considerably. Of course at the level of C code, we would not 
even have to use the temporary T, but could write the assignment to Z[i] 
as a 
single statement. However, here we are trying to model the intermediate-code 
level at which a vector processor would deal with the operations. 
Algorithm 11.68 
: 
Array contraction. 
INPUT: A program transformed by Algorithm 11.64. 
OUTPUT: An equivalent program with reduced array dimensions. 
METHOD: A dimension of an array can be contracted to a single element if 
1. Each independent partition uses only one element of the array, 
2. The value of the element upon entry to the partition is not used by the 
partition, and 
3. The value of the element is not live on exit from the partition. 
Identify the contractable dimensions - 
those that satisfy the three condi- 
tions above - 
and replace them with a single element. 
Algorithm 11.68 assumes that the program has first been transformed by Al- 
gorithm 11.64 to pull all the dependent operations into a partition and execute 
the partitions sequentially. It finds those array variables whose elements' live 
ranges in different iterations are disjoint. If these variables are not live after the 
loop, it contracts the array and has the processor operate on the same scalar 
location. After array contraction, it may be necessary to selectively expand 
arrays to accommodate for parallelism and other locality optimizations. 
The liveness analysis required here is more complex than that described in 
Section 9.2.5. If the array is declared as a global variable, or if it is a parameter, 
interprocedural analysis is required to ensure that the value on exit is not used. 
Furthermore, we need to compute the liveness of individual array elements, 
conservatively treating the array as a scalar would be too imprecise. 
11.10.3 Partition Interleaving 
Different partitions in a loop often read the same data, or read and write the 
same cache lines. In this and the next two sections, we discuss how to optimize 
for locality when reuse is found across partitions. 
3 
888 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
Reuse in Innermost Blocks 
We adopt the simple model that data can be found in the cache if it is reused 
within a small number of iterations. If the innermost loop has a large or un- 
known bound, only reuse across iterations of the innermost loop translates into 
a locality benefit. Blocking creates inner loops with small known bounds, al- 
lowing reuse within and across entire blocks of computation to be exploited. 
Thus, blocking has the effect of capitalizing on more dimensions of reuse. 
Example 11.69 : Consider the matrix-multiply code shown in Fig. 11.5 and 
its blocked version in Fig. 11.7. Matrix multiplication has reuse along every 
dimension of its three-dimensional iteration space. In the original code, the in- 
nermost loop has n iterations, where n is unknown and can be large. Our simple 
model assumes that only the data reused across iterations in the innermost loop 
is found in the cache. 
In the blocked version, the three innermost loops execute a three-dimension- 
al block of computation, with B iterations on each side. The block size B is 
chosen by the compiler to be small enough so that all the cache lines read and 
written within the block of computation fit into the cache. Thus reused data 
across iterations in the third outermost loop can be found in the cache. 
We refer to the innermost set of loops with small known bounds as the inner- 
most block. It is desirable that the innermost block include all the dimensions 
of the iteration space that carry reuse, if possible. Maximizing the lengths of 
each side of the block is not as important. For the matrix-multiply example, 3- 
dimensional blocking reduces the amount of data accessed for each matrix by a 
factor of B2. If reuse is present, it is better to accommodate higher-dimensional 
blocks with shorter sides than lower-dimensional blocks with longer sides. 
We can optimize locality of the innermost fully permutable loop nest by 
blocking the subset of loops that share reuse. We can generalize the notion of 
blocking to exploit reuses found among iterations of outer parallel loops, also. 
Observe that blocking primarily interleaves the execution of a small number 
of instances of the innermost loop. In matrix multiplication, each instance of 
the innermost loop computes one element of the array answer; there are n2 of 
them. Blocking interleaves the execution of a block of instances, computing B 
iterations from each instance at a time. Similarly, we can interleave iterations 
in parallel loops to take advantage of reuses between them. 
We define two primitives below that can reduce the distance between reuses 
across different iterations. We apply these primitives repeatedly, starting from 
the outermost loop until all the reuses are moved adjacent to each other in the 
innermost block. 
Interleaving Inner Loops in a Parallel Loop 
Consider the case where an outer parallelizable loop contains an inner loop. To 
exploit reuse across iterations of the outer loop, we interleave the executions of 
11.10. LOCALITY OPTIMIZATIONS 
889 
a fixed number of instances of the inner loop, as shown in Fig. 11.63. Creating 
two-dimensional inner blocks, this transformation reduces the distance between 
reuse of consecutive iterations of the outer loop. 
f o r  (i=O; i<n; i++) 
f o r  (ii=O; ii<n; ii+=4) 
f o r  (j=O; j<n; 
j++) 
f o r  (j=O; j<n; 
j++) 
<S> 
f o r  (i=ii; ii<rnin(n, ii+4) 
; ii+=4) 
<S> 
(a) Source program. 
(b) Transformed code. 
Figure 11.63: Interleaving 4 instances of the inner loop 
The step that turns a loop 
f o r  (i=O; i<n; i++) 
<s> 
into 
f o r  (ii=O; ii<n; ii+=4) 
f o r  (i=ii; ii<min(n, ii+4); ii+=4) 
<S> 
is known as stripmining. In the case where the outer loop in Fig. 11.63 has a 
small known bound, we need not stripmine it, but can simply permute the two 
loops in the original program. 
Interleaving Statements in a Parallel Loop 
Consider the case where a parallelizable loop contains a sequence of statements 
SI, 
s 2 ,  . 
. . , 
s,. 
If some of these statements are loops themselves, statements 
from consecutive iterations may still be separated by many operations. We 
can exploit reuse between iterations by again interleaving their executions, as 
shown in Fig. 11.64. This transformation distributes a stripmined loop across 
the statements. Again, if the outer loop has a small fixed number of iterations, 
we need not stripmine the loop but simply distribute the ariginal loop over all 
the statements. 
We use si 
(j) to denote the execution of statement si in iteration j. Instead of 
the original sequential execution order shown in Fig. 11.65(a), 
the code executes 
in the order shown in Fig. 11.65(b). 
Example 11.70: We now return to the multigrid example and show how 
we exploit reuse between iterations of outer parallel loops. We observe that 
references DW[1, 
k,j,i], 
DW[1, 
k-l,j,i], and DW[l, 
k+l, j,i] in the innermost 
loops of the code in Fig. 11.62 have rather poor spatial locality. From reuse 
analysis, as discussed in Section 11.5, the loop with index i carries spatial 
890 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
f o r  (i=O; i<n; i++) ( 
f o r  (ii=O; ii<n; ii+=4) ( 
<Sl> 
f o r  (i=ii; i<min(n,ii+4); i++) 
<S2> 
<S1> 
. 
. 
. 
f o r  (i=ii; i<min(n, ii+4) ; i++) 
1 
<S2> 
(a) Source program. 
(b) Transformed code. 
Figure 11.64: The statement-interleaving transformation 
locality and the loop with index k carries group reuse. The loop with index k 
is already the innermost loop, so we are interested in interleaving operations on 
DW from a block of partitions with consecutive i values. 
We apply the transform to interleave statements in the loop to obtain the 
code in Fig. 11.66, then apply the transform to interleave inner loops to obtain 
the code in Fig. 11.67. Notice that 3s we interleave B iterations from loop with 
index i, we need to expand variables AP, AM, T into arrays that hold B results 
at a time. 
11.10.4 Putting it All Together 
Algorithm 11.71 optimizes locality for a uniprocessor, and Algorithm 11.72 
optimizes both parallelism and locality for a multiprocessor. 
Algorithm 11.71 
: 
Optimize data locality on a uniprocessor. 
INPUT: A program with affine array accesses. 
OUTPUT: An equivalent program that maximizes data locality. 
METHOD: Do the followiag steps: 
1. Apply Algorithm 11.64 to optimize the temporal locality of computed 
results. 
2. Apply Algorithm 11.68 to contract arrays where possible. 
3. Determine the iteration subspace that may share the same data or cache 
lines using the technique described in Section 11.5. For each statement, 
identify those outer parallel loop dimensions that have data reuse. 
4. For each outer parallel loop carrying reuse, move a block of the iterations 
into the innermost block by applying the interleaving primitives repeat- 
edly. 
I1 
.lo. LOCALITY OPTIMIZATIONS 
(a) Original order. 
(b) Transformed order. 
Figure 11.65: Distributing a stripmined loop 
5. Apply blocking to the subset of dimensions in the innermost fully per- 
mutable loop nest that carries reuse. 
6. Block outer fully permutable loop nest for higher levels of memory hier- 
archies, such as the third-level cache or the physical memory. 
7. Expand scalars and arrays where necessary by the lengths of the blocks. 
Algorithm 11.72 : 
Optimize parallelism and data locality for multiprocessors. 
INPUT: A program with affine array accesses. 
OUTPUT: An equivalent program that maximizes parallelism and data locality. 
METHOD: Do the following: 
1. Use the Algorithm 11.64 to parallelize the program and create an SPMD 
program. 
892 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
for (j = 2, 
j <= jl, 
j++) 
for (ii = 2, ii <= il, ii+=b) ( 
for (i = ii; 
i <= min(ii+b-1,il); i++) ( 
ib 
= i-ii+l; 
AP 
L
i
b
]
 
- 
- ...) 
T 
= 1.0/(1.0 +
A
P
[
i
b
]
)
;
 
D 
[2, ibl 
= T*AP 
L
i
b
]
 ; 
DW[1,2, 
j 
,
i
]
 = T*DW[1,2, j 
,
i
]
 ; 
3 
for (i = ii; 
i <= min(ii+b-1,il); i++) ( 
for (k=3, k <= kl-1, 
k++) 
ib 
= i-ii+l; 
AM 
= AP 
L
i
b
]
 ; 
AP 
L
i
b
]
 
- 
- . . ,  
T 
= . 
. . 
AP 
[
i
b
]
 
-AM*D 
[ib, 
k-11. 
. 
. 
; 
D 
Lib, 
k
]
 
= T*AP; 
DW[l,k,j,i] = T*(DW[l,k,j,i]+D~[l,k-l,j,i]) ...; 
> 
. 
. 
. 
for (i = ii; 
i <= min(ii+b-1,il); i++) 
for (k=kl-1, 
k>=2, 
k--) ( 
DW[l,k,j ,
i
]
 = DW[l,k,j,i] +D[iw,k]*DW[l,k+l,j 
,
i
]
;
 
/* Ends code to be executed by processor (j,i) */ 
Figure 11.66: Excerpt of Fig. 11.23 after partitioning, array contraction, and 
blocking 
2. Apply Algorithm 11.71 to the SPMD program produced in Step 1 to 
optimize its locality. 
11.10.5 Exercises for Section 11.10 
Exercise 11.10.1 : Perform array contraction on the following vector opera- 
tions: 
for (i=O; 
i<n; 
i++) T
[
i
]
 
= A
[
i
]
 * B[il; 
for (i=O; 
i<n; 
i++) D
[
i
]
 
= T
[
i
]
 
+ C
[
i
]
;
 
Exercise 11.10.2 : 
Perform array contraction on the following vector opera- 
tions: 
11.21. OTHER USES OF AFFINE TRANSFORMS 
for (j = 2, 
j <= jl, j++) 
for (ii = 2, ii <= il, ii+=b) ( 
for (i = ii; 
i <= min(ii+b-1,il); i++) ( 
ib 
= i-ii+l; 
AP 
[
i
b
]
 
- 
- . . ,  
T 
= 1.0/(1.0 +
A
P
[
i
b
]
)
;
 
D 
[2, 
ibl 
= T*AP 
[
i
b
]
 ; 
DW[1,2, j 
,i] = T*DW[1,2, j 
,i] 
; 
j 
for (k=3, k <= kl-1, 
k++) 
for (i = ii; 
i <= min(ii+b-1,il); i++) ( 
ib 
= i-ii+l; 
AM 
= AP 
[
i
b
]
 ; 
AP 
[
i
b
]
 
- 
- . 
. 
. 
, 
T 
= . 
. 
. 
AP 
Lib] 
-AM*D 
[ib, 
k-11 
. 
. 
. 
; 
D 
[ib, 
k
]
 
= T*AP; 
DWClYky 
j 
,il = T*(DW[lYky j 
,il+DW[l,k-1, J ,
i
]
)
.
 . 
.; 
. 
. . 
for (k=kl-1, 
k>=2, 
k--) ( 
for (i = ii; 
i <= min(ii+b-1,il); i++) 
DW[l,k,j,il = DW[l,k,j,il +D[iw,k]*DW[l,k+l,j,i]; 
/* Ends code to be executed by processor (j,i) */ 
Figure 11.67: Excerpt of Fig. 11.23 after partitioning, array contraction, and 
blocking 
for (i=O; i<n; i++) T 
[
i
]
 = A 
[
i
]
 + B [
i
]
 ; 
for (i=O; i<n; i++) S
[
i
]
 
= C
[
i
]
 
+ D
[
i
]
;
 
for (i=O; icn; 
i++) E
[
i
]
 
= T
[
i
]
 * S
[
i
]
;
 
Exercise 11.10.3 
: 
Stripmine the outer loop 
for (i=n-1; i>=O; i--) 
for (j=O; 
j<n; 
j++) 
into strips of width 10. 
Other Uses of Affine Transforms 
So far we have focused on the architecture of shared memory machines, but 
the theory of affine loop transforms has many other applications. We can ap- 
ply affine transforms to other forms of parallelism including distributed memory 
894 CHAPTER 11. OPTIMIZING FOR PARALLELISM 
AND LOCALITY 
machines, vector instructions, SIMD (Single Instruction Multiple Data) instruc- 
tions, as well as multiple-instruction-issue machines. The reuse analysis intro- 
duced in this chapter also is useful for data prefetching, which is an effective 
technique for improving memory performance. 
11.11.1 Distributed Memory Machines 
For distributed memory machines, where processors communicate by sending 
messages to each other, it is even more important that processors be assigned 
large, independent units of computation, such as those generated by the afine- 
partitioning algorithm. Besides computation partitioning, a number of addi- 
tional compilation issues remain: 
1. Data allocation. If processors use different portions of an array, they each 
only have to allocate enough space to hold the portion used. We can use 
projection to determine the section of arrays used by each processor. The 
input is the system of linear inequalities representing the loop bounds, the 
array access functions, and the affine partitions that map the iterations 
to processor IDS. We project away the loop indices and find for each 
processor ID the set of array locations used. 
2. Communication code. We need to generate explicit code to send and 
receive data to and from other processors. At each synchronization point 
(a) Determine the data residing on one processor that is needed by other 
processors. 
(b) Generate the code that finds all the data to be sent and packs it into 
a buffer. 
(c) Similarly, determine the data needed by the processor, unpack re- 
ceived messages, and move the data to the right memory locations. 
Again, if all accesses are affine, these tasks can be performed by the 
compiler, using the affine framework. 
3. Optimization. It is not necessary for all the communications to take place 
at the synchronization points. It is preferable that each processor sends 
data as soon as it is available, and that each processor does not start 
waiting for data until it is needed. Such optimizations must be balanced 
by the goal of not generating too many messages, since there is a nontrivial 
overhead associated with processing each message. 
Techniques described here have other applications as well. For example, a 
special-purpose embedded system may use coprocessors to offload some of its 
computations. Or, instead of demand fetching data into the cache, an embedded 
system may use a separate controller to load and unload data into and out of 
the cache, or other data buffers, while the processor operates on other data. In 
these cases, similar techniques can be used to generate the code to move data 
around. 
11.11. OTHER USES OF AFFINE TRANSFORMS 
11.1 
1.2 Multi-Instruction-Issue Processors 
We can also use affine loop transforms to optimize the performance of multi- 
instruction-issue machines. As discussed in Chapter 10.5, the performance of 
a software-pipelined loop is limited by two factors: cycles in precedence con- 
straints and the usage of the critical resource. By changing the makeup of the 
innermost loop, we can improve these limits. 
First, we may be able to use loop transforms to create innermost paralleliz- 
able loops, thus eliminating precedence cycles altogether. Suppose a program 
has two loops, with the outer being parallelizable and the inner not. We can 
permute the two loops to make the inner loop parallelizable and so create more 
opportunities for instruction-level parallelism. Notice that it is not necessary 
for iterations in the innermost loop to be completely parallelizable. It is suffi- 
cient that the cycle of dependences in the loop be short enough so that all the 
hardware resources are fully utilized. 
We can also relax the limit due to resource usage by improving the usage 
balance inside a loop. Suppose one loop only uses the adder, and another uses 
only the multiplier. Or, suppose one loop is memory bound and another is 
compute bound. It is desirable to fuse each pair of loops in these examples 
together so as to utilize all the functional units at the same time. 
11.11.3 Vector and SIMD Instructions 
Besides multiple-instruction issue, there are two other important forms of in- 
struction-level parallelism: vector and SIMD operations. In both cases, the 
issue of just one instruction causes the same operation to be applied to a vector 
of data. 
As mentioned previously, many early supercomputers used vector instruc- 
tions. Vector operations are performed in a pipelined manner; the elements 
in the vector are fetched serially and computations on different elements are 
overlapped. In advanced vector machines, vector operations can be chained: 
as the elements of the vector results are produced, they are immediately con- 
sumed by operations of another vector instruction without having to wait for 
all the results to be ready. Moreover, in advanced machines with scatter/gather 
hardware, the elements of the vectors need not be contiguous; an index vector 
is used to specify where the elements are located. 
SIMD instructions specify that the same operation be performed on contigu- 
ous memory locations. These instructions load data from memory in parallel, 
store them in wide registers, and compute on them using parallel hardware. 
Many media, graphics, and digital-signal-processing applications can benefit 
from these operations. Low-end media processors can achieve instruction-level 
parallelism simply by issuing one SIMD instruction at a time. Higher-end pro- 
cessors can combine SIMD with multiple-instruction issue to achieve higher 
performance. 
SIMD and vector instruction generation share many similarities with locality 
896 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
optimization. As we find independent partitions that operate on contiguous 
memory locations, we stripmine those iterations and interleave these operations 
in innermost loops. 
SIMD instruction generation poses two additional difficulties. First, some 
machines require that the SIMD data fetched from memory be aligned. For 
example, they might require that 25,6-byte SIMD operands be placed in ad- 
dresses that are multiples of 256. If the sdurce loop operates on just one array 
of data, we can generate one main loop that operates on aligned data and ex- 
tra code before and after the loop to handle those elements at the boundary. 
For loops operating on more than one array, however, it may not be possible 
to align all the data at the same time. Second, data used by consecutive it- 
erations in a loop may not be contiguous. Examples include many important 
digital-signal-processing algorithms, such as Viterbi decoders and fast Fourier 
transforms. Additional operations to shuffle the data around may be necessary 
to take advantage of the SIMD instructions. 
11.1 
1.4 Prefetching 
No data-locality optimization can eliminate all memory accesses; for one, data 
used for the first time must be fetched from memory. To hide the latency 
of memory operations, prefetch instructions have been adopted in many high- 
performance processors. Prefetch is a machine instruction that indicates to the 
processor that certain data is likely to be used soon, and that it is desirable to 
load the data into the cache if it is not present already. 
The reuse analysis described in Section 11.5 can be used to estimate when 
caches misses are likely. There are two important considerations when gener- 
ating prefetch instructions. If contiguous memory locations are to be accessed, 
we need to issue only one prefetch instruction for each cache line. Prefetch 
instructions should be issued early enough so that the data is in the cache by 
the time it are used. However, we should not issue prefetch instructions too 
far in advance. The prefetch instructions can displace data that may still be 
needed; also the prefetched data may be flushed before it is used. 
Example 11.73 : 
Consider the following code: 
f o r  (i=O; ii<3; i++) 
f o r  (j=O; j<100; j++) 
A[i,j] = . 
. 
.; 
Suppose the target machine has a prefetch instruction that can fetch two words 
of data at a time, and that the latency of a prefetch instruction takes about 
the time to execute six iterations of the loop above. The prefetch code for the 
above example is shown in Fig. 11.68. 
We unroll the innermost loop twice, so a prefetch can be issued for each cache 
line. We use the concept of software pipelining to prefetch data six iterations 
before it is used. The prolog fetches the data used in the first six iterations. The 
11.12. SUMMARY OF CHAPTER 11 
f o r  (i=O; i i < 3 ;  i++) ( 
f o r  (j=O; j<6; j+=2) 
prefetch(&ACi, 
jl) ; 
f o r  (j=O; j<94; j+=2) C 
pref etch(&A 
[i, 
j+61) ; 
A[i,jI = . 
. 
.; 
~ [ i , j + l ]  
= . . .; 
1 
f o r  (j=94; j<100; j++) 
~ [ i , j ]  
= . . .; 
Figure 11.68: Code modified to prefetch data 
steady state loop prefetches six iterations ahead as it performs its computation. 
The epilog issues no prefetches, but simply executes the remaining iterations. 
1
7
 
11.12 Summary of Chapter 11 
+ Parallelism and Locality from Arrays. The most important opportunities 
for both parallelism and locality-based optimizations come from loops 
that access arrays. These loops tend to have limited dependences among 
accesses to array elements and tend to access arrays in a regular pattern, 
allowing efficient use of the cache for good locality. 
+ A f i n e  Accesses. Almost all theory and techniques for parallelism and 
locality optimization assume accesses to arrays are affine: the expressions 
for the array indexes are linear functions of the loop indexes. 
+ Iteration Spaces. A loop nest with d nested loops defines a d-dimensional 
iteration space. The points in the space are the d-tuples of values that 
the loop indexes can assume during the execution of the loop nest. In the 
affine case, the limits on each loop index are linear functions of the outer 
loop indexes, so the iteration space is a polyhedron. 
+ Fourier-Motxlcin Elimination. A key manipulation of iteration spaces is 
to reorder the loops that define the iteration space. Doing so requires that 
a polyhedral iteration space be projected onto a subset of its dimensions. 
The Fourier-Motzkin algorithm replaces the upper and lower limits on a 
given variable by inequalities between the limits themselves. 
+ Data Dependences and Array Accesses. A central problem we must solve 
in order to manipulate loops for parallelism and locality optimizations 
is whether two array accesses have a data dependence (can touch the 
898 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
same array element). When the accesses and loop bounds are affine, the 
problem can be expressed as whether there are solutions to a matrix-vector 
equation within the polyhedron that defines the iteration space. 
+ Matrix Rank and Data Reuse. The matrix that describes an array access 
can tell us several important things about that access. If the rank of the 
matrix is as large as possible (minimum of the number of rows and number 
of columns), then the access never touches the same element twice as the 
loops iterate. If the array is stored in row- (column-)major form, then the 
rank of the matrix with the last (first) row deleted tells us whether the 
access has good locality; i.e., elements in a single cache line are accessed 
at about the same time. 
+ Data Dependence and Diophantine Equations. Just because two accesses 
to the same array touch the same region of the array does not mean that 
they actually access any element in common. The reason is that each 
may skip some elements; e.g., one accesses even elements and the other 
accesses odd elements. In order to be sure that there is a data dependence, 
we must solve a Diophantine (integer solutions only) equation. 
+ Solving Diophantine Linear Equations. The key technique is to compute 
the greatest common divisor (GCD) of the coefficients of the variables. 
Only if that GCD divides the constaht term will there be integer solutions. 
+ Space-Partition Constraints. To parallelize the execution of a loop nest, 
we need to map the iterations of the loop to a space of processors, which 
can have one or more dimensions. The space-partition constraints say 
that if two accesses in two different iterations share a data dependence 
(i.e., they access the same array element), then they must map to the 
same processor. As long as the mapping of iterations to processors is 
affine, we can formulate the problem in matrix-vector terms. 
+ Primitive Code Transformations. The transformations used to parallelize 
programs with affine array accesses are combinations of seven primitives: 
loop fusion, loop fission, re-indexing (adding a constant to loop indexes), 
scaling (multiplying loop indexes by a constant), reversal (of a loop index), 
permutation (of the order of loops), and skewing (rewriting loops so the 
line of passage through the iteration space is no longer along one of the 
axes). 
+ Synchronization of Parallel Operations. Sometimes more parallelism can 
be obtained if we insert synchronization operations between steps of a 
program. For example, consecutive loop nests may have data depen- 
dences, but synchronizations between the loops can allow the loops to be 
parallelized separately. 
+ Pipelining. This parallelization technique allows processors to share data, 
by synchronously passing certain data (typically array elements) from one 
11.13. REFERENCES FOR CHAPTER 11 
processor to an adjacent processor in the processor space. The method 
can improve the locality of the data accessed by each processor. 
+ Time-Partition Constraints. To discover opportunities for pipelining, we 
need to discover solutions to the time-partition constraints. These say 
that whenever two array accesses can touch the same array element, then 
the access in the iteration that occurs first must be assigned to a stage 
in the pipeline that occurs no later than the stage to which the second 
access is assigned. 
+ Solving Time-Partition Constraints. Farkas' Lemma provides a power- 
ful technique for finding all the affine time-partition mappings that are 
allowed by a given loop nest with array accesses. The technique is es- 
sentially to replace the primal formulation of the linear inequalities that 
express the time-partition constraints by their dual. 
+ Blocking. This technique breaks each of several loops in a loop nest into 
two loops each. The advantage is that doing so may allow us to work on 
small sections (blocks) of a multidimensional array, one block at a time. 
That, in turn, improves the locality of the program, letting all the needed 
data reside in the cache while working on a single block. 
+ Strzprnining. Similar to blocking, this technique breaks only a subset of 
the loops of a loop nest into two loops each. A possible advantage is that 
a multidimensional array is accessed a "strip" at a time, which may lead 
to the best possible cache utilization. 
11.13 References for Chapter 11 
For detailed discussions of multiprocessor architectures, we refer the reader to 
the text by Hennessy and Patterson [9]. 
Lamport [13] and Kuck, Muraoka, and Chen [6] introduced the concept of 
data-dependence analysis. Early data-dependence tests used heuristics to prove 
a pair of references to be independent by determining if there are no solutions to 
Diophantine equations and systems of real linear inequalities: [5, 6, 261. May- 
dan, Hennessy, and Lam [18] formulated the data-dependence test as integer 
linear programming and showed that the problem can be solved exactly and 
efficiently in practice. The data-dependence analysis described here is based 
on work by Maydan, Hennessy, and Lam [18] and Pugh and Wonnacott [23], 
which in turn use techniques of Fourier-Motzkin elimination [7] and Shostak's 
algorithm [25]. 
The 70's and early 80's saw the use of loop transformations to improve 
vectorization and parallelization: loop fusion [3], loop fission [I], stripmining 
[17], and loop interchange [28]. There were three major experimental paral- 
lelizer/vectorizing projects going on at the time: Parafrase led by Kuck at the 
University of Illinois Urbana-Champaign [21], the PFC project led by Kennedy 
900 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
at Rice University [4], and the PTRAN project led by Allen at IBM Research 
121- 
McKellar and Coffman [I91 first discussed using blocking to improve data 
locality. Lam, Rothbert, and Wolf [12] provided the first in-depth empirical 
analysis of blocking on caches for modern architectures. Wolf and Lam [27] 
used linear-algebra techniques to compute data reuse in loops. Sarkar and Gao 
[24] introduced the optimization of array contraction. 
Lamport [13] 
was the first to model loops as iteration spaces and used hyper- 
planing (a special case of an affine transform) to find parallelism for multipro- 
cessors. Affine transforms have their root in systolic-array algorithm design [l 
1
1
 . 
Intended as parallel algorithms directly implemented in VLSI, systolic arrays 
require communication to be minimized along with parallelization. Algebraic 
techniques were developed to map the computation onto space and time coordi- 
nates. The concept of an affine schedule and the use of Farkas' Lemma in affine 
transformations were introduced by Feautrier [8]. The affine-transformation 
algorithm described here is based on work by Lim et al. [15, 14, 161. 
Porterfield [22] proposed one of the first compiler algorithms to prefetch 
data. Mowry, Lam, and Gupta [20] applied reuse analysis to minimize the 
prefetch overhead and gain an overall performance improvement. 
1. Abu-Sufah, W., D. J. Kuck, and D. H. Lawrie, "On the performance 
enhancement of paging systems through program analysis and transfor- 
mations," IEEE Trans. on Computing C-305 (1981), pp. 341-356. 
2. Allen, F. E., M. Burke, P. Charles, R. Cytron, and J. Ferrante, "An 
overview of the PTRAN analysis system for multiprocessing," J. Parallel 
and Distributed Computing 5:5 (1988), pp. 617-640. 
3. Allen, F. E. and J. Cocke, "A Catalogue of optimizing transformations," 
in Design and Optimization o
f
 Compilers (R. Rustin, ed.), pp. 1-30, 
Prentice-Hall, 1972. 
4. Allen, R. and K. Kennedy, "Automatic translation of Fortran programs to 
vector form," ACM Transactions 
on Programming Languages and Systems 
9:4 (1987), pp. 491-542. 
5. Banerjee, U., Data Dependence in Ordinary Programs, Master's thesis, 
Department of Computer Science, University of Illinois Urbana-Cham- 
paign, 1976. 
6. Banerjee, U., Speedup o
f
 Ordinary Programs, Ph.D. thesis, Department 
of Computer Science, University of Illinois Urbana-Champaign, 1979. 
7. Dantzig, G. and B. C. Eaves, "Fourier-Motzkin elimination and its dual," 
J. Combinatorial Theory, A(14) (1973), pp. 288-297. 
8. Feautrier, P., "Some efficient solutions to the affine scheduling problem: 
I. One-dimensional time," International J. Parallel Programming 215 
(1992), pp. 313-348, 
11.13. REFERENCES FOR CHAPTER 11 
90 
1 
9. Hennessy, J. L. and D. A. Patterson, Computer Architecture: A Quanti- 
tative Approach, Third Edition, Morgan Kaufman, San Francisco, 2003. 
10. Kuck, D., Y. Muraoka, and S. Chen, "On the number of operations 
simultaneously executable in Fortran-like programs and their resulting 
speedup," IEEE Transactions on Computers C-21:12 (1972), pp. 1293- 
1310. 
11. Kung, H. T. and C. E. Leiserson, "Systolic arrays (for VLSI)," in Duff, 
I. S. and G. W. Stewart (eds.), Sparse Matrix Proceedings pp. 256-282. 
Society for Industrial and Applied Mathematics, 1978. 
12. Lam, M. S., E. E. Rothberg, and M. E. Wolf, "The cache performance and 
optimization of blocked algorithms," Proc. Sixth International Conference 
on Architectural Support for Programming Languages and Operating Sys- 
tems (1991), pp. 63-74. 
13. Lamport, L., "The parallel execution of DO loops," Comm. A CM 17:2 
(1974), pp. 83-93. 
14. Lim, A. W., G. I. Cheong, and M. S. Lam, "An affine partitioning algo- 
rithm to maximize parallelism and minimize communication," Proc. 13th 
International Conference on Supercomputing (1999), pp. 228-237. 
15. Lim, A. W. and M. S. Lam, "Maximizing parallelism and minimizing 
synchronization with affine transforms," Proc. 24th ACM SIGPLAN-SIG- 
ACT Symposium on Principles o
f
 Programming Languages (1997), pp. 
201-214. 
16. Lim, A. W., S.-W. Liao, and M. S. Lam, "Blocking and array contrac- 
tion across arbitrarily nested loops using affine partitioning," Proc. ACM 
SIGPLAN Symposium on Principles and Practice o
f
 Parallel Program- 
ming (2001), pp. 103-112. 
17. Loveman. D. B., "Program improvement by source-to-source transforma- 
tion," J. ACM 24:l (1977), pp. 121-145. 
18. Maydan, D. E., J. L. Hennessy, and M. S. Lam, "An efficient method for 
exact dependence analysis," Proc. A CM SIGPLAN 1991 Conference on 
Programming Language Design and Implementation, pp. 1-14. 
19. McKeller, A. C. and E. G. Coffman, "The organization of matrices and 
matrix operations in a paged multiprogramming environment ," Comm. 
ACM, 12:3 (1969), pp. 153-165. 
20. Mowry, T. C., M. S. Lam, and A. Gupta, "Design and evaluation of a com- 
piler algorithm for prefetching," Proc. Fifth International Conference on 
Architectural Support for Programming Languages and Operating Systems 
(1992), pp. 62-73. 
902 CHAPTER 11. OPTIMIZING FOR PARALLELISM AND LOCALITY 
21. Padua, D. A. and M. J. Wolfe, "Advanced compiler optimziations for 
supercomputers," Comm. ACM, 29:12 (1986), pp. 1184-1201. 
22. Porterfield, A., Software Methods for Improving Cache Performance on 
Supercomputer Applications, Ph.D. Thesis, Department of Computer Sci- 
ence, Rice University, 1989. 
23. Pugh, W. and D. Wonnacott 
, 
"Eliminating false positives using the omega 
test ," Proc. A CM SIGPLAN 1992 Conference on Programming Language 
Design and Implementation, pp. 140-151. 
24. Sarkar, V. and G. Gao, "Optimization of array accesses by collective loop 
transformations," Proc. 5th International Conference on Supercomputing 
(1991), pp. 194-205. 
25. R. Shostak, "Deciding linear inequalities by computing loop residues," J. 
ACM, 28:4 
(1981), pp. 769-779. 
26. Towle, R. A., Control and Data Dependence for Program Transforma- 
tion, Ph.D . 
thesis, Department of Computer Science, University of Illinois 
Urbana-Champaign, 1976. 
27. Wolf, M. E. and M. S. Lam, "A data locality optimizing algorithm," 
Proc. SIGPLAN 1991 Conference on Programming Language Design and 
Implementation, pp. 30-44. 
28. Wolfe, M. J., Techniques for Improving the Inherent Parallelism in Pro- 
grams, Master's thesis, Department of Computer Science, University of 
Illinois Urbana-Champaign, 1978. 
Chapter 12 
Interprocedural Analysis 
In this chapter, we motivate the importance of interprocedural analysis by dis- 
cussing a number of important optimization problems that cannot be solved 
with intraprocedural analysis. We begin by describing the common forms of 
interprocedural analysis and explaining the difficulties in their implement ation. 
We then describe applications for interprocedural analysis. For widely used 
programming languages like C and Java, pointer alias analysis is key to any 
interprocedural analysis. Thus, for much of the chapter, we discuss techniques 
needed to compute pointer aliases. To start, we present Datalog, a notation 
that greatly hides the complexity of an efficient pointer analysis. We then de- 
scribe an algorithm for pointer analysis, and show how we use the abstraction 
of binary decision diagrams (BDD's) to implement the algorithm efficiently. 
Most compiler optimizations, including those described in Chapters 9, 10, 
and 11, 
are performed on procedures one at a time. We refer to such analyses as 
intraprocedural. These analyses conservatively assume that procedures invoked 
may alter the state of all the variables visible to the procedures and that they 
may create all possible side effects, 
such as modifyiag any of the variables visible 
to the procedure or generating exceptions that cause the unwinding of the 
call stack. Intraprocedural analysis is thus relatively simple, albeit imprecise. 
Some optimizations do not need interprocedural analysis, while others may yield 
almost no useful information without it. 
An interprocedural analysis operates across an entire program, flowing in- 
formation from the caller to its callees and vice versa. One relatively simple but 
useful technique is to inline procedures, that is, to replace a procedure invoca- 
tion by the body of the procedure itself with suitable modificatiohs to account 
for parameter passing and the return value. This method is applicable only if 
we know the target of the procedure call. 
If procedures are invoked indirectly through a pointer or via the method- 
dispatch mechanism prevalent in object-oriented programming, analysis of the 
program's pointers or references can in some cases determine the targets of 
the indirect invocations. If there is a unique target, inlining can be applied. 
904 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
Even if a unique target is determined for each procedure invocation, inlining 
must be applied judiciously. In general, it is not possible to inline recursive 
procedures directly, and even without recursion, inlining can expand the code 
size exponentially. 
12.1 Basic Concepts 
In this section, we introduce call graphs - 
graphs that tell us which procedures 
can call which. We also introduce the idea of "context sensitivity," where data- 
flow analyses are required to take cognizance of what the sequence of procedure 
calls has been. That is, context-sensitive analysis includes (a synopsis of) the 
current sequence of activation records on the stack, along with the current point 
in the program, when distinguishing among different "places" in the program. 
12.1.1 Call Graphs 
A call graph for a program is a set of nodes and edges such that 
1. There is one node for each procedure in the program. 
2, There is one node for each call site, that is, a place in the program where 
a procedure is invoked. 
3. If call site c may call procedure p, then there is an edge from the node for 
c to the node for p. 
Many programs written in languages like C and Fortran make procedure calls 
directly, so the call target of each invocation can be determined statically. In 
that case, each call site has an edge to exactly one procedure in the call graph. 
However, if the program includes the use of a procedure parameter or function 
pointer, the target generally is not known until the program is run and, in fact, 
may vary from one invocation to another. Then, a call site can link to many or 
all procedures in the call graph. 
Indirect calls are the norm for ob 
ject-oriented programming languages. In 
particular, when there is overriding of methods in subclasses, a use of method 
rn may refer to any of a number of different methods, depending on the subclass 
of the receiver object to which it was applied. The use of such virtual method 
invocations means that we need to know the type of the receiver before we can 
determine which method is invoked. 
Example 12.1 : 
Figure 12.1 shows a C program that declares pf to be a global 
pointer to a function whose type is "integer to integer." There are two functions 
of this type, f unl and f un2, and a main function that is not of the type that pf 
points to. The figure shows three call sites, denoted cl, c2, and c3; the labels 
are not part of the program. 
12.1. BASIC CONCEPTS 
int (*pf) 
(int) 
; 
cl: 
int funl 
(int x) ( 
if (x < 10) 
return (*pf) 
(x+l) 
; 
else 
return x; 
int fun2(int y) -( 
pf = &funl; 
return (*pf) 
(y) 
; 
1 
void main() ( 
pf = &fun2; 
(*pf 
) ( 5 )  ; 
1 
Figure 12.1: A program with a function pointer 
The simplest analysis of what pf could point to would simply observe the 
types of functions. Functions funl and fun2 
are of the same type as what 
pf points to, while main is not. Thus, a conservative call graph is shown in 
Fig. 12.2(a). A more careful analysis of the program would observe that pf is 
made to point to f 
un2 
in main 
and is made to point to f 
unl 
in f 
un2. 
But there 
are no other assignments to any pointer, so, in particular, there is no way for 
pf to point to main. 
This reasoning yields the same call graph as Fig. 12.2(a). 
An even more precise analysis would say that at c3, 
it is only possible for pf 
to point to f 
un2, 
because that call is preceded immediately by that assignment 
to pf 
. Similarly, at c2 
it is only possible for pf to point to f 
unl. As a result, 
the initial call to funl 
can come only from f 
un2, 
and funl 
does not change pf, 
so whenever we are within funl, 
pf points to f 
unl. In particular, at cl, 
we 
can be sure pf points to funl. 
Thus, Fig. 12.2(b) is a more precise, correct call 
graph. 
In general, the presence of references or pointers to functions or methods 
requires us to get a static approximation of the potential values of all procedure 
parameters, function pointers, and receiver object types. To make an accurate 
approximation, interprocedural analysis is necessary. The analysis is iterative, 
starting with the statically observable targets. As more targets are discov- 
ered, the analysis incorporates the new edges into the call graph and repeats 
discovering more targets until convergence is reached. 
CHAPTER 12. INTERPROCED URAL ANALYSIS 
main 
da 
Figure 12.2: Call graphs derived from Fig. 12.1 
12.1.2 
Context Sensitivity 
Interprocedural analysis is challenging because the behavior of each procedure is 
dependent upon the context in which it is called. Example 12.2 
uses the problem 
of interprocedural constant propagation on a small program to illustrate the 
significance of contexts. 
Example 12.2 : Consider the program fragment in Fig. 12.3. Function f is 
invoked at three call sites: ci, c2 and c3. Constant 0 is passed in as the 
actual parameter at ci, and constant 243 is passed in at c2 and c3 in each 
iteration; the constants 1 
and 244 are returned, respectively. Thus, function f 
is invoked with a constant in each of the contexts, but the value of the constant 
is context-dependent . 
As we shall see, it is not possible to tell that t 
1, 
t2, and t 3  
each are assigned 
constant values (and thus so is X[i]), 
unless we recognize that when called in 
context cl, f returns 1, and when called in the other two contexts, f returns 
244. A naive analysis would conclude that f can return either 1 
or 244 from 
any call. 
One simplistic but extremely inaccurate approach to interprocedural anal- 
ysis, known as context-insensitive analysis, is to treat each call and return 
statement as "goto" operations. We create a super control-flow graph where, 
besides the normal intraprocedural control flow edges, additional edges are cre- 
ated connecting 
1. Each call site to the beginning of the procedure it calls, and 
2. The return statements back to the call sites.' 
l ~ h e  
return is actually to the instruction following the call site. 
12.1. BASIC CONCEPTS 
f o r  (i 
= 0; i < n; i++) C 
c l :  
tl = f (0); 
c2: 
t 2  = f (243) ; 
c3: 
t 3  = f (243); 
X [ i ]  
= tl+t2+t3; 
i n t  f (int v) 
return (v+l) 
; 
> 
Figure 12.3: A program fragment illustrating the need for context-sensitive 
analysis 
Assignment statements are added to assign each actual parameter to its 
corresponding formal parameter and to assign the returned value to the variable 
receiving the result. We can then apply a standard analysis intended to be used 
within a procedure to the super control-flow graph to find context-insensitive 
interprocedural results. While simple, this model abstracts out the important 
relationship between input and output values in procedure invocations, causing 
the analysis to be imprecise. 
Example 12.3 : 
The super control-flow graph for the program in Fig. 12.3 is 
shown in Figure 12.4. Block B6 is the function f .  Block B3 contains the call 
site cl; 
it sets the formal parameter v to 0 and then jumps to the beginning of 
f ,  
at B6. Similarly, B4 and B5 represent the call sites c2 and c3, respectively. 
In B4, 
which is reached from the end of f (block B6), 
we take the return value 
from f and assign it to t l .  We then set formal parameter v to 243 and call f 
again, by jumping to Bg 
. Note that there is no edge from B3 to B4. Control 
must flow through f on the way from B3 to B4. 
B5 is similar to B4. It receives the return from f ,  
assigns the return value 
to t2, and initiates the third call to f .  
Block B7 represents the return from the 
third call and the assignment to X[i]. 
If we treat Fig. 12.4 as if it were the flow graph of a single procedure, then 
we would conclude that coming into B6, 
v can have the value 0 or 243. Thus, 
the most we can conclude about r e t v a l  is that it is assigned 1 
or 244, but no 
other value. Similarly, we can only conclude about t 
1, 
t2, and t 3  
that they can 
each be either 1 
or 244. Thus, X[i] 
appears to be either 3, 246, 489, or 732. In 
contrast, a context-sensitive analysis would separate the results for each of the 
calling contexts and produces the intuitive answer described in Example 12.2: 
t1 is always 1, 
t 2  and t 3  
are always 244, and X[i] 
is 489. 
C1 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
Figure 12.4: The control-flow graph for Fig. 12.3, treating function calls as 
control flow 
B 1  
12.1.3 Call Strings 
i
=
O
 
In Example 12.2, we can distinguish among the contexts by just knowing the 
call site that calls the procedure f .  In general, a calling context is defined by 
the contents of the entire call stack. We refer to the string of call sites on the 
stack as the call string. 
Example 12.4 : 
Figure 12.5 is a slight modification of Fig. 12.3. Here we have 
replaced the calls to f by calls to g ,  which then calls f with the same argument. 
There is an additional call site, c4, where g calls f .  
There are three call strings to f: (cl, 
c4), (c2, 
c4), and (c3, c4). As we see 
in this example, the value of v in function f depends not on the immediate or 
last site c4 on the call string. Rather, the constants are determined by the first 
element in each of the call strings. 
B7 
Example 12.4 illustrates that information relevant to the analysis can be 
introduced early in the call chain. In fact, it is sometimes necessary to consider 
the entire call string to compute the most precise answer, as illustrated in 
Example 12.5. 
B 3  
c1: v = 0 
t3 = retval 
t4 
= '"'2 
t 5  = t4+t3 
X C i I  = t5 
i = i+l - 
B2 
if i<n 
goto L
-
-
,
 
12.1. BASIC CONCEPTS 
f o r  ( i  = 0; i < n; i++) ( 
c l :  
tl = g(0); 
c2: 
t 2  = g(243) ; 
c3: 
t 3  = g(243); 
X [ i ]  
= t l + t 2 + t 3 ;  
3 
i n t  g ( i n t  V) ( 
c4: 
r e t u r n  f (v) ; 
3 
i n t  f ( i n t  v) ( 
r e t u r n  (v+l) 
; 
3 
Figure 12.5: Program fragment illustrating call strings 
f o r  ( i  = 0; i < n; i++) ( 
c l :  
tl = g(0); 
c2: 
t 2  = g(243) ; 
c3: 
t 3  = g(243); 
X [ i l  = t l + t 2 + t 3 ;  
3 
i n t  g ( i n t  V) ( 
i f  (v > 1) ( 
c4: 
return g(v-1) ; 
3 e l s e  ( 
c5: 
return f (v) ; 
i n t  f ( i n t  v) ( 
return (v+l) 
; 
Figure 12.6: Recursive program requiring analysis of complete call strings 
910 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
Example 12.5 : 
This example illustrates how the ability to reason about un- 
bounded call strings can yield more precise results. In Fig. 12.6 we see that if 
g is called with a positive value c, then g will be invoked recursively c times. 
Each time g is called, the value of its parameter u 
decreases by 1. Thus, the 
value of g's parameter u 
in the context whose call string is c2(c4)" is 243 - 
n. 
The effect of g is thus to increment 0 or any negative argument by 1, and to 
return 2 on any argument 1 
or greater. 
There are three possible call strings for f .  If we start with the call at c l ,  
then g calls f immediately, so (cl, 
c5) is one such string. If we start at c2 or 
c3, then we call g a total of 243 times, and then call f .  
These call strings are 
(c2, c4, c4,. . . , 
c5) and (c3, c4, c4,. 
. 
. , 
c5), where in each case there are 242 
c4's in the sequence. In the first of these contexts, the value of f's parameter 
u 
is 0, while in the other two contexts it is 1. 
In designing a context-sensitive analysis, we have a choice in precision. For 
example, instead of qualifying the results by the full call string, we may just 
choose to distinguish between contexts by their k most immediate call sites. 
This technique is known as k-limiting context analysis. Context-insensitive 
analysis is simply a special case of k-limiting context analysis, where k is 0. We 
can find all the constants in Example 12.2 using a 1-limiting analysis and all the 
constants in Example 12.4 using a 2-limiting analysis. However, no k-limiting 
analysis can find all the constants in Example 12.5, provided the constant 243 
were replaced by two different and arbitrarily large constants. 
Instead of choosing a fixed value k, another possibility is to be fully con- 
text sensitive for all acyclic call strings, which are strings that contain no re- 
cursive cycles. For call strings with recursion, we can collapse all recursive 
cycles, in order to bound the number of different contexts analyzed. In Ex- 
ample 12.5, the calls initiated at call site c2 may be approximated by the call 
string: (c2, c4*, 
c5). Note that, with this scheme, even for programs without 
recursion, the number of distinct calling contexts can be exponential in the 
number of procedures in the program. 
12.1.4 Cloning-Based Context-Sensitive Analysis 
Another approach to context-sensitive analysis is to clone the procedure con- 
ceptually, one for each unique context of interest. We can then apply a context- 
insensitive analysis to the cloned call graph. Examples 12.6 and 12.7 show the 
equivalent of a cloned version of Examples 12.4 and 12.5, respectively. In real- 
ity, we do not need to clone the code, we can simply use an efficient internal 
representation to keep track of the analysis results of each clone. 
Example 12.6 : 
The cloned version of Fig. 12.5 is shown in Fig. 12.7. Because 
every calling context refers to a distinct clone, there is no confusion. For ex- 
ample, g l  receives 0 as input and produces 1 as output, and g2 and g3 both 
receive 243 as input and produce 244 as output. 
12.1. BASIC CONCEPTS 
f o r  ( i  = 0; i < n; i++) ( 
tl = g l ( 0 ) ;  
t 2  = g2(243); 
t 3  = g3(243); 
X [ i ]  
= t l + t 2 + t 3 ;  
J 
i n t  g l  ( i n t  V) ( 
c4.1: 
return f I (v) ; 
J 
i n t  g2 ( i n t  v) ( 
c4.2: 
return f 
2 
(v) ; 
J 
i n t  g3 ( i n t  v) ( 
c4.3: 
r e t u r n  f 
3 
(v) ; 
i n t  f 1 ( i n t  v) ( 
r e t u r n  (v+l) ; 
> 
i n t  f 2  ( i n t  v) ( 
return (v+l) 
; 
3 
i n t  f 3  ( i n t  v) ( 
r e t u r n  (v+l) ; 
I 
Figure 12.7: Cloned version of Fig. 12.5 
Example 12.7 : 
The cloned version of Example 12.5 
is shown in Fig. 12.8. For 
procedure g, we create a clone to represent all instances of g that are first called 
from sites c l ,  c2, and c3. In this case, the analysis would determine that the 
invocation at call site c l  
returns 1, 
assuming the analysis can deduce that with 
v = 0, the test v > 1 
fails. This analysis does not handle recursion well enough 
to produce the constants for call sites c2 and c3, however. 
12.1.5 Summary-Based Context-Sensitive Analysis 
Summary-based interprocedural analysis is an extension of region-based anal- 
ysis. Basically, in a summary-based analysis each procedure is represented by 
a concise description ("summary") that encapsulates some observable behavior 
of the procedure. The primary purpose of the summary is to avoid reanalyzing 
@ 
a procedure's body at every call site that may invoke the procedure. 
Let us first consider the case where there is no recursion. Each procedure is 
modeled as a region with a single entry point, with each caller-callee pair sharing 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
f o r  ( i  = 0; i < n; i++) ( 
c l :  
tl = g l ( 0 ) ;  
c2: 
t 2  = g2 
(243) ; 
c3: 
t 3  = g3 
(243) ; 
X [ i ]  
= t l + t 2 + t 3 ;  
i n t  g1 ( i n t  V) ( 
i f  (v > 1) ( 
c4.1: 
r e t u r n  g l  
(v-I) ; 
) e l s e  ( 
c5.1: 
return f 1 
(v) ; 
1 
i n t  g2 ( i n t  v) ( 
i f  (v > 1) ( 
c4.2: 
return g2 (v-1) ; 
) e l s e  ( 
c5.2: 
return f 
2 
(v) ; 
i n t  g3 ( i n t  v) ( 
i f  (v > I )  ( 
r e t u r n  g3 
(v-1) ; 
) e l s e  ( 
r e t u r n  f 
3 
(v) ; 
i n t  f l  ( i n t  v) ( 
r e t u r n  (v+1) ; 
k 
i n t  f 2  ( i n t  v) ( 
r e t u r n  (v+1) ; 
J 
i n t  f 3  ( i n t  v) ( 
r e t u r n  (v+1) ; 
1 
Figure 12.8: Cloned version of Fig. 12.6 
12.1. BASIC CONCEPTS 
913 
an outer-inner region relationship. The only difference from the intraprocedural 
version is that, in the interprocedural case, a procedure region can be nested 
inside several different outer regions. 
The analysis consists of two parts: 
1. A bottom-up phase that computes a transfer function to summarize the 
effect of a procedure, and 
2. A top-down phase that propagates caller information to compute results 
of the callees. 
To get fully context-sensitive results, information from different calling contexts 
must propagate down to the callees individually. For a more efficient, but less 
precise calculation, information from all callers can be combined, using a meet 
operator, then propagated down to the callees. 
Example 12.8 : 
For constant propagation, each procedure is summarized by a 
transfer function specifying how it would propagate constants through its body. 
In Example 12.2, we can summarize f as a function that, given a constant c as 
an actual parameter to v, returns the constant c + 
1. Based on this information, 
the analysis would determine that t 
1, t2, and t 3  
have the constant values 1, 
244, and 244, respectively. Note that this analysis does not suffer the inaccuracy 
due to unrealizable call strings. 
Recall that Example 12.4 extends Example 12.2 by having g call f .  Thus, 
we could conclude that the transfer function for g is the same as the transfer 
function for f. Again we conclude that t 
1, 
t2, and t 3  
have the constant values 
1, 
244, and 244, respectively. 
Now, let us consider what is the value of parameter v in function f for 
Example 12.2. As a first cut, we can combine all the results for all calling 
contexts. Since v may have values 0 or 243, we can simply conclude that v is 
not a constant. This conclusion is fair, because there is no constant that can 
replace v in the code. 
If we desire more precise results, we can compute specific results for contexts 
of interest. Information must be passed down from the context of interest to 
determine the context-sensitive answer. This step is analogous to the top-down 
pass in region-based analysis. For example, the value of v is 0 at call site c l  
and 243 at sites c2 and c3. To get the advantage of constant propagation 
within f ,  
we need to capture this distinction by creating two clones, with the 
first specialized for input value 0 and the latter with value 243, as shown in 
Fig. 12.9. 
With Example 12.8, we see that, in the end, if we wish to compile the 
code differently in different contexts, we still need to clone the code. The 
difference is that in the cloning-based approach, cloning is performed prior to 
the analysis, based on the call strings. In the summary-based approach, the 
cloning is performed after the analysis, using the analysis results as a basis. 
914 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
f o r  ( i  = 0; i < n; i++) ( 
c l :  
t1 = fO(0); 
c2: 
t 2  = f 243 
(243) ; 
c3: 
t 3  = f243(243); 
X [ i ]  
= tl+t2+t3; 
1 
i n t  fO ( i n t  v) ( 
return ( I ) ;  
i n t  f243 ( i n t  v) ( 
r e t u r n  (244) ; 
3 
Figure 12.9: Result of propagating all possible constant arguments to the func- 
tion f 
Even if cloning is not applied, in the summary-based approach inferences about 
the effect of a called procedure are made accurately, without the problem of 
unrealizable paths. 
Instead of cloning a function, we could also inline the code. Inlining has the 
additional effect of eliminating the procedure-call overhead as well. 
We can handle recursion by computing the fixedpoint solution. In the pres- 
ence of recursion, we first find the strongly connected components in the call 
graph. In the bottom-up phase, we do not visit a strongly connected component 
unless all its successors have been visited. For a nontrivial strongly connected 
component, we iteratively compute the transfer functions for each procedure in 
the component until convergence is reached; that is, we iteratively update the 
transfer functions until no more changes occur. 
12.1.6 
Exercises for Section 12.1 
Exercise 12.1.1 : 
In Fig. 12.10 is a C program with two function pointers, p 
and q. N is a constant that could be less than or greater than 10. Note that 
the program results in an infinite sequence of calls, but that is of no concern 
for the purposes of this problem. 
a) Identify all the call sites in this program. 
b) For each call site, what can p point to? What can q point to? 
c) Draw the call graph for this program. 
! 
d) Describe all the call strings for f and g. 
12.1. BASIC CONCEPTS 
i n t  (*p)(int>; 
i n t  (*q) (int 
) ; 
i n t  f (int i )  ( 
i f  ( i  < 10) 
(p = &g; return (*q) ( i )  ; 
1 
else 
(p = &f 
; return (*p) ( i )  
; I  
i n t  g(int j) ( 
if ( j  < 10) 
(q = & f ;  return (*p)(j);) 
else 
(q = &g; return (*q) (j) ; 
3 
3 
void main() ( 
p = &f; 
q = k g ;  
(*p) ((*q) (N) 
) ; 
3 
Figure 12.10: Program for Exercise 12.1.1 
Exercise 12.1.2 : 
In Fig. 12.11 
is a function i d  
that is the "identity function7'; 
it returns exactly what it is given as an argument. We also see a code fragment 
consisting of a branch and following assignment that sums x + 
y. 
a) Examining the code, what can we tell about the value of x at the end? 
b) Construct the flow graph for the code fragment, treating the calls to i d  
as control flow. 
c) If we run a constant-propagation analysis, as in Section 9.4, on your flow 
graph from (b), what constant values are determined? 
d) What are all the call sites in Fig. 12.11? 
e) What are all the contexts in which i d  is called? 
f) Rewrite the code of Fig. 12.11 by cloning a new version of id for each 
context in which it is called. 
g) Construct the flow graph of your code from (f), 
treating the calls as control 
flow. 
916 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
i n t  i d ( i n t  x) ( return x;) 
. 
. 
. 
i f  (a == 1) ( x = id(2); y = id(3); ) 
e l s e  
( x = id(3); y = id(2); 3 
z = x+y; 
Figure 12.11: Code fragment for Exercise 12.1.2 
h) Perform a constant-propagation analysis on your flow graph from (g). 
What constant values are determined now? 
12.2 Why Interprocedural Analysis? 
Given how hard interprocedural analysis is, let us now address the important 
problem of why and when we wish to use interprocedural analysis. Although 
we used constant propagation to illustrate interprocedural analysis, this inter- 
procedural optimization is neither readily applicable nor particularly beneficial 
when it does occur. Most of the benefits of constant propagation can be ob- 
tained simply by performing intraprocedural analysis and inlining procedure 
calls of the most frequently executed sections of code. 
However, there are many reasons why interprocedural analysis is essential. 
Below, we describe several important applications of interprocedural analysis. 
12.2.1 Virtual Method Invocation 
As mentioned above, object-oriented programs have many small methods. If 
we only optimize one method at a time, then there are few opportunities for 
optimization. Resolving method invocation enables optimization. A language 
like Java dynamically loads its classes. As a result, we do not know at compile- 
time to which of (perhaps) many methods named m a use of "m" refers in an 
invocation such as x.m(). 
Many Java implementations use a just-in-time compiler to compile its byte- 
codes at run time. One common optimization is to profile the execution and 
determine which are the common receiver types. We can then inline the meth- 
ods that are most frequently invoked. The code includes a dynamic check on the 
type and executes the inlined methods if the run-time object has the expected 
type. 
Another approach to resolving uses of a method name m is possible as long 
as all the source code is available at compile time. Then, it is possible to 
perform an interprocedural analysis to determine the object types. If the type 
for a variable x turns out to be unique, then a use of x.m() can be resolved. 
12.2. WHY INTERPROCED URAL ANALYSIS? 
We know exactly what method m refers to in this context. In that case, we can 
in-line the code for this m, and the compiler does not even have to include a 
test for the type of x. 
12.2.2 Pointer Alias Analysis 
Even if we do not wish to perform interprocedural versions of the common data- 
flow analyses like reaching definitions, these analyses can in fact benefit from 
interprocedural pointer analysis. All the analyses presented in Chapter 9 apply 
only to local scalar variables that cannot have aliases. However, use of pointers 
is common, especially in languages like C. By knowing whether pointers can be 
aliases (can point to the same location), we can improve the accuracy of the 
techniques from Chapter 9. 
Example 12.9 : Consider the following sequence of three statements, which 
might form a basic block: 
Without knowing if p and q can point to the same location - 
that is, whether 
they can be aliases - 
we cannot conclude that x is equal to 1 
at the end of the 
block. 
12.2.3 Parallelization 
As discussed in Chapter 11, the most effective way to parallelize an applica- 
tion is to find the coarsest granularity of parallelism, such as that found in 
the outermost loops of a program. For this task, interprocedural analysis is 
of great importance. There is a significant difference between scalar optimiza- 
tions (those based on values of simple variables, as discussed in Chapter 9) 
and parallelization. In parallelization, just one spurious data dependence can 
render an entire loop not parallelizable, and greatly reduce the effectiveness 
of the optimization. Such amplification of inaccuracies is not seen in scalar 
optimizations. In scalar optimization, we only need to find the majority of 
the optimization opportunities. Missing one opportunity or two seldom makes 
much of a difference. 
12.2.4 Detection of Software Errors and Vulnerabilities 
Interprocedural analysis is not only important for optimizing code. The same 
techniques can be used to analyze existing software for many kinds of coding 
errors. These errors can render software unreliable; coding errors that hackers 
can exploit to take control of, or otherwise damage, a computer system can 
pose significant security vulnerability risks. 
918 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
Static analysis is useful in detecting occurrences of many common error 
patterns. For example, a data item must be guarded by a lock. As another 
example, disabling an interrupt in the operating system must be followed by a 
re-enabling of the interrupt. Since a significant source of errors is the incon- 
sistencies that span procedure boundaries, interprocedural analysis is of great 
importance. PREfix and Metal are two practical tools that use interprocedural 
analysis effectively to find many programming errors in large programs. Such 
tools find errors statically and can improve software reliability greatly. How- 
ever, these tools are both incomplete and unsound, in the sense that they may 
not find all errors, and not all reported warnings are real errors. Unfortunately, 
the interprocedural analysis used is sufficiently imprecise that, were the tools 
to report all potential errors, the large number of false warnings would render 
the tools unusable. Nevertheless, even though these tools are not perfect, their 
systematic use has been shown to greatly improve software reliability. 
When it comes to security vulnerabilities, it is highly desirable that we find 
all the potential errors in a program. In 2006, two of the "most popular" forms 
of intrusions used by hackers to compromise a system were 
1. Lack of input validation on Web applications: SQL injection is one of the 
most popular forms of such vulnerability whereby hackers gain control of 
a database by manipulating inputs accepted by web applications. 
2. Buffer overflows in C and C++ programs. Because C and C++ do not 
check if accesses to arrays are in bounds, hackers can write well-crafted 
strings into unintended areas and hence gain control of the program's 
execution. 
In the next section, we shall discuss how we can use interprocedural analysis to 
protect programs against such vulnerabilities. 
12.2.5 SQL Injection 
SQL injection refers to the vulnerability where hackers can manipulate user 
input to a Web application and gain unintended access to a database. For 
example, banks want their users to be able to make transactions online, provided 
they supply their correct password. A common architecture for such a system 
is to have the user enter strings into a Web form, and then to have those 
strings form part of a database query written in the SQL language. If systems 
developers are not careful, the strings provided by the user can alter the meaning 
of the SQL statement in unexpected ways. 
Example 12.10 : 
Suppose a bank offers its customers access to a relation 
AcctData(name, password, balance) 
That is, this relation is a table of triples, each consisting of the name of a 
customer, the password, and the balance of the account. The intent is that cus- 
tomers can see their account balance only if they provide both their name and 
12.2. W H Y  
INTERPROCED URAL 
ANALYSIS? 
919 
their correct password. Having a hacker see an account balance is not the worst 
thing that could occur, but this simple example is typical of more complicated 
situations where the hacker could execute payments from the account. 
The system might implement a balance inquiry as follows: 
1. Users invoke a Web form where they enter their name and password. 
2. The name is copied to a variable n and the password to a variable p. 
3. Later, perhaps in some other procedure, the following SQL query is exe- 
cuted: 
SELECT balance F
R
O
M
 AcctData 
W
H
E
R
E
 name = ' : n )  and password = ):p' 
For readers not familiar with SQL, this query says: "Find in the table AcctData 
a row with the first component (name) equal to the string currently in variable 
n and the second component (password) equal to the string currently in variable 
p; print the third component (balance) of that row." Note that SQL uses single 
quotes, not double quotes, to delimit strings, and the colons in front of n and 
p indicate that they are variables of the surrounding language. 
Suppose the hacker, who wants to find Charles Dickens' account balance, 
supplies the following values for the strings n and p: 
n = Charles Dickens' -- 
p = who cares 
The effect of these strange strings is to convert the query into 
SELECT balance F
R
O
M
 AcctData 
W
H
E
R
E
 name = 'Charles Dickens' --' and password = 'who cares' 
In many database systems -- 
is a comment-introducing token and has the effect 
of making whatever follows on that line a comment. As a result, the query now 
asks the database system to print the balance for every person whose name is 
'Charles Dickens', regardless of the password that appears with that name 
in a name-password-balance triple. That is, with comments eliminated, the 
query is: 
SELECT balance F
R
O
M
 AcctData 
W
H
E
R
E
 name = 'Charles Dickens' 
In Example 12.10, the "bad" strings were kept in two variables, which might 
be passed between procedures. However, in more realistic cases, these strings 
might be copied several times, or combined with others to form the full query. 
We cannot hope to detect coding errors that create SQL-injection vulnerabilities 
without doing a full interprocedural analysis of the entire program. 
920 
CHAPTER 12. INTERPROCED URAL ANALYSIS 
12.2.6 
Buffer Overflow 
A bufter overflow attack occurs when carefully crafted data supplied by the user 
writes beyond the intended buffer and manipulates the program execution. For 
example, a C program may read a string s from the user, and then copy it into 
a buffer b using the function call: 
If the string s is actually longer than the buffer b, then locations that are not 
part of b wilI have their values changed. That in itself will probably cause the 
program to malfunction or at least to produce the wrong answer, since some 
data used by the program will have been changed. 
But worse, the hacker who chose the string s can pick a value that will do 
more than cause an error. For example, if the buffer is on the run-time stack, 
then it is near the return address for its function. An insidiously chosen value 
of s 
may overwrite the return address, and when the function returns, it goes 
to a place chosen by the hacker. If hackers have detailed knowledge of the 
surrounding operating system and hardware, they may be able to execute a 
command that will give them control of the machine itself. In some situations, 
they may even have the ability to have the false return address transfer control 
to code that is part of the string s, thus allowing any sort of program to be 
inserted into the executing code. 
To prevent buffer overflows, every array-write operation must be statically 
proven to be within bounds, or a proper array-bounds check must be performed 
dynamically. Because these bounds checks need to be inserted by hand in C 
and C++ programs, it is easy to forget to insert the test or to get the test 
wrong. Heuristic tools have been developed that will check if at least some test, 
though not necessarily a correct test, has been performed before a strcpy is 
called. 
Dynamic bounds checking is unavoidable because it is impossible to deter- 
mine statically the size of users' input. All a static analysis can do is assure that 
the dynamic checks have been inserted properly. Thus, a reasonable strategy is 
to have the compiler insert dynamic bounds checking on every write, and use 
static analysis as a means to optimize away as many bounds check as possible. 
It is no longer necessary to catch every potential violation; moreover, we only 
need to optimize only those code regions that execute frequently. 
Inserting bounds checking into C programs is nontrivial, even if we do not 
mind the cost. A pointer may point into the middle of some array, and we do 
not know the extent of that array. Techniques have been developed to keep 
track of the extent of the buffer pointed to by each pointer dynamically. This 
information allows the compiler to insert array bounds checks for all accesses. 
Interestingly enough, it is not advisable to halt a program whenever a buffer 
overflow is detected. In fact, buffer overflows do occur in practice, and a pro- 
gram would likely fail if we disable all buffer overflows. The solution is to extend 
the size of the array dynamically to accommodate for the buffer overruns. 
12.3. A LOGICAL REPRESENTATION OF DATA FLOW 
921 
Interprocedural analysis can be used to speed up the cost of dynamic ar- 
ray bounds checks. For example, suppose we are interested only in catching 
buffer overflows involving user-input strings, we can use static analysis to de- 
termine which variables may hold contents provided by the user. Like SQL 
injection, being able to track an input as it is copied across procedures is useful 
in eliminating unnecessary bounds checks. 
12.3 A Logical Representation of Data Flow 
To this point, our representation of data-flow problems and solutions can be 
termed "set-theoretic." That is, we represent information as sets and compute 
results using operators like union and intersection. For instance, when we in- 
troduced the reaching-definitions problem in Section 9.2.4, we computed IN[B] 
and OUT[B] 
for a block B, and we described these as sets of definitions. We 
represented the contents of the block B by its gen and kill sets. 
To cope with the complexity of interprocedural analysis, we now introduce a 
more general and succinct notation based on logic. Instead of saying something 
like "definition D is in IN[B]," 
we shall use a notation like in(B, 
D) to mean 
the same thing. Doing so allows us to express succinct "rules7' about inferring 
program facts. It also allows us to implement these rules efficiently, in a way 
that generalizes the bit-vector approach to set-theoretic operations. Finally, 
the logical approach allows us to combine what appear to be several indepen- 
dent analyses into one, integrated algorithm. For example, in Section 9.5 we 
described partial-redundancy elimination by a sequence of four data-flow anal- 
yses and two other intermediate steps. In the logical notation, all these steps 
could be combined into one collection of logical rules that are solved simulta- 
neously. 
12.3.1 
Introduction to Datalog 
Datalog is a language that uses a Prolog-like notation, but whose semantics is 
far simpler than that of Prolog. To begin, the elements of Datalog are atoms 
of the form p(X1, 
X z , . .  
. , 
X,). Here, 
I. p is a predicate - 
a symbol that represents a type of statement such as 
"a definition reaches the beginning of a block." 
2. XI, 
X2,. 
. . , 
X, are terms such as variables or constants. We shall also 
allow simple expressions as arguments of a predicate.2 
A ground atom is a predicate with only constants as arguments. Every 
ground atom asserts a particular fact, and its value is either true or false. It 
2~ormally, 
such terms are built from function symbols and complicate the implementation 
of Datalog considerably. However, we shall use only a few operators, such as addition or 
subtraction of constants, in contexts that do not complicate matters. 
922 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
is often convenient to represent a predicate by a relation, or table of its true 
ground atoms. Each ground atom is represented by a single row, or tuple, of 
the relation. The columns of the relation are named by attributes, and each 
tuple has a component for each attribute. The attributes correspond to the 
components of the ground atoms represented by the relation. Any ground 
atom in the relation is true, and ground atoms not in the relation are false. 
Example 12.11 : 
Let us suppose the predicate in(B, 
D) means "definition D 
reaches the beginning of block B." Then we might suppose that, for a particular 
flow graph, in(bl, 
dl) is true, as are in(b2, 
dl) and in(b2, d2). We might also 
suppose that for this flow graph, all other in facts are false. Then the relation 
in Fig. 12.12 represents the value of this predicate for this flow graph. 
Figure 12.12: Representing the value of a predicate by a relation 
The attributes of the relation are B and D. The three tuples of the relation 
are (bl, dl), (b2, dl), and (b2, d2). 
We shall also see at times an atom that is really a comparison between 
variables and constants. An example would be X # Y or X = 10. In these 
examples, the predicate is really the comparison operator. That is, we can 
think of X = 10 as if it were written in predicate form: equals(X, 
10). There is 
an important difference between comparison predicates and others, however. A 
comparison predicate has its standard interpretation, while an ordinary pred- 
icate like in means only what it is defined to mean by a Datalog program 
(described next). 
A literal is either an atom or a negated atom. We indicate negation with 
the word N
O
T
 in front of the atom. Thus, N
O
T
 in(B, 
D) is an assertion that 
definition D does not reach the beginning of block B. 
12.3.2 Datalog Rules 
Rules are a way of expressing logical inferences. In Datalog, rules also serve to 
suggest how a computation of the true facts should be carried out. The form 
of a rule is 
H : - B 1 & B 2 & . - -  
&B, 
The components are as follows: 
H and B1, B2, 
. 
. . , 
B, are literals - 
either atoms or negated atoms. 
12.3. A LOGICAL REPRESENTATION OF DATA FLOW 
923 
Datalog Conventions 
We shall use the following conventions for Datalog programs: 
1. Variables begin with a capital letter. 
2. All other elements begin with lowercase letters or other symbols such 
as digits. These elements include predicates and constants that are 
arguments of predicates. 
H is the head and B1, B2, 
. . 
. , 
B, form the body of the rule. 
Each of the Bi7s 
is sometimes called a subgoal of the rule. 
We should read the : 
- symbol as "if." The meaning of a rule is "the head 
is true if the body is true." More precisely, we apply a rule to a given set of 
ground atoms as follows. Consider all possible substitutions of constants for 
the variables of the rule. If this substitution makes every subgoal of the body 
true (assuming that all and only the given ground atoms are true), then we can 
infer that the head with this substitution of constants for variables is a true 
fact. Substitutions that do not make all subgoals true give us no information; 
the head may or may not be true. 
A Datalog program is a collection of rules. This program is applied to "data," 
that is, to a set of ground atoms for some of the predicates. The result of the 
program is the set of ground atoms inferred by applying the rules until no more 
inferences can be made. 
Exarnple 12.12 : 
A simple example of a Datalog program is the computation 
of paths in a graph, given its (directed) edges. That is, there is one predicate 
edge(X, 
Y) that means "there is an edge from node X to node Y." Another 
predicate path(X, 
Y) means that there is a path from X to Y. The rules defining 
paths are: 
The first rule says that a single edge is a path. That is, whenever we replace 
variable X by a constant a and variable Y by a constant b, and edge(a, 
b) is 
true (i.e., there is an edge from node a to node b), then path(a, 
b) is also true 
(i.e., there is a path from a to b). The second rule says that if there is a path 
from some node X to some node Z, and there is also a path from Z 
to node Y, 
then there is a path from X to Y. This rule expresses "transitive closure." Note 
that any path can be formed by taking the edges along the path and applying 
the transitive closure rule repeatedly. 
924 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
For instance, suppose that the following facts (ground atoms) are true: 
edge(l,2), edge(2,3), and edge(3,4). Then we can use the first rule with 
three different substitutions to infer path(l,2), path(2,3), and path(3,4). As 
an example, substituting X = 1 and Y = 2 instantiates the first rule to be 
path(l,2) : - 
edge(l,2). Since edge(l,2) is true, we infer path(l,2). 
With these three path facts, we can use the second rule several times. If 
we substitute X = 1, Z 
= 2, and Y = 3, we instantiate the rule to be 
path(l,3) : - 
path(l,2) & path(2,3). Since both subgoals of the body have 
been inferred, they are known to be true, so we may infer the head: path(l,3). 
Then, the substitution X = 1, Z 
= 3, and Y = 4 lets us infer the head 
path(l,4); that is, there is a path from node 1 
to node 4. 
12.3.3 Intensional and Extensional Predicates 
It is conventional in Datalog programs to distinguish predicates as follows: 
1. EDB, or extensional database, predicates are those that are defined a- 
priori. That is, their true facts are either given in a relation or table, or 
they are given by the meaning of the predicate (as would be the case for 
a comparison predicate, e.g.). 
2. IDB, or intensional database, predicates are defined only by the rules. 
A predicate must be IDB or EDB, and it can be only one of these. As a result, 
any predicate that appears in the head of one or more rules must be an IDB 
predicate. Predicates appearing in the body can be either IDB or EDB. For 
instance, in Example 12.12, edge is an EDB predicate and path is an IDB 
predicate. Recall that we were given some edge facts, such as edge(l,2), but 
the path facts were inferred by the rules. 
When Datalog programs are used to express data-flow algorithms, the EDB 
predicates are computed from the flow graph itself. IDB predicates are then 
expressed by rules, and the data-flow problem is solved by inferring all possible 
IDB facts from the rules and the given EDB facts. 
Example 12.13 : 
Let us consider how reaching definitions might be expressed 
in Datalog. First, it makes sense to think on a statement level, rather than 
a block level; that is, the construction of gen and kill sets from a basic block 
will be integrated with the computation of the reaching definitions themselves. 
Thus, the block bl suggested in Fig. 12.13 is typical. Notice that we identify 
points within the block numbered 0,1, . 
. . , 
n, if n is the number of statements 
in the block. The ith definition is "at" point i, and there is no definition at 
point 0. 
A point in the program must be represented by a pair (b, 
n), where b is a 
block name and n is an integer between 0 and the number of statements in 
block b. Our formulation requires two EDB predicates: 
12.3. A LOGICAL REPRESENTATION OF DATA FLOW 
Figure 12.13: A basic block with points between statements 
1. def(B, N, 
X) is true if and only if the Nth statement in block B may define 
variable X .  For instance, in Fig. 12.13 def(bl, 1, 
x) is true, def(bl,3, 
x) is 
true, and def(bl, 2, Y) is true for every possible variable Y that p may 
point to at that point. For the moment, we shall assume that Y can be 
any variable of the type that p points to. 
2. succ(B, 
N, 
C) is true if and only if block C is a successor of block B in 
the flow graph, and B has N statements. That is, control can flow from 
the point N of B to the point 0 of C. For instance, suppose that b2 is 
a predecessor of block bl in Fig. 12.13, and ba has 5 statements. Then 
succ(bz, 
5, bl) is true. 
There is one IDB predicate, rd(B, 
N, C, 
M, 
X). It is intended to be true if 
and only if the definition of variable X at the Mth statement of block C reaches 
the point N in block B. The rules defining predicate rd are in Fig. 12.14. 
Figure 12.14: Rules for predicate rd 
Rule (1) says that if the Nth statement of block B defines X, then that 
definition of X reaches the Nth point of B (i.e., the point immediately after 
the statement). This rule corresponds to the concept of "gen" in our earlier, 
set-theoretic formulation of reaching definitions. 
Rule (2) represents the idea that a definition passes through a statement 
unless it is "killed," and the only way to kill a definition is to redefine its 
variable with 100% certainty. In detail, rule (2) says that the definition of 
variable X from the Mth statement of block C reaches the point N of block B 
if 
a) it reaches the previous point N - 
1 
of B, and 
926 
CHAPTER 12. INTERPROCED URAL ANALYSIS 
b) there is at least one variable Y ,  
other than X ,  
that may be defined at the 
Nth statement of B. 
Finally, rule (3) expresses the flow of control in the graph. It says that the 
definition of X at the Mth statement of block C reaches the point 0 of B if 
there is some block D with N statements, such that the definition of X reaches 
the end of D, and B is a successor of D. 
The EDB predicate succ from Example 12.13 clearly can be read off the flow 
graph. We can obtain deffrom the flow graph as well, if we are conservative and 
assume a pointer can point anywhere. If we want to limit the range of a pointer 
to variables of the appropriate type, then we can obtain type information from 
the symbol table, and use a smaller relation def. An option is to make def 
an IDB predicate and define it by rules. These rules will use more primitive 
EDB predicates, which can themselves be determined from the flow graph and 
symbol table. 
Example 12.14 : 
Suppose we introduce two new EDB predicates: 
1. assign(B, 
N, 
X )  is true whenever the Nth statement of block B has X 
on the left. Note that X can be a variable or a simple expression with an 
1-value, like *p. 
2. type(X, 
T) is true if the type of X is T. Again, X can be any expression 
with an 1-value, and T can be any expression for a legal type. 
Then, we can write rules for def, making it an IDB predicate. Figure 12.15 
is an expansion of Fig. 12.14, with two of the possible rules for def. Rule (4) 
says that the Nth statement of block B defines X ,  if X is assigned by the Nth 
statement. Rule (5) says that X can also be defined by the Nth statement of 
block B if that statement assigns to *P, and X is any of the variables of the 
type that P points to. Other kinds of assignments would need other rules for 
def. 
As an example of how we would make inferences using the rules of Fig. 12.15, 
let us re-examine the block bl of Fig. 12.13. The first statement assigns a 
value to variable x, so the fact assign(bl, 1, 
x) would be in the EDB. The third 
statement also assigns to x, so assign(b17 
3, x) is another EDB fact. The second 
statement assigns indirectly through p, so a third EDB fact is assign(b172, 
*p). 
Rule (4) then allows us to infer def(bl,l, 
x) and def(bl,3, x). 
Suppose that p is of type pointer-to-integer (*int), and x and y are integers. 
Then we may use rule (5), with B = 
b17 
N = 
2, P = 
p, T = 
int, and X equal to 
either x or y, to infer def(bl, 2, x) and def(b17 
2, y). Similarly, we can infer the 
same about any other variable whose type is integer or coerceable to an integer. 
12.3. A LOGICAL REPRESENTATION OF DATA FLOW 
Figure 12.15: Rules for predicates rd and def 
12.3.4 
Execution of Datalog Programs 
Every set of Datalog rules defines relations for its IDB predicates, as a function 
of the relations that are given for its EDB predicates. Start with the assumption 
that thg IDB relations are empty (i.e., the IDB predicates are false for all 
possible arguments). Then, repeatedly apply the rules, inferring new facts 
whenevqr the rules require us to do so. When the process converges, we are 
done, a+d the resulting IDB relations form the output of the program. This 
process is formalized in the next algorithm, which is similar to the iterative 
algorithps discussed in Chapter 9. 
~1~oritjhr.n 
12.15 : 
Simple evaluation of Datalog programs. 
INPUT: A Datalog program and sets of facts for each EDB predicate. 
I 
OUTPUIT: Sets of facts for each IDB predicate. 
METH D: For each predicate p in the program, let Rp be the relation of facts 
0 
true for that predicate. If p is an EDB predicate, then Rp is the set of 
en for that predicate. If p is an IDB predicate, we shall compute Rp. 
the algorithm in Fig. 12.16. 
Example 12.16 : 
The program in Example 12.12 computes paths in a graph. 
To applj. Algorithm 12.15, we start with EDB predicate edge holding all the 
edges of) 
the graph and with the relation for path empty. On the first round, 
rule (2) ields nothing, since there are no path facts. But rule (1) causes all the 
edge fac ! 
s to become path facts as well. That is, after the first rouqd, we know 
path(a, 8) if and only if there is an edge from a to b. 
928 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
for (each IDB predicate p) 
Rp = 0; 
while (changes to any Rp occur) { 
consider all possible substitutions of constants for 
variables in all the rules; 
determine, for each substitution, whether all the 
subgoals of the body are true, using the current 
Rp's to determine truth of EDB and IDB predicates; 
if (a substitution makes the body of a rule true) 
add the head to R, if q is the head predicate; 
Figure 12.16: Evaluation of Datalog programs 
On the second round, rule (1) 
yields no new paths facts, because the EDB 
relation edge never changes. However, now rule (2) lets us put together two 
paths of length 1 
to make paths of length 2. That is, after the second round, 
path(a, 
b) is true if and only if there is a path of length 1 or 2 from a to b. 
Similarly, on the third round, we can combine paths of length 2 or less to 
discover all paths of length 4 or less. On the fourth round, we discover paths of 
length up to to 8, and in general, after the ith round, path(a, 
b) is true if and 
only if there is a path from a to b of length 2i-1 or less. 
1
2
.
3
.
5
 Incremental Evaluation of Datalog Programs 
There is an efficiency enhancement of Algorithm 12.15 possible. Observe that a 
new IDB fact can only be discovered on round i if it is the result of substituting 
constants in a rule, such that at least one of the subgoals becomes a fact that 
was just discovered on round i - 
1. The proof of that claim is that if all the facts 
among the subgoals were known at round i - 
2, then the "new" fact would have 
been discovered when we made the same substitution of constants on round 
i - 
1. 
To take advantage of this observation, introduce for each IDB predicate p 
a predicate newP that will hold only the newly discovered p-facts from the 
previous round. Each rule that has one or more IDB predicates among its 
subgoals is replaced by a collection of rules. Each rule in the collection is 
formed by replacing exactly one occurrence of some IDB predicate q in the 
body by newQ. Finally, for all rules, we replace the head predicate h by newH. 
The resulting rules are said to be in incremental form. 
The relations for each IDB predicate p accumulates all the p-facts, as in 
Algorithm 12.15. In one round, we 
1. Apply the rules to evaluate the newP predicates. 
12.3. A LOGICAL REPRESENTATION OF DATA FLOW 
Incremental Evaluation of Sets 
It is also possible to solve set-theoretic data-flow problems incrementally. 
For example, in reaching definitions, a definition can only be newly dis- 
covered to be in IN[B] 
on the ith round if it was just discovered to be 
in OUT[P] 
for some predecessor P of B. The reason we do not generally 
try to solve such data-flow problems incrementally is that the bit-vector 
implementation of sets is so efficient. It is generally easier to fly through 
the complete vectors than to decide whether a fact is new or not. 
2. Then, subtract p from newP, to make sure the facts in newP are truly 
new. 
3. Add the facts in newP to p. 
4. Set all the newX relations to Q) for the next round. 
These ideas will be formalized in Algorithm 12.18. However, first, we shall give 
an example. 
Example 12.17 : 
Consider the Datalog program in Example 12.12 again. The 
incremental form of the rules is given in Fig. 12.17. Rule (I) does not change, 
except in the head because it has no IDB subgoals in the body. However, 
rule (2), with two IDB subgoals, becomes two different rules. In each rule, one 
of the occurrences of path in the body is replaced by newpath. Together, these 
rules enforce the idea that at least one of the two paths concatenated by the 
rule must have been discovered on the previous round. 
Figure 12.17: Incremental rules for the path Datalog program 
Algorithm 12.18 : 
Incremental evaluation of Datalog programs. 
INPUT: A Datalog program and sets of facts for each EDB predicate. 
OUTPUT: Sets of facts for each IDB predicate. 
930 
CHAPTER 12. INTERPROCED URAL ANALYSIS 
METHOD: For each predicate p in the program, let Rp be the relation of facts 
that are true for that predicate. If p is an EDB predicate, then Rp is the set of 
facts given for that predicate. If p is an IDB predicate, we shall compute Rp. 
In addition, for each IDB predicate p, let RnewP 
be a relation of "new" facts 
for predicate p. 
1. Modify the rules into the incremental form described above. 
2. Execute the algorithm ih Fig. 12.18. 
for (each IDB predicate p) { 
Rp = 0; 
Rnew~ 
= 0; 
1 
repeat { 
consider all possible substitutions of constants for 
variables in all the rules; 
determine, for each substitution, whet 
her all the 
subgoals of the body are true, using the current 
Rp7s 
and RneWP's 
to determine truth of EDB 
and IDB predicates; 
if (a substitution makes the body of a rule true) 
add the head to RnewH, 
where h is the head 
predicate; 
for (each predicate p) { 
Rnew~ 
= Rnew~ 
- 
Rp; 
Rp = Rp U Rnew~; 
1 
} until (all RneWp's 
are empty); 
Figure 12.18: Evaluation of Dat 
alog programs 
12.3.6 Problematic Datalog Rules 
There are certain Datalog rules or programs that technically have no meaning 
and should not be used. The two most important risks are 
1. Unsafe rules: those that have a variable in the head that does not appear 
in the body in a way that constrains that variable to take on only values 
that appear in the EDB. 
2. Unstratified programs: sets of rules that have a recursion involving a nega- 
t 
ion. 
We shall elaborate on each of these risks. 
12.3. A LOGICAL REPRESENTATION OF DATA FLOW 
Rule Safety 
Any variable that appears in the head of a rule must also appear in the body. 
Moreover, that appearance must be in a subgoal that is an ordinary IDB or 
EDB atom. It is not acceptable if the variable appears only in a negated atom, 
or only in a comparison operator. The reason for this policy is to avoid rules 
that let us infer an infinite number of facts. 
Example 12.19 : 
The rule 
p(X, 
Y) : 
- q(Z) & N
O
T
 r(X) & X # Y 
is unsafe for two reasons. Variable X appears only in the negated subgoal 
r(X) and the comparison X # Y. Y appears only in the comparison. The 
consequence is that p is true for an infinite number of pairs (X, 
Y), as long as 
r(X) is false and Y is anything other than X. 
Stratified Datalog 
In order for a program to make sense, recursion and negation must be separated. 
The formal requirement is as follows. We must be able to divide the IDB 
predicates into strata, so that if there is a rule with head predicate p and a 
subgoal of the form N
O
T
 q ( . . . ) ,  
then q is either EDB or an IDB predicate in 
a lower stratum than p. As long as this rule is satisfied, we can evaluate the 
strata, lowest first, by Algorithm 12.15 or 12.18, and then treat the relations 
for the IDB predicates of that strata as if they were EDB for the computation 
of higher strata. However, if we violate this rule, then the iterative algorithm 
may fail to converge, as the next example shows. 
Example 12.20 : Consider the Datalog program consisting of the one rule: 
Suppose e is an EDB predicate, and only e(1) is true. Is p(1) true? 
This program is not stratified. Whatever stratum we put p in, its rule has 
a subgoal that is negated and has an IDB predicate (namely p itself) that is 
surely not in a lower stratum than p. 
If we apply the iterative algorithm, we start with Rp = 0, so initially, the 
answer is "no; p(1) is not true." However, the first iteration lets us infer p(l), 
since both e(1) and N
O
T
 p(1) are true. But then the second iteration tells us 
p(1) is false. That is, substituting 1 
for X in the rule does not allow us to infer 
p(l), since subgoal N
O
T
 p(1) is false. Similarly, the third iteration says p(1) is 
true, the fourth says it is false, and so on. We conclude that this unstratified 
program is meaningless, and do not consider it a valid program. 
932 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
12.3.7 Exercises for Section 12.3 
! Exercise 1
2
.
3
.
1
 : In this problem, we shall consider a reaching-definitions 
data-flow analysis that is simpler than that in Example 12.13. Assume that each 
statement by itself is a block, and initially assume that each statement defines 
exactly one variable. The EDB predicate pred(I, J )  means that statement I is 
a predecessor of statement J .  The EDB predicate defines(1, X )  means that the 
variable defined by statement I 
is X. We shall use IDB predicates in(I, 
D) and 
out(I, 
D) to mean that definition D reaches the beginning or end of statement 
I, 
respectively. Note that a definition is really a statement number. Fig. 12.19 
is a datalog program that expresses the usual algorithm for computing reaching 
definitions. 
1) kill(I, 
D) 
: 
- 
defines 
(I, 
X) & defines 
(D, 
X )  
Figure 12.19: Datalog program for a simple reaching-definitions analysis 
Notice that rule (I) says that a statement kills itself, but rule (2) assures 
that a statement is in its own "out set" anyway. Rule (3) is the normal transfer 
function, and rule (4) allows confluence, since I can have several predecessors. 
Your problem is to modify the rules to handle the common case where a 
definition is ambiguous, e.g., an assignment through a pointer. In this situation, 
defines(1, 
X )  may be true for several different X's and one I. A definition is 
best represented by a pair (D, 
X), where D is a statement, and X is one of 
the variables that may be defined at D. As a result, in and out become three- 
argument predicates; e.g., in(1, 
D, 
X) means that the (possible) definition of X 
at statement D reaches the beginning of statement I .  
Exercise 12.3.2: Write a Datalog program analogous to Fig. 12.19 to com- 
pute available expressions. In addition to predicate defines, use a predicate 
eval 
(I, 
X, 0, 
Y) that says statement I 
causes expression XOY to be evaluated. 
Here, 0 is the operator in the expression, e.g., +. 
Exercise 1
2
.
3
.
3
 
: 
Write a Datalog program analogous to Fig. 12.19 
to compute 
live variables. In addition to predicate defines, assume a predicate use(I, 
X) 
that says statement I uses variable X. 
Exercise 12.3.4 
: In Section 9.5, we defined a data-flow calculation that in- 
volved six concepts: anticipated, available, earliest, postponable, latest, and 
used. Suppose. we had written a Datalog program to define each of these in 
12.4. A SIMPLE POINTER-ANALYSIS ALGORITHM 
933 
terms of EDB concepts derivable from the program (e.g., gen and kill infor- 
mation) and others of these six concepts. Which of the six depend on which 
others? Which of these dependences are negated? Would the resulting Datalog 
program be stratified? 
Exercise 12.3.5 : 
Suppose that the EDB predicate edge(X, 
Y) consists of the 
following facts: 
a) Simulate the Datalog program of Example 12.12 on this data, using the 
simple evaluation strategy of Algorithm 12.15. Show the path facts dis- 
covered at each round. 
b) Simulate the Datalog program of Fig. 12.17 on this data, as part of the 
incremental evaluation strategy of Algorithm 12.18. Show the path facts 
discovered at each round. 
Exercise 12.3.6 : 
The following rule 
is part of a larger Datalog program P. 
a) Identify the head, body, and subgoals of this rule. 
b) Which predicates are certainly IDB predicates of program P? 
! 
c) Which predicates are certainly EDB predicates of P? 
d) Is the rule safe? 
e) Is P stratified? 
Exercise 12.3.7 
: 
Convert the rules of Fig. 12.14 to incremental form. 
12.4 A Simple Pointer- 
Analysis Algorithm 
In this section, we begin the discussion of a very simple flow-insensitive pointer- 
alias analysis assuming that there are no procedure calls. We shall show in 
subsequent sections how to handle procedures first context insensitively, then 
context sensitively. Flow sensitivity adds a lot of complexity, and is less im- 
portant to context sensitivity for languages like Java where methods tend to be 
small. 
The fundamental question that we wish to ask in pointer-alias analysis is 
whether a given pair of pointers may be aliased. One way to answer this question 
is to compute for each pointer the answer to the question LLwhat 
objects can 
this pointer point to?" If two pointers can point to the same object, then the 
pointers may be aliased. 
934 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
12.4.1 Why is Pointer Analysis Difficult 
Pointer-alias analysis for C programs is particularly difficult, because C pro- 
grams can perform arbitrary computations on pointers. In fact, one can read in 
an integer and assign it to a pointer, which would render this pointer a potential 
alias of all other pointer variables in the program. Pointers in Java, known as 
references, are much simpler. No arithmetic is allowed, and pointers can only 
point to the beginning of an object. 
Pointer-alias analysis must be interprocedural. Without interprocedural 
analysis, one must assume that any method called can change the contents of 
all accessible pointer variables, thus rendering any intraprocedural pointer-alias 
analysis ineffective. 
Languages allowing indirect function calls present an additional challenge 
for pointer-alias analysis. In C, one can call a function indirectly by calling a 
dereferenced function pointer. We need to know what the function pointer can 
point to before we can analyze the function called. And clearly, after analyzing 
the function called, one may discover more functions that the function pointer 
can point to, and therefore the process needs to be iterated. 
While most functions are called directly in C, virtual methods in Java cause 
many invocations to be indirect. Given an invocation x . 
m
(
)
 in a Java program, 
there may be many classes to which object x might belong and that have a 
method named m. The more precise our knowledge of the actual type of x, the 
more precise our call graph is. Ideally, we can determine at compile time the 
exact class of x and thus know exactly which method m refers to. 
Example 12.2 
1 : 
Consider the following sequence of Java statements: 
Object o; 
o = new String(); 
n = o.length(); 
Here o is declared to be an Object. Without analyzing what o refers to, all 
possible methods called "length" declared for all classes must be considered as 
possible targets. Knowing that o points to a String 
will narrow interprocedural 
analysis to precisely the method declared for String. 
It is possible to apply approximations to reduce the number of targets. For 
example, statically we can determine what are all the types of objects created, 
and we can limit the analysis to those. But we can be more accurate if we can 
discover the call graph on the fly, based on the points-to analysis obtained at 
the same time. More accurate call graphs lead not only to more precise results 
but also can reduce greatly the analysis time otherwise needed. 
Points-to analysis is complicated. It is not one of those "easy" data flow 
problems where we only need to simulate the effect of going around a loop of 
statements once. Rather, as we discover new targets for a pointer, all statements 
assigning the contents of that pointer to another pointer need to be re-analyzed. 
12.4. A SIMPLE POINTER-ANALYSIS ALGORITHM 
For simplicity, we shall focus mainly on Java. We shall start with flow- 
insensitive and context-insensitive analysis, assuming for now that no methods 
are called in the program. Then, we describe how we can discover the call graph 
on the fly as the points-to results are computed. Finally, we describe one way 
of handling context sensitivity. 
12.4.2 A Model for Pointers and References 
Let us suppose that our language has the following ways to represent and ma- 
nipulate references: 
1. Certain program variables are of type "pointer to T" or "reference to T," 
where T is a type. These variables are either static or live on the run-time 
stack. We call them simply variables. 
2. There is a heap of objects. All variables point to heap objects, not to 
other variables. These objects will be referred to as heap objects. 
3. A heap object can have fields, and the value of a field can be a reference 
to a heap object (but not to a variable). 
Java is modeled well by this structure, alnd we shall use Java syntax in examples. 
Note that C is modeled less well, since pointer variables can point to other 
pointer variables in C, and in principle, any C value can be coerced into a 
pointer. 
Since we are performing an insensitive analysis, we only need to assert that 
a given variable v can point to a given heap object h; we do not have to address 
the issue of where in the program v can point to h, or in what contexts v can 
point to h. Note, however, that variables can be named by their full name. In 
Java, this name can incorporate the module, class, method, and block within 
a method, as well as the variable name itself. Thus, we can distinguish many 
variables that have the same identifier. 
Heap objects do not have names. Approximation often is used to name the 
objects, because an unbounded number of objects may be created dynamically. 
One convention is to refer to objects by the statement at which they are created. 
As a statement can be executed many times and create a new object each time, 
an assertion like "v can point to h" really means "v can point to one or more 
of the objects created at the statement labeled h." 
The goal of the analysis is to determine what each variable and each field 
of each heap object can point to. We refer to this as a points-to analysis; 
two pointers are aliased if their points-to sets intersect. We describe here an 
inclusion-based analysis; that is, a statement such as v = w causes variable v to 
point to all the objects w points to, but not vice versa. While this approach may 
seem obvious, there are other alternatives to how we define points-to analysis. 
For example, we can define an equivalence-based analysis such that a statement 
like v = w would turn variables v and w into one equivalence class, pointing 
936 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
to all the variables that each can point to. While this formulation does not 
approximate aliases well, it provides a quick, and often good, answer to the 
question of which variables point to the same kind of objects. 
12.4.3 Flow Insensitivity 
We start by showing a very simple example to illustrate the effect of ignoring 
control flow in points-to analysis. 
Example 12.22: In Fig. 12.20, three objects, h, i, and j, are created and 
assigned to variables a, b, and c, respectively. Thus, surely a points to h, b 
points to i, and c points to j by the end of line (3). 
1) h: 
a = n e w O b j e c t ( ) ;  
2) 
i: b = new Object(); 
3) 
j :  
c = new Object(); 
4 
a = b; 
5 
1 
b = c; 
6) 
c = a ;  
Figure 12.20: Java code for Example 12.22 
If you follow the statements (4) through (6), you discover that after line (4) 
a points only to i. After line (5), b points only to j 
, 
and after line (6), c points 
onlytoi. 
The above analysis is flow sensitive because we follow the control flow and 
compute what each variable can point to after each statement. In other words, 
in addition to considering what points-to information each statement "gener- 
ates 
," we also account for what points-to information each statement "kills." 
For instance, the statement b = c ; 
kills the previous fact "b points to j" and 
generates the new relationship "b points to what c points to.?' 
A flow-insensitive 
analysis ignores the control flow, which essentially assumes 
that every statement in the program can be executed in any order. It computes 
only one global points-to map indicating what each variable can possibly point 
to at any point of the program execution. If a variable can point to two different 
objects after two different statements in a program, we simply record that it can 
point to both objects. In other words, in flow-insensitive 
analysis, an assignment 
does not "kill7' 
any points-to relations but can only "generate7' 
more points-to 
relations. To compute the flow-insensitive 
results, we repeatedly add the points- 
to effects of each statement on the points-to relationships until no new relations 
are found. Clearly, lack of flow sensitivity weakens the analysis results greatly, 
but it tends to reduce the size of the representation of the results and make the 
algorithm converge faster. 
12.4. A SIMPLE POINTER-ANALYSIS ALGORITHM 
937 
Example 12.23 : 
Returning to Example 12.22, lines (1) 
through (3) again tell 
us a can point to h; b can point to i, and c can point to j. With lines (4) 
and ( 5 ) ,  
a can point to both h and i, and b can point to both i and j. With 
line (6), c can point to h, i, and j. This information affects line ( 5 ) ,  
which in 
turn affects line (4), In the end, we are left with the useless conclusion that 
anything can point to anything. 
O 
12.4.4 The Formulation in Datalog 
Let us now formalize a flow-insensitive pointer-alias analysis based on the dis- 
cussion above. We shall ignore procedure calls for now and concentrate on the 
four kinds of statements that can affect pointers: 
1. Object creation. h: 
T 
v = new T O  
; This statement creates a new heap 
object, and variable v can point to it. 
2. Copy statement. v = w; 
Here, v and w are variables. The statement 
makes v point to whatever heap object w currently points to; i.e., w is 
copied into v. 
3. Field store. v. 
f = w; 
The type of object that v points to must have a 
field f ,  
and this field must be of some reference type. Let v point to heap 
object h, and let w point to g .  This statement makes the field f, 
in h 
now point to g. Note that the variable v is unchanged. 
4. Field load. v = w. f ; Here, w is a variable pointing to some heap object 
that has a field f ,  
and f points to some heap object h. The statement 
makes variable v point to h. 
Note that compound field accesses in the source code such as v = w. 
f 
.g 
will be broken down into two primitive field-load statements: 
Let us now express the analysis formally in Datalog rules. First, there are 
only two IDB predicates we need to compute: 
1. ~ t s ( V ,  
H) means that variable V can point to heap object H. 
2. hpts(H, 
F, 
G) means that field F of heap object H can point to heap 
object G. 
The EDB relations are constructed from the program itself. Since the 
location of statements in a program is irrelevant when the analysis is flow- 
insensitive, we only have to assert in the EDB the existence of statements that 
have certain forms. In what follows, we shall make a convenient simplification. 
Instead of defining EDB relations to hold the information garnered from the 
938 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
program, we shall use a quoted statement form to suggest the EDB relation 
or relations that represent the existence of such a statement. For example, 
"H : T V = new T" is an EDB fact asserting that at statement H there is 
an assignment that makes variable V point to a new object of type T. We as- 
sume that in practice, there would be a corresponding EDB relation that would 
be populated with ground atoms, one for each statement of this form in the 
program. 
With this convention, all we need to write the Datalog program is one rule 
for each of the four types of statements. The program is shown in Fig. 12.21. 
Rule (1) 
says that variable V can point to heap object H if statement H is an 
assignment of a new object to V. Rule (2) says that if there is a copy statement 
V = W, and W can point to H ,  
then V can point to H. 
1) 
pts(V,H) 
:- " H :  T V  =newT" 
Figure 12.21: Datalog program for flow-insensitive 
pointer analysis 
Rule (3) says that if there is a statement of the form V . 
F = W, W can point 
to G, and V can point to H ,  then the F field of W 
can point to G. Finally, 
rule (4) says that if there is a statement of the form V = W. F, W can point to 
G, and the F field of G can point to H ,  
then V can point to H. Notice that pts 
and hpts are mutually recursive, but this Datalog program can be evaluated by 
either of the iterative algorithms discussed in Section 12.3.4. 
12.4.5 Using Type Information 
Because Java is type safe, variables can only point to types that are cornpat- 
ible to the declared types. For example, assigning an object belonging to a 
superclass of the declared type of a variable would raise a run-time exception. 
Consider the simple example in Fig. 12.22, where S is a subclass of T. This 
program will generate a run-time exception if p is true, because a cannot be 
assigned an object of class T. Thus, statically we can conclude that because of 
the type restriction, a can only point to h and not g. 
12.4. A SIMPLE POINTER-ANALYSIS ALGORITHM 
S a; 
T b; 
if (p) ( 
g 
: 
b = new T O ;  
) else 
h: 
b = new S O ;  
3 
a = b; 
Figure 12.22: Java program with a type error 
Thus, we introduce to our analysis three EDB predicates that reflect impor- 
tant type information in the code being analyzed. We shall use the following: 
1. vType(V, T) says that variable V is declared to have type T. 
2. hType(H, 
T) says that heap object H is allocated with type T. The type 
of a created object may not be known precisely if, for example, the object 
is returned by a native method. Such types are modeled conservatively 
as all possible types. 
3. assignable(T, S) means that an object of type S can be assigned to a 
variable with the type T. This information is generally gathered from the 
declaration of subtypes in the program, but also incorporates information 
about the predefined classes of the language. assignable(T, T) is always 
true. 
We can modify the rules from Fig. 12.21 to allow inferences only if the 
variable assigned gets a heap object of an assignable type. The rules are shown 
in Fig. 12.23. 
The first modification is to rule (2). The last three subgoals say that we can 
only conclude that V can point to H if there are types T and S that variable V 
and heap object H may respectively have, such that objects of type S can be 
assigned to variables that are references to type T. A similar additional restric- 
tion has been added to rule (4). Notice that there is no additional restriction 
in rule (3) because all stores must go through variables. Any type restriction 
would only catch one extra case, when the base object is a null constant. 
12.4.6 
Exercises for Section 12.4 
Exercise 12.4.1 : In Fig. 12.24, h and g are labels used to represent newly 
created objects, and are not part of the code. You may assume that objects of 
type T have a field f. 
Use the Datalog rules of this section to infer all possible 
pts and hpts facts. 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
1) 
pts(V,H) 
:- " H :  T V  =newTV 
Figure 12.23: Adding type restrictions to flow-insensitive pointer analysis 
h: T a = new TO; 
g: T  b  = new T O ;  
T  c  = a ;  
a.f = b; 
b.f = c; 
T  d = c.f; 
Figure 12.24: Code for Exercise 12.4.1 
! 
Exercise 12.4.2 : 
Applying the algorithm of this section to the code 
h: T a = new TO; 
g: 
b = n e w T ( ) ;  
T c = a ;  
would infer that both a and b can point to h and g. Had the code been written 
h: T  a  = new TO; 
g: 
b = n e w T ( ) ;  
T c = b ;  
we would infer accurately that a can point to h, and b and c can point to 
g. Suggest an intraprocedural data-flow analysis that can avoid this kind of 
inaccuracy. 
12.5. CONTEXT-INSENSITIVE INTERPROCED URAL ANALYSIS 
941 
t p ( t  x
)
 .C 
h: T a = new T; 
a.f = x; 
return a; 
1 
void main() ( 
g: T b = new T; 
b = p(b); 
b = b . f ;  
3 
Figure 12.25: Example code for pointer analysis 
! 
Exercise 12.4.3 : 
We can extend the analysis of this section to be interproce- 
dural if we simulate call and return as if they were copy operations, as in rule (2) 
of Fig. 12.21. That is, a call copies the actuals to their corresponding formals, 
and the return copies the variable that holds the return value to the variable 
that is assigned the result of the call. Consider the program of Fig. 12.25. 
a) Perform an insensitive analysis on this code. 
b) Some of the inferences made in (a) are actually "bogus," in the sense that 
they do not represent any event that can occur at run-time. The problem 
can be traced to the multiple assignments to variable b. Rewrite the code 
of Fig. 12.25 so that no variable is assigned more than once. Rerun the 
analysis and show that each inferred pts and hpts fact can occur at run 
time. 
12.5 Context-Insensitive Interprocedural 
Analysis 
We now consider method invocations. We first explain how points-to analysis 
can be used to compute a precise call graph, which is useful in computing precise 
points-to results. We then formalize on-the-fly call-graph discovery and show 
how Datalog can be used to describe the analysis succinctly. 
1
2
.
5
.
1
 Effects of a Method Invocation 
The effects of a method call such as x = y. n(z) in Java on the points-to rela- 
tions can be computed as follows: 
1. Determine the type of the receiver object, which is the object that y points 
to. Suppose its type is t. Let r
n
 
be the method named n in the narrowest 
CHAPTER 12. INTERPROCED 
URAL ANALYSIS 
superclass of t that has a method named n. Note that, in general, which 
method is invoked can only be determined dynamically. 
2. The formal parameters of m are assigned the objects pointed to by the ac- 
tual parameters. The actual parameters include not just the parameters 
passed in directly, but also the receiver object itself. Every method invo- 
cation assigns the receiver object to the t h i s  ~ariable.~ 
We refer to the 
t h i s  variables as the 0th formal parameters of methods. In x = y . 
n (2) 
, 
there are two formal parameters: the object pointed to by y is assigned 
to variable this, and the object pointed to by x is assigned to the first 
declared formal parameter of m. 
3. The returned object of m is assigned to the left-hand-side variable of the 
assignment statement. 
In context-insensitive analysis, parameters and returned values are modeled 
by copy statements. The interesting question that remains is how to determine 
the type of the receiver object. We can conservatively determine the type ac- 
cording to the declaration of the variable; for example, if the declared variable 
has type t, then only methods named n in subtypes of t can be invoked. Unfor- 
tunately, if the declared variable has type Object, 
then all methods with name 
n are all potential targets. In real-life programs that use object hierarchies ex- 
tensively and include many large libraries, such an approach can result in many 
spurious call targets, making the analysis both slow and imprecise. 
We need to know what the variables can point to in order to compute the 
call targets; but unless we know the call targets, we cannot find out what all the 
variables can point to. This recursive relationship requires that we discover the 
call targets on the fly as we compute the points-to set. The analysis continues 
until no new call targets and no new points-to relations are found. 
Example 12.24 : 
In the code in Fig. 12.26, 
r is a subtype of s, 
which itself is a 
subtype of t. Using only the declared type information, a. 
n () may invoke any 
of the three declared methods with name n since s and r are both subtypes of 
a's declared type, t. Furthermore, it appears that a may point to objects g, h, 
and i after line (5). 
By analyzing the points-to relationships, we first determine that a can point 
to j, an object of type t. Thus, the method declared in line (1) 
is a call target. 
Analyzing line (I), 
we determine that a also can point to g, an object of type 
r .  Thus, the method declared in line (3) may also be a call target, and a can 
now also point to i, another object of type r .  Since there are no more new 
call targets, the analysis terminates without analyzing the met 
hod declared in 
line (2) and without concluding that a can point to h. 
3~emember 
that variables are distinguished by the method to which they belong, so there 
is not just one variable named t h i s ,  but rather one such variable for each method in the 
program. 
CONTEXT-INSENSITIVE INTERPROCEDURAL ANALYSIS 
c l a s s  t ( 
1) g: 
t n() { return new r ( ) ;  1 
I- 
c l a s s  s extends t ( 
2) h: 
t n() ( return new s 
() ; 3 
I- 
c l a s s  r extends s ( 
3) i: 
t n() C return new r(); 3 
3 
main 0 ( 
4) j: 
t a = new t o ;  
5 
1 
a = a . n ( ) ;  
1 
Figure 12.26: A virtual method invocation 
12.5.2 Call Graph Discovery in Datalog 
To formulate the Dat 
alog rules for cont 
ext-insensitive interprocedural analysis, 
we introduce three EDB predicates, each of which is obtainable easily from the 
source code: 
1. actual(S, 
I, 
V) says V is the Ith actual parameter used in call site S. 
2. formal 
(M, 
I, 
V) says that V is Ith formal parameter declared in method 
M .  
3. cha(T, 
N, M )  says that M is the method called when N is invoked on a 
receiver object of type T. (cha stands for class hierarchy analysis). 
Each edge of the call graph is represented by an IDB predicate invokes. 
As we discover more call-graph edges, more points-to relations are created as 
the parameters are passed in and returned values are passed out. This effect is 
summarized by the rules shown in Figure 12.27. 
The first rule computes the call target of the call site. That is, "S : V.N( 
...)" 
says that there is a call site labeled S that invokes method named N on the 
receiver object pointed to by V. The subgoals say that if V can point to heap 
object H, which is allocated as type T, and M is the method used when N is 
invoked on objects of type T, then call site S may invoke method M .  
The second rule says that if site S can call method M ,  then each formal 
parameter of M can point to whatever the corresponding actual parameter of 
the call can point to. The rule for handling returned values is left as an exercise. 
Combining these two rules with those explained in Section 12.4 create a 
context-insensitive points-to analysis that uses a call graph that is computed 
on the fly. This analysis has the side effect of creating a call graph using a 
CHAPTER 12. INTERPROCED URAL ANALYSIS 
1) inuokes(S,M) 
:- "S: V.N( 
...)" & 
pts(V, H )  
& 
hType(H7 T) & 
cha(T7 
N, M) 
2) 
pts(V, H )  
: 
- inuokes(S, M) & 
f 
orrnal 
(M, 
I, 
V) & 
actual 
(S, 
I ,  
W) & 
pts(W, H )  
Figure 12.27: Datalog program for call-graph discovery 
context-insensitive and flow-insensitive points-to analysis. This call graph is 
significantly more accurate than one computed based only on type declarations 
and syntactic analysis. 
12.5.3 Dynamic Loading and Reflectioq 
Languages like Java allow dynamic loading of classes. It is impossible to an- 
alyze all the possible code executed by a program, and hence impossible to 
provide any conservative approximation of call graphs or pointer aliases stat- 
ically. Static analysis can only provide an approximation based on the code 
analyzed. Remember that all the analyses described here can be applied at the 
Java bytecode level, and thus it is not necessary to examine the source code. 
This option is especially significant because Java programs tend to use many 
libraries. 
Even if we assume that all the code to be executed is analyzed, there is 
one more complication that makes conservative analysis impossible: reflection. 
Reflection allows a program to determine dynamically the types of objects to 
be created, the names of methods invoked, as well as the names of the fields 
accessed. The type, method, and field names can be computed or derived 
from user input, so in general the only possible approximation is to assume the 
universe. 
Example 12.25 : 
The code below shows a common use of reflection: 
1) 
String className = . 
. 
. 
; 
2) 
Class c = Class.forName(className); 
3) 
Object o = c.newInstance() ; 
4) 
T t = (T) 0 ;  
The f orName 
method in the Class 
library takes a string containing the class 
name and returns the class. The method newInstance 
returns an instance of 
that class. Instead of leaving the object o with type Ob 
j ect, 
this object is cast 
to a superclass T of all the expected classes. 
12.6. CONTEXT-SENSITIVE POINTER ANALYSIS 
945 
While many large Java applications use reflection, they tend to use common 
idioms, such as the one shown in Example 12.25. As long as the application 
does not redefine the class loader, we can tell the class of the object if we know 
the value of className. 
If the value of className 
is defined in the program, 
because strings are immutable in Java, knowing what className points to 
will provide the name of the class. This technique is another use of points-to 
analysis. If the value of className is based on user input, then the points-to 
analysis can help locate where the value is entered, and the developer may be 
able to limit the scope of its value. 
Similarly, we can exploit the typecast statement, line (4) in Example 12.25, 
to approximate the type of dynamically created objects. Assuming that the 
typecast exception handler has not been redefined, the object must belong to a 
subclass of the class T. 
12.5.4 Exercises for Section 12.5 
Exercise 12.5.1 : 
For the code of Fig. 12.26 
a) Construct the EDB relations actual, formal, and cha. 
b) Make all possible inferences of pts and hpts facts. 
! Exercise 12.5.2 : How would you add to the EDB predicates and rules of 
Section 12.5.2 additional predicates and rules to take into account the fact that 
if a, method call returns an object, then the variable to which the result of the 
call is assigned can point to whatever the variable holding the return value can 
point to? 
12.6 Context-Sensitive Pointer Analysis 
As discussed in Section 12.1.2, context sensitivity can improve greatly the pre- 
cision of interprocedural analysis. We talked about two approaches to interpro- 
cedural analysis, one based on cloning (Section 12.1.4) and one on summaries 
(Section 12.1.5). Which one should we use? 
There are several difficulties in computing the summaries of points-to infor- 
mation. First, the summaries are large. Each method's summary must include 
the effect of all the updates that the function and all its callees can make, in 
terms of the incoming parameters. That is, a method can change the points-to 
sets of all data reachable through static variables, incoming parameters and all 
objects created by the method and its callees. While complicated schemes have 
been proposed, there is no known solution that can scale to large programs. 
Even if the summaries can be computed in a bottom-up pass, computing the 
points-to sets for all the exponentially many contexts in a typical top-down 
pass presents an even greater problem. Such information is necessary for global 
queries like finding all points in the code that touch a certain object. 
946 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
In this section, we discuss a cloning-based context-sensitive analysis. A 
cloning-based analysis simply clones the methods, one for each context of in- 
terest. We then apply the context-insensitive analysis to the cloned call graph. 
While this approach seems simple, the devil is in the details of handling the 
large number of clones. How many contexts are there? Even if we use the 
idea of collapsing all recursive cycles, as discussed in Section 12.1.3, it is not 
uncommon to find 1014 contexts in a Java application. Representing the results 
of these many contexts is the challenge. 
We separate the discussion of context sensitivity into two parts: 
1. How to handle context sensitivity logically? This part is easy, because we 
simply apply the context-insensitive algorithm to the cloned call graph. 
2. How to represent the exponentially many contexts? One way is to rep- 
resent the information as binary decision diagrams (BDD's), a highly- 
optimized data structure that has been used for many other applications. 
This approach to context sensitivity is an excellent example of the impor- 
tance of abstraction. As we are going to show, we eliminate algorithmic com- 
plexity by leveraging the years of work that went into the BDD abstraction. We 
can specify a context-sensitive points-to analysis in just a few lines of Datalog, 
which in turn takes advantage of many thousands of lines of existing code for 
BDD manipulation. This approach has several important advantages. First, 
it makes possible the easy expression of further analyses that use the results 
of the points-to analysis. After all, the points-to results on their own are not 
interesting. Second, it makes it much easier to write the analysis correctly, as 
it leverages many lines of well-debugged code. 
12.6.1 Contexts and Call Strings 
The context-sensitive points-to analysis described below assumes that a call 
graph has been already computed. This step helps make possible a compact 
representation of the many calling contexts. To get the call graph, we first run 
a context-insensitive points-to analysis that computes the call graph on the fly, 
as discussed in Section 12.5. We now describe how to create a cloned call graph. 
A context is a representation of the call string that forms the history of the 
active function calls. Another way to look at the context is that it is a summary 
of the sequence of calls whose activation records are currently on the run-time 
stack. If there are no recursive functions on the stack, then the call string - 
the sequence of locations from which the calls on the stack were made - 
is a 
complete representation. It is also an acceptable representation, in the sense 
that there is only a finite number of different contexts, although that number 
may be exponential in the number of functions in the program. 
However, if there are recursive functions in the program, then the number 
of possible call strings is infinite, and we cannot allow all possible call strings to 
represent distinct contexts. There are various ways we can limit the number of 
12.6. CONTEXT-SENSITIVE POINTER ANALYSIS 
distinct contexts. For example, we can write a regular expression that describes 
all possible call strings and convert that regular expression to a deterministic 
finite automaton, using the methods of Section 3.7. The contexts can then be 
identified with the states of this automaton. 
Here, we shall adopt a simpler scheme that captures the history of nonrecur- 
sive calls but considers recursive calls to be "too hard to unravel." We begin by 
finding all the mutually recursive sets of functions in the program. The process 
is simple and will not be elaborated in detail here. Think of a graph whose 
nodes are the functions, with an edge from p to q if function p calls function 
q. The strongly connected components (SCC7s) 
of this graph are the sets of 
mutually recursive functions. As a common special case, a function p that calls 
itself, but is not in an SCC with any other function is an SCC by itself. The 
nonrecursive functions are also SCC's by themselves. Call an SCC nontrivial 
if it either has more than one member (the mutually recursive case), or it has 
a single, recursive member. The SCC's that are single, nonrecursive functions 
are trivial SCC7s. 
Our modification of the rule that any call string is a context is as follows. 
Given a call string, delete the occurrence of a call site s if 
1. s is in a function p. 
2. Function q is called at site s (q = 
p is possible). 
3. p and q are in the same strong component (i.e., p and q are mutually 
recursive, or p = 
q and p is recursive). 
The result is that when a member of a nontrivial SCC S is called, the call site 
for that call becomes part of the context, but calls within S to other functions 
in the same SCC are not part of the context. Finally, when a call outside S is 
made, we record that call site as part of the context. 
Example 12.26 : 
In Fig. 12.28 is a sketch of five methods with some call sites 
and calls among them. An examination of the calls shows that q and r are 
mutually recursive. However, p, s, and t are not recursive at all. Thus, our 
contexts will be lists of all the call sites except s 3  and s5, where the recursive 
calls between q and r take place. 
Let us consider all the ways we could get from p to t, that is, all the contexts 
in which calls to t occur: 
1. p could call s at 92, and then s could call t at either s7 or s8. Thus, two 
possible call strings are (s2, s7) and (s2, 
s8). 
2. p could call q at s l .  Then, q and r could call each other recursively some 
number of times. We could break the cycle: 
(a) At s4, where t is called directly by q. This choice leads to only one 
context, (sl, 
s4). 
CHAPTER 12. INTERPROCED 
URAL ANALYSIS 
void p o  ( 
h: T a = new TO; 
sl: T b = a.q(); 
s2: 
b . s o ;  
T 
q o  ( 
s3: T c = t h i s . r ( ) ;  
i: T d = new TO; 
s4: 
d . t o ;  
return d; 
3 
T 
r o  C 
s5: T e = this.q(); 
~ 6 :  
@ . s o ;  
return e; 
void s o  C 
s7: T f = t h i s . t ( ) ;  
s8: 
f = f . t o ;  
3 
T 
t
0
C
 
j: T g =new TO; 
return g; 
3 
Figure 12.28: Methods and call sites for a running example 
(b) At s6, where r calls s. Here, we can reach t either by the call at s7 
or the call at s8. That gives us two more contexts, (s 
I, 
s6, s7) and 
(sl, 
s6, s8). 
There are thus five different contexts in which t can be called. Notice that all 
these contexts omit the recursive c 
sites, s3 
and s5. For example, the context 
(si, 
s4) actually represents the 
ite set of call strings (sl, 
s3, (s5, 
s3)", s4) 
forallnLO. 
We now describe how we derive the cloned call graph. Each cloned method 
is identified by the method in the program M and a context C. Edges can be 
derived by adding the corresponding contexts to each of the edges in the original 
call graph. Recall that there is an edge in the original call graph linking call 
site S with method M if the predicate involces(S, M) is true. To add contexts 
12.6. CONTEXT-SENSITIVE POINTER ANALYSIS 
949 
to identify the methods in the cloned call graph, we can define a corresponding 
CSinvokes predicate such that CSinvokes(S, 
C, M, 
D) is true if the call site S 
in context C calls the D context of method M. 
12.6.2 Adding Context to Datalog Rules 
To find context-sensitive points-to relations, we can simply apply the same 
context-insensitive points-to analysis to the cloned call graph. Since a method 
in the cloned call graph is represented by the original method and its context, 
we revise all the Datalog rules accordingly. For simplicity, the rules below do 
not include the type restriction, and the -'s are any new variables. 
1) 
pts(V,C,H) 
:- " H :  T V = 
new T()" & 
CSinvokes(H, 
C, -, 
-) 
5) 
pts(V, D, 
H )  
: 
- CSinvokes(S, 
C, M, 
D) & 
f 
orrnal 
(M, 
I, 
V) & 
actual 
(S, 
I ,  
W) & 
pts(W, 
C, 
H )  
Figure 12.29: Datalog program for context-sensitive points-to analysis 
An additional argument, representing the context, must be given to the IDB 
predicate pts. pts(V, C, 
H) says that variable V in context C can point to heap 
object H. All the rules are self-explanatory, perhaps with the exception of Rule 
5. Rule 5 says that if the call site S 
in context C calls method M of context D, 
then the formal parameters in method M of context D can point to the objects 
pointed to by the corresponding actual parameters in context C. 
12.6.3 Additional Observations About Sensitivity 
What we have described is one formulation of context sensitivity that has been 
shown to be practical enough to handle many large real-life Java programs, 
950 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
using the tricks described briefly in the next section. Nonetheless, this algorithm 
cannot yet handle the largest of Java applications. 
The heap objects in this formulation are named by their call site, but with- 
out context sensitivity. That simplification can cause problems. Consider the 
object-factory idiom where, all objects of the same type are allocated by the 
same routine. The current scheme would make all objects of that class share the 
same name. It is relatively simple to handle such cases by essentially inlining 
the allocation code. In general, it is desirable to increase the context sensitivity 
in the naming of objects. While it is easy to add context sensitivity of objects 
to the Datalog formulation, getting the analysis to scale to large programs is 
another matter. 
Another important form of sensitivity is object sensitivity. An object- 
sensitive technique can distinguish between methods invoked on different re- 
ceiver objects. Consider the scenario of a call site in a calling context where 
a variable is found to point to two different receiver objects of the same class. 
Their fields may point to different objects. Without distinguishing between the 
objects, a copy among fields of the t h i s  object reference will create spurious 
relationships unless we separate the analysis according to the receiver objects. 
Object sensitivity is more useful than context sensitivity for some analyses. 
12.6.4 
Exercises for Section 12.6 
void p o  ( 
h: T a = new T o ;  
i: T b = new T o ;  
c l :  T c = a.q(b); 
3 
T 
q ( T y ) C  
j: T d = new T o ;  
c2: 
d = this.q(d) ; 
c3: 
d = d . q ( y ) ;  
c4: 
d = d . r O ;  
return d; 
3 
T 
r O (  
return t h i s ;  
1 
Figure 12.30: Code for Exercises 12.6.1 and 12.6.2 
Exercise 1
2
.
6
.
1
 : 
What are all the contexts that would be distinguished if we 
apply the methods of this section to the code in Fig. 12.307 
12.7. DATALOG IMPLEMENTATIONBY BDD'S 
! 
Exercise 12.6.2 : 
Perform a context sensitive analysis of the code in Fig. 12.30. 
! Exercise 12.6.3 
: 
Extend the Datalog rules of this section to incorporate the 
type and subtype information, following the approach of Section 12.5. 
12.7 Datalog Implementation by BDD's 
Binary Decision Diagrams (BDD's) are a method for representing boolean func- 
tions by graphs. Since there are 22n boolean functions of n variables, no repre- 
sentation method is going to be very succinct on all boolean functions. However, 
the boolean functions that appear in practice tend to have a lot of regularity. It 
is thus common that one can find a succinct BDD for functions that one really 
wants to represent. 
It turns out that the boolean functions that are described by the Datalog 
programs that we have developed to analyze programs are no exception. While 
succinct BDD's representing information about a program often must be found 
using heuristics and/or techniques used in commercial BDD-manipulating pack- 
ages, the BDD approach has been quite successful in practice. In particular, 
it outperforms methods based on conventional database-management systems, 
because the latter are designed for the more irregular data patterns that appear 
in typical commercial data. 
It is beyond the scope of this book to cover all of the BDD technology that 
has been developed over the years. We shall here introduce you to the BDD 
notation. We then suggest how one represents relational data as BDD's and 
how one could manipulate BDD's to reflect the operations that are performed 
to execute Datalog programs by algorithms such as Algorithm 12.18. Finally, 
we describe how to represent the exponentially many contexts in BDD's, the 
key to the success of the use of BDD's in context-sensitive analysis. 
12.7.1 
Binary Decision Diagrams 
A BDD represents a boolean function by a rooted DAG. The interior nodes of 
the DAG are each labeled by one of the variables of the represented function. 
At the bottom are two leaves, one labeled 0 the other labeled 1. Each interior 
node has two edges to children; these edges are called "low" and "high." The 
low edge is associated with the case that the variable at the node has value 0, 
and the high edge is associated with the case where the variable has value 1. 
Given a truth assignment for the variables, we can start at the root, and 
at each node, say a node labeled x, follow the low or high edge, depending 
on whether the truth value for x is 0 or 1, respectively. If we arrive at the 
leaf labeled 1, 
then the represented function is true for this truth assignment; 
otherwise it is false. 
Example 12.27: In Fig. 12.31 we see a BDD. We shall see the function it 
represents shortly. Notice that we have labeled all the "low" edges with 0 and 
CHAPTER 12. INTERPROCED URAL ANALYSIS 
n 
Figure 12.31: A binary decision diagram 
all the "high" edges by 1. Consider the truth assignment for variables wxyx 
that sets w = 
x = y = 0 and x = 1. Starting at the root, since w = 0 we take 
the low edge, which gets us to the leftmost of the nodes labeled x. Since x = 
0, 
we again follow the low edge from this node, which takes us to the leftmost of 
the nodes labeled y. Since y = 0 we next move to the leftmost of the nodes 
labeled x. Now, since x = 1, we take the high edge and wind up at the leaf 
labeled 1. Our conclusion is that the function is true for this truth assignment. 
Now, consider the truth assignment wxyx = 0101, that is, w = y = 0 and 
x = z = I. We again start at the root. Since w = 0 we again move to the 
leftmost of the nodes labeled x. But now, since x = 1, 
we follow the high edge, 
which jumps to the 0 leaf. That is, we know not only that truth assignment 
0101 makes the function false, but since we never even looked at y or x, any 
truth assignment of the form Olyx will also make the function have value 0. 
This "short-circuiting" ability is one of the reasons BDD's tend to be succinct 
representations of boolean functions. 
In Fig. 12.31 
the interior nodes are in ranks - 
each rank having nodes with 
a particular variable as label. Although it is not an absolute requirement, it is 
convenient to restrict ourselves to ordered BDD's. 
In an ordered BDD, there is 
an order XI, xi,. . . , 
x, to the variables, and whenever there is an edge from a 
parent node labeled xi to a child labeled xj, then i < j .  We shall see that it 
is easier to operate on ordered BDD7s, 
and from here we assume all BDD's are 
12.7. DATALOG IMPLEMENTATION BY BDD 'S 
ordered. 
Notice also that BDD7s 
are DAG7s, 
not trees. Not only will the leaves 0 
and 1 
typically have many parents, but interior nodes also may have several 
parents. For example, the rightmost of the nodes labeled z in Fig. 12.31 has 
two parents. This combination of nodes that would result in the same decision 
is another reason that BDD7s 
tend to be succinct. 
12.7.2 
Transformations on BDD's 
We alluded, in the discussion above, to two simplifications on BDD7s 
that help 
make them more succinct: 
1. Short-Circuiting: If a node N has both its high and low edges go to the 
same node M ,  then we may eliminate N. Edges entering N go to M 
instead. 
2. Node-Merging: If two nodes N and M have low edges that go to the same 
node and also have high edges that go to the same node, then we may 
merge N with M .  Edges entering either N or M go to the merged node. 
It is also possible to run these transformations in the opposite direction. In 
particular, we can introduce a node along an edge from N to M. Both edges 
from the introduced node go to M ,  and the edge from N now goes to the 
introduced node. Note, however, that the variable assigned to the new node 
must be one of those that lies between the variables of N and M in the order. 
Figure 12.32 shows the two transformations schematically. 
(a) Short-circuiting 
(b) Node-merging 
Figure 12.32: Transformations on BDD7s 
954 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
12.7.3 Representing Relations by BDD's 
The relations with which we have been dealing have components that are taken 
frop "domains." A domain for a component of a relation is the set of possible 
values that tuples can have in that component. For example, the relation 
pts(V, 
H) has the domain of all program variables for its first component and 
the domain of all object-creating statements for the second component. If a 
domain has more than 2"-' 
possible values but no more than 2" values, then 
it requires n bits or boolean variables to represent values in that domain. 
A tuple in a relation may thus be viewed as a truth assignment to the 
variables that represent values in the domains for each of the components of the 
tuple. We may see a relation as a boolean function that returns the value true 
for all and only those truth assignments that represent tuples in the relation. 
An example should make these ideas clear. 
Example 12.28 : 
Consider a relation r (A, 
B) such that the domains of both 
A and B are {a, 
b, c, d). We shall encode a by bits 00, b by 01, c by 10, and d 
by 11. Let the tuples of relation r be: 
Let us use boolean variables wx 
to encode the first (A) component and variables 
yx to encode the second (B) component. Then the relation r becomes: 
That is, the relation r has been converted into the boolean function that is 
true for the three truth-assignments wxyx 
= 
0001, 0010, and 1110. Notice that 
these three sequences of bits are exactly those that label the paths from the 
root to the leaf 1 
in Fig. 12.31. That is, the BDD in that figure represents this 
relation r, if the encoding described above is used. 
12.7.4 Relational Operations as BDD Operations 
Now we see how to represent relations as BDD's. But to implement an algorithm 
like Algorithm 12.18 (incremental evaluation of Dat 
alog programs), we need 
to manipulate BDD's in a way that reflects how the relations themselves are 
manipulated. Here are the principal operations on relations that we need to 
perform: 
12.7. DATALOG IMPLEMENTATION B Y  BDD 'S 
955 
1. Initialixation: We need to create a BDD that represents a single tuple of 
a relation. We'll assemble these into BDD's that represent large relations 
by taking the union. 
2. Union: To take the union of relations, we take the logical OR of the 
boolean functions that represent the relations. This operation is needed 
not only to construct initial relations, but also to combine the results of 
several rules for the same head predicate, and to accumulate new facts 
into the set of old facts, as in the incremental Algorithm 12.18. 
3. Projection: When we evaluate a rule body, we need to construct the head 
relation that is implied by the true tuples of the body. In terms of the 
BDD that represents the relation, we need to eliminate the nodes that 
are labeled by those boolean variables that do not represent components 
of the head. We may also need to rename the variables in the BDD 
to correspond to the boolean variables for the components of the head 
relation. 
4. Join: To find the assignments of values to variables that make a rule 
body true, we need to "join" the relations corresponding to each of the 
subgoals. For example, suppose we have two subgoals r(A, 
B) & s(B, 
C). 
The join of the relations for these subgoals is the set of (a, 
b, c) triples 
such that (a, 
b) is a tuple in the relation for r, and (b, 
c) is a tuple in 
the relation for s. We shall see that, after renaming boolean variables in 
BDD's so the components for the two B7s 
agree in variable names, the 
operation on BDD7s 
is similar to the logical AND, which in turn is similar 
to the OR operation on BDD7s 
that implements the union. 
BDD's for Single Tuples 
To initialize a relation, we need to have a way to construct a BDD for the 
function that is true for a single truth assignment. Suppose the boolean vari- 
ables are XI, 
xz, . . 
. , 
x,, and the truth assignment is alaz . . . 
a,, where each ai 
is either 0 or 1. The BDD will have one node Ni for each xi. If ai = 0, then 
the high edge from Ni leads to the leaf 0, and the low edge leads to Ni+l7 
or to 
the leaf 1 
if i = 
n. If ai = 1, 
then we do the same, but the high and low edges 
are reversed. 
This strategy gives us a BDD that checks whether each xi has the correct 
value, for i = 1,2,. 
. 
. , 
n. As soon as we find an incorrect value, we jump 
directly to the 0 leaf. We only wind up at the 1 
leaf if all variables have their 
correct value. 
As an example, look ahead to Fig. 12.33(b). This BDD represents the 
function that is true if and only if x = 
y = 
0, i.e., the truth assignment 00. 
956 
CHAPTER 12. INTERPROCED URAL 
ANALYSIS 
Union 
We shall give in detail an algorithm for taking the logical OR of BDD's, that 
is, the union of the relations represented by the BDD's. 
Algorithm 12.29 : 
Union of BDD's. 
INPUT: Two ordered BDD's with the same set of variables, in the same order. 
OUTPUT: A BDD representing the function that is the logical OR of the two 
boolean functions represented by the input BDD's. 
METHOD: We shall describe a recursive procedure for combining two BDD's. 
The induction is on the size of the set of variables appearing in the BDD's. 
BASIS: Zero variables. The BDD's must both be leaves, labeled either 0 or 1. 
The output is the leaf labeled 1 
if either input is 1, 
or the leaf labeled 0 if both 
are 0. 
INDUCTION: Suppose there are k variables, yl, 
yz, . . 
. , 
y
k
 found among the 
two BDD's. Do the following: 
1. If necessary, use inverse short-circuiting to add a new root so that both 
BDD's have a root labeled yl. 
2. Let the two roots be N and M ;  
let their low children be No and Mo, and 
let their high children be Nl and MI. Recursively apply this algorithm to 
the BDD's rooted at No and Mo. Also, recursively apply this algorithm 
to the BDD's rooted at Nl and MI. The first of these BDD's represents 
the function that is true for all truth assignments that have yl = 0 and 
that make one or both of the given BDD's true. The second represents 
the same for the truth assignments with yl = 1. 
3. Create a new root node labeled yl. Its low child is the root of the first 
recursively constructed BDD, and its high child is the root of the second 
BDD. 
4. Merge the two leaves labeled 0 and the two leaves labeled 1 
in the com- 
bined BDD just constructed. 
5. Apply merging and short-circuiting where possible to simplify the BDD. 
Example 12.30 : In Fig. 12.33(a) and (b) are two simple BDD's. The first 
represents the function x OR y, and the second represents the function 
NOT x 
AND N
O
T
 y 
12.7. DATALOG IMPLEMENTATION BY BDD'S 
Figure 12.33: Constructing the BDD for a logical OR 
Notice that their logical OR is the function 1 that is always true. To apply 
Algorithm 12.29 to these two BDD's, we consider the low children of the two 
roots and the high children of the two roots; let us take up the latter first. 
The high child of the root in Fig. 12.33(a) is 1, 
and in Fig. 12.33(b) it is 0. 
Since these children are both at the leaf level, we do not have to insert nodes 
labeled y along each edge, although the result would be the same had we chosen 
to do so. The basis case for the union of 0 and 1 
is to produce a leaf labeled 1 
that will become the high child of the new root. 
The low children of the roots in Fig. 12.33(a) and (b) are both labeled y, 
so we can compute their union BDD recursively. These two nodes have low 
children labeled 0 and 1, so the combination of their low children is the leaf 
labeled 1. Likewise, their high children are 1 and 0, so the combination is 
again the leaf 1. When we add a new root labeled x, we have the BDD seen in 
Fig. 12.33(c). 
We are not done, since Fig. 12.33(c) can be simplified. The node labeled y 
has both children the node 1, so we can delete the node y and have the leaf 1 
be the low child of the root. Now, both children of the root are the leaf 1, so 
we can eliminate the root. That is, the simplest BDD for the union is the leaf 
1, 
all by itself. 
12.7.5 Using BDD's for Points-to Analysis 
Getting context-insensitive points-to analysis to work is already nontrivial. The 
ordering of the BDD variables can greatly change the size of the representation. 
Many considerations, as well as trial and error, are needed to come up with an 
ordering that allows the analysis to complete quickly. 
It is even harder to get context-sensitive points-to analysis to execute be- 
cause of the exponentially many contexts in the program. In particular, if we 
958 
CHAPTER 12. INTERPROCED URAL ANALYSIS 
arbitrarily assign numbers to represent contexts in a call graph, we cannot han- 
dle even small Java programs. It is important that the contexts be numbered 
so that the binary encoding of the points-to analysis can be made very com- 
pact. Two contexts of the same method with similar call paths share a lot of 
commonalities, so it is desirable to number the n contexts of a method consecu- 
tively. Similarly, because pairs of caller-callees for the same call site share many 
similarities, we wish to number the contexts such that the numeric difference 
between each caller-callee pair of a call site is always a constant. 
Even with a clever numbering scheme for the calling contexts, it is still hard 
to analyze large Java programs efficiently. Active machine learning has been 
found useful in deriving a variable ordering efficient enough to handle large 
applications. 
12.7.6 Exercises for Section 12.7 
Exercise 12.7.1 
: Using the encoding of symbols in Example 12.28, develop 
a BDD that represents the relation consisting of the tuples (b, b), (c, 
a), and 
(b, a). You may order the boolean variables in whatever way gives you the most 
succinct BDD. 
! 
Exercise 1
2
.
7
.
2
 
: As a function of n, how many nodes are there in the most 
succinct BDD that represents the exclusive-or function on n variables. That is, 
the function is true if an odd number of the n variables are true and false if an 
even number are true. 
Exercise 1
2
.
7
.
3
 
: 
Modify Algorithm 12.29 so it produces the intersection (log- 
ical AND) of two BDD's. 
! 
! Exercise 12.7.4 
: Find algorithms to perform the following relational opera- 
tions on the ordered BDD's that represent them: 
a) Project out some of the boolean variables. That is, the function repre- 
sented should be true for a given truth assignment a if there was any 
truth assignment for the missing variables that, together with a made 
the original function true. 
b) Join two relations r and s, by combining a tuple from r with one from 
s whenever these tuples agree on the attributes that r and s have in 
common. It is really sufficient to consider the case where the relations 
have only two components, and one from each relation matches; that is, 
the relations are r(A, B) and s(B, 
C). 
12.8 
Summary of Chapter 12 
+ Interprocedural Analysis: A data-flow analysis that tracks information 
across procedure boundaries is said to be interprocedural. Many analyses, 
12.8. SUMMARY OF CHAPTER 12 
such as points-to analysis, can only be done in a meaningful way if they 
are interprocedural. 
+ Call Sites: Programs call procedures at certain points referred to as call 
sites. The procedure called at a site may be obvious, or it may be am- 
biguous, should the call be indirect through a pointer or a call of a virtual 
met 
hod that has several implement at 
ions. 
+ Call Graphs: A call graph for a program is a bipartite graph with nodes 
for call sites and nodes for procedures. An edge goes from a call-site node 
to a procedure node if that procedure may be called at the site. 
+ Inlining: As long as there is no recursion in a program, we can in principle 
replace all procedure calls by copies of their code, and use intraprocedural 
analysis on the resulting program. This analysis is in effect, interproce- 
dural. 
+ Flow Sensitivity and Context-Sensitivity: A data-flow analysis that pro- 
duces facts that depend on location in the program is said to be flow- 
sensitive. If the analysis produces facts that depend on the history of 
procedure calls is said to be context-sensitive. A data-flow analysis can 
be either flow- or context-sensitive, both, or neither. 
+ Cloning-Based Context-Sensitive Analysis: In principle, once we establish 
the different contexts in which a procedure can be called, we can imagine 
that there is a clone of each procedure for each context. In that way, a 
context-insensitive analysis serves as a context-sensitive analysis. 
+ Summary-Based Context-Sensitive Analysis: Another approach to inter- 
procedural analysis extends the region-based analysis technique that was 
described for intraprocedural analysis. Each procedure has a transfer 
function and is treated as a region at each place where that procedure is 
called. 
+ Applications o
f
 Interprocedural Analysis: An important application re- 
quiring interprocedural analysis is the detection of software vulnerabili- 
ties. These are often characterized by having data read from an untrusted 
input source by one procedure and used in an exploitable way by another 
procedure. 
+ Datalog: The language Datalog is a simple notation for if-then rules that 
can be used to describe data-flow analyses at a high level. Collections of 
Datalog rules, or Datalog programs, can be evaluated using one of several 
standard algorithms. 
+ Datalog Rules: A Datalog rule consists of a body (antecedent) and head 
(consequent). The body is one or more atoms, and the head is an atom. 
Atoms are predicates applied to arguments that are variables or constants. 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
The atoms of the body are connected by logical AND, and an atom in the 
body may be negated. 
+ IDB and EDB Predicates: EDB predicates in a Datalog program have 
their true facts given a-priori. In a data-flow analysis, these predicates 
correspond to the facts that can be obtained from the code being analyzed. 
IDB predicates are defined by the rules themselves and correspond in a 
data-flow analysis to the information we are trying to extract from the 
code being analyzed. 
+ Evaluation of Datalog programs: We apply rules by substituting constants 
for variables that make the body true. Whenever we do so, we infer that 
the head, with the same substitution for variables, is also true. This 
operation is repeated, until no more facts can be inferred. 
+ Incremental Evaluation o
f
 Datalog Programs: An efficiency improvement 
is obtained by doing incremental evaluation. We perform a series of 
rounds. In one round, we consider only substitutions of constants for 
variables that make at least one atom of the body be a fact that was just 
discovered on the previous round. 
+ Java Pointer Analysis: We can model pointer analysis in Java by a frame- 
work in which there are reference variables that point to heap objects, 
which may have fields that point to other heap objects. An insensitive 
pointer analysis can be written as a Datalog program that infers two kinds 
of facts: a variable can point to a heap object, or a field of a heap object 
can point to another heap object. 
+ Type Information to Improve Pointer Analysis: We can get more precise 
pointer analysis if we take advantage of the fact that reference variables 
can only point to heap objects that are of the same type as the variable 
or a subtype. 
+ Interprocedural Pointer Analysis: To make the analysis interprocedural, 
we must add rules that reflect how parameters are passed and return 
values assigned to variables. These rules are essentially the same as the 
rules for copying one reference variable to another. 
+ Call-Graph Discovery: Since Java has virtual methods, interprocedural 
analysis requires that we first limit what procedures can be called at a 
given call site. The principal way to discover limits on what can be called 
where is to analyze the types of objects and take advantage of the fact 
that the actual method referred to by a virtual method call must belong 
to an appropriate class. 
+ Context-Sensitive Analysis: When procedures are recursive, we must con- 
dense the information contained in call strings into a finite number of 
contexts. An effective way to do so is to drop from the call string any 
22.9. REFERENCES FOR CHAPTER 12 
961 
call site where a procedure calls another procedure (perhaps itself) with 
which it is mutually recursive. Using this representation, we can mod- 
ify the rules for intraprocedural pointer analysis so the context is carried 
along in predicates; this approach simulates cloning-based analysis. 
+ Binary Decision Diagrams: BDD's are a succinct representation of bool- 
ean functions by rooted DAG7s. 
The interior nodes correspond to boolean 
variables and have two children, low (representing truth value 0) and high 
(representing 1). There are two leaves labeled 0 and 1. A truth assignment 
makes the represented function true if and only if the path from the root 
in which we go to the low child if the variable at a node is 0 and to the 
high child otherwise, leads to the 1 
leaf. 
+ BDD7s and Relations: A BDD can serve as a succinct representation of 
one of the predicates in a Datalog program. Constants are encoded as 
truth assignments to a collection of boolean variables, and the function 
represented by the BDD is true if an only if the boolean variables represent 
a true fact for that predicate. 
+ Implementing Data-Flow Analysis by BDD7s: 
Any data-flow analysis that 
can be expressed as Datalog rules can be implemented by manipulations 
on the BDD's that represent the predicates involved in those rules. Often, 
this representation leads to a more efficient implementation of the data- 
flow analysis than any other known approach. 
12.9 References for Chapter 12 
Some of the basic concepts in interprocedural analysis can be found in [I, 
6, 7, 
and 2 
1
1
.
 Callahan et al. [I 
I] describe an interprocedural constant-propagation 
algorithm. 
Steensgaard [22] published the first scalable pointer-alias analysis. It is 
context-insensitive, flow-insensitive, and equivalence-based. A context-insens- 
itive version of the inclusion-based points-to analysis was derived by Ander- 
sen [2]. Later, Heintze and Tardieu [15] described an efficient algorithm for this 
analysis. Fahndrich, Rehof, and Das [14] presented a context-sensitive, flow- 
insensitive, equivalence-based analysis that scales to large programs like gcc. 
Notable among previous attempts to create a context-sensitive, inclusion-based 
points-to analysis is Emami, Ghiya, and Hendren [13], which is a cloning-based 
context-sensitive, flow-sensitive, inclusion-based, points-to algorithm. 
Binary decision diagrams (BDD's) first appeared in Bryant [9]. Their first 
use for data-flow analysis was by Berndl et al. [4]. The application of BDD's to 
insensitive pointer analysis is reported by Zhu [25] and Berndl et al. (81. Whaley 
and Lam [24] describe the first context-sensitive, flow-insensitive, inclusion- 
based algorithm that has been shown to apply to real-life applications. The 
paper describes a tool called bddbddb that automatically translates analysis 
962 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
described in Datalog into BDD code. Object-sensitivity was introduced by 
Milanova, Rountev, and Ryder [18]. 
For a discussion of Datalog, see Ullman and Widom [23]. Also see Lam et 
al. [16] for a discussion of the connection of data-flow analysis to Datalog. 
The Metal code checker is described by Engler et al. [12] and the PREfix 
checker was created by Bush, Pincus, and Sielaff [lo]. Ball and Rajamani [4] 
developed a program analysis engine called SLAM using model checking and 
symbolic execution to simulate all possible behaviors of a system. Ball et al. [5] 
have created a static analysis tool called SDV based on SLAM to find API usage 
errors in C device-driver programs by applying BDD's to model checking. 
Livshits and Lam [17] describe how context-sensitive points-to analysis can 
be used to find SQL vulnerabilities in Java web applications. Ruwase and 
Lam [20] 
describe how to keep track of array extents and insert dynamic bounds 
checks automatically. Rinard et al. [19] describe how to extend arrays dynam- 
ically to accommodate for the overflowed contents. Avots et al. [3] extend the 
context-sensitive Java points-to analysis to C and show how it can be used to 
reduce the cost of dynamic detection of buffer overflows. 
1. Allen, F. E., "Interprocedural data flow analysis," Proc. IFIP Congress 
1974, pp. 398-402, North Holland, Amsterdam, 1974. 
2. Andersen, L., Program Analysis and Specialixation for the C Programming 
Language, Ph.D. thesis, DIKU, Univ. of Copenhagen, Denmark, 1994. 
3. Avots, D., M. Dalton, V. B. Livshits, and M. S. Lam, "Improving software 
security with a C pointer analysis," ICSE 2005: Proc. 27th International 
Conference on Sofiware Engineering, pp. 332-341. 
4. Ball, T. and S. K. Rajamani, "A symbolic model checker for boolean 
programs," Proc. SPIN 2000 Workshop on Model Checking of Software, 
pp. 113-130. 
5. Ball, T., E. Bounimova, B. Cook, V. Levin, J. Lichtenber, C. McGarvey, 
B. Ondrusek, S. Rajamani, and A. Ustuner, "Thorough static analysis of 
device drivers," EuroSys (2006), pp. 73-85. 
6. Banning, J. P., "An efficient way to find the side effects of procedural calls 
and the aliases of variables," Proc. Sixth Annual Symposium on Principles 
o
f
 Programming Languages (1979), pp. 29-41. 
7. Barth, J. M., "A practical interprocedural data flow analysis algorithm," 
Comm. ACM 21:9 (1978), pp. 724-736. 
8. Berndl, M., 0. Lohtak, F. Qian, L. Hendren, and N. Umanee, "Points- 
to analysis using BDD's," Proc. ACM SIGPLAN 2003 Conference on 
Programming Language Des2gn and Implementatzon, pp. 103-114. 
9. Bryant, R. E., "Graph-based algorithms for Boolean function manipula- 
tion," IEEE Trans. on Computers C-35% (1986), pp. 677-691. 
12.9. REFERENCES FOR CHAPTER 12 
963 
10. Bush, W. R., J. D. Pincus, and D. J. Sielaff, "A static analyzer for finding 
dynamic programming errors," Software-Practice and Experience, 30:7 
(2000), pp. 775-802. 
11. Callahan, D., K. D. Cooper, K. Kennedy, and L. Torczon, "Interprocedu- 
ral constant propagation," Proc. SIGPLAN 1986 Symposium on Compiler 
Construction, SIGPLAN Notices, 21:7 (1986), pp. 152-161. 
12. Engler, D., B. Chelf, A. Chou, and S. Hallem, "Checking system rules us- 
ing system-specific, programmer-written compiler extensions," Proc. Sixth 
USENIX Conference on Operating Systems Design and Implementation 
(2000). pp. 1-16. 
13. Emami, M., R. Ghiya, and L. J. Hendren, "Context-sensitive interproce- 
dural points-to analysis in the presence of function pointers," Proc. SIG- 
PLAN Conference on Programming Language Design and Implementation 
(1994), pp. 224-256. 
14. Fahndrich, M., J. Rehof, and M. Das, "Scalable context-sensitive flow 
analysis using instantiation constraints," Proc. SIGPLAN Conference on 
Programming Language Design and Implementation (2000), pp. 253-263. 
15. Heintze, N. and 0. 
Tardieu, ""Ultra-fast aliasing analysis using CLA: a 
million lines of C code in a second," Proc. o
f
 the SIGPLAN Conference on 
Programming Language Design and Implementation (2001), pp. 254-263. 
16. Lam, M. S., J. 
Whaley, V. B. Livshits, M. C. Martin, D. Avots, M. Carbin, 
and C. Unkel, "Context-sensitive program analysis as database queries," 
Proc. 2005 ACM Symposium on Principles o
f
 Database Systems, pp. 1-12. 
17. Livshits, V. B. and M. S. Lam, "Finding security vulnerabilities in Java 
applications using static analysis" Proc. 14th USENIX Security Sympo- 
sium (2005), pp. 271-286. 
18. Milanova, A., A. Rountev, and B. G. Ryder, "Parameterized object sen- 
sitivity for points-to and side-effect analyses for Java" Proc. 2002 ACM 
SIGSOFT International Symposium on Sofiware Testing and Analysis, 
pp. 1-11. 
19. Rinard, M., C. Cadar, D. Dumitran, D. Roy, and T. Leu, "A dynamic 
technique for eliminating buffer overflow vulnerabilities (and other mem- 
ory errors) ," Proc. 2004 Annual Computer Security Applications Confer- 
ence, pp. 82-90. 
20. Ruwase, 0. 
and M. S. Lam, "A practical dynamic buffer overflow detec- 
tor," Proc. 11th Annual Network and Distributed System Security Sym- 
posium (2004), pp. 159-169. 
964 
CHAPTER 12. INTERPROCEDURAL ANALYSIS 
21. Sharir, M. and A. Pnueli, "Two approaches to interprocedural data flow 
analysis," in S. Muchnick and N. Jones (eds.) Program Flow Analysis: 
Theory and Applications, Chapter 7, 
pp. 189-234. Prentice-Hall, Upper 
Saddle River NJ, 1981. 
22. Steensgaard, B., "Points-to analysis in linear time," Twenty- 
Third A CM 
Symposium on Principles o
f
 Programming Languages (1996). 
23. Ullman, J. D. and J. Widom, A First Course in Database Systems, Pren- 
tice-Hall, Upper Saddle River NJ, 2002. 
24. Whaley, J. and M. S. Lam, "Cloning-based context-sensitive pointer alias 
analysis using binary decision diagrams," Proc. ACM SIGPLAN 2004 
Conference on Programming Language Design and Implementation, pp. 
131-144. 
25. Zhu, J., "Symbolic Pointer Analysis," "Proc. International Conference in 
Computer-Aided Design (2002), pp. 150-157. 
Appendix A 
A Complete Front End 
The complete compiler front end in this appendix is based on the informally 
described simple compiler of Sections 2.5-2.8. The main difference from Chap- 
ter 2 is that the front end generates jumping code for boolean expressions, as 
in Section 6.6. We begin with the syntax of the source language, described by 
a grammar that needs to be adapted for top-down parsing. 
The Java code for the translator consists of five packages: main, lexer, 
symbol, parser, and inter. Package inter contains classes for the language 
constructs in the abstract syntax. Since the code for the parser interacts with 
the rest of the packages, it will be discussed last. Each package is stored as a 
separate directory with a file per class. 
Going into the parser, the source program consists of a stream of tokens, so 
object-orientation has little to do with the code for the parser. Coming out of 
the parser, the source program consists of a syntax tree, with constructs or nodes 
implemented as objects. These objects deal with all of the following: construct 
a syntax-tree node, check types, and generate three-address intermediate code 
(see package inter) 
. 
A.l The Source Language 
A program in the language consists of a block with optional declarations and 
statements. Token basic represents basic types. 
program + block 
block 
-+ 
( decls stmts ) 
decls + decls decl I E: 
decl 
-+ 
type id ; 
type + type C num 1 
( basic 
strnts + stmts stmt I 
c 
Treating assignments as statements, rather than as operators within expres- 
sions, simplifies translation. 
APPENDIX A. A COMPLETE FRONT END 
Object-Oriented Versus Phase-Oriented 
With an object-oriented approach, all the code for a construct is collected 
in the class for the construct. Alternatively, with a phase-oriented ap- 
proach, the code is grouped by phase, so a type checking procedure would 
have a case for each construct, and a code generation procedure would 
have a case for each construct, and so on. 
The tradeoff is that an object-oriented approach makes it easier to 
change or add a construct, such as "for" statements, and a phase-oriented 
approach makes it easier to change or add a phase, such as type checking. 
With objects, a new construct can be added by writing a self-contained 
class, but a change to a phase, such as inserting code for coercions, requires 
changes across all the affected classes. With phases, a new construct can 
result in changes across the procedures for the phases. 
stmt -+ 
I 
I 
loc 
-+ 
loc = bool ; 
if ( bool ) stmt 
if ( bool ) stmt else stmt 
while ( bool ) stmt 
do stmt while ( bool ) ; 
break ; 
block 
loc [ bool 1 I id 
The productions for expressions handle associativity and precedence of op- 
erators. They use a nonterminal for each level of precedence and a nonterminal, 
factor, for parent 
hesized expressions, identifiers, array references, and const 
ants. 
boo1 + bool I I join I join 
join 
join && equality 
( equality 
equality 
-+ 
equality == re1 I equality ! 
= re1 I re1 
re1 -+ 
e x p r c  expr I expr<= expr I expr>= expr I 
expr > expr 1 expr 
expr 
-+ 
expr + term I expr - term I term 
term 
-+ 
term * unary I term / unary I unary 
unary 
-+ 
! unary I - unary I factor 
factor 
-+ 
( bool ) I loc I num 1 real 1 true I false 
A.2 Main 
Execution begins in method main in class Main. Method main creates a lexical 
analyzer and a parser and then calls method program in the parser: 
1) 
package main; 
// File Main.java 
2) 
import java. 
io 
. 
* ; import lexer 
. 
* 
; import parser. 
* ; 
A.3. LEXICAL ANALYZER 
3) public class Main ( 
4) 
public static void main(String [I args) throws IOException ( 
5) 
Lexer lex = new LexerO; 
6) 
Parser parse = new Parser(1ex); 
7) 
parse 
.program() 
; 
8) 
System.out.write('\n'); 
9) 
3 
10) 3 
Lexical Analyzer 
Package lexer is an extension of the code for the lexical analyzer in Sec- 
tion 2.6.5. Class Tag defines constants for tokens: 
1) package lexer 
; 
// File Tag.java 
2
)
 
public class Tag ( 
3) 
public final static int 
4
,
 
AND 
= 256, BASIC = 257, BREAK = 258, DO 
= 259, 
ELSE = 260, 
5 1 
EQ 
= 261, FALSE = 262, GE 
= 263, ID 
= 264, IF 
= 265, 
6) 
INDEX = 266, LE 
= 267, MINUS = 268, NE 
= 269, 
NUM 
= 270, 
7, 
OR 
.= 271, REAL = 272, TEMP = 273, TRUE = 274, 
WHILE = 275; 
8) 3 
Three of the constants, INDEX, MINUS, and TEMP, are not lexical tokens; they 
will be used in syntax trees. 
Classes Token and N
u
r
n
 are as in Section 2.6.5, with method tostring added: 
1
)
 package lexer; 
// File Token.java 
2
)
 
public class Token ( 
3) 
public final int tag; 
4) 
public Token(int t
)
 ( tag = t; 
3 
5) 
public String tostring() (return "" + (char)tag;) 
6) 3 
1) package lexer; 
// File Num.java 
2) public class Num extends Token ( 
3) 
public final int value; 
4) 
public Num(int v
)
 (super(Tag.NUM); 
value = v; 
3 
5) 
public String tostring0 ( return "" + value; 
3 
6) 1 
Class Word manages lexemes for reserved words, identifiers, and composite 
tokens like &&. It is also useful for managing the written form of operators in 
the intermediate code like unary minus; for example, the source text -2 has the 
intermediate form minus 2. 
1) package lexer; 
// File Word.java 
2) public class Word extends Token ( 
3) 
public Stringlexeme = ""; 
4) 
public Word(String s, 
int tag) ( super(tag1; lexeme = s; 
3 
5 )  
public String 
tostring0 ( return lexeme; 
3 
6) 
public static final 
Word 
7) 
and = new W
o
r
d
(
 "&&", Tag. 
AND 1, 
or = new W
o
r
d
(
 " 
I 
1 " , Tag. 
OR 1, 
APPENDIX A. A COMPLETE FRONT END 
8) 
eq = new W
o
r
d
(
 "==", Tag.EQ ), ne = new Word( "!=", 
Tag.NE ), 
9 
> 
le = new Word( "<=", Tag.LE ), ge = new W
o
r
d
(
 ">=", Tag.GE ), 
10) 
minus = new Word( "minus", 
Tag.MINUS ), 
11) 
True 
= new Word 
( "true", Tag. 
TRUE ) , 
12) 
False = new W
o
r
d
(
 '
I
f
 
alse" 
, Tag 
.FALSE 
) , 
13) 
temp 
=newWord("tU, 
Tag.TEMP ); 
14) 3 
Class Real is for floating point numbers: 
1) package lexer; 
// File Real.java 
2) public class Real extends Token ( 
3) 
public final float value; 
4) 
public Real(f1oat v
)
 ( super(Tag.REAL) ; value = v; 
3 
5) 
public String tostring0 C return "" + value; 3 
6) ) 
The main method in class Lexer, function scan, recognizes numbers, iden- 
tifiers, and reserved words, as discussed in Section 2.6.5. 
Lines 9-13 in class Lexer reserve selected keywords. Lines 14-16 reserve 
lexemes for objects defined elsewhere. Objects Word. True and Word. False 
are defined in class Ward. Objects for the basic types int, char, bool, and 
float are defined in class Type, a subclass of Word. Class Type is from package 
symbols. 
1) package lexer 
; 
// File Lexer.java 
2) import java. 
io 
. 
* ; import java.uti1. 
* ; import symbols. 
* ; 
3) public class Lexer ( 
4) 
public static int line = 1; 
5) 
char peek = ' ' 
; 
6) 
Hashtable words = new Hashtable0 
; 
7) 
void reserve(Word w
)
 ( words.put(w.lexeme, w
)
;
 3 
8) 
public LexerO ( 
9 
1 
reserve( new Word("if", 
Tag. 
IF) 
; 
10) 
reserve 
( new Word 
("else", Tag. 
ELSE) ) ; 
11) 
reserve 
( new W~rd(~'while", 
Tag. 
WHILE) ) ; 
12) 
reserve( new Word("doU , 
Tag 
.DO) 
; 
13) 
reserve( new Word("breakM 
, Tag 
.BREAK) ) ; 
14) 
reserve 
( Word. 
True ) ; reserve 
( Word. 
False ) ; 
15) 
reserve 
( Type. 
Int ) ; reserve 
( Type. 
Char ) ; 
16) 
reserve 
( Type. 
Boo1 ) ; reserve 
( Type. 
Float ) ; 
17) 
3 
Function readch() (line 18) is used to read the next input character into 
variable peek. The name readch is reused or overloaded (lines 19-24) to 
help recognize composite tokens. For example, once input < is seen, the call 
readch('=') reads the next character into peek and checks whether it is =. 
18) 
void readch() throws IOException ( peek = (char)System. 
in.read0 
; 3 
19) 
boolean readch(char c
)
 throws IOException ( 
20) 
readch() ; 
21) 
if 
( peek ! 
= c ) return false; 
22) 
peek = ' ' 
; 
23) 
return true; 
24) 
3 
A.3. LEXICAL ANALYZER 
969 
Function scan begins by skipping white space (lines 26-30). It recognizes 
composite tokens like <= (lines 31-44) and numbers like 365 and 3.14 (lines 
45-58), before collecting words (lines 59-70). 
public Token s
c
a
n
(
)
 throws IOException ( 
for( ; ; readch0 ( 
i
f
(
 peek == ' ' I (  peek == '\t) ) continue; 
else i
f
(
 peek == '\n' ) line = line + 1; 
else break; 
3 
switch( peek ) ( 
case '&' : 
if 
( r
e
a
d
c
h
(
'
&
'
)
 
) return Word.and; else return new ~oken('&'); 
case '
1
'
:
 
if 
( readch(' 1 ') ) return Word. 
or; 
else return new Token(' 1 '1 ; 
case '=': 
i
f
 
( readch('=') 
) return Word.eq; 
else return new Token('=') ; 
case ' 
! ' 
: 
i
f
(
 readch('=') 
) return Word.ne; 
else return 
new T
o
k
e
n
(
'
!
'
)
;
 
case ' 0 :  
if 
( readch('=') ) return Word.le; 
else return new T
o
k
e
n
(
'
<
'
)
;
 
case '>': 
i
f
(
 readch('=') 
) return Word.ge; 
else return new T
o
k
e
n
(
'
>
'
)
;
 
3 
if 
( character. 
isDigit 
(peek) ) ( 
int v = 0; 
do i 
v = lO*v + Character.digit(peek, 1
0
)
;
 readcho; 
) while( Character.isDigit(peek) ) ;  
i
f
(
 peek != '.' ) return new N
u
m
(
v
)
;
 
float x = v; 
float d = 10; 
for(;;) € 
readch0 
; 
i
f
(
 ! Character.isDigit(peek) ) break; 
x = x + Character.digit(peek, 10) / d; 
d = d*lO; 
3 
return 
new Real(x) ; 
3 
if 
( Character. 
isLetter(peek) ) i 
StringBuffer b = new StringBufferO 
; 
do ( 
b. 
append(peek) ; readcho 
; 
3 while( Character.isLetterOrDigit(peek) ) ;  
String s = b.toString0; 
Word w = (Word)words.get(s); 
if 
( w != null ) return w; 
w = new Word(s, Tag.ID); 
words.put(s, w
)
;
 
return w; 
Finally, any remaining characters are returned as tokens (lines 71-72). 
71) 
Token tok = new Token(peek); peek = ' '; 
72) 
return tok; 
73) 
3 
74) 3 
970 
APPENDIX A. A COMPLETE FRONT END 
A.4 
Symbol Tables and Types 
Package symbols implements symbol tables and types. 
Class Env is essentially unchanged from Fig. 2.37. Whereas class Lexer maps 
strings to words, class Env maps word tokens to objects of class Id, which is 
defined in package i n t e r  along with the classes for expressions and statements. 
1
)
 
package symbols; 
// File Env.java 
2
)
 import j 
ava. 
ut 
il 
. 
* ; import lexer 
. 
* ; import inter. 
* ; 
3) 
public class Env C 
4
)
 
private Hashtable table 
; 
5
)
 
protected Env prev; 
6) 
public Env(Env n
)
 ( table = new Hashtable0 
; prev = n; 
3 
7
)
 
public void put(Token w, Id i
)
 ( table.put(w, i
)
 
; 3 
8) 
public Id get(Token w
)
 C 
9, 
for( Env e = this; 
e != null; 
e = e.prev ) ( 
10) 
Id found = (Id) 
(e. 
table 
.get 
(
w
)
)
 ; 
11) 
i
f
(
 found != null ) return found; 
12) 
3 
13) 
return 
null; 
14) 
3 
15) 
3 
We define class Type to be a subclass of Word since basic type names like i n t  
are simply reserved words, to be mapped from lexemes to appropriate objects by 
the lexical analyzer. The objects for the basic types are Type. Int, Type. Float, 
Type. Char, and Type. Bool (lines 7-10). All of them have inherited field t a g  
set to Tag. BASIC, so the parser treats them all alike. 
1
)
 
package symbols; 
// File Type.java 
2
)
 import lexer 
. 
* ; 
3)public class Type extends Word C 
4
)
 
public int width = 0; 
// width is used for storage allocation 
5 )  
public Type(String s, int tag, 
int w
)
 ( super(s, tag); width = W; 
3 
6) 
public static final Type 
7) 
Int 
= new Type( 'lint" 
, 
Tag. 
BASIC, 4 
1, 
8
,
 
Float = new Type( "float", 
Tag.BASIC, 
8 
1, 
9 
> 
Char = new Type( "char", Tag.BASIC, 1 1, 
10) 
Boo1 = new Type( "bool", Tag.BASIC, I ) ; 
Functions numeric (lines 11-14) and max (lines 15-20) are useful for type 
conversions. 
public static boolean numeric(Type p
)
 C 
if (
p
 == Type.Char I I  p == Type.Int I  I p == Type.Float) return true; 
else return false; 
3 
public static Type max(Type pl, 
Type p2 ) ( 
if ( ! numeric(p1) 
I  I  ! numeric(p2) ) return null; 
else if ( pl == Type.Float 1 )  p2 == Type.Float ) return Type.Float; 
else if ( pl == Type.Int 
1 I p2 == Type.Int 
) return Type.Int; 
else return Type.Char; 
3 
A.5. INTERMEDIATE CODE FOR EXPRESSIONS 
971 
Conversions are allowed between the "numeric" types Type. Char, Type. Int, 
and Type. Float. When an arithmetic operator is applied to two numeric types, 
the result is the "max" of the two types. 
Arrays are the only constructed type in the source language. The call to 
super on line 7 sets field width, which is essential for address calculations. It 
also sets lexeme and tok to default values that are not used. 
1
)
 package symbols 
; 
// File Array.java 
2
)
 
import lexer 
. 
* ; 
3)public class Array extends Type ( 
4
)
 
public Type of; 
// array *of* type 
5) 
public int size = 1; 
// number of elements 
6) 
public Array(int sz, 
Type p) ( 
7, 
~uper(~'~", 
Tag.INDEX, sz*p.width) 
; size = sz; of = p; 
8
)
 
3 
9) 
public String tostring0 i 
return " 
C" + size + "I " + of. 
tostring0 
; 3 
10) ) 
Intermediate Code for Expressions 
Package i n t e r  contains the Node class hierarchy. Node has two subclasses: Expr 
for expression nodes and Stmt for statement nodes. This section introduces 
Expr and its subclasses. Some of the methods in Expr deal with booleans and 
jumping code; they will be discussed in Section A.6, along with the remaining 
subclasses of Expr. 
Nodes in the syntax tree are implemented as objects of class Node. For error 
reporting, field lexline (line 4, file Node.java) saves the source-line number of 
the construct at this node. Lines 7-10 are used to emit three-address code. 
1
)
 package inter; 
// File Node.java 
2
)
 import lexer 
. 
* ; 
3
)
 
public class Node ( 
4
)
 
int lexline = 0; 
5) 
Node0 ( lexline = Lexer.line; ) 
6) 
void error(String s
)
 -( throw new Error("near line "+lexline+": "+s); 3 
7
)
 
static int labels = O ;  
8) 
public int newlabelo i 
return ++labels; 3 
9) 
public void emitlabel(int i
)
 ( System.out 
.print 
("L" + i + ":") ; 3 
10) 
public void emit(String s) ( System.out.println("\t" + s
)
;
 3 
11) 
3 
Expression constructs are implemented by subclasses of Expr. Class Expr 
has fields op and type (lines 4-5, file Expr.java), representing the operator and 
type, respectively, at a node. 
1
)
 package inter; 
// File Expr.java 
2
)
 import lexer 
. 
* ; import symbols. 
* ; 
3
)
 public class Expr extends Node ( 
4
)
 
public Token op; 
5
)
 
public Type type; 
6) 
Expr(Token tok, Type p
)
 ( op = tok; type = p; ) 
972 
APPENDIX A. A COMPLETE FRONT END 
Method gen 
(line 7) returns a "term" that can fit the right side of a three- 
address instruction. Given expression E = El + 
E2, 
method gen 
returns a term 
x1 
+x2, 
where XI 
and x2 are addresses for the values of El and E2, 
respectively. 
The return value this 
is appropriate if this object is an address; subclasses of 
Expr 
typically reimplement gen. 
Method reduce (line 8 )  computes or "reduces" an expression down to a 
single address; that is, it returns a constant, an identifier, or a temporary name. 
Given expression E, 
method reduce 
returns a temporary t holding the value of 
E. Again, this 
is an appropriate return value if this object is an address. 
We defer discussion of methods jumping 
and emitjumps 
(lines 9-18) until 
Section A.6; they generate jumping code for boolean expressions. 
public Expr gene 
return this; 3 
public Expr reduce0 C return this; 3 
public void jumping(int t, int f) ( emitjumps(toString~), t, 
f
)
;
 1 
public void emitjumps(String test, int t, 
int f) C 
i
f
(
 t != 0 && f != 0 )  
C 
emit(I1if 
+ test + I
'
 goto L" + t
)
;
 
emit("goto L" + f) 
; 
1 
else i
f
(
 t != 0 ) emit(I1if l1 + test + " goto L" + t
)
;
 
else i
f
(
 f != 0 ) emit("iffa1se 
+ test + 
goto L" + f
)
;
 
else ; // nothing since both t and f fall through 
3 
public String tostring() ( return op. 
tostring0 
; 3 
Class Id 
inherits the default implementations of gen 
and reduce 
in class 
Expr, 
since an identifier is an address. 
1
)
 package inter; 
// File Id.java 
2
)
 import lexer 
. 
* ; import symbols. 
* ; 
3
)
 public class Id extends Expr ( 
4
)
 
public int offset; 
// relative address 
5) 
public Id(Word id, Type p, int b) ( super(id, p
)
;
 offset = b; 3 
6) 3 
The node for an identifier of class Id 
is a leaf. The call super(id,p) (line 5, 
file Id.jaua) saves id 
and p 
in inherited fields op 
and type, 
respectively. Field 
off 
set 
(line 4
)
 
holds the relative address of this identifier. 
Class O
p
 provides an implementation of reduce 
(lines 5-10, file 0p.java) 
that is inherited by subclasses Arith 
for arithmetic operators, Unary 
for unary 
operators, and Access 
for array accesses. In each case, reduce 
calls gen 
to 
generate a term, emits an instruction to assign the term to a new temporary 
name, and returns the temporary. 
1
)
 package inter; 
// File Op.java 
2) import lexer 
. 
* 
; import symbols. 
* ; 
3
)
 
public class Op extends Expr ( 
4
)
 
public Op(Token tok, Type p
)
 ( super(tok, p
)
;
 1 
5) 
public Expr reduce0 C 
GI 
Expr x = gen 
(1 
; 
A. 5. INTERMEDIATE CODE FOR EXPRESSIONS 
7) 
Temp t = new ~emp(type) 
; 
8
,
 
emit( t 
.tostring() + " = " + x.tostring0 ) ; 
9) 
return t 
; 
10) 
3 
11) 3 
Class Arith implements binary operators like + and *. Constructor Arith 
begins by calling super (tok,null) (line 6), where tok is a token representing 
the operator and n u l l  is a placeholder for the type. The type is determined 
on line 7 by using Type.max, which checks whether the two operands can be 
coerced to a common numeric type; the code for Type .max is in Section A.4. 
I
f
 they can be coerced, type is set to the result type; otherwise, a type error 
is reported (line 8). This simple compiler checks types, but it does not insert 
type conversions. 
1) 
package inter 
; 
// File Arith.java 
2) 
import lexer 
. 
* ; import symbols. 
* ; 
3)public class Arith extends Op ( 
public Expr exprl, expr2; 
public Arith(Token tok, Expr XI, Expr x2) ( 
super 
(tok, null) 
; exprl = XI 
; expr2 = x2 
; 
type = Type. 
max 
(exprl 
. 
type, expr2. 
type) 
; 
if (type == null ) error("type error"); 
> 
public Expr gene ( 
return new Arith(op, exprl.reduce(), expr2.reduceO); 
public String tostring0 ( 
return exprl 
. 
tostring()+" "+op. 
tostring()+" I1+expr2. 
tostring0 
; 
3 
Method gen constructs the right side of a three-address instruction by reduc- 
ing the subexpressions to addresses and applying the operator to the addresses 
(line 11, file Arith.java). For example, suppose gen is called at the root for 
a+b*c. The calls to reduce return a as the address for subexpression a and a 
temporary t as the address for b*c. Meanwhile, reduce emits the instruction 
t=b*c. Method gen returns a new Arith node, with operator * and addresses 
a and t as operands.' 
It is worth noting that temporary names are typed, along with all other 
expressions. The constructor Temp is therefore called with a type as a parameter 
(line 6, file Temp.java) 
.2 
1) package inter; 
// File Temp.java 
2) import lexer 
. 
* ; import symbols. 
* ; 
3) 
public class Temp extends Expr ( 
l ~ o r  
error reporting, field lexline in class Node records the current lexical line number 
when a node is constructed. We leave it to the reader to track line numbers when new nodes 
are constructed during intermediate code generation. 
2
~
n
 
alternative approach might be for the constructor to take an expression node as a 
parameter, so it can copy the type and lexical position of the expression node. 
974 
APPENDIX A. A COMPLETE FRONT END 
4
)
 
static int count = O ;  
5) 
int number = 0; 
6
)
 
public Temp(Type p
)
 C super(Word.temp, p
)
 
; number = ++count; 
3 
7
)
 
public String tostring0 ( return "t" 
+ number; 3 
8) 3 
Class Unary is the one-operand counterpart of class Arith: 
1
)
 
package inter; 
// File Unary.java 
2
)
 import lexer 
. 
* 
; import symbols. 
* ; 
3
)
 public class Unary extends Op C 
4
)
 
public Expr expr; 
5) 
public Unary(Token tok, 
Expr x
)
 C 
// handles minus, for ! see Not 
6) 
super(tok, null) 
; expr = x; 
7, 
type = Type.max(Type.Int, expr.type); 
8) 
if (type == null ) error("type error"); 
9) 
3 
10) 
public Expr gene ( return new Unary(op, expr.reduce()); 
3 
11) 
public String tostring() ( return op.toString()+" 
"+expr. 
tostring0 
; 1 
12) 
3 
A.6 Jumping Code for Boolean Expressions 
Jumping code for a boolean expression B is generated by method jumping, 
which takes two labels t and f as parameters, called the true and false exits of 
B, respectively. The code contains a jump to t if B evaluates to true, and a 
jump to f if B evaluates to false. By convention, the special label 0 means that 
control falls through B to the next instruction after the code for B. 
We begin with class Constant. The constructor Constant on line 4 takes 
a token tok and a type p as parameters. It constructs a leaf in the syntax 
tree with label tok and type p. For convenience, the constructor Constant is 
overloaded (line 5) to create a constant object from an integer. 
1
)
 
package inter; 
// File Constant.java 
2
)
 import lexer 
. 
* ; import symbols. 
* ; 
3
)
 
public class Constant extends Expr C 
4
)
 
public Constant(Token tok, 
Type p
)
 ( super(tok, p
)
;
 3 
5) 
public Constant(int i
)
 ( super(new B
u
m
(
i
)
,
 
Type.Int); 3 
6) 
public static final Constant 
7
,
 
True = new Constant(Word.True, Type.Bool), 
8> 
False = new Constant(Word.False, Type.Boo1); 
9
)
 
public void jumping(int t, 
int f) ( 
lo> 
if ( this == True && t != 0 ) emit("goto L" + t
)
;
 
11) 
else if ( this == False && f != 0
)
 emit("goto L" + f
)
;
 
12) 
3 
13) 
3 
Method jumping (lines 9-12, file Constant.java) takes two parameters, labels 
t and f .  If this constant is the static object True (defined on line 7) and t is 
not the special label 0, then a jump to t is generated. Otherwise, if this is the 
object False (defined on line 8) and f is nonzero, then a jump to f is generated. 
A.6. JUMPING CODE FOR BOOLEAN EXPRESSIONS 
975 
Class Logical 
provides some common functionality for classes O r ,  And, 
and 
Not. Fields x 
and y (line 4) correspond to the operands of a logical opera- 
tor. (Although class Not 
implements a unary operator, for convenience, it is a 
subclass of ~ogical.) 
The constructor Logical 
(tok 
, 
a, 
b) (lines 5-10) 
builds 
a syntax node with operator tok 
and operands a 
and b. In doing so it uses 
function check 
to ensure that both a 
and b are booleans. Method gen 
will be 
discussed at the end of this section. 
1
)
 package inter 
; 
// File Logzcal.java 
2
)
 import lexer 
. 
* ; import symbols. 
* ; 
3
)
 
public class Logical extends Expr ( 
4
)
 
public Expr exprl, 
expr2; 
5
)
 
Logical(Token tok, Expr xl, Expr x2) ( 
6
)
 
super 
(tok 
, null) 
; 
// null type to start 
7
)
 
exprl = XI; expr2 = x2; 
8
)
 
type = check(expr1. 
type, expr2. 
type) 
; 
9 
> 
if (type == null ) error(l1type error"); 
10) 
3 
11) 
public Type check(Type pi, Type p2) ( 
12) 
if ( pl == Type.Boo1 
&& p2 == Type.Boo1 ) return Type.Boo1; 
13) 
else return null; 
14) 
3 
15) 
public Expr gen0 ( 
I
f
9
 
int f = newlabelo 
; int a = newlabelo 
; 
1
7
)
 
Temp temp = new Temp(type); 
Is> 
this. 
jumping(0,f) ; 
l9> 
emit 
(temp. 
tostring0 + " 
= true") 
; 
20) 
emit("goto L" + a
)
;
 
21) 
emitlabel(f) ; emit(temp.toString() 
+ " = false") 
; 
2
2
)
 
emit 
label 
(
a
)
 
; 
23) 
return temp; 
24) 
3 
25) 
public String tostring0 ( 
as) 
return exprl. 
tostring()+" 
"+op. 
tostring()+" 
I1+expr2. 
tostring0 
; 
27) 
3 
In class O r ,  method jumping (lines 5-10) 
generates jumping code for a 
boolean expression B = B1 
1 1 Bz. For the moment, suppose that neither the 
true exit t nor the false exit f of B is the special label 0. Since B is true if B1 
is true, the true exit of B1 must be t and the false exit corresponds to the first 
instruction of Bz. The true and false exits of B2 are the same as those of B. 
1
)
 package inter; 
/I 
File 0r.java 
2) import lexer 
. 
* ; import symbols. 
* ; 
3)public class Or extends Logical C 
4
)
 
public Or(Token tok, Expr XI, 
Expr x2) C super(tok, XI, x2); 3 
5
)
 
public void jumping(int t, 
int f) C 
6) 
int label = t ! 
= 0 ? t : newlabel 
() ; 
7, 
expri.jumping(labe1, 0
)
;
 
8
)
 
expr2. 
jumping(t ,f) 
; 
9> 
if 
( t == 0 
emitlabel(labe1); 
10) 
3 
11) 3 
976 
APPENDIX A. A COMPLETE FRONT END 
In the general case, t 
, 
the true exit of B, can be the special label 0. Variable 
label (line 6, file 0r.java) ensures that the true exit of B1 is set properly to 
the end of the code for B. If t is 0, then label is set to a new label that is 
emitted after code generation for both B1 and B2. 
The code for class And is similar to the code for Or. 
1
)
 
package inter; 
// File And.java 
2
)
 
import lexer 
. 
* ; import symbols. 
* ; 
3
)
 
public class And extends Logical ( 
4) 
public And(Token tok, 
Expr XI, 
Expr x2) (super(tok, XI, 
x
2
)
;
 3 
5
)
 
public void jumping(int t 
, int f) ( 
6
)
 
int label = f != 0 ? f : newlabelo; 
7
)
 
, 
exprl.jumping(O, label); 
8
)
 
expr2.jumping(t,f); 
9 
> 
if 
( f == 0 ) emitlabel(labe1) ; 
10) 
3 
11) 
3 
Class Not has enough in common with the other boolean operators that we 
make it a subclass of Logical, even though Not implements a unary operator. 
The superclass expects two operands, so b appears twice in the call to super 
on line 4. Only y (declared on line 4, file Logical.java) is used in the methods 
on lines 5-6. On line 5, method jumping simply calls y . 
jumping with the true 
and false exits reversed. 
1
)
 package inter; 
// File Not. 
java 
2
)
 import lexer 
. 
* ; import symbols. 
* ; 
3)public class Not extends Logical ( 
4
)
 
public Not 
(Token tok, 
Expr x2) ( super(tok, x2, 
x2) 
; ) 
5
)
 
public void jumping(int t, 
int f
)
 ( expr2. 
jumping(f, t
)
 
; 3 
6) 
public String tostring() ( return op.toString()+" 
"+expr2.toString(); 
7) 3 
Class Re1 implements the operators <, <=, ==, ! 
=, >=, and >. Function check 
(lines 5-9) checks that the two operands have the same type and that they are 
not arrays. For simplicity, coercions are not permitted. 
I
)
 
package inter; 
// File Re1.java 
2
)
 import lexer 
. 
* ; import symbols. 
* ; 
3)public class 
Re1 extends Logical ( 
4
)
 
public Rel(Token tok, 
Expr XI, 
Expr x2) ( super(tok, xl, 
x
2
)
;
 3 
5 )  
public Type check(Type pl, 
Type p2) ( 
6
,
 
i
f
 ( pi instanceof Array I I p2 instanceof Array ) return 
null; 
7
,
 
else i
f
 
( pl == p2 ) return Type 
.Bool; 
8
)
 
else return 
null; 
9) 
1 
10) 
public void jumping(int t, int f
)
 ( 
11> 
Expr a = exprl 
.reduce0 
; 
12> 
Expr b = expr2.reduceo; 
13) String test = a.toString() + 
" + op.toString() + " " + b.to~tring0; 
14) 
emitjumps(test, t, 
f); 
15) 
) 
1
6
)
 3 
A.6. JUMPING CODE FOR BOOLEAN EXPRESSIONS 
Method jumping 
(lines 10-15, 
file Rel. 
java) begins by generating code for the 
subexpressions x and y (lines 11-12). It then calls method emit 
jumps 
defined 
on lines 10-18, file Expr.java, in Section A.5. If neither t 
nor f is the special 
label 0, then emit 
jumps 
executes the following 
12) 
emit(Iiif " 
+ test + It goto L" + t); 
// File Expr.java 
13) 
emit(ligoto 
L" + f 
1; 
At most one instruction is generated if either t 
or f is the special label 0 (again, 
from file Expr. 
java) : 
15> 
else if( t != 0 ) emit("if " + test + " 
goto L" + t); 
16) 
else if 
( f != 0 ) emit(I1iffalse 
+ test + " goto L" + f) 
; 
17) 
else ; // nothing since both t and f fall through 
For another use of emit 
jumps, 
consider the code for class Access. The 
source language allows boolean values to be assigned to identifiers and array 
elements, so a boolean expression can be an array access. Class Access has 
method gen for generating "normal" code and method jumping 
for jumping 
code. Method jumping (line 11) calls emitjumps after reducing this array 
access to a temporary. The constructor (lines 6-9) is called with a flattened 
array a, an index i, 
and the type p 
of an element in the flattened array. Type 
checking is done during array address calculation. 
1) 
package inter; 
// File Access. 
java 
2) import lexer 
. 
* ; import symbols. 
* ; 
3) 
public class Access extends Op C 
4) 
public Id array; 
5) 
public Expr index; 
6) 
public Access(1d a, Expr i, Type p) i 
// p is element type after 
7
,
 
super(new Word(" [
I
 ", Tag. 
INDEX), p) 
; // flattening the array 
8) 
array = a; index = i; 
9) 
3 
10) 
public Expr g e n 0  ( return new Access 
(array, index.reduce0, type) 
; 3 
11) 
public void jumping(int t 
,int f) ( emitjumps(reduce() .tostring() 
,t 
,f) 
; ) 
12) 
public String tostring0 ( 
13) 
return array.toString0 + " [ 
+ index.toString() + 
If ]Ii; 
14) 
3 
15) 1 
Jumping code can also be used to return a boolean value. Class Logical, 
earlier in this section, has a method gen 
(lines 15-24) that returns a temporary 
temp, 
whose value is determined by the flow 
of control through the jumping code 
for this expression. At the true exit of this boolean expression, temp 
is assigned 
true; 
at the false exit, temp 
is assigned false. The temporary is declared on 
line 17. Jumping code for this expression is generated on line 18 with the true 
exit being the next instruction and the false exit being a new label f .  The next 
instruction assigns true 
to temp (line 19), followed by a jump to a new label 
a (line 20). The code on line 21 emits label f and an instruction that assigns 
false 
to temp. The code fragment ends with label a, 
generated on line 22. 
Finally, gen returns temp (line 23). 
978 
APPENDIX A. A COMPLETE FRONT END 
A.7 Intermediate Code for Statements 
Each statement construct is implemented by a subclass of Stmt 
. The fields for 
the components of a construct are in the relevant subclass; for example, class 
While has fields for a test expression and a substatement, as we shall see. 
Lines 3-4 in the following code for class Stmt deal with syntax-tree con- 
struction. The constructor Stmt () does nothing, since the work is done in the 
subclasses. The static object Stmt . 
Null (line 4) represents an empty sequence 
of statements. 
1) 
package i n t e r ;  
// File Strnt.java 
2) public class Stmt extends Node ( 
3) 
public Stmt 0 
( 3 
4) 
public s t a t i c  Stmt Null = new StmtO; 
5) 
public void gen(int b, i n t  a )  (3 // called with labels begin and a f t e r  
6) 
i n t  a f t e r  = 0 ;  
// saves label a f t e r  
7) 
public s t a t i c  Stmt Enclosing = Stmt.Nul1; 
// used f o r  break stmts 
8) 3 
Lines 5-7 deal with the generation of three-address code. The method gen is 
called with two labels b and a, where b marks the beginning of the code for this 
statement and a marks the first instruction after the code for this statement. 
Method gen (line 5) is a placeholder for the gen methods in the subclasses. 
The subclasses While and Do save their label a in the field a f t e r  (line 6) so 
it can be used by any enclosed break statement to jump out of its enclosing 
construct. The object Stmt .Enclosing is used during parsing to keep track 
of the enclosing construct. (For a source language with continue statements, 
we can use the same approach to keep track of the enclosing construct for a 
continue statement 
.) 
The constructor for class If builds a node for a statement if ( E )  
S. Fields 
expr and s t m t  hold the nodes for E  and S, respectively. Note that expr in 
lower-case letters names a field of class Expr; similarly, s t m t  names a field of 
class Stmt 
. 
1) 
package i n t e r ;  
// File If.3ava 
2) import symbols. * ; 
3) public class If extends Stmt ( 
4) 
Expr expr; Stmt stmt; 
5) 
public If (Expr x, Stmt s) ( 
6) 
expr = x; 
stmt = s ;  
7, 
i f  ( expr . 
type ! 
= Type. Boo1 ) expr . 
e r r o r  
("boolean required i n  i f  ") ; 
8) 
3 
9) 
public void gen(int b, i n t  a) { 
lo> 
i n t  label = newlabelo; // label f o r  the code f o r  stmt 
11> 
expr. jumping(O, a) ; 
// f a l l  through on t r u e ,  goto a on f a l s e  
la> 
emitlabel(labe1) ; stmt .gen(label, a) ; 
13) 
3 
14) 3 
The code for an If object consists of jumping code for expr followed by the 
code for s t m t  
. As discussed in Section A.6, the call expr . 
jumping (0, 
f ) on line 
A. 7. INTERMEDIATE CODE FOR STATEMENTS 
11 specifies that control must fall through the code for expr if expr evaluates 
to true, and must flow to label a otherwise. 
The implementation of class Else, which handles conditionals with else 
parts, is analogous to that of class If: 
1
)
 package inter; 
// File Else.java 
2
)
 import symbols. 
* ; 
3
)
 
public class Else extends Stmt ( 
4
)
 
Expr expr; 
Stmt stmtl, stmt2; 
5
)
 
public Else(Expr x, 
Stmt sl, 
Stmt s2) ( 
6) 
expr = x; 
stmtl = SI; stmt2 
= s2; 
7
,
 
i
f
(
 expr.type != Type.Boo1 ) expr.error("boolean required in 
8) 
3 
9) 
public void gen(int b, int a
)
 ( 
lo> 
int label1 
= newlabelo; 
// label1 
for stmtl 
11> 
int label2 
= newlabel0 
; 
// label2 
for stmt2 
1
2
)
 
expr. 
jumping(0 
,label2) 
; 
// fall 
through to stmtl on true 
13) 
emitlabel(label1) ; stmt 
I 
.gen(labell , a
)
 
; emit 
( 
"goto L" + a
)
 
; 
1
4
)
 
emitlabel 
(label2) 
; stmt2. 
gen(label2, a
)
 
; 
15) 
3 
16) 1 
The construction of a While object is split between the constructor While 0 ,  
which creates a node with null children (line 5), and an initialization function 
i n i t  
(x , 
s )  
, 
which sets child expr to x and child s t m t  to s (lines 6-9). Function 
gen 
(b , 
a) for generating three-address code (line 10-16) is in the spirit of the 
corresponding function gen 0 in class If. 
The difference is that label a is saved 
in field after (line 11) and that the code for s t m t  is followed by a jump to b 
(line 15) for the next iteration of the while loop. 
1
)
 
package inter; 
// File While. 
java 
2
)
 import symbols. 
* ; 
3) 
public class While extends Stmt ( 
4
)
 
Expr expr; Stmt stmt; 
5) 
public While0 ( expr = null; stmt = null; ) 
6) 
public void init(Expr x, Stmt s
)
 ( 
7
)
 
expr = x; stmt = s 
; 
8> 
i
f
(
 expr.type != Type.Boo1 ) expr.error("boolean required in while"); 
9) 
10) 
public void gen(int b, int a
)
 ( 
11) 
after = a; 
// save label a 
12) 
expr. 
jumping(0, a) 
; 
13) 
int label = newlabelo 
; 
// label for stmt 
14) 
emitlabel(labe1) ; stmt 
.gen(label, b
)
 ; 
15) 
emit("goto L" + b
)
;
 
16) 
3 
17) 1 
Class D
o
 is very similar to class While. 
1
)
 
package inter; 
// File Do.java 
2
)
 
import symbols. 
* ; 
3) 
public class Do extends Stmt { 
4) 
Expr expr; Stmt stmt; 
APPENDIX A. A COMPLETE FRONT END 
public D o 0  
( expr = null; stmt = null; 3 
public void init(Stmt s, 
Expr x
)
 { 
expr = x; 
stmt = s; 
i
f
(
 expr.type != Type.Boo1 ) expr.error("boolean required in do" 
3 
public void gen(int b, int a
)
 { 
after = a; 
int label = newlabel 
() 
; 
// label for expr 
stmt 
.gen(b, 
label) 
; 
emit 
label 
(label) 
; 
expr. 
jumping(b,O) ; 
3 
Class Set 
implements assignments with an identifer on the left side and an 
expression on the right. Most of the code in class Set 
is for constructing a node 
and checking types (lines 5-13). Function gen 
emits a three-address instruction 
(lines 14- 
16). 
1
)
 package inter; 
// File Set.java 
2
)
 import lexer 
. 
* ; import symbols. 
* ; 
3
)
 public class Set extends Stmt ( 
4) 
public Id id; 
public Expr expr; 
5) 
public Set(Id i, 
Expr x
)
 C 
6
,
 
id = i; 
expr = x; 
7
,
 
if ( check(id. 
type, 
expr. 
type) == null ) error(Ittype error") 
; 
8) 
3 
9) 
public Type check(Type pi, Type p2) ( 
10) 
if ( Type.numeric(p1) && Type.numeric(p2) ) return 
p2; 
11> 
else if ( pi == Type.Boo1 
&& p2 == Type.Boo1 ) return p2; 
12) 
else return 
null; 
13) 
3 
14) 
public void gen(int b, int a
)
 ( 
15> 
emit( id.toString() + It = It + expr.gen() .tostring() 1; 
16) 
3 
17) 3 
Class SetElem 
implements assignments to an array element: 
1
)
 package inter; 
// File SetElem.java 
2
)
 
import lexer 
. 
* ; import symbols. 
* ; 
3
)
 
public class SetElem extends Stmt C 
4
)
 
public Id array; 
public Expr index; 
public Expr expr; 
5) 
public SetElem(Access x, Expr y) < 
6) 
array = x.array; 
index = x.index; 
expr = y; 
7
)
 
if ( check(x. type, expr. 
type) == null ) error(Ittype 
error") 
; 
8) 
3 
9) 
public Type check(Type pi, Type p2) ( 
lo> 
if ( pi instanceof Array I I p2 instanceof Array ) return null; 
11) 
else if ( pi == p2 ) return p2; 
12) 
else if ( Type.numeric(p1) 
&& ~ype.numeric(p2) ) return 
p2; 
13) 
else return 
null; 
14) 
3 
15) 
public void gen(int b, int a
)
 ( 
1
6
)
 
String sl = index.reduce() .tostring() ; 
17) 
String s2 
= expr 
.reduce() .tostring() ; 
A.8. PARSER 
Class Seq implements a sequence of statements. The tests for null state- 
ments on lines 6-7 are for avoiding labels. Note that no code is generated for 
the null statement, Stmt . 
Null, since method gen in class Stmt does nothing. 
1
)
 
package inter 
; 
// File Seq.java 
2
)
 public class Seq extends Stmt C 
3
)
 
Stmt stmtl; 
Stmt stmt2; 
4
)
 
public Seq(Stmt sl, 
Stmt s2) ( stmtl = sl; 
stmt2 
= s2; 
5
)
 
public void gen(int b, int a
)
 ( 
6, 
if ( stmtl == Stmt.Nul1 ) stmt2.gen(b, a
)
;
 
7, 
else if ( stmt2 
== Stmt 
.Null ) stmtl 
. 
gen(b, a
)
 
; 
8) 
else c 
9 
> 
int label 
= newlabel0 
; 
lo> 
stmtl.gen(b,label) 
; 
11> 
emitlabel 
(label) 
; 
12) 
stmt2. 
gen(labe1, 
a
)
 
; 
13) 
3 
14) 
3 
15) 
1 
A break statement sends control out of an enclosing loop or switch state- 
ment. Class Break uses field s t m t  to save the enclosing statement construct 
(the parser ensures that Stmt . 
Enclosing denotes the syntax-tree node for 
the enclosing construct). The code for a Break object is a jump to the la- 
bel s t m t  . 
a f t e r ,  which marks the instruction immediately after the code for 
stmt. 
1
)
 package inter; 
// File Break.java 
2) 
public class Break extends Stmt C 
3
)
 
Stmt stmt; 
4
)
 
public Break0 C 
5
)
 
if 
( Stmt 
.Enclosing 
== null ) error("unenc1osed break") 
; 
6) 
stmt = Stmt.Enclosing; 
7) 
3 
8
)
 
public void gen(int b, int a
)
 ( 
9> 
emit( "goto L" + stmt 
.after) 
; 
10) 
1 
11) 3 
Parser 
The parser reads a stream of tokens and builds a syntax tree by calling the 
appropriate constructor functions from Sections A 
.5-A.7. The current symbol 
table is maintained as in the translation scheme in Fig. 2.38 in Section 2.7. 
Package p a r s e r  contains one class, Parser: 
1
)
 package parser; 
// File Parser.java 
2
)
 import j 
ava. 
io 
. 
* ; import lexer 
. 
* ; import symbols. 
* ; import inter. 
* ; 
982 
APPENDIX A. A COMPLETE FRONT END 
3
)
 
public class Parser C 
4
)
 
private Lexer lex; 
// lexical analyzer for this parser 
5
)
 
private Token look; // lookahead tagen 
6) 
Env top = null; 
// current or top symbol table 
7
)
 
int used = 0; 
// storage used for declarations 
8
)
 
public Parser(Lexer 1
)
 throws IOException C lex = 1; 
m
o
v
e
(
)
;
 
3 
9) 
void move0 throws IOException C look = lex.scan(); ) 
10) 
void error(String s
)
 { throw 
new Error(I1near line "+lex.line+": "
+
s
)
;
 ) 
11) 
void match(int t
)
 throws IOException C 
12) 
if 
( 1ook.tag 
== t ) m
o
v
e
(
)
;
 
13) 
else error("syntax error"); 
14) 
3 
Like the simple expression translator in Section 2.5, class Parser has a pro- 
cedure for each nonterminal. The procedures are based on a grammar formed 
by removing left recursion from the source-language grammar in Section A.1. 
Parsing begins with a call to procedure program, which calls b l o c k 0  (line 
16) to parse the input stream and build the syntax tree. Lines 17-18 generate 
intermediate code. 
15) 
public void program() throws IOException ( // program -> block 
Is> 
Stmt s 
= b
l
o
c
k
(
)
 ; 
17) 
int begin = s.newlabel() ; int after = s 
.newlabel0 
; 
1
8
)
 
s.emitlabel(begin); 
s.gen(begin, after); 
s.emitlabel(after); 
19) 
3 
Symbol-table handling is shown explicitly in procedure block.3 Variable 
top (declared on line 5) holds the top symbol table; variable savedEnv (line 21) 
is a link to the previous symbol table. 
20) 
Stmt b
l
o
c
k
(
)
 throws IOException C // block -> ( decls stmts 3 
all 
m
a
t
c
h
(
'
{
'
)
;
 
Env savedEnv = top; top = new ~nv(top); 
22> 
d
e
c
l
s
(
)
;
 
Stmt s = stmtso; 
23> 
m
a
t
c
h
(
'
)
'
)
;
 
top = savedEnv; 
24) 
return s; 
25) 
3 
Declarations result in symbol-table entries for identifiers (see line 36). Al- 
though not shown here, declarations can also result in instructions to reserve 
storage for the identifiers at run time. 
void decls 
() throws IOExcept 
ion C 
while( 1ook.tag == Tag.BASIC 
) ( // D -> type ID ; 
Type p = t
y
p
e
(
)
;
 Token tok = look; 
match(Tag.ID); match('; '1; 
Id id = new I
d
(
(
W
o
r
d
)
t
o
k
,
 
p, 
used); 
top.put( tok, 
id 1; 
used = used + p.width; 
.
t
 
3 
Type t
y
p
e
(
)
 throws IOException C 
Type p = (Type)look; 
// expect 1ook.tag == Tag.BASIC 
3
~
n
 
attractive alternative is to add methods push 
and pop 
to class Env, 
with the current 
table accessible through a static variable Env. 
top 
A.8. PARSER 
match(Tag. 
BASIC) 
; 
i
f
(
 1ook.tag != '[' ) return 
p; // T -> basic 
else return dims 
(p) ; 
// return array type 
3 
Type dims 
(Type p
)
 throws IOExcept 
ion ( 
rnatch('C2); 
Token tok = look; match(~ag.~~~); 
m
a
t
c
h
(
'
]
'
)
;
 
i
f
(
 1ook.tag == 'C' ) 
p = dims(p) ; 
return 
new Array(((Num)tok).value, 
p
)
;
 
3 
Procedure s t m t  has a switch statement with cases corresponding to the 
productions for nonterminal Stmt. Each case builds a node for a construct, 
using the constructor functions discussed in Section A.7. The nodes for while 
and do statements are constructed when the parser sees the opening keyword. 
The nodes are constructed before the statement is parsed to allow any enclosed 
break statement to point back to its enclosing loop. Nested loops are handled 
by using variable S t m t  
.Enclosing in class S t m t  and s a v e d s t m t  (declared on 
line 52) to maintain the current enclosing loop. 
Stmt stmts() throws IOException ( 
if ( 1ook.tag 
== '3' ) return Stmt.Nul1; 
else return new Seq(stmt 0, stmts 
0 ; 
3 
Stmt s
t
m
t
(
)
 throws IOException ( 
Expr x; Stmt s, 
sl, 
s2; 
Stmt savedstmt 
; 
// save enclosing loop for breaks 
switch( 1ook.tag ) ( 
case ';': 
move 
() 
; 
return Stmt.Nul1; 
case Tag. 
IF: 
match(Tag.IF); m
a
t
c
h
(
'
(
'
)
;
 
x = boolo; 
m
a
t
c
h
(
'
)
'
)
;
 
sl = stmt() ; 
if 
( look. 
tag != Tag.ELSE ) return new If 
(x, sl) 
; 
match(Tag.ELSE); 
s2 
= s
t
m
t
(
)
 ; 
return 
new Else(x, sl, 
s
2
)
;
 
case Tag. 
WHILE: 
While whilenode = new W
h
i
l
e
(
)
;
 
savedstmt = Stmt.Enclosing; 
Stmt.Enclosing 
= whilenode; 
match(Tag.WHILE); m
a
t
c
h
(
'
(
'
)
;
 
x = boolo; 
m
a
t
c
h
(
'
)
'
)
;
 
sl = stmt0; 
whilenode. 
init 
(x, sl) 
; 
Stmt.Enclosing 
= savedstmt; // reset Stmt.Enclosing 
return whilenode; 
case Tag.DO: 
Do donode = new D o O ;  
savedstmt = Stmt.Enclosing; 
Stmt.Enclosing 
= donode; 
match(Tag.DO) ; 
sl = s
t
m
t
(
)
;
 
match(Tag.WHILE); m
a
t
c
h
(
'
(
'
)
;
 
x = boolo; 
m
a
t
c
h
(
'
)
'
)
;
 
m
a
t
c
h
(
'
;
'
)
;
 
donode. 
init 
(sl, x) 
; 
Stmt.Enclosing 
= savedstmt; // reset Stmt.Enclosing 
return donode; 
APPENDIX A. A COMPLETE FRONT END 
case Tag.BREAK: 
match(Tag.BREAK); m
a
t
c
h
(
'
;
'
)
;
 
return new Break0 
; 
case '-0: 
return 
b
l
o
c
k
(
)
;
 
default 
: 
return assign0 
; 
1 
For cogvenience, 
the code for assignments appears in an auxiliary procedure, 
assign. 
90) 
Stmt a
s
s
i
g
n
(
)
 throws IOException ( 
91) 
Stmt stmt; Token t = look; 
g
2
)
 
match(Tag . 
ID) 
; 
93) 
Id id = top.get(t); 
94) 
if 
( id == null ) error(t . 
tostring0 + " undeclared") 
; 
95) 
if 
( look. 
tag == '=' ) ( 
// S -> id = E ; 
96) 
m
o
v
e
(
)
;
 
stmt = new Set(id, bool0); 
97) 
3 
98) 
else i 
/ / S - > L = E ;  
99) 
Access x = offset(id1; 
100) 
m
a
t
c
h
(
'
=
'
)
 ; stmt = new SetElem(x, b
o
o
l
(
)
)
 ; 
101) 
3 
102) 
m
a
t
c
h
(
'
;
 ' I ;  
103) 
return stmt; 
104) 
3 
The parsing of arithmetic and boolean expressions is similar. In each case, 
an appropriate syntax-tree node is created. Code generation for the two is 
different, as discussed in Sections A.5-A.6. 
Expr boo10 throws IOException ( 
Expr x = j
o
i
n
(
)
;
 
while( 1ook.tag 
== Tag.OR ) ( 
Token tok = look; m
o
v
e
(
)
;
 
x = new Or(tok, x, 
j
o
i
n
(
)
)
;
 
3 
return x; 
3 
Expr j
o
i
n
(
)
 throws IOExcept 
ion ( 
Expr x = equalityo; 
while( 1ook.tag == Tag.AND ) ( 
Token tok = look; m
o
v
e
(
)
 ; x = new And(tok, x, 
equalityo) 
; 
3 
return x; 
3 
Expr equality() throws IOExcept 
ion i 
Expr x = relo 
; 
while( look. 
tag == Tag 
.EQ 1 I look. 
tag == Tag.NE ) ( 
Token tok = look; m
o
v
e
(
)
;
 
x = new Rel(tok, x, 
relo); 
3- 
return x; 
Expr relo throws IOException ( 
Expr x = expr 
(1 
; 
A.8. PARSER 
switch( 1ook.tag ) ( 
case '<': case Tag.LE: case Tag.GE: case '>': 
Token tok = look; m
o
v
e
(
)
 ; return 
new Rel(tok, x, 
expro) 
; 
default 
: 
return x; 
3 
3 
Expr expr 
() throws IOExcept 
ion ( 
Expr x = term0 
; 
while( look.tag == '+' 
1 1  1ook.tag == '-' ) ( 
Token tok = look; m
o
v
e
(
)
;
 
x = new Arith(tok, x, 
t
e
r
m
(
)
)
;
 
1 
return x; 
1 
Expr t
e
r
m
(
)
 throws IOException ( 
Expr x = unary 
( 
) ; 
while(1ook. tag == '*' I  
1 look. 
tag == '/' ) ( 
Token tok = look; m
o
v
e
(
)
;
 
x = new Arith(tok, x, 
u
n
a
r
y
(
)
)
;
 
1 
return x; 
J 
Expr unary() throws IOException ( 
i
f
(
 1ook.tag == '-' ) ( 
m
o
v
e
(
)
;
 
return new Unary(Word.minus, u
n
a
r
y
(
)
)
;
 
3 
else i
f
(
 1ook.tag 
== ' ! '  ) ( 
Token tok = look; m
o
v
e
(
)
 ; return new Not 
(tok, 
u
n
a
r
y
(
)
)
 ; 
1 
else return factor() ; 
1 
The rest of the code in the parser deals with "factors" in expressions. The 
auxiliary procedure off set generates code for array address calculations, as 
discussed in Section 6.4.3. 
Expr factor() throws IOException ( 
Expr x = null; 
switch( 1ook.tag ) ( 
case '0: 
m
o
v
e
(
)
;
 x = boolo; 
m
a
t
c
h
(
'
)
'
)
;
 
return x; 
case Tag 
.NUM: 
x = new Constant(look, Type.Int); 
m
o
v
e
(
)
;
 
return x; 
case Tag.REAL: 
x = new Constant(look, Type.Float); m
o
v
e
(
)
;
 return x; 
case Tag.TRUE: 
x = Constant.True; 
m
o
v
e
(
)
;
 return x; 
case Tag. 
FALSE 
: 
x = Constant 
.False; 
m
o
v
e
(
)
;
 
return x; 
default 
: 
error("syntax error") 
; 
return 
x; 
case Tag. 
ID: 
String s = look.toString(); 
Id id = top.get(look) ; 
i
f
(
 id == null ) error(look.toString() + " undeclared"); 
986 
APPENDIX A. A COMPLETE FRONT END 
179) 
m
o
v
e
(
)
 ; 
180) 
if 
( 1ook.tag != 'C' 
) return id; 
181) 
else return off 
set 
(id) 
; 
182) 
3 
183) 
3 
184) 
Access offset(1d a
)
 throws IOException ( // I -> [
E
l
 I [
E
l
 I 
185) 
Expr i; 
Expr w; 
Expr ti, 
t2; 
Expr loc; // inherit id 
186) 
Type type = a.type; 
187) 
m
a
t
c
h
(
'
[
'
)
;
 
i = boolo; 
m
a
t
c
h
(
'
]
 '); 
// first index, 
I -> [ E 1 
188) 
type = ( 
(Array) 
type) 
. 
of 
; 
189) 
w = new Constant(type.width); 
190) 
ti = new Arith(new T
o
k
e
n
(
'
*
'
)
,
 
i, 
w
)
;
 
191) 
loc = ti; 
192) 
while( 1ook.tag == ' 
[' 
( 
// multi-dimensional I -> [ E I I 
193) 
m
a
t
c
h
(
'
[
'
)
;
 
i = boolo; 
m
a
t
c
h
(
'
]
'
)
;
 
194) 
type = (
(
A
r
r
a
y
)
 type) 
.of 
; 
195) 
w = new Constant(type.width); 
196) 
ti = new Arith(new T
o
k
e
n
(
'
*
'
)
,
 
i, 
w
)
;
 
197) 
t2 
= new Arith(new T
o
k
e
n
(
'
+
'
)
,
 
loc, 
t
i
)
;
 
198) 
loc = t2; 
199) 
3 
200) 
return 
new Access(a, loc, 
type) 
; 
201) 
3 
202) 1 
A.9 
Creating the Front End 
The code for the packages appears in five directories: main, lexer, symbol, 
parser, and inter. The commands for creating the compiler vary from system 
to system. The following are from a UNIX implementation: 
j 
avac lexerj* 
. 
j 
ava 
javac symbols/*. 
java 
javac inter/*. 
j 
ava 
javac parser/*. 
j 
ava 
j 
avac main/*. 
j 
ava 
The javac command creates . 
c l a s s  files for each class. The translator can 
then be exercised by typing j ava main. Main followed by the source program to 
be translated; e.g., the contents of file t e s t  
// File test 
int i; 
int j 
; float v; 
float x; 
float 
[
l
o
o
]
 a; 
while( true ) ( 
do i = i+l 
; while 
( a
[
i
l
 < v
)
 ; 
do j 
= j-1; 
while( a
[
j
l
 > v
)
;
 
if 
( i >= j ) break; 
x = a
[
i
]
 ; a
[
i
]
 
= a
[
j
l
;
 a
[
j
l
 
= x; 
3 
On this input, the front end produces 
A.9. CREATING THE FRONT END 
i = i + l  
t i = i * 8  
t 2 = a  [ t i ]  
if t 2  < v goto L3 
j = j - l  
t 3 = j * 8  
t 4 = a  C t 3 1  
i f  t 4  > v goto L
4
 
iffalse i >= j goto L8 
got0 L2 
t 5 = i * 8  
x = a C t 5 1  
t 6 = i * 8  
t 7 = j * 8  
t 8 = a  C t 7 1  
a  C t 6 1  = t 8  
t 9 = j * 8  
a C t 9 1 = x  
got0 L 1  
Try it. 
Appendix B 
Finding Linearly 
Independent Solutions 
Algorithm. 
B. 
1 : Finds a maximal set of linearly independent solutions for 
A 2  2 6, and expresses them as rows of matrix B. 
INPUT: An m x n matrix A. 
OUTPUT: A matrix B of linearly independent solutions to A 2  2 6
.
 
METHOD: The algorithm is shown in pseudocode below. Note that X[y] 
de- 
notes the yth row of matrix X, 
X[y 
: 
z] denotes rows y through z of matrix X, 
and X[y 
: 
z][u 
: v] 
denotes the rectangle of matrix X 
in rows y through z and 
columns u 
through v. 
990 
APPENDIX B. FINDING LINEARLY INDEPENDENT SOLUTIONS 
M = AT; 
r0 = 1; 
c
o
 = 1; 
B = I
,
,
,
;
 
/* 
an n-by-n identity matrix */ 
while ( true ) ( 
/* 1. Make M[ro 
: 
r' - 
I][co 
: 
c
'
 - 
I ]  into a diagonal matrix with 
positive diagonal entries and M[rl 
: 
n][co 
: rn] = 
0. 
M [r' : 
n] 
are solutions. */ 
r' = 
T o ;  
C
'
 = cb; 
while ( there exists M [r] 
[c] 
# 0 such that 
r - 
r' and c - 
c
'
 are both 2 
0 ) ( 
Move pivot M[r] 
[c] 
to M[r1] 
[c'] 
by row and column 
interchange; 
Interchange row r with row r' in B; 
if ( M[r'][c'] 
< 0 ) ( 
M[rl] 
= -1 * M[r1]; 
B[r1] 
= -1 * B[rl]; 
1 
for ( row = ro to n ) ( 
if ( TOW # r' and M[row][c'] 
# 0 ( 
u = -(M[row][c']/M[rl][c']); 
M[row] 
= M[row] 
+ 
u * M[rl]; 
/* 2. Find a solution besides M[r' : 
n]. 
It must be a 
nonnegative combination of M[ro 
: 
r' - 
l][co 
: 
m] */ 
Find k,,, . 
. 
. , 
k,t-1 2 0 such that 
k,, M [ro] 
[c' : 
m ]  
+ . 
- + 
k , ~  M [r' - 
I ]  
[c' : 
m] 
2 0; 
i f  ( there exists a nontrivial solution, say k, > 0 ) ( 
M[T] 
= k r o M [ ~ O ]  
+ . 
. . + 
krt-lM[r' - 
11; 
NoMoreSoln = false; 
) 
else /* M [r' : 
n] 
are the only solutions */ 
NoMoreSoln = true; 
/* 3. Make M[ro 
: r ,  - 
l][co: 
m] 
2 
0 */ 
i f  ( NoMoreSoln ) { /* Move solutions M[r' : 
n] 
to M[ro 
: 
r, - 
1
1
 */ 
for ( r  
= 
r' t o n )  
Interchange rows r and ro + 
r - 
r' in M and B ;  
r, = ro + 
71 - 
r' + 
1; 
else /* 
Use row addition to find more solutions */ 
r, = n + l ;  
for ( col = c
'
 to m ) 
i f  
( there exists M [row] 
[col] 
< 0 such that row 2 
ro ) 
i f  
( there exists M [r] 
[col] 
> 0 such that r 2 
ro ) 
for ( row = 
ro to r, - 
1 ) 
i f  ( M [row] 
[col] 
< 0 ) ( 
u = [(- M [row] 
[col]/M 
[r] 
[col])1 
; 
M[row] 
= M[row] 
+ 
u * M[r]; 
B[row] 
= B[row] 
+ 
u * B[r]; 
else 
for ( row = r, - 
1 to ro step -1 
) 
i f  ( M[row][col] 
< 0 ( 
rn = r n -  1; 
Interchange M [row] 
with M[r,]; 
Interchange B [row] 
with B [r,] ; 
) 
/* 
4. Make M[ro 
: 
rn - 
1][1 
: co - 
I ]  2 
0 */ 
for ( row = ro to r, - 
1 
) 
for ( col = 1 to co - 
1 ) 
i f  
( M [row] 
[col] 
< 0 { 
Pick an r such that M[r][col] 
> 0 and r < ro; 
u = [(-M[row][col]/M[r][col])l; 
M[row] 
= M[row] 
+ 
u * M[r]; 
B[row] 
= 
B[row] 
+ 
u * B[r]; 
1 
992 
APPENDIX B. FINDING LINEARLY INDEPENDENT SOLUTIONS 
/* 5. I
f
 necessary, repeat with rows M[r, : 
n] */ 
if ( (NoMoreSoln 
or r, > n or r, == ro) 
C 
Remove rows r, to n from B; 
return B; 
3 
else C 
c, = 
m + 1 ;  
for ( col = rn to 1 
step -1 ) 
if ( there is no M[r][col] 
> 0 such that r < r, C 
Cn = en- 1; 
Interchange column col with c, in M ;  
3 
To = 
T n ;  
Co = cn; 
3 
Index 
Abstract syntax tree 
See Syntax tree 
Abu-Sufah, W. 900 
Acceptance 149 
Accepting state 
See Final state 
Access link 434, 445-449 
Action 58-59, 249, 327 
Activation record 433-452 
Activation tree 430-433 
Actual parameter 33, 434, 942 
Acyclic call string 910 
Acyclic path 667 
Acyclic test 821-822 
Ada 391 
Address 364, 374 
Address descriptor 543, 545-547 
Address space 427 
Advancing edge 661 
Affine array access 781,801-804,815- 
826 
Affine expression 687, 770 
Affine partitioning 781 
Affine space partition 830-838 
Affine transformation 778-782,846- 
851 
Aho, A. V. 189-190, 301, 579-580 
Aho-Corasick algorithm 138-140 
Algebraic identities 536, 552 
Alias 35, 713, 903, 917, 933 
Alignment 374, 428 
Allen, F. E. 704, 899-900, 962 
Allen, R. 900 
Allocation, of memory 453 
Alpha 703 
Alphabet 117 
Ambiguous grammar 47, 203-204, 
210-212,255,278-283,291- 
294 
Amdahl's law 774 
Analysis 4 
Ancestor 46 
Andersen, L. 962 
Annotated parse tree 54 
Anticipated expression 645-648,653 
Antidependence 711, 816 
Antisymmetry 619 
Antlr 300, 302 
Architecture 19-22 
Arithmetic expression 49-50? 68-69? 
378-381, 971-974 
Array 373-375, 381-384, 537-539, 
541,584,712-713,770, 920 
See also Affine array access 
Array contraction 884-887 
ASCII 117 
Assembler 3 
Assembly language 13, 508 
Associativity 48, 122, 279-281, 293, 
619 
Atom 921 
Attribute 54, 112 
See also Inherited attribute, Main 
attribute, Synthesized at- 
tribute 
Attribute grammar 306 
Augmented grammar 243 
Auslander, M. A. 580 
Auto-increment 739 
Automaton 147 
INDEX 
See also Deterministic finite au- 
tomaton, LR(0) automaton, 
Nondeterministic finite au- 
tomaton 
Available expression 610-615, 648- 
649, 653 
Avots, D. 962-963 
Back edge 662,664-665 
Back end 4, 357 
Backpat 
ching 4 
10-4 1 
7 
Backus, J. W. 300-301 
Backus-Naur form 
See BNF 
Backward flow 610, 615, 618, 627, 
669 
Baker, H. G. Jr. 502 
Baker's algorithm 475-476, 482 
Ball, T. 962 
Banerjee, U. 900 
Banning, J. P. 962 
Barth, J. M. 962 
Base address 381 
Base register 762 
Basic block 525-541, 597, 600-601, 
721-726 
Basic type 371 
Bauer, F. L. 354-355 
BDD 951-958 
Bddbddb 961 
Bergin, T. J. 38 
Berndl, M. 961-962 
Bernstein, D. 766-767 
Best-fit 458 
Big-oh 159 
Binary alphabet 117 
Binary decision diagram 
See BDD 
Binary translation 22 
Binning, of chunks 458 
Birman, A. 301 
Bison 300 
Block 29, 86-87,95 
See also Basic block 
Block structure 
See Static scope 
Blocking 770-771,785-787,877-880, 
888 
BNF 
See Context-free grammar 
Body 43,197, 923 
Body region 673 
Boolean expression 399-400, 403- 
409,411-413,974-977 
Bottom element 619, 622 
Bottom-up parser 233-240 
See also LR parser, Shift-reduce 
parser 
Boundary condition 615 
Boundary tag 459 
Bounds checking 19, 24, 920-921 
Bounimova, E. 962 
Branch 
See Jump 
Branch-and-bound 824-825 
Break-statement 416-417 
Brooker, R. A. 354 
Bryant, R. E. 961-962 
Buffer 115-117 
Buffer overflow 918, 920-921 
Burke, M. 900 
Bus 772-773 
Bush, W. R. 962-963 
Bytecode 2 
C 13, 18, 25, 28-29, 381, 498, 903, 
934 
Cache 20, 454-455, 457, 772, 783- 
785 
Cache interference 788 
Cadar, C. 963 
Call 365,423-424,467,518-522,539, 
541 
Call graph 904-906,943-944 
Call site 904, 950 
Call string 908-910, 946-949 
Callahan, D. 961, 963 
Call-by-name 35 
INDEX 
Call-by-reference 34 
Call-by-value 34 
Calling sequence 436-438 
Canonical derivation 
See Rightmost derivation 
Canonical LR parser 259, 265-266, 
283 
Canonical LR(1) set of items 260- 
264 
Canonical LR(0) set of items 243, 
247 
Cantor, D. C. 300-301 
Carbin, M. 963 
Case sensitivity 125 
CFG 
See Grammar 
Chaitin, G. J. 580 
Chandra, A. K. 580 
Character class 123, 126 
Charles, P. 900 
Chelf, B. 963 
Chen, S. 766-767,899, 901 
Cheney, C. J. 502-503 
Cheney's algorithm 479-482 
Cheong, G. I. 901 
Child 46 
Chomsky, N. 300-301 
Chomsky Normal Form 232, 300 
Chou, A. 963 
Chow, F. 579-580 
Chunk 457-459 
Church, A. 502-503 
Circular dependency 307 
CISC 21, 507-508 
Class 33, 376 
Class variable 25-26 
Clock 708 
Cloning 910-9 1 
1 
Closure 119,121-122,243-245,261- 
262 
See also Positive closure 
Closure, of transfer functions 679 
Coalescing, of chunks 459-460 
Cocke, J. 301, 579-580, 704, 900 
Cocke-Younger-Kasami algorithm 232, 
301 
Code generation 10-11,505-581,707- 
767 
See also Scheduling 
Code motion 592 
See also Downward code mo- 
tion, Loop-invariant expres- 
sion, Partial redundancy elim- 
ination, Upward code mo- 
t 
ion 
Code optimization 5, 10, 15-19,368, 
583-705, 769-963 
Code scheduling 
See Scheduling 
Coercion 99, 388-389 
Coffman, E. G. 900-901 
Coherent cache protocol 772-773 
Collins, G. E. 503 
Coloring 556-557 
Column-major order 382, 785 
Comment 77 
Common subexpression 533-535,588- 
590, 611, 639-641 
Communication 828, 881-882, 894 
Commutativity 122, 619 
Compile time 25 
Complex instruction-set computer 
See CISC 
Composition 624, 678, 693 
Computer architecture 
See Architecture 
Concatenation 119, 121-122 
Concurrent garbage collection 495- 
497 
Conditional jump 513, 527 
Configuration 249 
Conflict 144, 565 
See also Reduce-reduce conflict, 
Shift 
-reduce conflict 
Conservative data-flow analysis 603 
Constant 78-79 
Constant folding 536, 632-637 
Const 
ant propagation 
See Constant folding 
INDEX 
Constraint 
See Control-dependence constraint, 
Data dependence, Resource 
constraint 
Context sensitivity 906-908,945-951 
Context-free grammar 
See Grammar 
Context-free language 200,215-216 
Context-sensitive analysis 906-907, 
945-950 
Contiguous evauation 574 
Continue-statement 416-41 7 
Control equivalence 728 
Control flow 399-409,413-417, 525 
Control link 434 
Control-dependence constraint 710, 
716-717 
Control-flow equation 600, 605 
Convex polyhedron 789-790, 795- 
796 
Cook, B. 962 
Cooper, K. D. 580, 963 
Copy propagation 590-591 
Copy statement 544, 937 
Copying garbage collector 478-482, 
488, 497-498 
Corasick, M. J. 189-190 
Cousot, P. 704 
Cousot, R. 704 
C++ 13, 18, 34, 498 
Critical cycle 758 
Critical edge 643 
Critical path 725 
Cross edge 662 
CUP 300, 302 
Cutset 645 
Cyclic garbage 488 
CYK algorithm 
See Cocke-Younger-Kasami 
al- 
gorit 
hm 
Cytron, R. 704, 900 
DAG 359-362,533-541, 951 
Dain, J. 300-301 
Dalton, M. 962 
Dangling else 210-212, 281-283 
Dangling pointer 461 
Dantzig, G. 900 
Das, M. 961, 963 
Data abstraction 18 
Data dependence 71 
1-715,732,747- 
749,771,781-782,804-805, 
815-826 
See also Antidependence, Out- 
put dependence, True de- 
pendence 
Data locality 891-892 
See also Locality 
Data reuse 804-815, 887-888 
Data space 779-780 
Data-dependence graph 722-723 
Data-flow analysis 18, 23, 597-705, 
921 
Data-flow analysis framework 618 
Datalog 921-933 
Datalog program 923 
Davidson, E. S. 767 
Dead code 533, 535, 550, 591-592 
Dead state 172, 183 
Dead variable 608 
Deallocation, of memory 453, 460- 
463 
Declaration 32, 373, 376 
Declarative language 13 
Decode 708 
Def 609 
Definition 32 
Definition, of a variable 601 
Dependency graph 310-312 
Depth, of a flow graph 665 
Depth-first order 660 
Depth-first search 57 
Depth-first spanning tree 660 
Dereferencing 46 
1 
DeRemer, F. 300-301 
Derivation 44-46, 199-202 
See also Leftmost derivation, Right- 
most derivation 
Descendant 46 
INDEX 
Deterministic finite automaton 149- 
156,164-166,170-186,206 
DFA 
See Deterministic finite automa- 
ton 
Dijkstra, E. W. 502-503 
Diophantine equation 818-820 
Directed acyclic graph 
See DAG 
Direct-mapped cache 457, 788 
Display 449-451 
Distributive framework 625,635-636 
Distributivity 122 
Do-across loop 743-745 
Do-all loop 738 
Domain, of a data-flow analysis 599, 
615 
Domain, of a relation 954 
Dominator 656-659, 672, 728 
Dominator tree 657 
Donnelly, C. 301 
Downward code motion 731-732 
Dumitran, D. 963 
Dynamic access 816 
Dynamic loading 944 
Dynamic policy 25 
Dynamic programming 573-577 
See also Cocke-Younger-Kasami 
algorithm 
Dynamic RAM 456 
Dynamic scheduler 719, 737 
Dynamic scope 31-33 
Dynamic storage 429 
See also Heap, Run-time stack 
Earley, J. 301 
Earliest expression 649-650, 654 
Eaves, B. C. 900 
EDB 
See Extensional database pred- 
icate 
Edge 
See Advancing edge, Back edge, 
Critical edge, Cross edge, 
Retreating edge 
Emami, M. 961, 963 
Empty string 44, 118, 121 
Engler, D. 963 
Entry node 531, 605 
Environment 26-28 
Epilog 742 
€ 
See Empty string 
€-free grammar 232 
c-production 63, 65-66 
Eqn 331 
Equivalence-based analysis 935 
Error correction 113-114, 192-196, 
228-231 
Error production 196 
Error recovery 283-284, 295-297 
Ershov, A. P 426, 579-580, 705 
Ershov number 567-572 
Euclidean algorithm 820 
Execution path 597, 628 
Exit block 677 
Exit node 605 
Expression 94,96-97, 101-105,359, 
568-572 
See also Arithmetic expression, 
Boolean expression, Infix 
expression, Postfix expres- 
sion, Prefix expression, Reg- 
ular expression, Type ex- 
pression 
Extensional database predicate 924 
Fahndrich, M. 961, 963 
Fall-through code 406 
Farkas' lemma 872-875 
Feautrier, P. 900 
Feldrnan, S. I. 426 
Fenichel, R. R. 502-503 
Ferrante, J. 704, 900 
Fetch 708 
Field 377, 584, 935 
Field load 937 
Field store 937 
INDEX 
Fift 
h-generation language 13 
Final state 130-131, 147, 205 
Finite automaton 
See Automaton 
FIRST 220-222 
First-fit 458 
First-generation language 13 
Firstpos 175-177 
Fischer, C. N. 580 
Fisher, J. A. 766-767 
Fission 848, 850, 854 
Fixedpoint 
See Maximum fixedpoint 
Flex 189-190 
Floating garbage 484 
Flow graph 529-531 
See also Reducible flow graph, 
Super control-flow graph 
Flow sensitivity 933, 936-937 
Floyd, R. W. 300-301 
FOLLOW 220-222 
Followpos 177-1 79 
Formal parameter 33, 942 
Fortran 113, 382, 779, 886 
Fortran H 703 
Forward flow 615, 618, 627, 668 
Fourier-Motzkin algorithm 796-797 
Fourt 
h-generation language 13 
Fragmentation 457-460 
Framework 
See Data-flow analysis frame- 
work, Distributive frame- 
work, Monotone framework 
Fraser, C. W. 580 
Free chunk 457 
Free list 459-460,471 
Free state 473 
Frege, G. 502-503 
Front end 4, 40-41, 357, 986 
Frontier 
See Yield 
Full redundancy 645 
Fully permutable loops 861, 864- 
867, 875-876 
Fully ranked matrix 808 
Function 29 
See also Procedure 
Function call 
See Call 
Function type 371,423 
Functional language 443 
Fusion 848, 850 
Ganapathi, M. 579-580 
Gao, G. 902 
Garbage collection 25, 430,463-499 
See also Mark-and-compact , 
Mark- 
and-sweep, Short-pause garbage 
collection 
GCD 818-820 
Gear, C. W. 705 
Gen 603,611 
Generational garbage collection 483, 
488-489 
Gen-kill form 603 
Geschke, C. M. 705 
Ghiya, R. 961, 963 
Gibson, R. G. 38 
Glaeser, C. D. 767 
Glanville, R. S. 579-580 
Global code optimization 
See Code optimization 
Global variable 442 
GNU 38,426 
Gosling, J. 426 
GOT0 246, 249, 261 
Graham, S. L. 579-580 
Grammar 42-50, 197-199,204-205 
See also Ambiguous grammar, 
Augmented grammar 
Grammar symbol 199 
Granularity 917 
Granularity, of parallelism 773-775 
Graph 
See Call graph, DAG, Data-de- 
pendence graph, Dependency 
graph, Flow graph, Program- 
dependence graph 
Graph coloring 
INDEX 
See Coloring 
Greatest common divisor 
See GCD 
Greatest lower bound 620, 622 
Gross, T. R. 766-767 
Ground atom 921 
Group reuse 806, 811-813 
Grune, D. 302 
Gupta, A. 900-901 
Hallem, S. 963 
Halstead, M. H. 426 
Handle 235-236 
Hanson, D. R. 580 
Hardware register renaming 714 
Hardware synthesis 22 
Head 42, 197, 923 
Header 665, 672 
Heap 428-430,452-463,518, 935 
Hecht, M. S. 705 
Height, of a semilattice 623, 626, 
628 
Heintze, N, 961, 963 
Hendren, L. J. 961-963 
Hennessy, J. L. 38, 579-580, 766- 
767, 899, 901 
Hewitt, C. 502-503 
Hierarchical reduction 761-762 
Hierarchical time 857-859 
Higher-order function 444 
Hoare, C. A. R. 302 
Hobbs, S. 0 .  
705 
Hole 457 
Hopcroft, J. E. 189-190, 302 
Hopkins, M. E. 580 
Hudson, R. L. 582-503 
Hudson, S. E. 302 
Huffman, D. A. 189-190 
Huskey, H. D. 426 
IDB 
See Intensional database pred- 
icate 
Ideal solution, to a data-flow prob- 
lem 628-630 
Idempotence 122, 619 
Identifier 28, 79-80 
Identity function 624 
If-statement 401 
Immediate dominator 657-658 
Imperative language 13 
Inclusion-based analysis 935 
Increment instruction 509 
Incremental evaluation 928-930 
Incremental garbage collection 483- 
487 
Incremental translation 
See On-the-fly generation 
Independent variables test 820-821 
Index 365 
Indexed address 513 
Indirect address 513 
Indirect triples 368-369 
Induction variable 592-596,687-688 
Infix expression 40, 52-53 
Ingerman, P. Z. 302 
Inheritance 18 
Inherited attribute 55,304-305,307 
Initial state 
See Start state 
Initialization 615 
Initiation interval 745 
Iulining 903-904, 9 
14 
Input buffering 
See Buffer 
Instruction pipeline 708-709 
See also Software pipelining 
Integer linear programming 817-825 
Intensional database predicate 924 
Interleaving 887-890 
Intermediate code 9, 91-105, 357- 
426, 507, 971-981 
Interpreter 2 
Interprocedural analysis 713, 903- 
964 
Interrupt 526 
Intersection 612-613, 615, 620, 650 
Intraprocedural analysis 903 
INDEX 
Irons, E. T. 354 
Item 242-243 
See also Kernel item, Set of items, 
Valid item 
Iteration space 779-780, 788-799 
Iterative data-flow algorithm 605- 
607, 610, 614, 626-628 
J 
Jacobs, C. J. H. 302 
Java 2, 13, 18-19, 25, 34, 76, 381, 
903, 934, 944 
Java virtual machine 507-508 
Jazayeri, M. 354 
JFlex 189-190 
Johnson, R. K. 705 
Johnson, S. C. 300-302, 355, 426, 
502-503,579-580 
Join 621, 955 
Jump 513,527, 551-552 
Jumping code 408, 974-977 
Just-in-time compilation 508 
JVM 
See Java virtual machine 
Kam, J. B. 705 
Kasami, T. 301-302, 705 
Kennedy, K. 899-900,963 
Kernel 777 
Kernel item 245, 272-273 
Kernighan, B. W. 189-190 
Keyword 50-51, 79-80, 132-133 
Kill 601, 603, 611 
Killdall, G. 704-705 
Kleene closure 
See Closure 
Kleene, S. C. 189-190 
Knoop, J. 705 
Knuth, D. E. 189-190,300,302,354- 
355, 502-503 
Knuth-Morris-Pratt algorithm 136- 
138 
Korenjak, A. J. 300, 302 
Kosaraju, S. R. 705 
K u c ~ ,  
D. J. 766-767,899-901 
Kung, H. T. 901 
Label 46, 364, 366 
LALR parser 259,266-275,283,287 
Lam, M. S. 767, 899-902, 961-964 
Lamport, L. 503, 766-767, 899-901 
Language 44, 118 
See also Java, Source language, 
Target language 
Lastpos 175-177 
Latest expression 649, 654 
Lattice 621 
See also Semilattice 
Lattice diagram 621-622 
L-attributed definition 313-314,331- 
352 
Law 
See Associativity, Commutativ- 
ity, Distributivity, Idempo- 
tence 
Lawrie, D. H. 900 
Lazy code motion 
See Partial redundancy elimi- 
nation 
Lea 458 
Leader 526-527 
Leaf 45-46 
Leaf region 673 
Least upper bound 621 
LeBlanc, R. J. 580 
Left side 
See Head 
Left-associativity 48 
Left-factoring 2 
14-2 15 
Leftmost derivation 201 
Left-recursion 67-68, 71, 212-214, 
328-331 
Left-sentential form 201 
Leiserson, C. E. 901 
Lesk, M. E. 189-190 
Leu, T. 963 
Levin, V. 962 
INDEX 
Lewis, P. M. I1 300, 302, 355 
Lex 126-127,140-145,166-167,189- 
190, 294-295 
Lexeme 111 
Lexical analyzer 5-7, 41, 76-84, 86, 
109-190,209-210,294-295, 
967-969 
Lexical error 194 
Lexical scope 
See Static scope 
Lexicographic order 79 
1 
Liao, S.-W. 901 
Lichtenber, J. 962 
Lieberman, H. 502-503 
Lim, A. W. 901 
Linear programming 
See Integer linear programming 
List scheduling 723-726 
Literal 922 
Live variable 528-529,608-610,615 
Livshits, V. B. 962-963 
LL grammar 223 
LL parser 
See Predictive parser 
LLgen 300 
Load instruction 512 
Loader 3 
Local code optimization 
See Basic block 
Locality 455, 769 
See also Spatial locality, Tem- 
poral locality 
Location 26-28 
Logical address 427 
Logical error 194 
Lohtak, 0. 
962 
Lookahead 78,144-145,171-172,272- 
275 
Lookahead-LR parser 
See LALR parser 
Loop 531, 554, 556, 655-656, 775 
See also Do-all loop, Fully per- 
mutable loops, Natural loop 
Loop fission 
See Fission 
Loop fusion 
See Fusion 
Loop nest 780, 791, 797, 862 
Loop region 674 
Loop reversal 
See Reversal 
Loop unrolling 735, 740-741, 743 
Loop-invariant expression 641-642 
Loop-residue test 822-823 
Loveman, D. B. 901 
Lowry, E. S. 579-580, 705 
LR(0) automaton 243,247-248,252 
LR parser 53-252,275-277,325,348- 
352 
See also Canonical LR parser, 
LALR parser, SLR parser 
L-value 26, 98 
See also Location 
Machine language 508 
Macro 13 
Main attribute 341 
Mark-and-compact 476-482 
Mark-and-sweep 471-476, 482 
Marker nonterminal 349 
Markstein, P. W. 580 
Martin, A. J. 503 
Martin, M. C. 963 
Matrix multiplication 782-788 
Maximum fixedpoint 626-628,630- 
631 
Maydan, D. E. 899, 901 
McArthur, R. 426 
McCarthy, J. 189-190, 502-503 
McClure, R. M. 302 
McCullough, W. S. 189-190 
McGarvey, C. 962 
McKellar, A. C. 900-901 
McNaughton, R. 189-190 
McNaughton-Yarnada-Thompson al- 
gorithm 159-161 
Medlock, C. W. 579-580, 705 
Meet 605, 615, 618-619, 622-623, 
633, 678, 695 
INDEX 
Meet-over-paths solution 629-631 
N 
Memaization 823 
Memory 20, 772-773 
See also Heap, Physical mem- 
ory, Storage, Virtual mem- 
ory 
Memory hierarchy 20, 454-455 
Memory leak 25, 461 
Message-passing machine 773, 894 
META 300 
Metal 918, 962 
Method 29 
See also Procedure, Virtual method 
Method call 
See Call 
Method invocation 33 
MGU 
See Most general unifier 
Milanova, A. 962-963 
Milner, R. 426 
Minimization, of states 180-185 
Minsky, M. 503 
ML 387, 443-445 
Mock, 0 .  
426 
Modular resource-reservation table 
746-747, 758 
Modular variable expansion 758-761 
Monotone framework 624-628, 635 
Moore, E. F. 189-190 
MOP 
See Meet-over-paths solution 
Morel, E. 705 
 orris, D. 354 
Morris, J. H. 189-190 
Mbss, J. E. B. 502-503 
Most general unifier 393 
See also Unification 
Motwani, R. 189-190,302 
Mowry, T. C. 900-901 
Multiprocessor 772-773, 895 
See also SIMD, Single-program 
multiple data 
Muraoka, Y. 766-767,899, 901 
Mptator 464 
NAA 690 
NAC 633 
Name 26-28 
Narrowing 388-389 
Natural loop 667, 673 
Naur, P. 300, 302 
Neighborhood compaction 736 
Neliac 425 
Nested procedure declarations 442- 
445 
Next-fit 458-459 
NFA 
See Nondeterministic finite au- 
tomaton 
Node 46 
Node merging 953 
Nondeterministic finite automaton 
147-148,152-175,205,257 
Nonreducible flow graph 
See Reducible flow graph 
Nonterminal 42-43, 45, 197-198 
See also Marker nonterminal 
Nonuniform memory access 773 
Null space 808-809 
Nullable 175-1 77 
Nullity 808 
NUMA 
See Nonuniform memory access 
Object code 358 
See also Code generation 
Object creation 937 
Object ownership 462 
Object program 427-428 
Object sensitivity 950 
Ob 
ject-oriented language 
See C++, Java 
Offset 377-378 
Ogden, W. F. 354 
Olsztyn, J. 426 
Ondrusek, B. 962 
INDEX 
On-the-fly generation 340-343,380- 
381 
Optimization 
See Code optimization 
Ordered BDD 952 
Output dependence 711, 816 
Overloading 99, 390-391 
Paakki, J. 354-355 
Padua, D. A. 902 
Panic-mode garbage collection 492- 
493 
Panic-mode recovery 195-196, 228- 
230, 283-284 
Panini 300 
Parafrase 899 
Parallel garbage collection 495-497 
Parallelism 19-20, 707-902, 917 
Parameter 422 
See also Actual parameter, For- 
mal parameter, Procedure 
parameter 
Parameter passing 33-35, 365 
Parametric polymorphism 391 
See also Polymorphism 
Parent 46 
Parr, T. 302 
Parse tree 45-48, 201-204 
See also Annotated parse tree 
Parser 8, 41, 45, 60-61, 110-111, 
191-302,981-986 
See also Bottom-up parser, Top- 
down parser 
Parser generator 
See Antlr, Bison, CUP, LLgen, 
Yacc 
Parser state 241-242 
See also Set of items 
Partial garbage collection 483, 487- 
494 
Partial order 619-621, 623 
Partial redundancy elimination 639- 
655 
Partially dead variable 655 
Partially ordered set 
See Poset 
Pass 11 
Patel, J. H. 767 
Path 
See Acyclic path, Critical path, 
Execution path, Meet-over- 
paths solution, Weight, of 
a path 
Pattern 111 
Pattern matching, of trees 563-567 
Patterson, D. A. 38, 579-580, 766- 
767, 899, 901 
Pause time 465 
See also Short-pause garbage col- 
lection 
P-code 386 
PDG 
See Program-dependence 
graph 
Peephole optimization 549-552 
Pelegri-Llopart , 
E. 580 
Permuation 849-850 
Peterson, W. W. 705 
PFC 899 
Phase I1 
Phoenix 38 
Phrase-level recovery 196, 231 
Physical address 427 
Physical memory 454-455 
Pierce, B. C. 426 
Pincus, J. D. 962-963 
Pipeline 
See Instruction pipeline, Pipelin- 
ing, Software pipelining 
Pipelining 861-884 
Pitts, W. 189-190 
Pnueli, A. 964 
Pointer 365, 373, 514, 539, 935 
See also Dangling pointer, Stack 
pointer 
Pointer analysis 713, 903, 917, 933- 
95 
1 
Poison bit 718 
Polyhedron 
See Convex polyhedron 
INDEX 
Polymorphism 391-395 
Porterfield, A. 900, 902 
Poset 619 
Positive closure 123 
Post 
dominator 728 
Postfix expression 40, 53-54 
Postfix translation scheme 324-327 
Postorder traversal 58, 432 
See also Depth-first order 
Postponable expression 646,649,65 
1- 
654 
Power set 620 
Pratt, V. R. 189-190 
PRE 
See Partial redundancy elimi- 
nation 
Precedence 48, 121-122, 279-281, 
293-294 
Predecessor 529 
Predicate 921-922 
Predicated execution 718, 761 
Predictive parser 64-68,222-231,343- 
348 
Prefetch 457 
Prefetching 718, 896 
Prefix 119, 918, 962 
Prefix expression 327 
Preorder traversal 58, 432 
Preprocessor 3 
Prioritized topological order 725-726 
Private 31 
Privatizable variable 758 
Procedure 29, 422-424 
Procedure call 
See Call 
Procedure parameter 448-449 
Processor space 779-781, 838-841 
Product lattice 622-623 
Production 42-43, 45, 197, 199 
See also Error production 
Proebsting, T. A. 580 
Program-dependence graph 854-857 
Programming language 12-14, 25- 
35 
See also Ada, C, C++, Fortran, 
Java, ML 
Projection 955 
Prolog 742 
Prosser, R. T. 705 
Protected 31 
Pseudoregister 713 
PTRAN 900 
Public 31 
Pugh, W. 899, 902 
Purify 25, 462 
Qian, F. 962 
Quadruple 366-368 
Quicksort 431-432, 585 
Rabin, M. 0. 189-190 
Rajamani, S. K. 962 
Randell, B. 502-503 
Rank, of a matrix 807-809 
Rau, B. R. 767 
Reaching definitions 601-608, 615 
Read barrier 486 
Record 371, 376-378, 584 
Recursive descent 338-343 
Recursive type 372 
Recursive-descent parser 64,219-222 
Reduced instruction-set computer 
See RISC 
Reduce-reduce conflict 238-240,293 
Reducible flow graph 662,664,673- 
677, 684-685 
Reduction 234, 324 
Reduction in strength 536,552, 592- 
596 
Reference 
See Pointer 
Reference count 462-463,466,468- 
470 
Reference variable 34, 686-689 
Reflection 944-945 
Reflexivity 619 
INDEX 
Region 672-686, 694-699, 733-734, 
Root 46 
911 
Root set 466-467,488 
Region-based allocation 463 
Rosen, B. K. 704 
Register 18, 20, 454-455, 542-543, 
Rosenkrantz, D. J. 355 
714-715 
Rotating register file 762 
See also Pseudoregister, Rot 
at- 
Rothberg, E. E. 900-901 
ing register file 
Rounds, W. C. 354 
Register allocation 510-512,553-557, 
Rountev, A. 962-963 
570-572,716, 743 
ROW 
Register assignment 510, 556 
See Tuple 
Register descriptor 543, 545-547 
Row-major order 382, 785 
Register pair 510 
Roy, D. 963 
Register renaming 
Rule 922-923 
See Hardware register renam- 
Run time 25 
ing 
Run-time environment 427 
Regular definition 123 
Run-time stack 428-451, 468 
Regular expression 116-122,159-163, 
Russell, L. J. EdB-503 
179-180, 189, 210 
Ruwase, 0. 
962-963 
Rehof, J. 961, 963 
R-value 26, 98 
Re-indexing 848, 850 
Ryder, B. G. 962-963 
Relation 922, 954 
Relative address 371, 373, 381 
S 
Remembered set 491 
Sadgupta, S. 767 
Renvoise, C. 705 
Safety 
Reserved word 
See Conservative data-flow anal- 
See Keyword 
ysis 
Resource constraint 71 
1 
Samelson, K. 354-355 
Resource-reservation table 719-720 
Sarkar, V. 902 
See also Modular resource-reservation S-attributed definition 306, 312-313, 
table 
324 
Retreating edge 661, 664-665 
Scaling 848, 850 
Return 365, 467, 518-522, 906, 942 
Scanned state 474 
Return value 434 
Scanning 110 
Reuse 
See also Lexical analyzer 
See Data reuse 
SCC 
Reversal 849-850 
See Strongly connected compo- 
Right side 
nent 
See Body 
Scheduling 710-71 1, 716 
Right-associativity 48 
Scholten, C. S. 503 
Rightmost derivation 201 
Schorre, D. V. 302 
Right-sentential form 201, 256 
Schwartz, J. T. 579, 581, 704 
Rinard, M. 962-963 
Scope 86 
RISC 21, 507-508 
Scott, D. 189-190 
Ritchie, D. M. 426, 502-503 
Scott, M. L. 38 
Rodeh, M. 766-767 
Scripting language 13-14 

Preface         9
Audience
This textbook is for a first course on computer networking. It can be used in both 
computer science and electrical engineering departments. In terms of programming 
languages, the book assumes only that the student has experience with C, C++, Java, 
or Python (and even then only in a few places). Although this book is more precise 
and analytical than many other introductory computer networking texts, it rarely uses 
any mathematical concepts that are not taught in high school. We have made a delib-
erate effort to avoid using any advanced calculus, probability, or stochastic process 
concepts (although we’ve included some homework problems for students with this 
advanced background). The book is therefore appropriate for undergraduate courses 
and for first-year graduate courses. It should also be useful to practitioners in the 
telecommunications industry.
What Is Unique About This Textbook?
The subject of computer networking is enormously complex, involving many con-
cepts, protocols, and technologies that are woven together in an intricate manner. 
To cope with this scope and complexity, many computer networking texts are often 
organized around the “layers” of a network architecture. With a layered organization, 
students can see through the complexity of computer networking—they learn about 
the distinct concepts and protocols in one part of the architecture while seeing the 
big picture of how all parts fit together. From a pedagogical perspective, our personal 
experience has been that such a layered approach indeed works well. Nevertheless, 
we have found that the traditional approach of teaching—bottom up; that is, from the 
physical layer towards the application layer—is not the best approach for a modern 
course on computer networking.
A Top-Down Approach
Our book broke new ground 16 years ago by treating networking in a top-down 
­
manner—that is, by beginning at the application layer and working its way down 
toward the physical layer. The feedback we received from teachers and students alike 
have confirmed that this top-down approach has many advantages and does indeed 
work well pedagogically. First, it places emphasis on the application layer (a “high 
growth area” in networking). Indeed, many of the recent revolutions in ­
computer 
networking—including the Web, peer-to-peer file sharing, and media streaming—
have taken place at the application layer. An early emphasis on application-layer 
issues differs from the approaches taken in most other texts, which have only a 
small amount of material on network applications, their requirements, application-
layer paradigms (e.g., client-server and peer-to-peer), and application programming 
10         Preface
­
interfaces. ­
Second, our experience as instructors (and that of many instructors who 
have used this text) has been that teaching networking applications near the begin-
ning of the course is a powerful motivational tool. Students are thrilled to learn about 
how networking applications work—applications such as e-mail and the Web, which 
most students use on a daily basis. Once a student understands the applications, the 
student can then understand the network services needed to support these applica-
tions. The student can then, in turn, examine the various ways in which such services 
might be provided and implemented in the lower layers. Covering applications early 
thus provides motivation for the remainder of the text.
Third, a top-down approach enables instructors to introduce network applica-
tion development at an early stage. Students not only see how popular applica-
tions and protocols work, but also learn how easy it is to create their own network 
­
applications and application-level protocols. With the top-down approach, students 
get early ­
exposure to the notions of socket programming, service models, and 
­
protocols—important concepts that resurface in all subsequent layers. By providing 
socket programming examples in Python, we highlight the central ideas without 
confusing students with complex code. Undergraduates in electrical engineering 
and computer science should not have difficulty following the Python code.
An Internet Focus
Although we dropped the phrase “Featuring the Internet” from the title of this book 
with the fourth edition, this doesn’t mean that we dropped our focus on the Internet. 
Indeed, nothing could be further from the case! Instead, since the Internet has become 
so pervasive, we felt that any networking textbook must have a significant focus on 
the Internet, and thus this phrase was somewhat unnecessary. We continue to use the 
Internet’s architecture and protocols as primary vehicles for studying fundamental 
computer networking concepts. Of course, we also include concepts and protocols 
from other network architectures. But the spotlight is clearly on the Internet, a fact 
reflected in our organizing the book around the Internet’s five-layer architecture: the 
application, transport, network, link, and physical layers.
Another benefit of spotlighting the Internet is that most computer science and 
electrical engineering students are eager to learn about the Internet and its protocols. 
They know that the Internet has been a revolutionary and disruptive technology and 
can see that it is profoundly changing our world. Given the enormous relevance of 
the Internet, students are naturally curious about what is “under the hood.” Thus, it 
is easy for an instructor to get students excited about basic principles when using the 
Internet as the guiding focus.
Teaching Networking Principles
Two of the unique features of the book—its top-down approach and its focus on the 
Internet—have appeared in the titles of our book. If we could have squeezed a third 
Preface         11
phrase into the subtitle, it would have contained the word principles. The field of 
networking is now mature enough that a number of fundamentally important issues 
can be identified. For example, in the transport layer, the fundamental issues include 
reliable communication over an unreliable network layer, connection establishment/ 
teardown and handshaking, congestion and flow control, and multiplexing. Three fun-
damentally important network-layer issues are determining “good” paths between two 
routers, interconnecting a large number of heterogeneous networks, and managing the 
complexity of a modern network. In the link layer, a fundamental problem is sharing a 
multiple access channel. In network security, techniques for providing confidentiality, 
authentication, and message integrity are all based on cryptographic fundamentals. 
This text identifies fundamental networking issues and studies approaches towards 
addressing these issues. The student learning these principles will gain knowledge 
with a long “shelf life”—long after today’s network standards and protocols have 
become obsolete, the principles they embody will remain important and relevant. We 
believe that the combination of using the Internet to get the student’s foot in the door 
and then emphasizing fundamental issues and solution approaches will allow the stu-
dent to quickly understand just about any networking technology.
The Website
Each new copy of this textbook includes twelve months of access to a Companion 
­
Website for all book readers at http://www.pearsonglobaleditions.com/kurose, which 
includes:
• Interactive learning material. The book’s Companion Website contains 
­
VideoNotes—video presentations of important topics throughout the book 
done by the authors, as well as walkthroughs of solutions to problems similar to 
those at the end of the chapter. We’ve seeded the Web site with VideoNotes and 
­
online problems for chapters 1 through 5 and will continue to actively add and 
update this material over time. As in earlier editions, the Web site contains the 
interactive Java applets that animate many key networking concepts. The site also 
has interactive quizzes that permit students to check their basic understanding of 
the subject matter. Professors can integrate these interactive features into their 
lectures or use them as mini labs.
• Additional technical material. As we have added new material in each edition of 
our book, we’ve had to remove coverage of some existing topics to keep the book 
at manageable length. For example, to make room for the new ­
material in this 
­
edition, we’ve removed material on FTP, distributed hash tables, and ­
multicasting, 
Material that appeared in earlier editions of the text is still of ­
interest, and thus can 
be found on the book’s Web site.
• Programming assignments. The Web site also provides a number of detailed 
programming assignments, which include building a multithreaded Web ­
server, 
12         Preface
building an e-mail client with a GUI interface, programming the sender and 
­
receiver sides of a reliable data transport protocol, programming a distributed 
routing algorithm, and more.
• Wireshark labs. One’s understanding of network protocols can be greatly 
­
deepened by seeing them in action. The Web site provides numerous Wireshark 
assignments that enable students to actually observe the sequence of messages 
exchanged between two protocol entities. The Web site includes separate Wire-
shark labs on HTTP, DNS, TCP, UDP, IP, ICMP, Ethernet, ARP, WiFi, SSL, and 
 
on tracing all protocols involved in satisfying a request to fetch a Web page. We’ll 
continue to add new labs over time.
In addition to the Companion Website, the authors maintain a public Web site, 
http://gaia.cs.umass.edu/kurose_ross/interactive, containing interactive exercises 
that create (and present solutions for) problems similar to selected end-of-chapter 
problems. Since students can generate (and view solutions for) an unlimited number 
of similar problem instances, they can work until the material is truly mastered.
Pedagogical Features
We have each been teaching computer networking for more than 30 years. Together, 
we bring more than 60 years of teaching experience to this text, during which time 
we have taught many thousands of students. We have also been active researchers 
in computer networking during this time. (In fact, Jim and Keith first met each other 
as master’s students in a computer networking course taught by Mischa Schwartz 
in 1979 at Columbia University.) We think all this gives us a good perspective on 
where networking has been and where it is likely to go in the future. Nevertheless, 
we have resisted temptations to bias the material in this book towards our own pet 
research projects. We figure you can visit our personal Web sites if you are interested 
in our research. Thus, this book is about modern computer networking—it is about 
contemporary protocols and technologies as well as the underlying principles behind 
these protocols and technologies. We also believe that learning (and teaching!) about 
networking can be fun. A sense of humor, use of analogies, and real-world examples 
in this book will hopefully make this material more fun.
Supplements for Instructors
We provide a complete supplements package to aid instructors in teaching this 
course. This material can be accessed from Pearson’s Instructor Resource Center 
 
(http://www.pearsonglobaleditions.com/kurose). Visit the Instructor Resource Cen-
ter for ­
information about accessing these instructor’s supplements.
Preface         13
• PowerPoint® slides. We provide PowerPoint slides for all nine chapters. The 
slides have been completely updated with this seventh edition. The slides cover 
each chapter in detail. They use graphics and animations (rather than relying only 
on monotonous text bullets) to make the slides interesting and visually appealing. 
We provide the original PowerPoint slides so you can customize them to best suit 
your own teaching needs. Some of these slides have been contributed by other 
instructors who have taught from our book.
• Homework solutions. We provide a solutions manual for the homework prob-
lems in the text, programming assignments, and Wireshark labs. As noted 
­
earlier, we’ve introduced many new homework problems in the first six chapters 
of the book.
Chapter Dependencies
The first chapter of this text presents a self-contained overview of computer net-
working. Introducing many key concepts and terminology, this chapter sets the stage 
for the rest of the book. All of the other chapters directly depend on this first chapter. 
After completing Chapter 1, we recommend instructors cover Chapters 2 through 6 
in sequence, following our top-down philosophy. Each of these five chapters lever-
ages material from the preceding chapters. After completing the first six chapters, 
the instructor has quite a bit of flexibility. There are no interdependencies among 
the last three chapters, so they can be taught in any order. However, each of the last 
three chapters depends on the material in the first six chapters. Many instructors first 
teach the first six chapters and then teach one of the last three chapters for “dessert.”
One Final Note: We’d Love to Hear from You
We encourage students and instructors to e-mail us with any comments they might 
have about our book. It’s been wonderful for us to hear from so many instructors 
and students from around the world about our first five editions. We’ve incorporated 
many of these suggestions into later editions of the book. We also encourage instruc-
tors to send us new homework problems (and solutions) that would complement the 
current homework problems. We’ll post these on the instructor-only portion of the 
Web site. We also encourage instructors and students to create new Java applets that 
illustrate the concepts and protocols in this book. If you have an applet that you think 
would be appropriate for this text, please submit it to us. If the applet (including nota-
tion and terminology) is appropriate, we’ll be happy to include it on the text’s Web 
site, with an appropriate reference to the applet’s authors.
So, as the saying goes, “Keep those cards and letters coming!” Seriously, please 
do continue to send us interesting URLs, point out typos, disagree with any of our 
14         Preface
claims, and tell us what works and what doesn’t work. Tell us what you think should 
or shouldn’t be included in the next edition. Send your e-mail to kurose@cs.umass 
.edu and keithwross@nyu.edu.
Acknowledgments
Since we began writing this book in 1996, many people have given us invaluable 
help and have been influential in shaping our thoughts on how to best organize and 
teach a networking course. We want to say A BIG THANKS to everyone who has 
helped us from the earliest first drafts of this book, up to this seventh edition. We are 
also very thankful to the many hundreds of readers from around the world—students, 
faculty, practitioners—who have sent us thoughts and comments on earlier editions 
of the book and suggestions for future editions of the book. Special thanks go out to:
Al Aho (Columbia University)
Hisham Al-Mubaid (University of Houston-Clear Lake)
Pratima Akkunoor (Arizona State University)
Paul Amer (University of Delaware)
Shamiul Azom (Arizona State University)
Lichun Bao (University of California at Irvine)
Paul Barford (University of Wisconsin)
Bobby Bhattacharjee (University of Maryland)
Steven Bellovin (Columbia University)
Pravin Bhagwat (Wibhu)
Supratik Bhattacharyya (previously at Sprint)
Ernst Biersack (Eurécom Institute)
Shahid Bokhari (University of Engineering & Technology, Lahore)
Jean Bolot (Technicolor Research)
Daniel Brushteyn (former University of Pennsylvania student)
Ken Calvert (University of Kentucky)
Evandro Cantu (Federal University of Santa Catarina)
Jeff Case (SNMP Research International)
Jeff Chaltas (Sprint)
Vinton Cerf (Google)
Byung Kyu Choi (Michigan Technological University)
Bram Cohen (BitTorrent, Inc.)
Constantine Coutras (Pace University)
John Daigle (University of Mississippi)
Edmundo A. de Souza e Silva (Federal University of Rio de Janeiro)
Philippe Decuetos (Eurécom Institute)
Christophe Diot (Technicolor Research)
Prithula Dhunghel (Akamai)
Preface         15
Deborah Estrin (University of California, Los Angeles)
Michalis Faloutsos (University of California at Riverside)
Wu-chi Feng (Oregon Graduate Institute)
Sally Floyd (ICIR, University of California at Berkeley)
Paul Francis (Max Planck Institute)
David Fullager (Netflix)
Lixin Gao (University of Massachusetts)
JJ Garcia-Luna-Aceves (University of California at Santa Cruz)
Mario Gerla (University of California at Los Angeles)
David Goodman (NYU-Poly)
Yang Guo (Alcatel/Lucent Bell Labs)
Tim Griffin (Cambridge University)
Max Hailperin (Gustavus Adolphus College)
Bruce Harvey (Florida A&M University, Florida State University)
Carl Hauser (Washington State University)
Rachelle Heller (George Washington University)
Phillipp Hoschka (INRIA/W3C)
Wen Hsin (Park University)
Albert Huang (former University of Pennsylvania student)
Cheng Huang (Microsoft Research)
Esther A. Hughes (Virginia Commonwealth University)
Van Jacobson (Xerox PARC)
Pinak Jain (former NYU-Poly student)
Jobin James (University of California at Riverside)
Sugih Jamin (University of Michigan)
Shivkumar Kalyanaraman (IBM Research, India)
Jussi Kangasharju (University of Helsinki)
Sneha Kasera (University of Utah)
Parviz Kermani (formerly of IBM Research)
Hyojin Kim (former University of Pennsylvania student)
Leonard Kleinrock (University of California at Los Angeles)
David Kotz (Dartmouth College)
Beshan Kulapala (Arizona State University)
Rakesh Kumar (Bloomberg)
Miguel A. Labrador (University of South Florida)
Simon Lam (University of Texas)
Steve Lai (Ohio State University)
Tom LaPorta (Penn State University)
Tim-Berners Lee (World Wide Web Consortium)
Arnaud Legout (INRIA)
Lee Leitner (Drexel University)
Brian Levine (University of Massachusetts)
Chunchun Li (former NYU-Poly student)
16         Preface
Yong Liu (NYU-Poly)
William Liang (former University of Pennsylvania student)
Willis Marti (Texas A&M University)
Nick McKeown (Stanford University)
Josh McKinzie (Park University)
Deep Medhi (University of Missouri, Kansas City)
Bob Metcalfe (International Data Group)
Sue Moon (KAIST)
Jenni Moyer (Comcast)
Erich Nahum (IBM Research)
Christos Papadopoulos (Colorado Sate University)
Craig Partridge (BBN Technologies)
Radia Perlman (Intel)
Jitendra Padhye (Microsoft Research)
Vern Paxson (University of California at Berkeley)
Kevin Phillips (Sprint)
George Polyzos (Athens University of Economics and Business)
Sriram Rajagopalan (Arizona State University)
Ramachandran Ramjee (Microsoft Research)
Ken Reek (Rochester Institute of Technology)
Martin Reisslein (Arizona State University)
Jennifer Rexford (Princeton University)
Leon Reznik (Rochester Institute of Technology)
Pablo Rodrigez (Telefonica)
Sumit Roy (University of Washington)
Dan Rubenstein (Columbia University)
Avi Rubin (Johns Hopkins University)
Douglas Salane (John Jay College)
Despina Saparilla (Cisco Systems)
John Schanz (Comcast)
Henning Schulzrinne (Columbia University)
Mischa Schwartz (Columbia University)
Ardash Sethi (University of Delaware)
Harish Sethu (Drexel University)
K. Sam Shanmugan (University of Kansas)
Prashant Shenoy (University of Massachusetts)
Clay Shields (Georgetown University)
Subin Shrestra (University of Pennsylvania)
Bojie Shu (former NYU-Poly student)
Mihail L. Sichitiu (NC State University)
Peter Steenkiste (Carnegie Mellon University)
Tatsuya Suda (University of California at Irvine)
Kin Sun Tam (State University of New York at Albany)
Preface         17
Don Towsley (University of Massachusetts)
David Turner (California State University, San Bernardino)
Nitin Vaidya (University of Illinois)
Michele Weigle (Clemson University)
David Wetherall (University of Washington)
Ira Winston (University of Pennsylvania)
Di Wu (Sun Yat-sen University)
Shirley Wynn (NYU-Poly)
Raj Yavatkar (Intel)
Yechiam Yemini (Columbia University)
Dian Yu (NYU Shanghai)
Ming Yu (State University of New York at Binghamton)
Ellen Zegura (Georgia Institute of Technology)
Honggang Zhang (Suffolk University)
Hui Zhang (Carnegie Mellon University)
Lixia Zhang (University of California at Los Angeles)
Meng Zhang (former NYU-Poly student)
Shuchun Zhang (former University of Pennsylvania student)
Xiaodong Zhang (Ohio State University)
ZhiLi Zhang (University of Minnesota)
Phil Zimmermann (independent consultant)
Mike Zink (University of Massachusetts)
Cliff C. Zou (University of Central Florida)
We also want to thank the entire Pearson team—in particular, Matt Goldstein and 
Joanne Manning—who have done an absolutely outstanding job on this seventh 
­
edition (and who have put up with two very finicky authors who seem congenitally 
­
unable to meet deadlines!). Thanks also to our artists, Janet Theurer and Patrice 
Rossi Calkin, for their work on the beautiful figures in this and earlier editions of 
our book, and to Katie Ostler and her team at Cenveo for their wonderful production 
work on this edition. Finally, a most special thanks go to our previous two editors 
at ­
Addison-Wesley—Michael Hirsch and Susan Hartman. This book would not be 
what it is (and may well not have been at all) without their graceful management, 
constant encouragement, nearly infinite patience, good humor, and perseverance.
18         Preface
Acknowledgments for the Global Edition
Pearson would like to thank and acknowledge the following people for their 
contributions to the Global Edition.
Contributors 
Mario De Francesco (Aalto University)
Reviewers
Arif Ahmed (National Institute of Technology Silchar)
Kaushik Goswami (St. Xavier’s College Kolkata)
Moumita Mitra Manna (Bangabasi College)
Chapter 1	 Computer Networks and the Internet	
29
1.1	
What Is the Internet?	
30
1.1.1	
A Nuts-and-Bolts Description	
30
1.1.2	
A Services Description	
33
1.1.3	
What Is a Protocol?	
35
1.2	
The Network Edge	
37
1.2.1	
Access Networks	
40
1.2.2	
Physical Media	
46
1.3	
The Network Core	
49
1.3.1	
Packet Switching	
51
1.3.2	
Circuit Switching	
55
1.3.3	
A Network of Networks	
59
1.4	
Delay, Loss, and Throughput in Packet-Switched Networks	
63
1.4.1	
Overview of Delay in Packet-Switched Networks	
63
1.4.2	
Queuing Delay and Packet Loss	
67
1.4.3	
End-to-End Delay	
69
1.4.4	
Throughput in Computer Networks	
71
1.5	
Protocol Layers and Their Service Models	
75
1.5.1	
Layered Architecture	
75
1.5.2	
Encapsulation	
81
1.6	
Networks Under Attack	
83
1.7	
History of Computer Networking and the Internet	
87
1.7.1	
The Development of Packet Switching: 1961–1972	
87
1.7.2	
Proprietary Networks and Internetworking: 1972–1980	
88
1.7.3	
A Proliferation of Networks: 1980–1990	
90
1.7.4	
The Internet Explosion: The 1990s	
91
1.7.5	
The New Millennium	
92
1.8	
Summary	
93
Homework Problems and Questions	
95
Wireshark Lab	
105
Interview: Leonard Kleinrock	
 107
Table of Contents
19
20         Table of Contents
Chapter 2	 Application Layer	
111
2.1	
Principles of Network Applications	
112
2.1.1	
Network Application Architectures	
114
2.1.2	
Processes Communicating	
116
2.1.3	
Transport Services Available to Applications	
118
2.1.4	
Transport Services Provided by the Internet	
121
2.1.5	
Application-Layer Protocols	
124
2.1.6	
Network Applications Covered in This Book	
125
2.2	
The Web and HTTP	
126
2.2.1	
Overview of HTTP	
126
2.2.2	
Non-Persistent and Persistent Connections	
128
2.2.3	
HTTP Message Format	
131
2.2.4	
User-Server Interaction: Cookies	
136
2.2.5	
Web Caching	
138
2.3	
Electronic Mail in the Internet	
144
2.3.1	
SMTP	
146
2.3.2	
Comparison with HTTP	
149
2.3.3	
Mail Message Formats	
149
2.3.4	
Mail Access Protocols	
150
2.4	
DNS—The Internet’s Directory Service	
154
2.4.1	
Services Provided by DNS	
155
2.4.2	
Overview of How DNS Works	
157
2.4.3	
DNS Records and Messages	
163
2.5	
Peer-to-Peer Applications	
168
2.5.1	
P2P File Distribution	
168
2.6	
Video Streaming and Content Distribution Networks	
175
2.6.1	
Internet Video	
176
2.6.2	
HTTP Streaming and DASH	
176
2.6.3	
Content Distribution Networks	
177
2.6.4	
Case Studies: Netflix, YouTube, and Kankan	
181
2.7	
Socket Programming: Creating Network Applications	
185
2.7.1	
Socket Programming with UDP	
187
2.7.2	
Socket Programming with TCP	
192
2.8	
Summary	
198
Homework Problems and Questions	
199
Socket Programming Assignments	
208
Wireshark Labs: HTTP, DNS	
210
Interview: Marc Andreessen	
212
Table of Contents         21
Chapter 3	 Transport Layer	
215
3.1	
Introduction and Transport-Layer Services	
216
3.1.1	
Relationship Between Transport and Network Layers	
216
3.1.2	
Overview of the Transport Layer in the Internet	
219
3.2	
Multiplexing and Demultiplexing	
221
3.3	
Connectionless Transport: UDP	
228
3.3.1	
UDP Segment Structure	
232
3.3.2	
UDP Checksum	
232
3.4	
Principles of Reliable Data Transfer	
234
3.4.1	
Building a Reliable Data Transfer Protocol	
236
3.4.2	
Pipelined Reliable Data Transfer Protocols	
245
3.4.3	
Go-Back-N (GBN)	
249
3.4.4	
Selective Repeat (SR)	
254
3.5	
Connection-Oriented Transport: TCP	
261
3.5.1	
The TCP Connection	
261
3.5.2	
TCP Segment Structure	
264
3.5.3	
Round-Trip Time Estimation and Timeout	
269
3.5.4	
Reliable Data Transfer	
272
3.5.5	
Flow Control	
280
3.5.6	
TCP Connection Management	
283
3.6	
Principles of Congestion Control	
289
3.6.1	
The Causes and the Costs of Congestion	
289
3.6.2	
Approaches to Congestion Control	
296
3.7	
TCP Congestion Control	
297
3.7.1	
Fairness	
307
3.7.2	
Explicit Congestion Notification (ECN): Network-assisted  
Congestion Control	
310
3.8	
Summary	
312
Homework Problems and Questions	
314
Programming Assignments	
329
Wireshark Labs: Exploring TCP, UDP	
330
Interview: Van Jacobson	
331
Chapter 4	 The Network Layer: Data Plane	
333
4.1	
Overview of Network Layer	
334
4.1.1	
Forwarding and Routing: The Network Data and Control Planes	
334
4.1.2	
Network Service Models	
339
4.2	
What’s Inside a Router?	
341
4.2.1	
Input Port Processing and Destination-Based Forwarding	
344
4.2.2	
Switching	
347
4.2.3	
Output Port Processing	
349
22         Table of Contents
4.2.4	
Where Does Queuing Occur?	
349
4.2.5	
Packet Scheduling	
353
4.3	
The Internet Protocol (IP): IPv4, Addressing, IPv6, and More	
357
4.3.1	
IPv4 Datagram Format	
358
4.3.2	
IPv4 Datagram Fragmentation	
360
4.3.3	
IPv4 Addressing	
362
4.3.4	
Network Address Translation (NAT)	
373
4.3.5	
IPv6	
376
4.4	
Generalized Forwarding and SDN	
382
4.4.1	
Match	
384
4.4.2	
Action	
386
4.4.3	
OpenFlow Examples of Match-plus-action in Action	
386
4.5	
Summary	
389
Homework Problems and Questions	
389
Wireshark Lab	
398
Interview: Vinton G. Cerf	
399
Chapter 5	 The Network Layer: Control Plane	
401
5.1	
Introduction	
402
5.2	
Routing Algorithms	
404
5.2.1	
The Link-State (LS) Routing Algorithm	
407
5.2.2	
The Distance-Vector (DV) Routing Algorithm	
412
5.3	
Intra-AS Routing in the Internet: OSPF	
419
5.4	
Routing Among the ISPs: BGP	
423
5.4.1	
The Role of BGP	
423
5.4.2	
Advertising BGP Route Information	
424
5.4.3	
Determining the Best Routes	
426
5.4.4	
IP-Anycast	
430
5.4.5	
Routing Policy	
431
5.4.6	
Putting the Pieces Together: Obtaining Internet Presence	
434
5.5	
The SDN Control Plane	
435
5.5.1	
The SDN Control Plane: SDN Controller and SDN Control  
Applications	
438
5.5.2	
OpenFlow Protocol	
440
5.5.3	
Data and Control Plane Interaction: An Example	
442
5.5.4	
SDN: Past and Future	
443
5.6	
ICMP: The Internet Control Message Protocol	
447
5.7	
Network Management and SNMP	
449
5.7.1	
The Network Management Framework	
450
5.7.2	
The Simple Network Management Protocol (SNMP)	
452
5.8	
Summary	
454
Table of Contents         23
Homework Problems and Questions	
455
Socket Programming Assignment	
461
Programming Assignment	
462
Wireshark Lab	
463
Interview: Jennifer Rexford	
464
Chapter 6	 The Link Layer and LANs	
467
6.1	
Introduction to the Link Layer	
468
6.1.1	
The Services Provided by the Link Layer	
470
6.1.2	
Where Is the Link Layer Implemented?	
471
6.2	
Error-Detection and -Correction Techniques	
472
6.2.1	
Parity Checks	
474
6.2.2	
Checksumming Methods	
476
6.2.3	
Cyclic Redundancy Check (CRC)	
477
6.3	
Multiple Access Links and Protocols	
479
6.3.1	
Channel Partitioning Protocols	
481
6.3.2	
Random Access Protocols	
483
6.3.3	
Taking-Turns Protocols	
492
6.3.4	
DOCSIS: The Link-Layer Protocol for Cable Internet Access	
493
6.4	
Switched Local Area Networks	
495
6.4.1	
Link-Layer Addressing and ARP	
496
6.4.2	
Ethernet	
502
6.4.3	
Link-Layer Switches	
509
6.4.4	
Virtual Local Area Networks (VLANs)	
515
6.5	
Link Virtualization: A Network as a Link Layer	
519
6.5.1	
Multiprotocol Label Switching (MPLS)	
520
6.6	
Data Center Networking	
523
6.7	
Retrospective: A Day in the Life of a Web Page Request	
528
6.7.1	
Getting Started: DHCP, UDP, IP, and Ethernet	
528
6.7.2	
Still Getting Started: DNS and ARP	
530
6.7.3	
Still Getting Started: Intra-Domain Routing to the DNS Server	
531
6.7.4	
Web Client-Server Interaction: TCP and HTTP	
532
6.8	
Summary	
534
Homework Problems and Questions	
535
Wireshark Lab	
543
Interview: Simon S. Lam	
544
Chapter 7	 Wireless and Mobile Networks	
547
7.1	
Introduction	
548
7.2	
Wireless Links and Network Characteristics	
553
7.2.1	
CDMA	
556
24         Table of Contents
7.3	
WiFi: 802.11 Wireless LANs	
560
7.3.1	
The 802.11 Architecture	
561
7.3.2	
The 802.11 MAC Protocol	
565
7.3.3	
The IEEE 802.11 Frame	
570
7.3.4	
Mobility in the Same IP Subnet	
574
7.3.5	
Advanced Features in 802.11	
575
7.3.6	
Personal Area Networks: Bluetooth and Zigbee	
576
7.4	
Cellular Internet Access	
579
7.4.1	
An Overview of Cellular Network Architecture	
579
7.4.2	
3G Cellular Data Networks: Extending the Internet  
to Cellular Subscribers	
582
7.4.3	
On to 4G: LTE	
585
7.5	
Mobility Management: Principles	
588
7.5.1	
Addressing	
590
7.5.2	
Routing to a Mobile Node	
592
7.6	
Mobile IP	
598
7.7	
Managing Mobility in Cellular Networks	
602
7.7.1 Routing Calls to a Mobile User	
604
7.7.2 Handoffs in GSM	
605
7.8	
Wireless and Mobility: Impact on Higher-Layer Protocols	
608
7.9	
Summary	
610
Homework Problems and Questions	
611
Wireshark Lab	
616
Interview: Deborah Estrin	
617
Chapter 8	 Security in Computer Networks	
621
8.1	
What Is Network Security?	
622
8.2	
Principles of Cryptography	
624
8.2.1	
Symmetric Key Cryptography	
626
8.2.2	
Public Key Encryption	
632
8.3	
Message Integrity and Digital Signatures	
638
8.3.1	
Cryptographic Hash Functions	
639
8.3.2	
Message Authentication Code	
641
8.3.3	
Digital Signatures	
642
8.4	
End-Point Authentication	
649
8.4.1	
Authentication Protocol ap1.0	
650
8.4.2	
Authentication Protocol ap2.0	
650
8.4.3	
Authentication Protocol ap3.0	
651
8.4.4	
Authentication Protocol ap3.1	
651
8.4.5	
Authentication Protocol ap4.0	
652
Table of Contents         25
8.5	
Securing E-Mail	
654
8.5.1	
Secure E-Mail	
655
8.5.2	
PGP	
658
8.6	
Securing TCP Connections: SSL	
659
8.6.1	
The Big Picture	
660
8.6.2	
A More Complete Picture	
663
8.7	
Network-Layer Security: IPsec and Virtual Private Networks	
665
8.7.1	
IPsec and Virtual Private Networks (VPNs)	
666
8.7.2	
The AH and ESP Protocols	
668
8.7.3	
Security Associations	
668
8.7.4	
The IPsec Datagram	
669
8.7.5	
IKE: Key Management in IPsec	
673
8.8	
Securing Wireless LANs	
674
8.8.1	
Wired Equivalent Privacy (WEP)	
674
8.8.2	
IEEE 802.11i	
676
8.9	
Operational Security: Firewalls and Intrusion Detection Systems	
679
8.9.1	
Firewalls	
679
8.9.2	
Intrusion Detection Systems	
687
8.10	
Summary	
690
Homework Problems and Questions	
692
Wireshark Lab	
700
IPsec Lab	
700
Interview: Steven M. Bellovin	
701
Chapter 9	 Multimedia Networking	
703
9.1	
Multimedia Networking Applications	
704
9.1.1	
Properties of Video	
704
9.1.2	
Properties of Audio	
705
9.1.3	
Types of Multimedia Network Applications	
707
9.2	
Streaming Stored Video	
709
9.2.1	
UDP Streaming	
711
9.2.2	
HTTP Streaming	
712
9.3	
Voice-over-IP	
716
9.3.1	
Limitations of the Best-Effort IP Service	
716
9.3.2	
Removing Jitter at the Receiver for Audio	
719
9.3.3	
Recovering from Packet Loss	
722
9.3.4	
Case Study: VoIP with Skype	
725
9.4	
Protocols for Real-Time Conversational Applications	
728
9.4.1	
RTP	
728
9.4.2	
SIP	
731
26         Table of Contents
9.5	
Network Support for Multimedia	
737
9.5.1	
Dimensioning Best-Effort Networks	
739
9.5.2	
Providing Multiple Classes of Service	
740
9.5.3	
Diffserv	
747
9.5.4	
Per-Connection Quality-of-Service (QoS) Guarantees:  
Resource Reservation and Call Admission	
751
9.6	
Summary	
754
Homework Problems and Questions	
755
Programming Assignment	
763
Interview: Henning Schulzrinne	
765
	
References	
769
	
Index	
811
COMPUTER 
NETWORKING
A Top-Down Approach
SEVENTH EDITION
Global Edition
This page intentionally left blank
29
Today’s Internet is arguably the largest engineered system ever created by ­
mankind, 
with hundreds of millions of connected computers, communication links, and 
 
switches; with billions of users who connect via laptops, tablets, and smartphones; 
and with an array of new Internet-connected “things” including game consoles, sur-
veillance systems, watches, eye glasses, thermostats, body scales, and cars. Given 
that the Internet is so large and has so many diverse components and uses, is there 
any hope of understanding how it works? Are there guiding principles and struc-
ture that can provide a foundation for understanding such an amazingly large and 
complex system? And if so, is it possible that it actually could be both interesting 
and fun to learn about computer networks? Fortunately, the answer to all of these 
questions is a resounding YES! Indeed, it’s our aim in this book to provide you with 
a modern introduction to the dynamic field of computer networking, giving you the 
principles and practical insights you’ll need to understand not only today’s networks, 
but tomorrow’s as well.
This first chapter presents a broad overview of computer networking and the 
Internet. Our goal here is to paint a broad picture and set the context for the rest 
of this book, to see the forest through the trees. We’ll cover a lot of ground in this 
introductory chapter and discuss a lot of the pieces of a computer network, without 
losing sight of the big picture.
We’ll structure our overview of computer networks in this chapter as follows. 
After introducing some basic terminology and concepts, we’ll first examine the basic 
hardware and software components that make up a network. We’ll begin at the net-
work’s edge and look at the end systems and network applications running in the 
network. We’ll then explore the core of a computer network, examining the links 
1
Chapter
Computer 
Networks and 
the Internet
30         Chapter 1    •    Computer Networks and the Internet
and the switches that transport data, as well as the access networks and physical 
media that connect end systems to the network core. We’ll learn that the Internet is 
a network of networks, and we’ll learn how these networks connect with each other.
After having completed this overview of the edge and core of a computer net-
work, we’ll take the broader and more abstract view in the second half of this chap-
ter. We’ll examine delay, loss, and throughput of data in a computer network and 
provide simple quantitative models for end-to-end throughput and delay: models 
that take into account transmission, propagation, and queuing delays. We’ll then 
introduce some of the key architectural principles in computer networking, namely, 
protocol layering and service models. We’ll also learn that computer networks are 
vulnerable to many different types of attacks; we’ll survey some of these attacks and 
consider how computer networks can be made more secure. Finally, we’ll close this 
chapter with a brief history of computer networking.
1.1	 What Is the Internet?
In this book, we’ll use the public Internet, a specific computer network, as our prin-
cipal vehicle for discussing computer networks and their protocols. But what is the 
Internet? There are a couple of ways to answer this question. First, we can describe 
the nuts and bolts of the Internet, that is, the basic hardware and software components 
that make up the Internet. Second, we can describe the Internet in terms of a network-
ing infrastructure that provides services to distributed applications. Let’s begin with 
the nuts-and-bolts description, using Figure 1.1 to illustrate our discussion.
1.1.1	A Nuts-and-Bolts Description
The Internet is a computer network that interconnects billions of computing devices 
throughout the world. Not too long ago, these computing devices were primarily 
traditional desktop PCs, Linux workstations, and so-called servers that store and 
transmit information such as Web pages and e-mail messages. Increasingly, how-
ever, nontraditional Internet “things” such as laptops, smartphones, tablets, TVs, 
gaming consoles, thermostats, home security systems, home appliances, watches, 
eye glasses, cars, traffic control systems and more are being connected to the Inter-
net. Indeed, the term computer network is beginning to sound a bit dated, given the 
many nontraditional devices that are being hooked up to the Internet. In Internet 
jargon, all of these devices are called hosts or end systems. By some estimates, in 
2015 there were about 5 billion devices connected to the Internet, and the number 
 
will reach 25 billion by 2020 [Gartner 2014]. It is estimated that in 2015 there 
 
were over 3.2 billion Internet users worldwide, approximately 40% of the world 
population [ITU 2015].
1.1    •    What Is the Internet?         31
Figure 1.1  ♦  Some pieces of the Internet
Key:
Host
(= end system)
Server
Mobile
Router
Link-layer
switch
Modem
Base
station
Smartphone
Tablet
Trafﬁc light
Thermostat
Fridge
Flat computer
monitor
Keyboard
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Cell phone
tower
Home Network
32         Chapter 1    •    Computer Networks and the Internet
End systems are connected together by a network of communication links and 
packet switches. We’ll see in Section 1.2 that there are many types of communica-
tion links, which are made up of different types of physical media, including coaxial 
cable, copper wire, optical fiber, and radio spectrum. Different links can transmit 
data at different rates, with the transmission rate of a link measured in bits/second. 
When one end system has data to send to another end system, the sending end system 
segments the data and adds header bytes to each segment. The resulting packages 
of information, known as packets in the jargon of computer networks, are then sent 
through the network to the destination end system, where they are reassembled into 
the original data.
A packet switch takes a packet arriving on one of its incoming communication 
links and forwards that packet on one of its outgoing communication links. Packet 
switches come in many shapes and flavors, but the two most prominent types in 
today’s Internet are routers and link-layer switches. Both types of switches forward 
packets toward their ultimate destinations. Link-layer switches are typically used in 
access networks, while routers are typically used in the network core. The sequence 
of communication links and packet switches traversed by a packet from the sending 
end system to the receiving end system is known as a route or path through the 
network. Cisco predicts annual global IP traffic will pass the zettabyte (1021 bytes) 
threshold by the end of 2016, and will reach 2 zettabytes per year by 2019 [Cisco 
VNI 2015].
Packet-switched networks (which transport packets) are in many ways similar 
to transportation networks of highways, roads, and intersections (which transport 
vehicles). Consider, for example, a factory that needs to move a large amount of 
cargo to some destination warehouse located thousands of kilometers away. At the 
factory, the cargo is segmented and loaded into a fleet of trucks. Each of the trucks 
then independently travels through the network of highways, roads, and intersections 
to the destination warehouse. At the destination warehouse, the cargo is unloaded 
and grouped with the rest of the cargo arriving from the same shipment. Thus, in 
many ways, packets are analogous to trucks, communication links are analogous to 
highways and roads, packet switches are analogous to intersections, and end systems 
are analogous to buildings. Just as a truck takes a path through the transportation 
network, a packet takes a path through a computer network.
End systems access the Internet through Internet Service Providers (ISPs), 
including residential ISPs such as local cable or telephone companies; corporate 
ISPs; university ISPs; ISPs that provide WiFi access in airports, hotels, coffee shops, 
and other public places; and cellular data ISPs, providing mobile access to our 
smartphones and other devices. Each ISP is in itself a network of packet switches 
and communication links. ISPs provide a variety of types of network access to the 
end systems, including residential broadband access such as cable modem or DSL, 
high-speed local area network access, and mobile wireless access. ISPs also provide 
­
Internet access to content providers, connecting Web sites and video servers directly 
to the Internet. The Internet is all about connecting end systems to each other, so the 
1.1    •    What Is the Internet?         33
ISPs that provide access to end systems must also be interconnected. These lower-
tier ISPs are interconnected through national and international upper-tier ISPs such 
as Level 3 Communications, AT&T, Sprint, and NTT. An upper-tier ISP consists of 
high-speed routers interconnected with high-speed fiber-optic links. Each ISP net-
work, whether upper-tier or lower-tier, is managed independently, runs the IP pro-
tocol (see below), and conforms to certain naming and address conventions. We’ll 
examine ISPs and their interconnection more closely in Section 1.3.
End systems, packet switches, and other pieces of the Internet run protocols that 
control the sending and receiving of information within the Internet. The Transmission 
 
Control Protocol (TCP) and the Internet Protocol (IP) are two of the most impor-
tant protocols in the Internet. The IP protocol specifies the format of the packets 
that are sent and received among routers and end systems. The Internet’s principal 
protocols are collectively known as TCP/IP. We’ll begin looking into protocols in 
this introductory chapter. But that’s just a start—much of this book is concerned with 
computer network protocols!
Given the importance of protocols to the Internet, it’s important that everyone 
agree on what each and every protocol does, so that people can create systems and 
products that interoperate. This is where standards come into play. Internet ­
standards 
are developed by the Internet Engineering Task Force (IETF) [IETF 2016]. The IETF 
standards documents are called requests for comments (RFCs). RFCs started out 
as general requests for comments (hence the name) to resolve network and protocol 
design problems that faced the precursor to the Internet [Allman 2011]. RFCs tend 
to be quite technical and detailed. They define protocols such as TCP, IP, HTTP (for 
the Web), and SMTP (for e-mail). There are currently more than 7,000 RFCs. Other 
bodies also specify standards for network components, most notably for network 
links. The IEEE 802 LAN/MAN Standards Committee [IEEE 802 2016], for exam-
ple, specifies the Ethernet and wireless WiFi standards.
1.1.2	A Services Description
Our discussion above has identified many of the pieces that make up the Internet. 
But we can also describe the Internet from an entirely different angle—namely, as 
an infrastructure that provides services to applications. In addition to traditional 
applications such as e-mail and Web surfing, Internet applications include mobile 
smartphone and tablet applications, including Internet messaging, mapping with 
real-time road-traffic information, music streaming from the cloud, movie and tel-
evision streaming, online social networks, video conferencing, multi-person games, 
and location-based recommendation systems. The applications are said to be distrib-
uted applications, since they involve multiple end systems that exchange data with 
each other. Importantly, Internet applications run on end systems—they do not run 
in the packet switches in the network core. Although packet switches facilitate the 
exchange of data among end systems, they are not concerned with the application 
that is the source or sink of data.
34         Chapter 1    •    Computer Networks and the Internet
Let’s explore a little more what we mean by an infrastructure that provides 
­
services to applications. To this end, suppose you have an exciting new idea for a dis-
tributed Internet application, one that may greatly benefit humanity or one that may 
simply make you rich and famous. How might you go about transforming this idea 
into an actual Internet application? Because applications run on end systems, you are 
going to need to write programs that run on the end systems. You might, for example, 
write your programs in Java, C, or Python. Now, because you are developing a dis-
tributed Internet application, the programs running on the different end systems will 
need to send data to each other. And here we get to a central issue—one that leads 
to the alternative way of describing the Internet as a platform for applications. How 
does one program running on one end system instruct the Internet to deliver data to 
another program running on another end system?
End systems attached to the Internet provide a socket interface that specifies 
how a program running on one end system asks the Internet infrastructure to deliver 
data to a specific destination program running on another end system. This Internet 
socket interface is a set of rules that the sending program must follow so that the 
Internet can deliver the data to the destination program. We’ll discuss the Internet 
socket interface in detail in Chapter 2. For now, let’s draw upon a simple analogy, 
one that we will frequently use in this book. Suppose Alice wants to send a letter to 
Bob using the postal service. Alice, of course, can’t just write the letter (the data) and 
drop the letter out her window. Instead, the postal service requires that Alice put the 
letter in an envelope; write Bob’s full name, address, and zip code in the center of the 
envelope; seal the envelope; put a stamp in the upper-right-hand corner of the enve-
lope; and finally, drop the envelope into an official postal service mailbox. Thus, the 
postal service has its own “postal service interface,” or set of rules, that Alice must 
follow to have the postal service deliver her letter to Bob. In a similar manner, the 
Internet has a socket interface that the program sending data must follow to have the 
Internet deliver the data to the program that will receive the data.
The postal service, of course, provides more than one service to its customers. It 
provides express delivery, reception confirmation, ordinary use, and many more ser-
vices. In a similar manner, the Internet provides multiple services to its applications. 
When you develop an Internet application, you too must choose one of the Internet’s 
services for your application. We’ll describe the Internet’s services in Chapter 2.
We have just given two descriptions of the Internet; one in terms of its hardware 
and software components, the other in terms of an infrastructure for providing ser-
vices to distributed applications. But perhaps you are still confused as to what the 
Internet is. What are packet switching and TCP/IP? What are routers? What kinds of 
communication links are present in the Internet? What is a distributed application? 
How can a thermostat or body scale be attached to the Internet? If you feel a bit over-
whelmed by all of this now, don’t worry—the purpose of this book is to introduce 
you to both the nuts and bolts of the Internet and the principles that govern how and 
why it works. We’ll explain these important terms and questions in the following 
sections and chapters.
1.1    •    What Is the Internet?         35
1.1.3	What Is a Protocol?
Now that we’ve got a bit of a feel for what the Internet is, let’s consider another 
important buzzword in computer networking: protocol. What is a protocol? What 
does a protocol do?
A Human Analogy
It is probably easiest to understand the notion of a computer network protocol by 
first considering some human analogies, since we humans execute protocols all of 
the time. Consider what you do when you want to ask someone for the time of day. 
A typical exchange is shown in Figure 1.2. Human protocol (or good manners, at 
least) dictates that one first offer a greeting (the first “Hi” in Figure 1.2) to initiate 
communication with someone else. The typical response to a “Hi” is a returned “Hi” 
message. Implicitly, one then takes a cordial “Hi” response as an indication that one 
can proceed and ask for the time of day. A different response to the initial “Hi” (such 
as “Don’t bother me!” or “I don’t speak English,” or some unprintable reply) might 
Figure 1.2  ♦  A human protocol and a computer network protocol
GET http://www.pearsonglobaleditions.com/
kurose
TCP connection request
Time
Time
TCP connection reply
<ﬁle>
Hi
Got the time?
Time
Time
Hi
2:00
36         Chapter 1    •    Computer Networks and the Internet
indicate an unwillingness or inability to communicate. In this case, the human proto-
col would be not to ask for the time of day. Sometimes one gets no response at all to 
a question, in which case one typically gives up asking that person for the time. Note 
that in our human protocol, there are specific messages we send, and specific actions 
we take in response to the received reply messages or other events (such as no reply 
within some given amount of time). Clearly, transmitted and received messages, and 
actions taken when these messages are sent or received or other events occur, play 
a central role in a human protocol. If people run different protocols (for example, if 
one person has manners but the other does not, or if one understands the concept of 
time and the other does not) the protocols do not interoperate and no useful work can 
be accomplished. The same is true in networking—it takes two (or more) communi-
cating entities running the same protocol in order to accomplish a task.
Let’s consider a second human analogy. Suppose you’re in a college class (a 
computer networking class, for example!). The teacher is droning on about protocols 
and you’re confused. The teacher stops to ask, “Are there any questions?” (a message 
that is transmitted to, and received by, all students who are not sleeping). You raise 
your hand (transmitting an implicit message to the teacher). Your teacher acknowl-
edges you with a smile, saying “Yes . . .” (a transmitted message encouraging you 
to ask your question—teachers love to be asked questions), and you then ask your 
question (that is, transmit your message to your teacher). Your teacher hears your 
question (receives your question message) and answers (transmits a reply to you). 
Once again, we see that the transmission and receipt of messages, and a set of con-
ventional actions taken when these messages are sent and received, are at the heart 
of this question-and-answer protocol.
Network Protocols
A network protocol is similar to a human protocol, except that the entities exchang-
ing messages and taking actions are hardware or software components of some 
device (for example, computer, smartphone, tablet, router, or other network-capable 
device). All activity in the Internet that involves two or more communicating remote 
entities is governed by a protocol. For example, hardware-implemented protocols in 
two physically connected computers control the flow of bits on the “wire” between 
the two network interface cards; congestion-control protocols in end systems control 
the rate at which packets are transmitted between sender and receiver; protocols in 
routers determine a packet’s path from source to destination. Protocols are running 
everywhere in the Internet, and consequently much of this book is about computer 
network protocols.
As an example of a computer network protocol with which you are probably 
familiar, consider what happens when you make a request to a Web server, that 
is, when you type the URL of a Web page into your Web browser. The scenario 
is illustrated in the right half of Figure 1.2. First, your computer will send a con-
nection request message to the Web server and wait for a reply. The Web server 
1.2    •    The Network Edge         37
will eventually receive your connection request message and return a connection 
reply message. Knowing that it is now OK to request the Web document, your 
computer then sends the name of the Web page it wants to fetch from that Web 
server in a GET message. Finally, the Web server returns the Web page (file) to 
your computer.
Given the human and networking examples above, the exchange of messages 
and the actions taken when these messages are sent and received are the key defining 
elements of a protocol:
A protocol defines the format and the order of messages exchanged between two 
or more communicating entities, as well as the actions taken on the transmission 
and/or receipt of a message or other event.
The Internet, and computer networks in general, make extensive use of pro-
tocols. Different protocols are used to accomplish different communication tasks. 
As you read through this book, you will learn that some protocols are simple and 
straightforward, while others are complex and intellectually deep. Mastering the 
field of computer networking is equivalent to understanding the what, why, and how 
of networking protocols.
1.2	 The Network Edge
In the previous section we presented a high-level overview of the Internet and net-
working protocols. We are now going to delve a bit more deeply into the components 
of a computer network (and the Internet, in particular). We begin in this section at 
the edge of a network and look at the components with which we are most ­
familiar—
namely, the computers, smartphones and other devices that we use on a daily basis. 
In the next section we’ll move from the network edge to the network core and exam-
ine switching and routing in computer networks.
Recall from the previous section that in computer networking jargon, the com-
puters and other devices connected to the Internet are often referred to as end sys-
tems. They are referred to as end systems because they sit at the edge of the Internet, 
 
as shown in Figure 1.3. The Internet’s end systems include desktop computers 
 
(e.g., desktop PCs, Macs, and Linux boxes), servers (e.g., Web and e-mail servers), 
 
and mobile devices (e.g., laptops, smartphones, and tablets). Furthermore, an 
increasing number of non-traditional “things” are being attached to the Internet as 
end ­
systems (see the Case History feature).
End systems are also referred to as hosts because they host (that is, run) appli-
cation programs such as a Web browser program, a Web server program, an e-mail 
client program, or an e-mail server program. Throughout this book we will use the 
38         Chapter 1    •    Computer Networks and the Internet
Figure 1.3  ♦  End-system interaction
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Home Network
1.2    •    The Network Edge         39
terms hosts and end systems interchangeably; that is, host = end system. Hosts are 
sometimes further divided into two categories: clients and servers. Informally, cli-
ents tend to be desktop and mobile PCs, smartphones, and so on, whereas serv-
ers tend to be more powerful machines that store and distribute Web pages, stream 
video, relay e-mail, and so on. Today, most of the servers from which we receive 
search results, e-mail, Web pages, and videos reside in large data centers. For exam-
ple, Google has 50-100 data centers, including about 15 large centers, each with 
more than 100,000 servers.
THE INTERNET OF THINGS
Can you imagine a world in which just about everything is wirelessly connected to 
the Internet? A world in which most people, cars, bicycles, eye glasses, watches, 
toys, hospital equipment, home sensors, classrooms, video surveillance systems, 
atmospheric sensors, store-shelf products, and pets are connected? This world of the 
Internet of Things (IoT) may actually be just around the corner.
By some estimates, as of 2015 there are already 5 billion things connected to 
the Internet, and the number could reach 25 billion by 2020 [Gartner 2014]. These 
things include our smartphones, which already follow us around in our homes, offices, 
and cars, reporting our geo-locations and usage data to our ISPs and Internet applica-
tions. But in addition to our smartphones, a wide-variety of non-traditional “things” are 
already available as products. For example, there are Internet-connected wearables, 
including watches (from Apple and many others) and eye glasses. Internet-connected 
glasses can, for example, upload everything we see to the cloud, allowing us to share 
our visual experiences with people around the world in real-time. There are Internet-
connected things already available for the smart home, including Internet-connected 
thermostats that can be controlled remotely from our smartphones, and Internet-
connected body scales, enabling us to graphically review the progress of our diets 
from our smartphones. There are Internet-connected toys, including dolls that  
recognize and interpret a child’s speech and respond appropriately.
The IoT offers potentially revolutionary benefits to users. But at the same time there 
are also huge security and privacy risks. For example, attackers, via the Internet, 
might be able to hack into IoT devices or into the servers collecting data from IoT 
devices. For example, an attacker could hijack an Internet-connected doll and talk 
directly with a child; or an attacker could hack into a database that stores ­
personal 
health and activity information collected from wearable devices. These security 
and privacy concerns could undermine the consumer confidence necessary for the 
­
technologies to meet their full potential and may result in less widespread adoption 
[FTC 2015].
CASE HISTORY
40         Chapter 1    •    Computer Networks and the Internet
1.2.1	Access Networks
Having considered the applications and end systems at the “edge of the network,” 
let’s next consider the access network—the network that physically connects an end 
system to the first router (also known as the “edge router”) on a path from the end 
system to any other distant end system. Figure 1.4 shows several types of access 
Figure	1.4  ♦  Access networks
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Home Network
1.2    •    The Network Edge         41
networks with thick, shaded lines and the settings (home, enterprise, and wide-area 
mobile wireless) in which they are used.
Home Access: DSL, Cable, FTTH, Dial-Up, and Satellite
In developed countries as of 2014, more than 78 percent of the households have Internet 
access, with Korea, Netherlands, Finland, and Sweden leading the way with more than 
80 percent of households having Internet access, almost all via a high-speed broadband 
connection [ITU 2015]. Given this widespread use of home access networks let’s begin 
our overview of access networks by considering how homes connect to the Internet.
Today, the two most prevalent types of broadband residential access are digital 
subscriber line (DSL) and cable. A residence typically obtains DSL Internet access 
from the same local telephone company (telco) that provides its wired local phone 
access. Thus, when DSL is used, a customer’s telco is also its ISP. As shown in 
Figure 1.5, each customer’s DSL modem uses the existing telephone line (twisted-
pair copper wire, which we’ll discuss in Section 1.2.2) to exchange data with a digi-
tal subscriber line access multiplexer (DSLAM) located in the telco’s local central 
office (CO). The home’s DSL modem takes digital data and translates it to high-­
frequency tones for transmission over telephone wires to the CO; the analog signals 
from many such houses are translated back into digital format at the DSLAM.
The residential telephone line carries both data and traditional telephone signals 
simultaneously, which are encoded at different frequencies:
•	 A high-speed downstream channel, in the 50 kHz to 1 MHz band
•	 A medium-speed upstream channel, in the 4 kHz to 50 kHz band
•	 An ordinary two-way telephone channel, in the 0 to 4 kHz band
This approach makes the single DSL link appear as if there were three separate links, so 
that a telephone call and an Internet connection can share the DSL link at the same time. 
Figure 1.5  ♦  DSL Internet access
Home PC
Home
phone
DSL
modem
Internet
Telephone
network
Splitter
Existing phone line:
0-4KHz phone; 4-50KHz
upstream data; 50KHz–
1MHz downstream data
Central
ofﬁce
DSLAM
42         Chapter 1    •    Computer Networks and the Internet
(We’ll describe this technique of frequency-division multiplexing in Section 1.3.1.) 
On the customer side, a splitter separates the data and telephone signals arriving to the 
home and forwards the data signal to the DSL modem. On the telco side, in the CO, the 
DSLAM separates the data and phone signals and sends the data into the Internet. Hun-
dreds or even thousands of households connect to a single DSLAM [Dischinger 2007].
The DSL standards define multiple transmission rates, including 12 Mbps down-
stream and 1.8 Mbps upstream [ITU 1999], and 55 Mbps downstream and 15 Mbps 
upstream [ITU 2006]. Because the downstream and upstream rates are different, the 
access is said to be asymmetric. The actual downstream and upstream transmission 
rates achieved may be less than the rates noted above, as the DSL provider may pur-
posefully limit a residential rate when tiered service (different rates, available at dif-
ferent prices) are offered. The maximum rate is also limited by the distance between 
the home and the CO, the gauge of the twisted-pair line and the degree of electrical 
interference. Engineers have expressly designed DSL for short distances between the 
home and the CO; generally, if the residence is not located within 5 to 10 miles of the 
CO, the residence must resort to an alternative form of Internet access.
While DSL makes use of the telco’s existing local telephone infrastructure, 
cable Internet access makes use of the cable television company’s existing cable 
television infrastructure. A residence obtains cable Internet access from the same 
company that provides its cable television. As illustrated in Figure 1.6, fiber optics 
connect the cable head end to neighborhood-level junctions, from which traditional 
coaxial cable is then used to reach individual houses and apartments. Each neighbor-
hood junction typically supports 500 to 5,000 homes. Because both fiber and coaxial 
cable are employed in this system, it is often referred to as hybrid fiber coax (HFC).
Figure 1.6  ♦  A hybrid fiber-coaxial access network
Fiber
cable
Coaxial cable
Hundreds
of homes
Cable head end
Hundreds
of homes
Fiber
node
Fiber
node
Internet
CMTS
1.2    •    The Network Edge         43
Cable Internet access requires special modems, called cable modems. As with 
a DSL modem, the cable modem is typically an external device and connects to 
the home PC through an Ethernet port. (We will discuss Ethernet in great detail in 
Chapter 6.) At the cable head end, the cable modem termination system (CMTS) 
serves a similar function as the DSL network’s DSLAM—turning the analog signal 
sent from the cable modems in many downstream homes back into digital format. 
Cable modems divide the HFC network into two channels, a downstream and an 
upstream channel. As with DSL, access is typically asymmetric, with the downstream 
 
channel typically allocated a higher transmission rate than the upstream channel. The 
­
DOCSIS 2.0 standard defines downstream rates up to 42.8 Mbps and upstream rates 
of up to 30.7 Mbps. As in the case of DSL networks, the maximum achievable rate 
may not be realized due to lower contracted data rates or media impairments.
One important characteristic of cable Internet access is that it is a shared broad-
cast medium. In particular, every packet sent by the head end travels downstream on 
every link to every home and every packet sent by a home travels on the upstream 
channel to the head end. For this reason, if several users are simultaneously down-
loading a video file on the downstream channel, the actual rate at which each user 
receives its video file will be significantly lower than the aggregate cable down-
stream rate. On the other hand, if there are only a few active users and they are all 
Web surfing, then each of the users may actually receive Web pages at the full cable 
downstream rate, because the users will rarely request a Web page at exactly the 
same time. Because the upstream channel is also shared, a distributed multiple access 
protocol is needed to coordinate transmissions and avoid collisions. (We’ll discuss 
this collision issue in some detail in Chapter 6.)
Although DSL and cable networks currently represent more than 85 percent 
of residential broadband access in the United States, an up-and-coming technol-
ogy that provides even higher speeds is fiber to the home (FTTH) [FTTH Coun-
cil 2016]. As the name suggests, the FTTH concept is simple—provide an optical 
fiber path from the CO directly to the home. Many countries today—including 
 
the UAE, South Korea, Hong Kong, Japan, Singapore, Taiwan, Lithuania, and 
 
Sweden—now have household penetration rates exceeding 30% [FTTH Council 2016].
There are several competing technologies for optical distribution from the CO 
to the homes. The simplest optical distribution network is called direct fiber, with 
one fiber leaving the CO for each home. More commonly, each fiber leaving the 
central office is actually shared by many homes; it is not until the fiber gets rela-
tively close to the homes that it is split into individual customer-specific fibers. There 
are two competing optical-distribution network architectures that perform this split-
ting: active optical networks (AONs) and passive optical networks (PONs). AON is 
essentially switched Ethernet, which is discussed in Chapter 6.
Here, we briefly discuss PON, which is used in Verizon’s FIOS service. 
Fig­
ure 1.7 shows FTTH using the PON distribution architecture. Each home has 
an optical network terminator (ONT), which is connected by dedicated optical fiber 
to a neighborhood splitter. The splitter combines a number of homes (typically less 
 
44         Chapter 1    •    Computer Networks and the Internet
than 100) onto a single, shared optical fiber, which connects to an optical line 
­
terminator (OLT) in the telco’s CO. The OLT, providing conversion between opti-
cal and electrical signals, connects to the Internet via a telco router. In the home, 
users connect a home router (typically a wireless router) to the ONT and access the 
­
Internet via this home router. In the PON architecture, all packets sent from OLT to 
the splitter are replicated at the splitter (similar to a cable head end).
FTTH can potentially provide Internet access rates in the gigabits per second 
range. However, most FTTH ISPs provide different rate offerings, with the higher 
rates naturally costing more money. The average downstream speed of US FTTH 
customers was approximately 20 Mbps in 2011 (compared with 13 Mbps for cable 
access networks and less than 5 Mbps for DSL) [FTTH Council 2011b].
Two other access network technologies are also used to provide Internet access 
to the home. In locations where DSL, cable, and FTTH are not available (e.g., in 
some rural settings), a satellite link can be used to connect a residence to the Inter-
net at speeds of more than 1 Mbps; StarBand and HughesNet are two such satellite 
access providers. Dial-up access over traditional phone lines is based on the same 
model as DSL—a home modem connects over a phone line to a modem in the ISP. 
Compared with DSL and other broadband access networks, dial-up access is excru-
ciatingly slow at 56 kbps.
Access in the Enterprise (and the Home): Ethernet and WiFi
On corporate and university campuses, and increasingly in home settings, a local area 
network (LAN) is used to connect an end system to the edge router. Although there 
are many types of LAN technologies, Ethernet is by far the most prevalent access 
technology in corporate, university, and home networks. As shown in Figure 1.8, 
 
Ethernet users use twisted-pair copper wire to connect to an Ethernet switch, a tech-
nology discussed in detail in Chapter 6. The Ethernet switch, or a network of such 
Figure 1.7  ♦  FTTH Internet access
Internet
Central ofﬁce
Optical
splitter
ONT
ONT
ONT
OLT
Optical
ﬁbers
1.2    •    The Network Edge         45
interconnected switches, is then in turn connected into the larger Internet. With Eth-
ernet access, users typically have 100 Mbps or 1 Gbps access to the Ethernet switch, 
whereas servers may have 1 Gbps or even 10 Gbps access.
Increasingly, however, people are accessing the Internet wirelessly from lap-
tops, smartphones, tablets, and other “things” (see earlier sidebar on “Internet of 
Things”). In a wireless LAN setting, wireless users transmit/receive packets to/from 
an access point that is connected into the enterprise’s network (most likely using 
wired Ethernet), which in turn is connected to the wired Internet. A wireless LAN 
user must typically be within a few tens of meters of the access point. Wireless LAN 
access based on IEEE 802.11 technology, more colloquially known as WiFi, is now 
just about everywhere—universities, business offices, cafes, airports, homes, and 
even in airplanes. In many cities, one can stand on a street corner and be within range 
of ten or twenty base stations (for a browseable global map of 802.11 base stations 
that have been discovered and logged on a Web site by people who take great enjoy-
ment in doing such things, see [wigle.net 2016]). As discussed in detail in Chapter 7, 
802.11 today provides a shared transmission rate of up to more than 100 Mbps.
Even though Ethernet and WiFi access networks were initially deployed in enter-
prise (corporate, university) settings, they have recently become relatively common 
components of home networks. Many homes combine broadband residential access 
(that is, cable modems or DSL) with these inexpensive wireless LAN technologies 
to create powerful home networks [Edwards 2011]. Figure 1.9 shows a typical home 
network. This home network consists of a roaming laptop as well as a wired PC; a base 
station (the wireless access point), which communicates with the wireless PC and other 
wireless devices in the home; a cable modem, providing broadband access to the Inter-
net; and a router, which interconnects the base station and the stationary PC with the 
cable modem. This network allows household members to have broadband access to the 
Internet with one member roaming from the kitchen to the backyard to the bedrooms.
Figure 1.8  ♦  Ethernet Internet access
Ethernet
switch
Institutional
router
100 Mbps
100 Mbps
100 Mbps
Server
To Institution’s
ISP
46         Chapter 1    •    Computer Networks and the Internet
Wide-Area Wireless Access: 3G and LTE
Increasingly, devices such as iPhones and Android devices are being used to mes-
sage, share photos in social networks, watch movies, and stream music while on the 
run. These devices employ the same wireless infrastructure used for cellular teleph-
ony to send/receive packets through a base station that is operated by the cellular 
network provider. Unlike WiFi, a user need only be within a few tens of kilometers 
(as opposed to a few tens of meters) of the base station.
Telecommunications companies have made enormous investments in so-called 
third-generation (3G) wireless, which provides packet-switched wide-area wire-
less Internet access at speeds in excess of 1 Mbps. But even higher-speed wide-area 
access technologies—a fourth-generation (4G) of wide-area wireless networks—are 
already being deployed. LTE (for “Long-Term Evolution”—a candidate for Bad 
Acronym of the Year Award) has its roots in 3G technology, and can achieve rates in 
excess of 10 Mbps. LTE downstream rates of many tens of Mbps have been reported 
in commercial deployments. We’ll cover the basic principles of wireless networks 
and mobility, as well as WiFi, 3G, and LTE technologies (and more!) in Chapter 7.
1.2.2	Physical Media
In the previous subsection, we gave an overview of some of the most important 
network access technologies in the Internet. As we described these technologies, 
we also indicated the physical media used. For example, we said that HFC uses a 
combination of fiber cable and coaxial cable. We said that DSL and Ethernet use 
copper wire. And we said that mobile access networks use the radio spectrum. In this 
subsection we provide a brief overview of these and other transmission media that 
are commonly used in the Internet.
In order to define what is meant by a physical medium, let us reflect on the brief life 
of a bit. Consider a bit traveling from one end system, through a series of links and rout-
ers, to another end system. This poor bit gets kicked around and transmitted many, many 
Figure 1.9  ♦  A typical home network
Cable
head end
House
Internet
1.2    •    The Network Edge         47
times! The source end system first transmits the bit, and shortly thereafter the first router 
in the series receives the bit; the first router then transmits the bit, and shortly thereafter 
the second router receives the bit; and so on. Thus our bit, when traveling from source 
to destination, passes through a series of transmitter-receiver pairs. For each transmitter-
receiver pair, the bit is sent by propagating electromagnetic waves or optical pulses 
across a physical medium. The physical medium can take many shapes and forms and 
does not have to be of the same type for each transmitter-receiver pair along the path. 
Examples of physical media include twisted-pair copper wire, coaxial cable, multimode 
fiber-optic cable, terrestrial radio spectrum, and satellite radio spectrum. Physical media 
fall into two categories: guided media and unguided media. With guided media, the 
waves are guided along a solid medium, such as a fiber-optic cable, a twisted-pair cop-
per wire, or a coaxial cable. With unguided media, the waves propagate in the atmos-
phere and in outer space, such as in a wireless LAN or a digital satellite channel.
But before we get into the characteristics of the various media types, let us say a 
few words about their costs. The actual cost of the physical link (copper wire, fiber-optic 
cable, and so on) is often relatively minor compared with other networking costs. In par-
ticular, the labor cost associated with the installation of the physical link can be orders 
of magnitude higher than the cost of the material. For this reason, many builders install 
twisted pair, optical fiber, and coaxial cable in every room in a building. Even if only one 
medium is initially used, there is a good chance that another medium could be used in 
the near future, and so money is saved by not having to lay additional wires in the future.
Twisted-Pair Copper Wire
The least expensive and most commonly used guided transmission medium is twisted-
pair copper wire. For over a hundred years it has been used by telephone networks. 
In fact, more than 99 percent of the wired connections from the telephone handset to 
the local telephone switch use twisted-pair copper wire. Most of us have seen twisted 
pair in our homes (or those of our parents or grandparents!) and work environments. 
Twisted pair consists of two insulated copper wires, each about 1 mm thick, arranged 
in a regular spiral pattern. The wires are twisted together to reduce the electrical inter-
ference from similar pairs close by. Typically, a number of pairs are bundled together 
in a cable by wrapping the pairs in a protective shield. A wire pair constitutes a single 
communication link. Unshielded twisted pair (UTP) is commonly used for computer 
networks within a building, that is, for LANs. Data rates for LANs using twisted pair 
today range from 10 Mbps to 10 Gbps. The data rates that can be achieved depend on 
the thickness of the wire and the distance between transmitter and receiver.
When fiber-optic technology emerged in the 1980s, many people disparaged 
twisted pair because of its relatively low bit rates. Some people even felt that fiber-
optic technology would completely replace twisted pair. But twisted pair did not give 
up so easily. Modern twisted-pair technology, such as category 6a cable, can achieve 
data rates of 10 Gbps for distances up to a hundred meters. In the end, twisted pair 
has emerged as the dominant solution for high-speed LAN networking.
48         Chapter 1    •    Computer Networks and the Internet
As discussed earlier, twisted pair is also commonly used for residential Internet 
access. We saw that dial-up modem technology enables access at rates of up to 56 
kbps over twisted pair. We also saw that DSL (digital subscriber line) technology 
has enabled residential users to access the Internet at tens of Mbps over twisted pair 
(when users live close to the ISP’s central office).
Coaxial Cable
Like twisted pair, coaxial cable consists of two copper conductors, but the two con-
ductors are concentric rather than parallel. With this construction and special insula-
tion and shielding, coaxial cable can achieve high data transmission rates. Coaxial 
cable is quite common in cable television systems. As we saw earlier, cable televi-
sion systems have recently been coupled with cable modems to provide residential 
users with Internet access at rates of tens of Mbps. In cable television and cable 
Internet access, the transmitter shifts the digital signal to a specific frequency band, 
and the resulting analog signal is sent from the transmitter to one or more receivers. 
Coaxial cable can be used as a guided shared medium. Specifically, a number of 
end systems can be connected directly to the cable, with each of the end systems 
receiving whatever is sent by the other end systems.
Fiber Optics
An optical fiber is a thin, flexible medium that conducts pulses of light, with each 
pulse representing a bit. A single optical fiber can support tremendous bit rates, up 
to tens or even hundreds of gigabits per second. They are immune to electromagnetic 
interference, have very low signal attenuation up to 100 kilometers, and are very hard 
to tap. These characteristics have made fiber optics the preferred long-haul guided 
transmission media, particularly for overseas links. Many of the long-distance tele-
phone networks in the United States and elsewhere now use fiber optics exclusively. 
Fiber optics is also prevalent in the backbone of the Internet. However, the high cost 
of optical devices—such as transmitters, receivers, and switches—has hindered their 
deployment for short-haul transport, such as in a LAN or into the home in a residen-
tial access network. The Optical Carrier (OC) standard link speeds range from 51.8 
Mbps to 39.8 Gbps; these specifications are often referred to as OC-n, where the link 
speed equals n ∞ 51.8 Mbps. Standards in use today include OC-1, OC-3, OC-12, 
OC-24, OC-48, OC-96, OC-192, OC-768. [Mukherjee 2006, Ramaswami 2010] 
provide coverage of various aspects of optical networking.
Terrestrial Radio Channels
Radio channels carry signals in the electromagnetic spectrum. They are an attractive 
medium because they require no physical wire to be installed, can penetrate walls, 
provide connectivity to a mobile user, and can potentially carry a signal for long 
1.3    •    The Network Core         49
distances. The characteristics of a radio channel depend significantly on the propaga-
tion environment and the distance over which a signal is to be carried. Environmental 
considerations determine path loss and shadow fading (which decrease the signal 
strength as the signal travels over a distance and around/through obstructing objects), 
multipath fading (due to signal reflection off of interfering objects), and interference 
(due to other transmissions and electromagnetic signals).
Terrestrial radio channels can be broadly classified into three groups: those that 
operate over very short distance (e.g., with one or two meters); those that operate in 
local areas, typically spanning from ten to a few hundred meters; and those that oper-
ate in the wide area, spanning tens of kilometers. Personal devices such as wireless 
headsets, keyboards, and medical devices operate over short distances; the wireless 
LAN technologies described in Section 1.2.1 use local-area radio channels; the cel-
lular access technologies use wide-area radio channels. We’ll discuss radio channels 
in detail in Chapter 7.
Satellite Radio Channels
A communication satellite links two or more Earth-based microwave transmitter/ 
receivers, known as ground stations. The satellite receives transmissions on one fre-
quency band, regenerates the signal using a repeater (discussed below), and transmits 
the signal on another frequency. Two types of satellites are used in communications: 
geostationary satellites and low-earth orbiting (LEO) satellites [Wiki Satellite 2016].
Geostationary satellites permanently remain above the same spot on Earth. This 
stationary presence is achieved by placing the satellite in orbit at 36,000 kilometers 
above Earth’s surface. This huge distance from ground station through satellite back 
to ground station introduces a substantial signal propagation delay of 280 millisec-
onds. Nevertheless, satellite links, which can operate at speeds of hundreds of Mbps, 
are often used in areas without access to DSL or cable-based Internet access.
LEO satellites are placed much closer to Earth and do not remain permanently 
above one spot on Earth. They rotate around Earth (just as the Moon does) and may 
communicate with each other, as well as with ground stations. To provide continuous 
coverage to an area, many satellites need to be placed in orbit. There are currently 
 
many low-altitude communication systems in development. LEO satellite ­
technology 
may be used for Internet access sometime in the future.
1.3	 The Network Core
Having examined the Internet’s edge, let us now delve more deeply inside the net-
work core—the mesh of packet switches and links that interconnects the Internet’s 
end systems. Figure 1.10 highlights the network core with thick, shaded lines.
50         Chapter 1    •    Computer Networks and the Internet
Figure 1.10  ♦  The network core
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Home Network
1.3    •    The Network Core         51
1.3.1	Packet Switching
In a network application, end systems exchange messages with each other. Mes-
sages can contain anything the application designer wants. Messages may perform 
a control function (for example, the “Hi” messages in our handshaking example in 
Figure 1.2) or can contain data, such as an e-mail message, a JPEG image, or an 
MP3 audio file. To send a message from a source end system to a destination end 
system, the source breaks long messages into smaller chunks of data known as pack-
ets. Between source and destination, each packet travels through communication 
links and packet switches (for which there are two predominant types, routers and 
link-layer switches). Packets are transmitted over each communication link at a rate 
equal to the full transmission rate of the link. So, if a source end system or a packet 
switch is sending a packet of L bits over a link with transmission rate R bits/sec, then 
the time to transmit the packet is L / 
R seconds.
Store-and-Forward Transmission
Most packet switches use store-and-forward transmission at the inputs to the 
links. Store-and-forward transmission means that the packet switch must receive 
the entire packet before it can begin to transmit the first bit of the packet onto the 
outbound link. To explore store-and-forward transmission in more detail, consider 
a simple network consisting of two end systems connected by a single router, as 
shown in Figure 1.11. A router will typically have many incident links, since its 
job is to switch an incoming packet onto an outgoing link; in this simple example, 
the router has the rather simple task of transferring a packet from one (input) link 
to the only other attached link. In this example, the source has three packets, each 
consisting of L bits, to send to the destination. At the snapshot of time shown in 
Figure 1.11, the source has transmitted some of packet 1, and the front of packet 1 
has already arrived at the router. Because the router employs store-and-forwarding, 
at this instant of time, the router cannot transmit the bits it has received; instead it 
must first buffer (i.e., “store”) the packet’s bits. Only after the router has received 
all of the packet’s bits can it begin to transmit (i.e., “forward”) the packet onto the 
outbound link. To gain some insight into store-and-forward transmission, let’s now 
calculate the amount of time that elapses from when the source begins to send the 
packet until the destination has received the entire packet. (Here we will ignore 
propagation delay—the time it takes for the bits to travel across the wire at near 
the speed of light—which will be discussed in Section 1.4.) The source begins to 
transmit at time 0; at time L/R seconds, the source has transmitted the entire packet, 
and the entire packet has been received and stored at the router (since there is no 
propagation delay). At time L/R seconds, since the router has just received the entire 
packet, it can begin to transmit the packet onto the outbound link towards the des-
tination; at time 2L/R, the router has transmitted the entire packet, and the entire 
packet has been received by the destination. Thus, the total delay is 2L/R. If the 
52         Chapter 1    •    Computer Networks and the Internet
switch instead forwarded bits as soon as they arrive (without first receiving the entire 
packet), then the total delay would be L/R since bits are not held up at the router. 
But, as we will discuss in Section 1.4, routers need to receive, store, and process the 
entire packet before forwarding.
Now let’s calculate the amount of time that elapses from when the source begins 
to send the first packet until the destination has received all three packets. As before, 
at time L/R, the router begins to forward the first packet. But also at time L/R the 
source will begin to send the second packet, since it has just finished sending the 
entire first packet. Thus, at time 2L/R, the destination has received the first packet 
and the router has received the second packet. Similarly, at time 3L/R, the destina-
tion has received the first two packets and the router has received the third packet. 
Finally, at time 4L/R the destination has received all three packets!
Let’s now consider the general case of sending one packet from source to des-
tination over a path consisting of N links each of rate R (thus, there are N-1 routers 
between source and destination). Applying the same logic as above, we see that the 
end-to-end delay is:
	
dend@to@end = N L
R
(1.1)
You may now want to try to determine what the delay would be for P packets sent 
over a series of N links.
Queuing Delays and Packet Loss
Each packet switch has multiple links attached to it. For each attached link, the 
packet switch has an output buffer (also called an output queue), which stores 
packets that the router is about to send into that link. The output buffers play a key 
role in packet switching. If an arriving packet needs to be transmitted onto a link but 
finds the link busy with the transmission of another packet, the arriving packet must 
wait in the output buffer. Thus, in addition to the store-and-forward delays, packets 
suffer output buffer queuing delays. These delays are variable and depend on the 
level of congestion in the network. Since the amount of buffer space is finite, an 
Figure 1.11  ♦  Store-and-forward packet switching
Source
R bps
1
2
Destination
Front of packet 1
stored in router,
awaiting remaining
bits before forwarding
3
1.3    •    The Network Core         53
arriving packet may find that the buffer is completely full with other packets waiting 
for transmission. In this case, packet loss will occur—either the arriving packet or 
one of the already-queued packets will be dropped.
Figure 1.12 illustrates a simple packet-switched network. As in Figure 1.11, 
packets are represented by three-dimensional slabs. The width of a slab represents 
the number of bits in the packet. In this figure, all packets have the same width and 
hence the same length. Suppose Hosts A and B are sending packets to Host E. Hosts 
A and B first send their packets along 100 Mbps Ethernet links to the first router. 
The router then directs these packets to the 15 Mbps link. If, during a short interval 
of time, the arrival rate of packets to the router (when converted to bits per second) 
exceeds 15 Mbps, congestion will occur at the router as packets queue in the link’s 
output buffer before being transmitted onto the link. For example, if Host A and B 
each send a burst of five packets back-to-back at the same time, then most of these 
packets will spend some time waiting in the queue. The situation is, in fact, entirely 
analogous to many common-day situations—for example, when we wait in line for a 
bank teller or wait in front of a tollbooth. We’ll examine this queuing delay in more 
detail in Section 1.4.
Forwarding Tables and Routing Protocols
Earlier, we said that a router takes a packet arriving on one of its attached communi-
cation links and forwards that packet onto another one of its attached communication 
links. But how does the router determine which link it should forward the packet 
onto? Packet forwarding is actually done in different ways in different types of 
 
computer networks. Here, we briefly describe how it is done in the Internet.
Figure 1.12  ♦  Packet switching
100 Mbps Ethernet
Key:
Packets
A
B
C
D
E
15 Mbps
Queue of
packets waiting
for output link
54         Chapter 1    •    Computer Networks and the Internet
In the Internet, every end system has an address called an IP address. When a 
source end system wants to send a packet to a destination end system, the source 
includes the destination’s IP address in the packet’s header. As with postal addresses, 
this address has a hierarchical structure. When a packet arrives at a router in the net-
work, the router examines a portion of the packet’s destination address and forwards 
the packet to an adjacent router. More specifically, each router has a forwarding 
table that maps destination addresses (or portions of the destination addresses) to that 
router’s outbound links. When a packet arrives at a router, the router examines the 
address and searches its forwarding table, using this destination address, to find the 
appropriate outbound link. The router then directs the packet to this outbound link.
The end-to-end routing process is analogous to a car driver who does not use 
maps but instead prefers to ask for directions. For example, suppose Joe is driving 
from Philadelphia to 156 Lakeside Drive in Orlando, Florida. Joe first drives to his 
neighborhood gas station and asks how to get to 156 Lakeside Drive in Orlando, 
Florida. The gas station attendant extracts the Florida portion of the address and tells 
Joe that he needs to get onto the interstate highway I-95 South, which has an entrance 
just next to the gas station. He also tells Joe that once he enters Florida, he should ask 
someone else there. Joe then takes I-95 South until he gets to Jacksonville, Florida, 
at which point he asks another gas station attendant for directions. The attendant 
extracts the Orlando portion of the address and tells Joe that he should continue on 
I-95 to Daytona Beach and then ask someone else. In Daytona Beach, another gas 
station attendant also extracts the Orlando portion of the address and tells Joe that 
he should take I-4 directly to Orlando. Joe takes I-4 and gets off at the Orlando exit. 
Joe goes to another gas station attendant, and this time the attendant extracts the 
Lakeside Drive portion of the address and tells Joe the road he must follow to get to 
Lakeside Drive. Once Joe reaches Lakeside Drive, he asks a kid on a bicycle how to 
get to his destination. The kid extracts the 156 portion of the address and points to 
the house. Joe finally reaches his ultimate destination. In the above analogy, the gas 
station attendants and kids on bicycles are analogous to routers.
We just learned that a router uses a packet’s destination address to index a for-
warding table and determine the appropriate outbound link. But this statement begs 
yet another question: How do forwarding tables get set? Are they configured by hand 
in each and every router, or does the Internet use a more automated procedure? This 
issue will be studied in depth in Chapter 5. But to whet your appetite here, we’ll note 
now that the Internet has a number of special routing protocols that are used to auto-
matically set the forwarding tables. A routing protocol may, for example, determine 
the shortest path from each router to each destination and use the shortest path results 
to configure the forwarding tables in the routers.
How would you actually like to see the end-to-end route that packets take in 
the Internet? We now invite you to get your hands dirty by interacting with the 
Trace-route program. Simply visit the site www.traceroute.org, choose a source in 
a particular country, and trace the route from that source to your computer. (For a 
discussion of Traceroute, see Section 1.4.)
1.3    •    The Network Core         55
1.3.2	Circuit Switching
There are two fundamental approaches to moving data through a network of links 
and switches: circuit switching and packet switching. Having covered packet-
switched networks in the previous subsection, we now turn our attention to circuit-
switched networks.
In circuit-switched networks, the resources needed along a path (buffers, link 
transmission rate) to provide for communication between the end systems are 
reserved for the duration of the communication session between the end systems. 
 
In packet-switched networks, these resources are not reserved; a session’s messages 
use the resources on demand and, as a consequence, may have to wait (that is, queue) 
for access to a communication link. As a simple analogy, consider two restaurants, 
one that requires reservations and another that neither requires reservations nor 
accepts them. For the restaurant that requires reservations, we have to go through 
the hassle of calling before we leave home. But when we arrive at the restaurant we 
can, in principle, immediately be seated and order our meal. For the restaurant that 
does not require reservations, we don’t need to bother to reserve a table. But when 
we arrive at the restaurant, we may have to wait for a table before we can be seated.
Traditional telephone networks are examples of circuit-switched networks. 
­
Consider what happens when one person wants to send information (voice or facsimile) 
 
to another over a telephone network. Before the sender can send the information, 
the network must establish a connection between the sender and the receiver. This 
is a bona fide connection for which the switches on the path between the sender and 
receiver maintain connection state for that connection. In the jargon of telephony, 
this connection is called a circuit. When the network establishes the circuit, it also 
reserves a constant transmission rate in the network’s links (representing a fraction 
of each link’s transmission capacity) for the duration of the connection. Since a given 
transmission rate has been reserved for this sender-to-receiver connection, the sender 
can transfer the data to the receiver at the guaranteed constant rate.
Figure 1.13 illustrates a circuit-switched network. In this network, the four 
circuit switches are interconnected by four links. Each of these links has four cir-
cuits, so that each link can support four simultaneous connections. The hosts (for 
example, PCs and workstations) are each directly connected to one of the switches. 
When two hosts want to communicate, the network establishes a dedicated end-
to-end connection between the two hosts. Thus, in order for Host A to communi-
cate with Host B, the network must first reserve one circuit on each of two links. 
In this example, the dedicated end-to-end connection uses the second circuit in 
the first link and the fourth circuit in the second link. Because each link has four 
circuits, for each link used by the end-to-end connection, the connection gets one 
fourth of the link’s total transmission capacity for the duration of the connection. 
Thus, for example, if each link between adjacent switches has a transmission rate of 
 
1 Mbps, then each end-to-end circuit-switch connection gets 250 kbps of dedicated 
transmission rate.
56         Chapter 1    •    Computer Networks and the Internet
Figure 1.13  ♦  
A simple circuit-switched network consisting of four switches 
and four links
In contrast, consider what happens when one host wants to send a packet to 
another host over a packet-switched network, such as the Internet. As with circuit 
switching, the packet is transmitted over a series of communication links. But dif-
ferent from circuit switching, the packet is sent into the network without reserving 
any link resources whatsoever. If one of the links is congested because other packets 
need to be transmitted over the link at the same time, then the packet will have to 
wait in a buffer at the sending side of the transmission link and suffer a delay. The 
Internet makes its best effort to deliver packets in a timely manner, but it does not 
make any guarantees.
Multiplexing in Circuit-Switched Networks
A circuit in a link is implemented with either frequency-division multiplexing 
(FDM) or time-division multiplexing (TDM). With FDM, the frequency spectrum 
of a link is divided up among the connections established across the link. Specifi-
cally, the link dedicates a frequency band to each connection for the ­
duration of the 
connection. In telephone networks, this frequency band typically has a width of 4 
kHz (that is, 4,000 hertz or 4,000 cycles per second). The width of the band is called, 
not surprisingly, the bandwidth. FM radio stations also use FDM to share the fre-
quency spectrum between 88 MHz and 108 MHz, with each station being allocated 
a specific frequency band.
For a TDM link, time is divided into frames of fixed duration, and each frame is 
divided into a fixed number of time slots. When the network establishes a connection 
across a link, the network dedicates one time slot in every frame to this connection. 
These slots are dedicated for the sole use of that connection, with one time slot avail-
able for use (in every frame) to transmit the connection’s data.
1.3    •    The Network Core         57
Figure 1.14 illustrates FDM and TDM for a specific network link supporting 
up to four circuits. For FDM, the frequency domain is segmented into four bands, 
each of bandwidth 4 kHz. For TDM, the time domain is segmented into frames, with 
four time slots in each frame; each circuit is assigned the same dedicated slot in the 
revolving TDM frames. For TDM, the transmission rate of a circuit is equal to the 
frame rate multiplied by the number of bits in a slot. For example, if the link trans-
mits 8,000 frames per second and each slot consists of 8 bits, then the transmission 
rate of each circuit is 64 kbps.
Proponents of packet switching have always argued that circuit switching is waste-
ful because the dedicated circuits are idle during silent periods. For example, when one 
person in a telephone call stops talking, the idle network resources (frequency bands or 
time slots in the links along the connection’s route) cannot be used by other ongoing 
connections. As another example of how these resources can be underutilized, consider 
a radiologist who uses a circuit-switched network to remotely access a series of x-rays. 
The radiologist sets up a connection, requests an image, contemplates the image, and 
then requests a new image. Network resources are allocated to the connection but are 
not used (i.e., are wasted) during the radiologist’s contemplation periods. Proponents 
of packet switching also enjoy pointing out that establishing end-to-end circuits and 
reserving end-to-end transmission capacity is complicated and requires complex sign-
aling software to coordinate the operation of the switches along the end-to-end path.
Figure 1.14  ♦  
With FDM, each circuit continuously gets a fraction of the 
bandwidth. With TDM, each circuit gets all of the bandwidth 
periodically during brief intervals of time (that is, during slots)
4KHz
TDM
FDM
Link
Frequency
4KHz
Slot
Key:
All slots labeled “2” are dedicated
to a speciﬁc sender-receiver pair.
Frame
1
2
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
Time 
58         Chapter 1    •    Computer Networks and the Internet
Before we finish our discussion of circuit switching, let’s work through a numer-
ical example that should shed further insight on the topic. Let us consider how long 
it takes to send a file of 640,000 bits from Host A to Host B over a circuit-switched 
network. Suppose that all links in the network use TDM with 24 slots and have a bit 
rate of 1.536 Mbps. Also suppose that it takes 500 msec to establish an end-to-end 
circuit before Host A can begin to transmit the file. How long does it take to send 
the file? Each circuit has a transmission rate of (1.536 Mbps)/24 = 64 kbps, so it 
takes (640,000 bits)/(64 kbps) = 10 seconds to transmit the file. To this 10 seconds 
we add the circuit establishment time, giving 10.5 seconds to send the file. Note 
that the transmission time is independent of the number of links: The transmission 
time would be 10 seconds if the end-to-end circuit passed through one link or a 
hundred links. (The actual end-to-end delay also includes a propagation delay; see 
Section 1.4.)
Packet Switching Versus Circuit Switching
Having described circuit switching and packet switching, let us compare the two. 
Critics of packet switching have often argued that packet switching is not suita-
ble for real-time services (for example, telephone calls and video conference calls) 
because of its variable and unpredictable end-to-end delays (due primarily to vari-
able and unpredictable queuing delays). Proponents of packet switching argue that 
(1) it offers better sharing of transmission capacity than circuit switching and (2) it 
is simpler, more efficient, and less costly to implement than circuit switching. An 
 
interesting discussion of packet switching versus circuit switching is [Molinero- 
Fernandez 2002]. Generally speaking, people who do not like to hassle with ­
restaurant 
reservations prefer packet switching to circuit switching.
Why is packet switching more efficient? Let’s look at a simple example. Sup-
pose users share a 1 Mbps link. Also suppose that each user alternates between peri-
ods of activity, when a user generates data at a constant rate of 100 kbps, and periods 
of inactivity, when a user generates no data. Suppose further that a user is active only 
10 percent of the time (and is idly drinking coffee during the remaining 90 percent 
of the time). With circuit switching, 100 kbps must be reserved for each user at all 
times. For example, with circuit-switched TDM, if a one-second frame is divided 
into 10 time slots of 100 ms each, then each user would be allocated one time slot 
per frame.
Thus, the circuit-switched link can support only 10 (= 1 Mbps/100 kbps) simul-
taneous users. With packet switching, the probability that a specific user is active 
is 0.1 (that is, 10 percent). If there are 35 users, the probability that there are 11 or 
more simultaneously active users is approximately 0.0004. (Homework Problem P8 
outlines how this probability is obtained.) When there are 10 or fewer simultane-
ously active users (which happens with probability 0.9996), the aggregate arrival 
rate of data is less than or equal to 1 Mbps, the output rate of the link. Thus, when 
there are 10 or fewer active users, users’ packets flow through the link essentially 
1.3    •    The Network Core         59
without delay, as is the case with circuit switching. When there are more than 10 
simultaneously active users, then the aggregate arrival rate of packets exceeds the 
output capacity of the link, and the output queue will begin to grow. (It continues to 
grow until the aggregate input rate falls back below 1 Mbps, at which point the queue 
will begin to diminish in length.) Because the probability of having more than 10 
simultaneously active users is minuscule in this example, packet switching provides 
essentially the same performance as circuit switching, but does so while allowing for 
more than three times the number of users.
Let’s now consider a second simple example. Suppose there are 10 users and 
that one user suddenly generates one thousand 1,000-bit packets, while other users 
remain quiescent and do not generate packets. Under TDM circuit switching with 10 
slots per frame and each slot consisting of 1,000 bits, the active user can only use its 
one time slot per frame to transmit data, while the remaining nine time slots in each 
frame remain idle. It will be 10 seconds before all of the active user’s one million 
bits of data has been transmitted. In the case of packet switching, the active user can 
continuously send its packets at the full link rate of 1 Mbps, since there are no other 
users generating packets that need to be multiplexed with the active user’s packets. 
In this case, all of the active user’s data will be transmitted within 1 second.
The above examples illustrate two ways in which the performance of packet 
switching can be superior to that of circuit switching. They also highlight the cru-
cial difference between the two forms of sharing a link’s transmission rate among 
multiple data streams. Circuit switching pre-allocates use of the transmission link 
regardless of demand, with allocated but unneeded link time going unused. Packet 
switching on the other hand allocates link use on demand. Link transmission capacity 
will be shared on a packet-by-packet basis only among those users who have packets 
that need to be transmitted over the link.
Although packet switching and circuit switching are both prevalent in today’s 
telecommunication networks, the trend has certainly been in the direction of packet 
switching. Even many of today’s circuit-switched telephone networks are slowly 
migrating toward packet switching. In particular, telephone networks often use 
packet switching for the expensive overseas portion of a telephone call.
1.3.3	A Network of Networks
We saw earlier that end systems (PCs, smartphones, Web servers, mail servers, and 
so on) connect into the Internet via an access ISP. The access ISP can provide either 
wired or wireless connectivity, using an array of access technologies including DSL, 
cable, FTTH, Wi-Fi, and cellular. Note that the access ISP does not have to be a 
telco or a cable company; instead it can be, for example, a university (providing 
Internet access to students, staff, and faculty), or a company (providing access for 
its employees). But connecting end users and content providers into an access ISP is 
only a small piece of solving the puzzle of connecting the billions of end systems that 
make up the Internet. To complete this puzzle, the access ISPs themselves must be 
60         Chapter 1    •    Computer Networks and the Internet
interconnected. This is done by creating a network of networks—understanding this 
phrase is the key to understanding the Internet.
Over the years, the network of networks that forms the Internet has evolved into 
a very complex structure. Much of this evolution is driven by economics and national 
policy, rather than by performance considerations. In order to understand today’s 
Internet network structure, let’s incrementally build a series of network structures, 
with each new structure being a better approximation of the complex Internet that we 
have today. Recall that the overarching goal is to interconnect the access ISPs so that 
all end systems can send packets to each other. One naive approach would be to have 
each access ISP directly connect with every other access ISP. Such a mesh design is, 
of course, much too costly for the access ISPs, as it would require each access ISP 
to have a separate communication link to each of the hundreds of thousands of other 
access ISPs all over the world.
Our first network structure, Network Structure 1, interconnects all of the access 
ISPs with a single global transit ISP. Our (imaginary) global transit ISP is a network 
of routers and communication links that not only spans the globe, but also has at least 
one router near each of the hundreds of thousands of access ISPs. Of course, it would 
be very costly for the global ISP to build such an extensive network. To be profitable, 
it would naturally charge each of the access ISPs for connectivity, with the pricing 
reflecting (but not necessarily directly proportional to) the amount of traffic an access 
ISP exchanges with the global ISP. Since the access ISP pays the global transit ISP, the 
access ISP is said to be a customer and the global transit ISP is said to be a provider.
Now if some company builds and operates a global transit ISP that is profit-
able, then it is natural for other companies to build their own global transit ISPs 
and compete with the original global transit ISP. This leads to Network Structure 2, 
 
which consists of the hundreds of thousands of access ISPs and multiple global 
­
transit ISPs. The access ISPs certainly prefer Network Structure 2 over Network 
Structure 1 since they can now choose among the competing global transit providers 
as a function of their pricing and services. Note, however, that the global transit ISPs 
themselves must interconnect: Otherwise access ISPs connected to one of the global 
transit providers would not be able to communicate with access ISPs connected to the 
 
other global transit providers.
Network Structure 2, just described, is a two-tier hierarchy with global transit 
providers residing at the top tier and access ISPs at the bottom tier. This assumes 
that global transit ISPs are not only capable of getting close to each and every access 
ISP, but also find it economically desirable to do so. In reality, although some ISPs 
do have impressive global coverage and do directly connect with many access ISPs, 
no ISP has presence in each and every city in the world. Instead, in any given region, 
there may be a regional ISP to which the access ISPs in the region connect. Each 
regional ISP then connects to tier-1 ISPs. Tier-1 ISPs are similar to our (imaginary) 
global transit ISP; but tier-1 ISPs, which actually do exist, do not have a presence 
in every city in the world. There are approximately a dozen tier-1 ISPs, including 
Level 3 Communications, AT&T, Sprint, and NTT. Interestingly, no group officially 
1.3    •    The Network Core         61
sanctions tier-1 status; as the saying goes—if you have to ask if you’re a member of 
a group, you’re probably not.
Returning to this network of networks, not only are there multiple competing 
tier-1 ISPs, there may be multiple competing regional ISPs in a region. In such a 
hierarchy, each access ISP pays the regional ISP to which it connects, and each 
regional ISP pays the tier-1 ISP to which it connects. (An access ISP can also connect 
directly to a tier-1 ISP, in which case it pays the tier-1 ISP). Thus, there is customer-
provider relationship at each level of the hierarchy. Note that the tier-1 ISPs do not 
pay anyone as they are at the top of the hierarchy. To further complicate matters, in 
some regions, there may be a larger regional ISP (possibly spanning an entire coun-
try) to which the smaller regional ISPs in that region connect; the larger regional 
ISP then connects to a tier-1 ISP. For example, in China, there are access ISPs in 
 
each city, which connect to provincial ISPs, which in turn connect to national ISPs, 
which finally connect to tier-1 ISPs [Tian 2012]. We refer to this multi-tier hierarchy, 
which is still only a crude approximation of today’s Internet, as Network Structure 3.
To build a network that more closely resembles today’s Internet, we must add 
points of presence (PoPs), multi-homing, peering, and Internet exchange points 
(IXPs) to the hierarchical Network Structure 3. PoPs exist in all levels of the hier-
archy, except for the bottom (access ISP) level. A PoP is simply a group of one or 
more routers (at the same location) in the provider’s network where customer ISPs 
can connect into the provider ISP. For a customer network to connect to a provider’s 
PoP, it can lease a high-speed link from a third-party telecommunications provider 
to directly connect one of its routers to a router at the PoP. Any ISP (except for tier-1 
ISPs) may choose to multi-home, that is, to connect to two or more provider ISPs. So, 
for example, an access ISP may multi-home with two regional ISPs, or it may multi-
home with two regional ISPs and also with a tier-1 ISP. Similarly, a regional ISP may 
multi-home with multiple tier-1 ISPs. When an ISP multi-homes, it can continue to 
send and receive packets into the Internet even if one of its providers has a failure.
As we just learned, customer ISPs pay their provider ISPs to obtain global Inter-
net interconnectivity. The amount that a customer ISP pays a provider ISP reflects 
the amount of traffic it exchanges with the provider. To reduce these costs, a pair 
of nearby ISPs at the same level of the hierarchy can peer, that is, they can directly 
connect their networks together so that all the traffic between them passes over the 
direct connection rather than through upstream intermediaries. When two ISPs peer, 
it is typically settlement-free, that is, neither ISP pays the other. As noted earlier, 
tier-1 ISPs also peer with one another, settlement-free. For a readable discussion of 
peering and customer-provider relationships, see [Van der Berg 2008]. Along these 
same lines, a third-party company can create an Internet Exchange Point (IXP), 
which is a meeting point where multiple ISPs can peer together. An IXP is typically 
in a stand-alone building with its own switches [Ager 2012]. There are over 400 
IXPs in the Internet today [IXP List 2016]. We refer to this ecosystem—consisting of 
access ISPs, regional ISPs, tier-1 ISPs, PoPs, multi-homing, peering, and IXPs—as 
Network Structure 4.
62         Chapter 1    •    Computer Networks and the Internet
We now finally arrive at Network Structure 5, which describes today’s Internet. 
Network Structure 5, illustrated in Figure 1.15, builds on top of Network Structure 
4 by adding content-provider networks. Google is currently one of the leading 
examples of such a content-provider network. As of this writing, it is estimated that 
Google has 50–100 data centers distributed across North America, Europe, Asia, 
South America, and Australia. Some of these data centers house over one hundred 
thousand servers, while other data centers are smaller, housing only hundreds of 
servers. The Google data centers are all interconnected via Google’s private TCP/IP 
network, which spans the entire globe but is nevertheless separate from the public 
Internet. Importantly, the Google private network only carries traffic to/from Google 
servers. As shown in Figure 1.15, the Google private network attempts to “bypass” 
the upper tiers of the Internet by peering (settlement free) with lower-tier ISPs, either 
by directly connecting with them or by connecting with them at IXPs [Labovitz 
2010]. However, because many access ISPs can still only be reached by transiting 
through tier-1 networks, the Google network also connects to tier-1 ISPs, and pays 
those ISPs for the traffic it exchanges with them. By creating its own network, a con-
tent provider not only reduces its payments to upper-tier ISPs, but also has greater 
control of how its services are ultimately delivered to end users. Google’s network 
infrastructure is described in greater detail in Section 2.6.
In summary, today’s Internet—a network of networks—is complex, consisting 
of a dozen or so tier-1 ISPs and hundreds of thousands of lower-tier ISPs. The ISPs 
are diverse in their coverage, with some spanning multiple continents and oceans, 
and others limited to narrow geographic regions. The lower-tier ISPs connect to the 
higher-tier ISPs, and the higher-tier ISPs interconnect with one another. Users and 
content providers are customers of lower-tier ISPs, and lower-tier ISPs are customers 
of higher-tier ISPs. In recent years, major content providers have also created their 
own networks and connect directly into lower-tier ISPs where possible.
Figure 1.15  ♦  Interconnection of ISPs
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
Regional
ISP
Tier 1
ISP
Content provider
(e.g., Google)
Tier 1
ISP
IXP
Regional
ISP
IXP
IXP
1.4    •    Delay, Loss, and Throughput in Packet-Switched Networks          63
1.4	 Delay, Loss, and Throughput  
in Packet-Switched Networks
Back in Section 1.1 we said that the Internet can be viewed as an infrastructure that 
provides services to distributed applications running on end systems. Ideally, we 
would like Internet services to be able to move as much data as we want between any 
two end systems, instantaneously, without any loss of data. Alas, this is a lofty goal, 
one that is unachievable in reality. Instead, computer networks necessarily constrain 
throughput (the amount of data per second that can be transferred) between end sys-
tems, introduce delays between end systems, and can actually lose packets. On one 
hand, it is unfortunate that the physical laws of reality introduce delay and loss as 
well as constrain throughput. On the other hand, because computer networks have 
these problems, there are many fascinating issues surrounding how to deal with the 
problems—more than enough issues to fill a course on computer networking and to 
motivate thousands of PhD theses! In this section, we’ll begin to examine and quan-
tify delay, loss, and throughput in computer networks.
1.4.1	Overview of Delay in Packet-Switched Networks
Recall that a packet starts in a host (the source), passes through a series of routers, 
and ends its journey in another host (the destination). As a packet travels from one 
node (host or router) to the subsequent node (host or router) along this path, the 
packet suffers from several types of delays at each node along the path. The most 
important of these delays are the nodal processing delay, queuing delay, transmis-
sion delay, and propagation delay; together, these delays accumulate to give a total 
nodal delay. The performance of many Internet applications—such as search, Web 
browsing, e-mail, maps, instant messaging, and voice-over-IP—are greatly affected 
by network delays. In order to acquire a deep understanding of packet switching and 
computer networks, we must understand the nature and importance of these delays.
Types of Delay
Let’s explore these delays in the context of Figure 1.16. As part of its end-to-end 
route between source and destination, a packet is sent from the upstream node 
through router A to router B. Our goal is to characterize the nodal delay at router A. 
Note that router A has an outbound link leading to router B. This link is preceded 
by a queue (also known as a buffer). When the packet arrives at router A from the 
upstream node, router A examines the packet’s header to determine the appropriate 
outbound link for the packet and then directs the packet to this link. In this exam-
ple, the outbound link for the packet is the one that leads to router B. A packet can 
be transmitted on a link only if there is no other packet currently being transmitted 
on the link and if there are no other packets preceding it in the queue; if the link is 
64         Chapter 1    •    Computer Networks and the Internet
­
currently busy or if there are other packets already queued for the link, the newly 
arriving packet will then join the queue.
Processing Delay
The time required to examine the packet’s header and determine where to direct 
the packet is part of the processing delay. The processing delay can also include 
other factors, such as the time needed to check for bit-level errors in the packet 
that occurred in transmitting the packet’s bits from the upstream node to router A. 
Processing delays in high-speed routers are typically on the order of microseconds 
or less. After this nodal processing, the router directs the packet to the queue that 
precedes the link to router B. (In Chapter 4 we’ll study the details of how a router 
operates.)
Queuing Delay
At the queue, the packet experiences a queuing delay as it waits to be transmitted 
onto the link. The length of the queuing delay of a specific packet will depend on the 
number of earlier-arriving packets that are queued and waiting for transmission onto 
the link. If the queue is empty and no other packet is currently being transmitted, then 
our packet’s queuing delay will be zero. On the other hand, if the traffic is heavy and 
many other packets are also waiting to be transmitted, the queuing delay will be long. 
We will see shortly that the number of packets that an arriving packet might expect 
to find is a function of the intensity and nature of the traffic arriving at the queue. 
­
Queuing delays can be on the order of microseconds to milliseconds in practice.
Transmission Delay
Assuming that packets are transmitted in a first-come-first-served manner, as is com-
mon in packet-switched networks, our packet can be transmitted only after all the 
packets that have arrived before it have been transmitted. Denote the length of the 
Figure 1.16  ♦  The nodal delay at router A
A
B
Nodal
processing
Queueing
(waiting for
transmission)
Transmission
Propagation
1.4    •    Delay, Loss, and Throughput in Packet-Switched Networks          65
packet by L bits, and denote the transmission rate of the link from router A to router 
B by R bits/sec. For example, for a 10 Mbps Ethernet link, the rate is R = 10 Mbps; 
for a 100 Mbps Ethernet link, the rate is R = 100 Mbps. The transmission delay is 
L/R. This is the amount of time required to push (that is, transmit) all of the packet’s 
bits into the link. Transmission delays are typically on the order of microseconds to 
milliseconds in practice.
Propagation Delay
Once a bit is pushed into the link, it needs to propagate to router B. The time required 
to propagate from the beginning of the link to router B is the propagation delay. The 
bit propagates at the propagation speed of the link. The propagation speed depends 
on the physical medium of the link (that is, fiber optics, twisted-pair copper wire, and 
so on) and is in the range of
	
2 # 108 meters/sec to 3 # 108 meters/sec
which is equal to, or a little less than, the speed of light. The propagation delay is the 
distance between two routers divided by the propagation speed. That is, the propaga-
tion delay is d/s, where d is the distance between router A and router B and s is the 
propagation speed of the link. Once the last bit of the packet propagates to node B, 
it and all the preceding bits of the packet are stored in router B. The whole process 
then continues with router B now performing the forwarding. In wide-area networks, 
propagation delays are on the order of milliseconds.
Comparing Transmission and Propagation Delay
Newcomers to the field of computer networking sometimes have difficulty under-
standing the difference between transmission delay and propagation delay. The dif-
ference is subtle but important. The transmission delay is the amount of time required 
for the router to push out the packet; it is a function of the packet’s length and the 
transmission rate of the link, but has nothing to do with the distance between the two 
routers. The propagation delay, on the other hand, is the time it takes a bit to propa-
gate from one router to the next; it is a function of the distance between the two rout-
ers, but has nothing to do with the packet’s length or the transmission rate of the link.
An analogy might clarify the notions of transmission and propagation delay. 
Consider a highway that has a tollbooth every 100 kilometers, as shown in Figure 
1.17. You can think of the highway segments between tollbooths as links and the 
tollbooths as routers. Suppose that cars travel (that is, propagate) on the highway 
at a rate of 100 km/hour (that is, when a car leaves a tollbooth, it instantaneously 
accelerates to 100 km/hour and maintains that speed between tollbooths). Suppose 
next that 10 cars, traveling together as a caravan, follow each other in a fixed order. 
You can think of each car as a bit and the caravan as a packet. Also suppose that each 
VideoNote
Exploring propagation 
delay and transmission 
delay
66         Chapter 1    •    Computer Networks and the Internet
tollbooth services (that is, transmits) a car at a rate of one car per 12 seconds, and that 
it is late at night so that the caravan’s cars are the only cars on the highway. Finally, 
suppose that whenever the first car of the caravan arrives at a tollbooth, it waits at 
the entrance until the other nine cars have arrived and lined up behind it. (Thus the 
entire caravan must be stored at the tollbooth before it can begin to be forwarded.) 
The time required for the tollbooth to push the entire caravan onto the highway is 
 
(10 cars)/(5 cars/minute) = 2 minutes. This time is analogous to the transmission 
delay in a router. The time required for a car to travel from the exit of one tollbooth 
to the next tollbooth is 100 km/(100 km/hour) = 1 hour. This time is analogous to 
propagation delay. Therefore, the time from when the caravan is stored in front of a 
tollbooth until the caravan is stored in front of the next tollbooth is the sum of trans-
mission delay and propagation delay—in this example, 62 minutes.
Let’s explore this analogy a bit more. What would happen if the tollbooth ser-
vice time for a caravan were greater than the time for a car to travel between toll-
booths? For example, suppose now that the cars travel at the rate of 1,000 km/hour 
and the tollbooth services cars at the rate of one car per minute. Then the traveling 
delay between two tollbooths is 6 minutes and the time to serve a caravan is 10 min-
utes. In this case, the first few cars in the caravan will arrive at the second tollbooth 
before the last cars in the caravan leave the first tollbooth. This situation also arises 
in packet-switched networks—the first bits in a packet can arrive at a router while 
many of the remaining bits in the packet are still waiting to be transmitted by the 
preceding router.
If a picture speaks a thousand words, then an animation must speak a million 
words. The Web site for this textbook provides an interactive Java applet that nicely 
illustrates and contrasts transmission delay and propagation delay. The reader is 
highly encouraged to visit that applet. [Smith 2009] also provides a very readable 
discussion of propagation, queueing, and transmission delays.
If we let dproc, dqueue, dtrans, and dprop denote the processing, queuing, transmis-
sion, and propagation delays, then the total nodal delay is given by
	
dnodal = dproc + dqueue + dtrans + dprop
The contribution of these delay components can vary significantly. For example, 
dprop can be negligible (for example, a couple of microseconds) for a link connecting 
Figure 1.17  ♦  Caravan analogy
Ten-car
caravan
Toll
booth
Toll
booth
100 km
100 km
1.4    •    Delay, Loss, and Throughput in Packet-Switched Networks          67
two routers on the same university campus; however, dprop is hundreds of millisec-
onds for two routers interconnected by a geostationary satellite link, and can be the 
dominant term in dnodal. Similarly, dtrans can range from negligible to significant. Its 
contribution is typically negligible for transmission rates of 10 Mbps and higher (for 
example, for LANs); however, it can be hundreds of milliseconds for large Internet 
packets sent over low-speed dial-up modem links. The processing delay, dproc, is 
often negligible; however, it strongly influences a router’s maximum throughput, 
which is the maximum rate at which a router can forward packets.
1.4.2	Queuing Delay and Packet Loss
The most complicated and interesting component of nodal delay is the queuing 
delay,  dqueue. In fact, queuing delay is so important and interesting in computer net-
working that thousands of papers and numerous books have been written about it 
[Bertsekas 1991; Daigle 1991; Kleinrock 1975, Kleinrock 1976; Ross 1995]. We 
give only a high-level, intuitive discussion of queuing delay here; the more curious 
reader may want to browse through some of the books (or even eventually write a 
PhD thesis on the subject!). Unlike the other three delays (namely, dproc,  dtrans, and 
 dprop), the queuing delay can vary from packet to packet. For example, if 10 packets 
arrive at an empty queue at the same time, the first packet transmitted will suffer no 
queuing delay, while the last packet transmitted will suffer a relatively large queuing 
delay (while it waits for the other nine packets to be transmitted). Therefore, when 
characterizing queuing delay, one typically uses statistical measures, such as average 
queuing delay, variance of queuing delay, and the probability that the queuing delay 
exceeds some specified value.
When is the queuing delay large and when is it insignificant? The answer to this 
question depends on the rate at which traffic arrives at the queue, the transmission 
rate of the link, and the nature of the arriving traffic, that is, whether the traffic arrives 
periodically or arrives in bursts. To gain some insight here, let a denote the average 
rate at which packets arrive at the queue (a is in units of packets/sec). Recall that R 
is the transmission rate; that is, it is the rate (in bits/sec) at which bits are pushed out 
of the queue. Also suppose, for simplicity, that all packets consist of L bits. Then the 
average rate at which bits arrive at the queue is La bits/sec. Finally, assume that the 
queue is very big, so that it can hold essentially an infinite number of bits. The ratio 
La/R, called the traffic intensity, often plays an important role in estimating the 
extent of the queuing delay. If La/R > 1, then the average rate at which bits arrive at 
the queue exceeds the rate at which the bits can be transmitted from the queue. In this 
unfortunate situation, the queue will tend to increase without bound and the queuing 
delay will approach infinity! Therefore, one of the golden rules in traffic engineering 
is: Design your system so that the traffic intensity is no greater than 1.
Now consider the case La/R ≤ 1. Here, the nature of the arriving traffic impacts 
the queuing delay. For example, if packets arrive periodically—that is, one packet 
arrives every L/R seconds—then every packet will arrive at an empty queue and 
68         Chapter 1    •    Computer Networks and the Internet
there will be no queuing delay. On the other hand, if packets arrive in bursts but 
periodically, there can be a significant average queuing delay. For example, sup-
pose N packets arrive simultaneously every (L/R)N seconds. Then the first packet 
transmitted has no queuing delay; the second packet transmitted has a queuing delay 
of L/R seconds; and more generally, the nth packet transmitted has a queuing delay 
of (n - 1)L/R seconds. We leave it as an exercise for you to calculate the average 
queuing delay in this example.
The two examples of periodic arrivals described above are a bit academic. 
­
Typically, the arrival process to a queue is random; that is, the arrivals do not fol-
low any pattern and the packets are spaced apart by random amounts of time. In this 
more realistic case, the quantity La/R is not usually sufficient to fully characterize the 
queuing delay statistics. Nonetheless, it is useful in gaining an intuitive understand-
ing of the extent of the queuing delay. In particular, if the traffic intensity is close to 
zero, then packet arrivals are few and far between and it is unlikely that an arriving 
packet will find another packet in the queue. Hence, the average queuing delay will 
be close to zero. On the other hand, when the traffic intensity is close to 1, there will 
be intervals of time when the arrival rate exceeds the transmission capacity (due to 
variations in packet arrival rate), and a queue will form during these periods of time; 
when the arrival rate is less than the transmission capacity, the length of the queue 
will shrink. Nonetheless, as the traffic intensity approaches 1, the average queue 
length gets larger and larger. The qualitative dependence of average queuing delay 
on the traffic intensity is shown in Figure 1.18.
One important aspect of Figure 1.18 is the fact that as the traffic intensity 
approaches 1, the average queuing delay increases rapidly. A small percentage 
increase in the intensity will result in a much larger percentage-wise increase in 
delay. Perhaps you have experienced this phenomenon on the highway. If you regu-
larly drive on a road that is typically congested, the fact that the road is typically 
Figure 1.18  ♦  Dependence of average queuing delay on traffic intensity
Average queuing delay
La/R
1
1.4    •    Delay, Loss, and Throughput in Packet-Switched Networks          69
congested means that its traffic intensity is close to 1. If some event causes an even 
slightly larger-than-usual amount of traffic, the delays you experience can be huge.
To really get a good feel for what queuing delays are about, you are encouraged 
once again to visit the textbook Web site, which provides an interactive Java applet 
for a queue. If you set the packet arrival rate high enough so that the traffic intensity 
exceeds 1, you will see the queue slowly build up over time.
Packet Loss
In our discussions above, we have assumed that the queue is capable of holding an 
infinite number of packets. In reality a queue preceding a link has finite capacity, 
although the queuing capacity greatly depends on the router design and cost. Because 
the queue capacity is finite, packet delays do not really approach infinity as the traffic 
intensity approaches 1. Instead, a packet can arrive to find a full queue. With no place 
to store such a packet, a router will drop that packet; that is, the packet will be lost. 
This overflow at a queue can again be seen in the Java applet for a queue when the 
traffic intensity is greater than 1.
From an end-system viewpoint, a packet loss will look like a packet having 
been transmitted into the network core but never emerging from the network at the 
destination. The fraction of lost packets increases as the traffic intensity increases. 
Therefore, performance at a node is often measured not only in terms of delay, but 
also in terms of the probability of packet loss. As we’ll discuss in the subsequent 
chapters, a lost packet may be retransmitted on an end-to-end basis in order to ensure 
that all data are eventually transferred from source to destination.
1.4.3	End-to-End Delay
Our discussion up to this point has focused on the nodal delay, that is, the delay at a 
single router. Let’s now consider the total delay from source to destination. To get a 
handle on this concept, suppose there are N - 1 routers between the source host and 
the destination host. Let’s also suppose for the moment that the network is uncon-
gested (so that queuing delays are negligible), the processing delay at each router 
and at the source host is dproc, the transmission rate out of each router and out of the 
source host is R bits/sec, and the propagation on each link is dprop. The nodal delays 
accumulate and give an end-to-end delay,
	
dend-end = N (dproc + dtrans + dprop)
(1.2)
where, once again, dtrans = L/R, where L is the packet size. Note that Equation 1.2 
is a generalization of Equation 1.1, which did not take into account processing and 
propagation delays. We leave it to you to generalize Equation 1.2 to the case of 
­
heterogeneous delays at the nodes and to the presence of an average queuing delay 
at each node.
70         Chapter 1    •    Computer Networks and the Internet
Traceroute
To get a hands-on feel for end-to-end delay in a computer network, we can make use 
of the Traceroute program. Traceroute is a simple program that can run in any Inter-
net host. When the user specifies a destination hostname, the program in the source 
host sends multiple, special packets toward that destination. As these packets work 
their way toward the destination, they pass through a series of routers. When a router 
receives one of these special packets, it sends back to the source a short message that 
contains the name and address of the router.
More specifically, suppose there are N - 1 routers between the source and the 
destination. Then the source will send N special packets into the network, with each 
packet addressed to the ultimate destination. These N special packets are marked 1 
through N, with the first packet marked 1 and the last packet marked N. When the 
nth router receives the nth packet marked n, the router does not forward the packet 
toward its destination, but instead sends a message back to the source. When the 
destination host receives the Nth packet, it too returns a message back to the source. 
The source records the time that elapses between when it sends a packet and when it 
receives the corresponding return message; it also records the name and address of 
the router (or the destination host) that returns the message. In this manner, the source 
can reconstruct the route taken by packets flowing from source to destination, and the 
source can determine the round-trip delays to all the intervening routers. Traceroute 
actually repeats the experiment just described three times, so the source actually 
sends 3 • N packets to the destination. RFC 1393 describes Traceroute in detail.
Here is an example of the output of the Traceroute program, where the route was 
being traced from the source host gaia.cs.umass.edu (at the University of ­
Massachusetts) 
to the host cis.poly.edu (at Polytechnic University in Brooklyn). The output has six 
columns: the first column is the n value described above, that is, the number of the 
router along the route; the second column is the name of the router; the third column is 
the address of the router (of the form xxx.xxx.xxx.xxx); the last three columns are the 
round-trip delays for three experiments. If the source receives fewer than three messages 
from any given router (due to packet loss in the network), Traceroute places an asterisk 
just after the router number and reports fewer than three round-trip times for that router.
VideoNote
Using Traceroute to 
discover network  
paths and measure 
network delay
1  cs-gw (128.119.240.254) 1.009 ms 0.899 ms 0.993 ms
2  128.119.3.154 (128.119.3.154) 0.931 ms 0.441 ms 0.651 ms
3  -border4-rt-gi-1-3.gw.umass.edu (128.119.2.194) 1.032 ms 0.484 ms 0.451 ms
4  -acr1-ge-2-1-0.Boston.cw.net (208.172.51.129) 10.006 ms 8.150 ms 8.460 ms
5  -agr4-loopback.NewYork.cw.net (206.24.194.104) 12.272 ms 14.344 ms 13.267 ms
6  -acr2-loopback.NewYork.cw.net (206.24.194.62) 13.225 ms 12.292 ms 12.148 ms
7  -pos10-2.core2.NewYork1.Level3.net (209.244.160.133) 12.218 ms 11.823 ms 11.793 ms
8 
 
 
-gige9-1-52.hsipaccess1.NewYork1.Level3.net (64.159.17.39) 13.081 ms 11.556 ms 13.297 ms
9  -p0-0.polyu.bbnplanet.net (4.25.109.122) 12.716 ms 13.052 ms 12.786 ms
10 cis.poly.edu (128.238.32.126) 14.080 ms 13.035 ms 12.802 ms
1.4    •    Delay, Loss, and Throughput in Packet-Switched Networks          71
In the trace above there are nine routers between the source and the destination. 
Most of these routers have a name, and all of them have addresses. For exam-
ple, the name of Router 3 is border4-rt-gi-1-3.gw.umass.edu and its 
address is 128.119.2.194. Looking at the data provided for this same router, 
we see that in the first of the three trials the round-trip delay between the source 
and the router was 1.03 msec. The round-trip delays for the subsequent two trials 
were 0.48 and 0.45 msec. These round-trip delays include all of the delays just 
discussed, including transmission delays, propagation delays, router processing 
delays, and queuing delays. Because the queuing delay is varying with time, the 
round-trip delay of packet n sent to a router n can sometimes be longer than the 
round-trip delay of packet n+1 sent to router n+1. Indeed, we observe this phe-
nomenon in the above example: the delays to Router 6 are larger than the delays 
to Router 7!
Want to try out Traceroute for yourself? We highly recommended that you visit 
http://www.traceroute.org, which provides a Web interface to an extensive list of 
sources for route tracing. You choose a source and supply the hostname for any 
destination. The Traceroute program then does all the work. There are a number of 
free software programs that provide a graphical interface to Traceroute; one of our 
favorites is PingPlotter [PingPlotter 2016].
End System, Application, and Other Delays
In addition to processing, transmission, and propagation delays, there can be addi-
tional significant delays in the end systems. For example, an end system wanting 
to transmit a packet into a shared medium (e.g., as in a WiFi or cable modem sce-
nario) may purposefully delay its transmission as part of its protocol for sharing the 
medium with other end systems; we’ll consider such protocols in detail in Chapter 6. 
 
Another important delay is media packetization delay, which is present in Voice-
over-IP (VoIP) applications. In VoIP, the sending side must first fill a packet with 
encoded digitized speech before passing the packet to the Internet. This time to fill a 
packet—called the packetization delay—can be significant and can impact the user-
perceived quality of a VoIP call. This issue will be further explored in a homework 
problem at the end of this chapter.
1.4.4	Throughput in Computer Networks
In addition to delay and packet loss, another critical performance measure in com-
puter networks is end-to-end throughput. To define throughput, consider transferring 
a large file from Host A to Host B across a computer network. This transfer might 
be, for example, a large video clip from one peer to another in a P2P file sharing 
system. The instantaneous throughput at any instant of time is the rate (in bits/
sec) at which Host B is receiving the file. (Many applications, including many P2P 
72         Chapter 1    •    Computer Networks and the Internet
file sharing ­
systems, display the instantaneous throughput during downloads in the 
user interface—perhaps you have observed this before!) If the file consists of F bits 
and the transfer takes T seconds for Host B to receive all F bits, then the aver-
age throughput of the file transfer is F/T bits/sec. For some applications, such as 
Internet telephony, it is desirable to have a low delay and an instantaneous through-
put consistently above some threshold (for example, over 24 kbps for some Internet 
telephony applications and over 256 kbps for some real-time video applications). For 
other applications, including those involving file transfers, delay is not critical, but it 
is desirable to have the highest possible throughput.
To gain further insight into the important concept of throughput, let’s consider 
a few examples. Figure 1.19(a) shows two end systems, a server and a client, con-
nected by two communication links and a router. Consider the throughput for a file 
transfer from the server to the client. Let Rs denote the rate of the link between the 
 
server and the router; and Rc denote the rate of the link between the router and 
 
the client. Suppose that the only bits being sent in the entire network are those 
from the server to the client. We now ask, in this ideal scenario, what is the server- 
to-client throughput? To answer this question, we may think of bits as fluid and com-
munication links as pipes. Clearly, the server cannot pump bits through its link at a 
rate faster than Rs bps; and the router cannot forward bits at a rate faster than Rc bps. 
If Rs 6 Rc, then the bits pumped by the server will “flow” right through the router 
and arrive at the client at a rate of Rs bps, giving a throughput of Rs bps. If, on the 
other hand, Rc 6 Rs, then the router will not be able to forward bits as quickly as it 
receives them. In this case, bits will only leave the router at rate Rc, giving an end-
to-end throughput of Rc. (Note also that if bits continue to arrive at the router at rate 
Rs, and continue to leave the router at Rc, the backlog of bits at the router waiting 
Figure 1.19  ♦ Throughput for a file transfer from server to client
Server
Rs
R1
R2
RN
Rc
Client
Server
a.
b.
Client
1.4    •    Delay, Loss, and Throughput in Packet-Switched Networks          73
for transmission to the client will grow and grow—a most undesirable situation!) 
Thus, for this simple two-link network, the throughput is min{Rc, Rs}, that is, it is the 
transmission rate of the bottleneck link. Having determined the throughput, we can 
now approximate the time it takes to transfer a large file of F bits from server to cli-
ent as F/min{Rs, Rc}. For a specific example, suppose you are downloading an MP3 
file of F = 32 million bits, the server has a transmission rate of Rs = 2 Mbps, and 
you have an access link of Rc = 1 Mbps. The time needed to transfer the file is then 
32 seconds. Of course, these expressions for throughput and transfer time are only 
approximations, as they do not account for store-and-forward and processing delays 
as well as protocol issues.
Figure 1.19(b) now shows a network with N links between the server and the 
client, with the transmission rates of the N links being R1, R2, c, RN. Applying 
the same analysis as for the two-link network, we find that the throughput for a file 
transfer from server to client is min{R1, R2, c, RN}, which is once again the trans-
mission rate of the bottleneck link along the path between server and client.
Now consider another example motivated by today’s Internet. Figure 1.20(a) 
shows two end systems, a server and a client, connected to a computer network. 
Consider the throughput for a file transfer from the server to the client. The server is 
connected to the network with an access link of rate Rs and the client is connected to 
the network with an access link of rate Rc. Now suppose that all the links in the core 
of the communication network have very high transmission rates, much higher than 
Rs and Rc. Indeed, today, the core of the Internet is over-provisioned with high speed 
links that experience little congestion. Also suppose that the only bits being sent in 
the entire network are those from the server to the client. Because the core of the 
computer network is like a wide pipe in this example, the rate at which bits can flow 
from source to destination is again the minimum of Rs and Rc, that is, throughput = 
min{Rs, Rc}. Therefore, the constraining factor for throughput in today’s Internet is 
typically the access network.
For a final example, consider Figure 1.20(b) in which there are 10 servers and 
10 clients connected to the core of the computer network. In this example, there are 
10 simultaneous downloads taking place, involving 10 client-server pairs. Suppose 
that these 10 downloads are the only traffic in the network at the current time. As 
shown in the figure, there is a link in the core that is traversed by all 10 downloads. 
Denote R for the transmission rate of this link R. Let’s suppose that all server access 
links have the same rate Rs, all client access links have the same rate Rc, and the 
transmission rates of all the links in the core—except the one common link of rate 
R—are much larger than Rs, Rc, and R. Now we ask, what are the throughputs of 
the downloads? Clearly, if the rate of the common link, R, is large—say a hun-
dred times larger than both Rs and Rc—then the throughput for each download will 
once again be min{Rs, Rc}. But what if the rate of the common link is of the same 
order as Rs and Rc? What will the throughput be in this case? Let’s take a look at 
a specific example. Suppose Rs = 2 Mbps, Rc = 1 Mbps, R = 5 Mbps, and the 
74         Chapter 1    •    Computer Networks and the Internet
common link divides its transmission rate equally among the 10 downloads. Then 
the bottleneck for each download is no longer in the access network, but is now 
instead the shared link in the core, which only provides each download with 500 
kbps of throughput. Thus the end-to-end throughput for each download is now 
reduced to 500 kbps.
The examples in Figure 1.19 and Figure 1.20(a) show that throughput depends 
on the transmission rates of the links over which the data flows. We saw that when 
there is no other intervening traffic, the throughput can simply be approximated as 
the minimum transmission rate along the path between source and destination. The 
example in Figure 1.20(b) shows that more generally the throughput depends not 
only on the transmission rates of the links along the path, but also on the interven-
ing traffic. In particular, a link with a high transmission rate may nonetheless be the 
bottleneck link for a file transfer if many other data flows are also passing through 
that link. We will examine throughput in computer networks more closely in the 
homework problems and in the subsequent chapters.
Figure 1.20  ♦ 
End-to-end throughput: (a) Client downloads a file from 
­
server; (b) 10 clients ­
downloading with 10 servers
Server
Rs
Rc
a.
b.
Client
10 Clients
10 Servers
Bottleneck
link of
capacity R
1.5    •    Protocol Layers and Their Service Models         75
1.5	 Protocol Layers and Their Service Models
From our discussion thus far, it is apparent that the Internet is an extremely com-
plicated system. We have seen that there are many pieces to the Internet: numerous 
applications and protocols, various types of end systems, packet switches, and vari-
ous types of link-level media. Given this enormous complexity, is there any hope of 
organizing a network architecture, or at least our discussion of network architecture? 
Fortunately, the answer to both questions is yes.
1.5.1	Layered Architecture
Before attempting to organize our thoughts on Internet architecture, let’s look 
for a human analogy. Actually, we deal with complex systems all the time in our 
everyday life. Imagine if someone asked you to describe, for example, the air-
line system. How would you find the structure to describe this complex system 
that has ticketing agents, baggage checkers, gate personnel, pilots, airplanes, 
air traffic control, and a worldwide system for routing airplanes? One way to 
describe this system might be to describe the series of actions you take (or oth-
ers take for you) when you fly on an airline. You purchase your ticket, check 
your bags, go to the gate, and eventually get loaded onto the plane. The plane 
takes off and is routed to its destination. After your plane lands, you deplane at 
the gate and claim your bags. If the trip was bad, you complain about the flight 
to the ticket agent (getting nothing for your effort). This scenario is shown in 
Figure 1.21.
Figure 1.21  ♦  Taking an airplane trip: actions
Ticket (purchase)
Baggage (check)
Gates (load)
Runway takeoff
Airplane routing
Ticket (complain)
Baggage (claim)
Gates (unload)
Runway landing
Airplane routing
Airplane routing
76         Chapter 1    •    Computer Networks and the Internet
Already, we can see some analogies here with computer networking: You are 
being shipped from source to destination by the airline; a packet is shipped from 
source host to destination host in the Internet. But this is not quite the analogy we 
are after. We are looking for some structure in Figure 1.21. Looking at Figure 1.21, 
we note that there is a ticketing function at each end; there is also a baggage func-
tion for already-ticketed passengers, and a gate function for already-ticketed and 
already-baggage-checked passengers. For passengers who have made it through the 
gate (that is, passengers who are already ticketed, baggage-checked, and through the 
gate), there is a takeoff and landing function, and while in flight, there is an airplane-
routing function. This suggests that we can look at the functionality in Figure 1.21 in 
a horizontal manner, as shown in Figure 1.22.
Figure 1.22 has divided the airline functionality into layers, providing a frame-
work in which we can discuss airline travel. Note that each layer, combined with the 
layers below it, implements some functionality, some service. At the ticketing layer 
and below, airline-counter-to-airline-counter transfer of a person is accomplished. At 
the baggage layer and below, baggage-check-to-baggage-claim transfer of a person 
and bags is accomplished. Note that the baggage layer provides this service only to an 
already-ticketed person. At the gate layer, departure-gate-to-arrival-gate transfer of 
a person and bags is accomplished. At the takeoff/landing layer, runway-to-runway 
 
transfer of people and their bags is accomplished. Each layer provides its service 
by (1) performing certain actions within that layer (for example, at the gate layer, 
loading and unloading people from an airplane) and by (2) using the services of the 
layer directly below it (for example, in the gate layer, using the runway-to-runway 
passenger transfer service of the takeoff/landing layer).
A layered architecture allows us to discuss a well-defined, specific part of a 
large and complex system. This simplification itself is of considerable value by 
providing modularity, making it much easier to change the implementation of the 
service provided by the layer. As long as the layer provides the same service to the 
layer above it, and uses the same services from the layer below it, the remainder of 
Figure 1.22  ♦  Horizontal layering of airline functionality
Ticket (purchase)
Baggage (check)
Gates (load)
Runway takeoff
Airplane routing
Airplane routing
Airplane routing
Ticket (complain)
Baggage (claim)
Gates (unload)
Runway landing
Airplane routing
Ticket
Baggage
Gate
Takeoff/Landing
Departure airport
Intermediate air-trafﬁc
control centers
1.5    •    Protocol Layers and Their Service Models         77
the system remains unchanged when a layer’s implementation is changed. (Note 
that changing the implementation of a service is very different from changing the 
service itself!) For example, if the gate functions were changed (for instance, to have 
people board and disembark by height), the remainder of the airline system would 
remain unchanged since the gate layer still provides the same function (loading and 
unloading people); it simply implements that function in a different manner after the 
change. For large and complex systems that are constantly being updated, the ability 
to change the implementation of a service without affecting other components of the 
system is another important advantage of layering.
Protocol Layering
But enough about airlines. Let’s now turn our attention to network protocols. To 
provide structure to the design of network protocols, network designers organize 
protocols—and the network hardware and software that implement the protocols—
in layers. Each protocol belongs to one of the layers, just as each function in the 
airline architecture in Figure 1.22 belonged to a layer. We are again interested in 
the services that a layer offers to the layer above—the so-called service model of 
a layer. Just as in the case of our airline example, each layer provides its service 
by (1) performing certain actions within that layer and by (2) using the services 
of the layer directly below it. For example, the services provided by layer n may 
include reliable delivery of messages from one edge of the network to the other. 
This might be implemented by using an unreliable edge-to-edge message delivery 
service of layer n - 1, and adding layer n functionality to detect and retransmit 
lost messages.
A protocol layer can be implemented in software, in hardware, or in a combina-
tion of the two. Application-layer protocols—such as HTTP and SMTP—are almost 
always implemented in software in the end systems; so are transport-layer protocols. 
Because the physical layer and data link layers are responsible for handling commu-
nication over a specific link, they are typically implemented in a network interface 
card (for example, Ethernet or WiFi interface cards) associated with a given link. The 
network layer is often a mixed implementation of hardware and software. Also note 
that just as the functions in the layered airline architecture were distributed among 
the various airports and flight control centers that make up the system, so too is a 
layer n protocol distributed among the end systems, packet switches, and other com-
ponents that make up the network. That is, there’s often a piece of a layer n protocol 
in each of these network components.
Protocol layering has conceptual and structural advantages [RFC 3439]. As 
we have seen, layering provides a structured way to discuss system components. 
Modularity makes it easier to update system components. We mention, however, 
that some researchers and networking engineers are vehemently opposed to layering 
[Wakeman 1992]. One potential drawback of layering is that one layer may duplicate 
lower-layer functionality. For example, many protocol stacks provide error recovery 
78         Chapter 1    •    Computer Networks and the Internet
on both a per-link basis and an end-to-end basis. A second potential drawback is that 
functionality at one layer may need information (for example, a timestamp value) 
that is present only in another layer; this violates the goal of separation of layers.
When taken together, the protocols of the various layers are called the protocol 
stack. The Internet protocol stack consists of five layers: the physical, link, network, 
transport, and application layers, as shown in Figure 1.23(a). If you examine the 
Table of Contents, you will see that we have roughly organized this book using the 
layers of the Internet protocol stack. We take a top-down approach, first covering 
the application layer and then proceeding downward.
Application Layer
The application layer is where network applications and their application-layer pro-
tocols reside. The Internet’s application layer includes many protocols, such as the 
HTTP protocol (which provides for Web document request and transfer), SMTP 
(which provides for the transfer of e-mail messages), and FTP (which provides for 
the transfer of files between two end systems). We’ll see that certain network func-
tions, such as the translation of human-friendly names for Internet end systems like 
www.ietf.org to a 32-bit network address, are also done with the help of a specific 
application-layer protocol, namely, the domain name system (DNS). We’ll see in 
Chapter 2 that it is very easy to create and deploy our own new application-layer 
protocols.
An application-layer protocol is distributed over multiple end systems, with the 
application in one end system using the protocol to exchange packets of information 
with the application in another end system. We’ll refer to this packet of information 
at the application layer as a message.
Figure 1.23  ♦ The Internet protocol stack (a) and OSI reference model (b)
Transport
Application
Network
Link
Physical
a.  Five-layer
 
Internet
 
protocol stack
Transport
Session
Application
Presentation
Network
Link
Physical
b.  
Seven-layer
 
ISO OSI
 
reference model
1.5    •    Protocol Layers and Their Service Models         79
Transport Layer
The Internet’s transport layer transports application-layer messages between applica-
tion endpoints. In the Internet there are two transport protocols, TCP and UDP, either of 
which can transport application-layer messages. TCP provides a ­
connection-oriented 
service to its applications. This service includes guaranteed delivery of application-
layer messages to the destination and flow control (that is, sender/receiver speed 
matching). TCP also breaks long messages into shorter ­
segments and provides a 
congestion-control mechanism, so that a source throttles its transmission rate when 
the network is congested. The UDP protocol provides a connectionless service to its 
applications. This is a no-frills service that provides no reliability, no flow control, 
and no congestion control. In this book, we’ll refer to a transport-layer packet as a 
segment.
Network Layer
The Internet’s network layer is responsible for moving network-layer packets known 
as datagrams from one host to another. The Internet transport-layer protocol (TCP 
or UDP) in a source host passes a transport-layer segment and a destination address 
to the network layer, just as you would give the postal service a letter with a destina-
tion address. The network layer then provides the service of delivering the segment 
to the transport layer in the destination host.
The Internet’s network layer includes the celebrated IP protocol, which defines 
the fields in the datagram as well as how the end systems and routers act on these 
fields. There is only one IP protocol, and all Internet components that have a network 
layer must run the IP protocol. The Internet’s network layer also contains routing 
protocols that determine the routes that datagrams take between sources and destina-
tions. The Internet has many routing protocols. As we saw in Section 1.3, the Internet 
is a network of networks, and within a network, the network administrator can run 
any routing protocol desired. Although the network layer contains both the IP pro-
tocol and numerous routing protocols, it is often simply referred to as the IP layer, 
reflecting the fact that IP is the glue that binds the Internet together.
Link Layer
The Internet’s network layer routes a datagram through a series of routers between 
the source and destination. To move a packet from one node (host or router) to the 
next node in the route, the network layer relies on the services of the link layer. In 
particular, at each node, the network layer passes the datagram down to the link 
layer, which delivers the datagram to the next node along the route. At this next node, 
the link layer passes the datagram up to the network layer.
The services provided by the link layer depend on the specific link-layer proto-
col that is employed over the link. For example, some link-layer protocols provide 
80         Chapter 1    •    Computer Networks and the Internet
reliable delivery, from transmitting node, over one link, to receiving node. Note that 
this reliable delivery service is different from the reliable delivery service of TCP, 
which provides reliable delivery from one end system to another. Examples of link-
layer protocols include Ethernet, WiFi, and the cable access network’s DOCSIS pro-
tocol. As datagrams typically need to traverse several links to travel from source to 
destination, a datagram may be handled by different link-layer protocols at different 
links along its route. For example, a datagram may be handled by Ethernet on one 
link and by PPP on the next link. The network layer will receive a different service 
from each of the different link-layer protocols. In this book, we’ll refer to the link-
layer packets as frames.
Physical Layer
While the job of the link layer is to move entire frames from one network element to 
an adjacent network element, the job of the physical layer is to move the individual 
bits within the frame from one node to the next. The protocols in this layer are again 
link dependent and further depend on the actual transmission medium of the link (for 
example, twisted-pair copper wire, single-mode fiber optics). For example, Ether-
net has many physical-layer protocols: one for twisted-pair copper wire, another for 
coaxial cable, another for fiber, and so on. In each case, a bit is moved across the link 
in a different way.
The OSI Model
Having discussed the Internet protocol stack in detail, we should mention that it is not 
the only protocol stack around. In particular, back in the late 1970s, the International 
Organization for Standardization (ISO) proposed that computer networks be organ-
ized around seven layers, called the Open Systems Interconnection (OSI) model 
[ISO 2016]. The OSI model took shape when the protocols that were to become the 
Internet protocols were in their infancy, and were but one of many different protocol 
suites under development; in fact, the inventors of the original OSI model probably 
did not have the Internet in mind when creating it. Nevertheless, beginning in the 
late 1970s, many training and university courses picked up on the ISO mandate and 
organized courses around the seven-layer model. Because of its early impact on net-
working education, the seven-layer model continues to linger on in some networking 
textbooks and training courses.
The seven layers of the OSI reference model, shown in Figure 1.23(b), are: 
application layer, presentation layer, session layer, transport layer, network layer, 
data link layer, and physical layer. The functionality of five of these layers is roughly 
the same as their similarly named Internet counterparts. Thus, let’s consider the two 
additional layers present in the OSI reference model—the presentation layer and the 
session layer. The role of the presentation layer is to provide services that allow com-
municating applications to interpret the meaning of data exchanged. These services 
1.5    •    Protocol Layers and Their Service Models         81
include data compression and data encryption (which are self-explanatory) as well as 
data description (which frees the applications from having to worry about the inter-
nal format in which data are represented/stored—formats that may differ from one 
computer to another). The session layer provides for delimiting and synchronization 
of data exchange, including the means to build a checkpointing and recovery scheme.
The fact that the Internet lacks two layers found in the OSI reference model 
poses a couple of interesting questions: Are the services provided by these layers 
unimportant? What if an application needs one of these services? The Internet’s 
answer to both of these questions is the same—it’s up to the application developer. 
It’s up to the application developer to decide if a service is important, and if the ser-
vice is important, it’s up to the application developer to build that functionality into 
the application.
1.5.2	Encapsulation
Figure 1.24 shows the physical path that data takes down a sending end system’s 
protocol stack, up and down the protocol stacks of an intervening link-layer switch 
Figure 1.24  ♦  
Hosts, routers, and link-layer switches; each contains 
a ­
different set of layers, reflecting their differences in 
­
functionality
M
M
M
M
Ht
Ht
Ht
Hn
Hn
Hl
Ht
Hn
Hl
Link-layer switch
Router
Application
Transport
Network
Link
Physical
Message
Segment
Datagram
Frame
M
M
M
M
Ht
Ht
Ht
Hn
Hn
Hl
Link
Physical
Source
Network
Link
Physical
Destination
Application
Transport
Network
Link
Physical
M
Ht
Hn
Hl
M
Ht
Hn
M
Ht
Hn
M
Ht
Hn
Hl
M
Ht
Hn
Hl
M
82         Chapter 1    •    Computer Networks and the Internet
and router, and then up the protocol stack at the receiving end system. As we discuss 
later in this book, routers and link-layer switches are both packet switches. Similar 
to end systems, routers and link-layer switches organize their networking hardware 
and software into layers. But routers and link-layer switches do not implement all of 
the layers in the protocol stack; they typically implement only the bottom layers. As 
shown in Figure 1.24, link-layer switches implement layers 1 and 2; routers imple-
ment layers 1 through 3. This means, for example, that Internet routers are capable of 
implementing the IP protocol (a layer 3 protocol), while link-layer switches are not. 
We’ll see later that while link-layer switches do not recognize IP addresses, they are 
capable of recognizing layer 2 addresses, such as Ethernet addresses. Note that hosts 
implement all five layers; this is consistent with the view that the Internet architec-
ture puts much of its complexity at the edges of the network.
Figure 1.24 also illustrates the important concept of encapsulation. At the send-
ing host, an application-layer message (M in Figure 1.24) is passed to the transport 
layer. In the simplest case, the transport layer takes the message and appends addi-
tional information (so-called transport-layer header information, Ht in Figure 1.24) 
that will be used by the receiver-side transport layer. The application-layer message 
and the transport-layer header information together constitute the transport-layer 
segment. The transport-layer segment thus encapsulates the application-layer mes-
sage. The added information might include information allowing the receiver-side 
transport layer to deliver the message up to the appropriate application, and error-
detection bits that allow the receiver to determine whether bits in the message have 
been changed in route. The transport layer then passes the segment to the network 
layer, which adds network-layer header information (Hn in Figure 1.24) such as 
source and destination end system addresses, creating a network-layer datagram. 
The datagram is then passed to the link layer, which (of course!) will add its own 
link-layer header information and create a link-layer frame. Thus, we see that at 
each layer, a packet has two types of fields: header fields and a payload field. The 
payload is typically a packet from the layer above.
A useful analogy here is the sending of an interoffice memo from one corpo-
rate branch office to another via the public postal service. Suppose Alice, who is in 
one branch office, wants to send a memo to Bob, who is in another branch office. 
The memo is analogous to the application-layer message. Alice puts the memo in 
an interoffice envelope with Bob’s name and department written on the front of 
the envelope. The interoffice envelope is analogous to a transport-layer segment—it 
contains header information (Bob’s name and department number) and it encap-
sulates the application-layer message (the memo). When the sending branch-office 
mailroom receives the interoffice envelope, it puts the interoffice envelope inside 
yet another envelope, which is suitable for sending through the public postal service. 
The sending mailroom also writes the postal address of the sending and receiving 
branch offices on the postal envelope. Here, the postal envelope is analogous to the 
datagram—it encapsulates the transport-layer segment (the interoffice envelope), 
which encapsulates the original message (the memo). The postal service delivers the 
1.6    •    Networks Under Attack         83
postal envelope to the receiving branch-office mailroom. There, the process of de-
encapsulation is begun. The mailroom extracts the interoffice memo and forwards it 
to Bob. Finally, Bob opens the envelope and removes the memo.
The process of encapsulation can be more complex than that described above. 
For example, a large message may be divided into multiple transport-layer segments 
(which might themselves each be divided into multiple network-layer datagrams). 
At the receiving end, such a segment must then be reconstructed from its constituent 
datagrams.
1.6	 Networks Under Attack
The Internet has become mission critical for many institutions today, including large 
and small companies, universities, and government agencies. Many individuals also 
rely on the Internet for many of their professional, social, and personal activities. 
Billions of “things,” including wearables and home devices, are currently being con-
nected to the Internet. But behind all this utility and excitement, there is a dark side, 
a side where “bad guys” attempt to wreak havoc in our daily lives by damaging our 
Internet-connected computers, violating our privacy, and rendering inoperable the 
Internet services on which we depend.
The field of network security is about how the bad guys can attack computer 
networks and about how we, soon-to-be experts in computer networking, can defend 
networks against those attacks, or better yet, design new architectures that are 
immune to such attacks in the first place. Given the frequency and variety of exist-
ing attacks as well as the threat of new and more destructive future attacks, network 
security has become a central topic in the field of computer networking. One of the 
features of this textbook is that it brings network security issues to the forefront.
Since we don’t yet have expertise in computer networking and Internet protocols, 
we’ll begin here by surveying some of today’s more prevalent security-related prob-
lems. This will whet our appetite for more substantial discussions in the upcoming 
chapters. So we begin here by simply asking, what can go wrong? How are computer 
networks vulnerable? What are some of the more prevalent types of attacks today?
The Bad Guys Can Put Malware into Your Host Via the Internet
We attach devices to the Internet because we want to receive/send data from/to the 
Internet. This includes all kinds of good stuff, including Instagram posts, Internet 
search results, streaming music, video conference calls, streaming movies, and 
so on. But, unfortunately, along with all that good stuff comes malicious stuff—­
collectively known as malware—that can also enter and infect our devices. Once 
malware infects our device it can do all kinds of devious things, including deleting 
 
our files and installing spyware that collects our private information, such as social 
84         Chapter 1    •    Computer Networks and the Internet
­
security numbers, passwords, and keystrokes, and then sends this (over the Internet, 
of course!) back to the bad guys. Our compromised host may also be enrolled in 
a network of thousands of similarly compromised devices, collectively known as 
a botnet, which the bad guys control and leverage for spam e-mail distribution or 
distributed denial-of-service attacks (soon to be discussed) against targeted hosts.
Much of the malware out there today is self-replicating: once it infects one host, 
from that host it seeks entry into other hosts over the Internet, and from ­
the newly 
infected hosts, it seeks entry into yet more hosts. In this manner, self-­
replicating mal-
ware can spread exponentially fast. Malware can spread in the form of a virus or a 
worm. Viruses are malware that require some form of user interaction to infect the 
user’s device. The classic example is an e-mail attachment containing malicious exe-
cutable code. If a user receives and opens such an attachment, the user inadvertently 
runs the malware on the device. Typically, such e-mail viruses are self-replicating: once 
executed, the virus may send an identical message with an identical malicious attach-
ment to, for example, every recipient in the user’s address book. Worms are malware 
that can enter a device without any explicit user interaction. For example, a user may 
be running a vulnerable network application to which an attacker can send malware. 
In some cases, without any user intervention, the application may accept the malware 
from the Internet and run it, creating a worm. The worm in the newly infected device 
then scans the Internet, searching for other hosts running the same vulnerable network 
application. When it finds other vulnerable hosts, it sends a copy of itself to those hosts. 
Today, malware, is pervasive and costly to defend against. As you work through this 
textbook, we encourage you to think about the following question: What can computer 
network designers do to defend Internet-attached devices from malware attacks?
The Bad Guys Can Attack Servers and Network Infrastructure
Another broad class of security threats are known as denial-of-service (DoS) 
attacks. As the name suggests, a DoS attack renders a network, host, or other piece 
of infrastructure unusable by legitimate users. Web servers, e-mail servers, DNS 
servers (discussed in Chapter 2), and institutional networks can all be subject to DoS 
attacks. Internet DoS attacks are extremely common, with thousands of DoS attacks 
occurring every year [Moore 2001]. The site Digital Attack Map allows use to visu-
alize the top daily DoS attacks worldwide [DAM 2016]. Most Internet DoS attacks 
fall into one of three categories:
•	 Vulnerability attack.   This involves sending a few well-crafted messages to a 
vulnerable application or operating system running on a targeted host. If the right 
sequence of packets is sent to a vulnerable application or operating system, the 
service can stop or, worse, the host can crash.
•	 Bandwidth flooding.   The attacker sends a deluge of packets to the targeted 
host—so many packets that the target’s access link becomes clogged, preventing 
legitimate packets from reaching the server.
1.6    •    Networks Under Attack         85
•	 Connection flooding.   The attacker establishes a large number of half-open or 
fully open TCP connections (TCP connections are discussed in Chapter 3) at the 
target host. The host can become so bogged down with these bogus connections 
that it stops accepting legitimate connections.
Let’s now explore the bandwidth-flooding attack in more detail. Recalling our 
delay and loss analysis discussion in Section 1.4.2, it’s evident that if the server 
has an access rate of R bps, then the attacker will need to send traffic at a rate of 
approximately R bps to cause damage. If R is very large, a single attack source 
may not be able to generate enough traffic to harm the server. Furthermore, if all 
the traffic emanates from a single source, an upstream router may be able to detect 
the attack and block all traffic from that source before the traffic gets near the 
server. In a distributed DoS (DDoS) attack, illustrated in Figure 1.25, the attacker 
controls multiple sources and has each source blast traffic at the target. With this 
approach, the aggregate traffic rate across all the controlled sources needs to be 
approximately R to cripple the ­
service. DDoS attacks leveraging botnets with thou-
sands of comprised hosts are a common occurrence today [DAM 2016]. DDos 
attacks are much harder to detect and defend against than a DoS attack from a 
single host.
We encourage you to consider the following question as you work your way 
through this book: What can computer network designers do to defend against DoS 
attacks? We will see that different defenses are needed for the three types of DoS 
attacks.
Figure 1.25  ♦  A distributed denial-of-service attack
Attacker
“start
    attack”
Slave
Slave
Slave
Victim
Slave
Slave
86         Chapter 1    •    Computer Networks and the Internet
The Bad Guys Can Sniff Packets
Many users today access the Internet via wireless devices, such as WiFi-connected 
laptops or handheld devices with cellular Internet connections (covered in Chapter 7). 
 
While ubiquitous Internet access is extremely convenient and enables marvelous 
new applications for mobile users, it also creates a major security vulnerability—by 
placing a passive receiver in the vicinity of the wireless transmitter, that receiver 
can obtain a copy of every packet that is transmitted! These packets can contain all 
kinds of sensitive information, including passwords, social security numbers, trade 
secrets, and private personal messages. A passive receiver that records a copy of 
every packet that flies by is called a packet sniffer.
Sniffers can be deployed in wired environments as well. In wired broadcast envi-
ronments, as in many Ethernet LANs, a packet sniffer can obtain copies of broadcast 
packets sent over the LAN. As described in Section 1.2, cable access technologies 
also broadcast packets and are thus vulnerable to sniffing. Furthermore, a bad guy 
who gains access to an institution’s access router or access link to the Internet may be 
able to plant a sniffer that makes a copy of every packet going to/from the organiza-
tion. Sniffed packets can then be analyzed offline for sensitive information.
Packet-sniffing software is freely available at various Web sites and as commercial 
products. Professors teaching a networking course have been known to assign lab exer-
cises that involve writing a packet-sniffing and application-layer data reconstruction 
program. Indeed, the Wireshark [Wireshark 2016] labs associated with this text (see the 
introductory Wireshark lab at the end of this chapter) use exactly such a packet sniffer!
Because packet sniffers are passive—that is, they do not inject packets into the 
channel—they are difficult to detect. So, when we send packets into a wireless chan-
nel, we must accept the possibility that some bad guy may be recording copies of our 
packets. As you may have guessed, some of the best defenses against packet sniffing 
involve cryptography. We will examine cryptography as it applies to network secu-
rity in Chapter 8.
The Bad Guys Can Masquerade as Someone You Trust
It is surprisingly easy (you will have the knowledge to do so shortly as you proceed 
through this text!) to create a packet with an arbitrary source address, packet content, 
and destination address and then transmit this hand-crafted packet into the Internet, 
which will dutifully forward the packet to its destination. Imagine the unsuspecting 
receiver (say an Internet router) who receives such a packet, takes the (false) source 
address as being truthful, and then performs some command embedded in the pack-
et’s contents (say modifies its forwarding table). The ability to inject packets into the 
Internet with a false source address is known as IP spoofing, and is but one of many 
ways in which one user can masquerade as another user.
To solve this problem, we will need end-point authentication, that is, a mecha-
nism that will allow us to determine with certainty if a message originates from 
1.7    •    History of Computer Networking and the Internet          87
where we think it does. Once again, we encourage you to think about how this can 
be done for network applications and protocols as you progress through the chapters 
of this book. We will explore mechanisms for end-point authentication in Chapter 8.
In closing this section, it’s worth considering how the Internet got to be such 
an insecure place in the first place. The answer, in essence, is that the Internet was 
originally designed to be that way, based on the model of “a group of mutually trust-
ing users attached to a transparent network” [Blumenthal 2001]—a model in which 
(by definition) there is no need for security. Many aspects of the original Internet 
architecture deeply reflect this notion of mutual trust. For example, the ability for 
one user to send a packet to any other user is the default rather than a requested/
granted capability, and user identity is taken at declared face value, rather than being 
authenticated by default.
But today’s Internet certainly does not involve “mutually trusting users.” None-
theless, today’s users still need to communicate when they don’t necessarily trust 
each other, may wish to communicate anonymously, may communicate indirectly 
through third parties (e.g., Web caches, which we’ll study in Chapter 2, or mobility-
assisting agents, which we’ll study in Chapter 7), and may distrust the hardware, 
software, and even the air through which they communicate. We now have many 
security-related challenges before us as we progress through this book: We should 
seek defenses against sniffing, end-point masquerading, man-in-the-middle attacks, 
DDoS attacks, malware, and more. We should keep in mind that communication 
among mutually trusted users is the exception rather than the rule. Welcome to the 
world of modern computer networking!
1.7	 History of Computer Networking and  
the Internet
Sections 1.1 through 1.6 presented an overview of the technology of computer net-
working and the Internet. You should know enough now to impress your family and 
friends! However, if you really want to be a big hit at the next cocktail party, you 
should sprinkle your discourse with tidbits about the fascinating history of the Inter-
net [Segaller 1998].
1.7.1	The Development of Packet Switching: 1961–1972
The field of computer networking and today’s Internet trace their beginnings back to 
the early 1960s, when the telephone network was the world’s dominant communica-
tion network. Recall from Section 1.3 that the telephone network uses circuit switch-
ing to transmit information from a sender to a receiver—an appropriate choice given 
that voice is transmitted at a constant rate between sender and receiver. Given the 
88         Chapter 1    •    Computer Networks and the Internet
increasing importance of computers in the early 1960s and the advent of timeshared 
computers, it was perhaps natural to consider how to hook computers together so that 
they could be shared among geographically distributed users. The traffic generated 
by such users was likely to be bursty—intervals of activity, such as the sending of a 
command to a remote computer, followed by periods of inactivity while waiting for 
a reply or while contemplating the received response.
Three research groups around the world, each unaware of the others’ work 
[Leiner 1998], began inventing packet switching as an efficient and robust alterna-
tive to circuit switching. The first published work on packet-switching techniques 
was that of Leonard Kleinrock [Kleinrock 1961; Kleinrock 1964], then a graduate 
student at MIT. Using queuing theory, Kleinrock’s work elegantly demonstrated the 
effectiveness of the packet-switching approach for bursty traffic sources. In 1964, 
Paul Baran [Baran 1964] at the Rand Institute had begun investigating the use of 
packet switching for secure voice over military networks, and at the National Physi-
cal Laboratory in England, Donald Davies and Roger Scantlebury were also devel-
oping their ideas on packet switching.
The work at MIT, Rand, and the NPL laid the foundations for today’s Internet. 
But the Internet also has a long history of a let’s-build-it-and-demonstrate-it attitude 
that also dates back to the 1960s. J. C. R. Licklider [DEC 1990] and Lawrence Rob-
erts, both colleagues of Kleinrock’s at MIT, went on to lead the computer science 
program at the Advanced Research Projects Agency (ARPA) in the United States. 
Roberts published an overall plan for the ARPAnet [Roberts 1967], the first packet-
switched computer network and a direct ancestor of today’s public Internet. On 
Labor Day in 1969, the first packet switch was installed at UCLA under Kleinrock’s 
supervision, and three additional packet switches were installed shortly thereafter at 
the Stanford Research Institute (SRI), UC Santa Barbara, and the University of Utah 
(Figure 1.26). The fledgling precursor to the Internet was four nodes large by the end 
of 1969. Kleinrock recalls the very first use of the network to perform a remote login 
from UCLA to SRI, crashing the system [Kleinrock 2004].
By 1972, ARPAnet had grown to approximately 15 nodes and was given its 
first public demonstration by Robert Kahn. The first host-to-host protocol between 
ARPAnet end systems, known as the network-control protocol (NCP), was com-
pleted [RFC 001]. With an end-to-end protocol available, applications could now be 
written. Ray Tomlinson wrote the first e-mail program in 1972.
1.7.2	 Proprietary Networks and Internetworking:  
1972–1980
The initial ARPAnet was a single, closed network. In order to communicate with an 
ARPAnet host, one had to be actually attached to another ARPAnet IMP. In the early 
to mid-1970s, additional stand-alone packet-switching networks besides ARPAnet 
came into being: ALOHANet, a microwave network linking universities on the 
Hawaiian islands [Abramson 1970], as well as DARPA’s packet-satellite [RFC 829] 
 
1.7    •    History of Computer Networking and the Internet          89
and packet-radio networks [Kahn 1978]; Telenet, a BBN commercial packet-­
switching 
network based on ARPAnet technology; Cyclades, a French packet-switching net-
work pioneered by Louis Pouzin [Think 2012]; Time-sharing networks such as 
Tymnet and the GE Information Services network, among others, in the late 1960s 
and early 1970s [Schwartz 1977]; IBM’s SNA (1969–1974), which paralleled the 
ARPAnet work [Schwartz 1977].
The number of networks was growing. With perfect hindsight we can see that the 
time was ripe for developing an encompassing architecture for connecting networks 
together. Pioneering work on interconnecting networks (under the sponsorship of 
the Defense Advanced Research Projects Agency (DARPA)), in essence creating 
Figure 1.26  ♦  An early packet switch
90         Chapter 1    •    Computer Networks and the Internet
a network of networks, was done by Vinton Cerf and Robert Kahn [Cerf 1974]; the 
term internetting was coined to describe this work.
These architectural principles were embodied in TCP. The early versions of 
TCP, however, were quite different from today’s TCP. The early versions of TCP 
combined a reliable in-sequence delivery of data via end-system retransmission (still 
part of today’s TCP) with forwarding functions (which today are performed by IP). 
Early experimentation with TCP, combined with the recognition of the importance 
of an unreliable, non-flow-controlled, end-to-end transport service for applications 
such as packetized voice, led to the separation of IP out of TCP and the development 
of the UDP protocol. The three key Internet protocols that we see today—TCP, UDP, 
and IP—were conceptually in place by the end of the 1970s.
In addition to the DARPA Internet-related research, many other important net-
working activities were underway. In Hawaii, Norman Abramson was developing 
 
ALOHAnet, a packet-based radio network that allowed multiple remote sites 
on the Hawaiian Islands to communicate with each other. The ALOHA protocol 
 
[Abramson 1970] was the first multiple-access protocol, allowing geographically 
 
distributed users to share a single broadcast communication medium (a radio 
­
frequency). Metcalfe and Boggs built on Abramson’s multiple-access protocol work 
when they developed the Ethernet protocol [Metcalfe 1976] for wire-based shared 
broadcast networks. Interestingly, Metcalfe and Boggs’ Ethernet protocol was moti-
vated by the need to connect multiple PCs, printers, and shared disks [Perkins 1994]. 
Twenty-five years ago, well before the PC revolution and the explosion of networks, 
Metcalfe and Boggs were laying the foundation for today’s PC LANs.
1.7.3	A Proliferation of Networks: 1980–1990
By the end of the 1970s, approximately two hundred hosts were connected to the 
ARPAnet. By the end of the 1980s the number of hosts connected to the public 
­
Internet, a confederation of networks looking much like today’s Internet, would 
reach a hundred thousand. The 1980s would be a time of tremendous growth.
Much of that growth resulted from several distinct efforts to create computer 
networks linking universities together. BITNET provided e-mail and file transfers 
among several universities in the Northeast. CSNET (computer science network) 
was formed to link university researchers who did not have access to ARPAnet. In 
1986, NSFNET was created to provide access to NSF-sponsored supercomputing 
centers. Starting with an initial backbone speed of 56 kbps, NSFNET’s backbone 
would be running at 1.5 Mbps by the end of the decade and would serve as a primary 
backbone linking regional networks.
In the ARPAnet community, many of the final pieces of today’s Internet archi-
tecture were falling into place. January 1, 1983 saw the official deployment of TCP/
IP as the new standard host protocol for ARPAnet (replacing the NCP protocol). 
The transition [RFC 801] from NCP to TCP/IP was a flag day event—all hosts 
were required to transfer over to TCP/IP as of that day. In the late 1980s, important 
1.7    •    History of Computer Networking and the Internet          91
extensions were made to TCP to implement host-based congestion control [Jacobson 
1988]. The DNS, used to map between a human-readable Internet name (for exam-
ple, gaia.cs.umass.edu) and its 32-bit IP address, was also developed [RFC 1034].
Paralleling this development of the ARPAnet (which was for the most part a 
US effort), in the early 1980s the French launched the Minitel project, an ambitious 
plan to bring data networking into everyone’s home. Sponsored by the French gov-
ernment, the Minitel system consisted of a public packet-switched network (based 
on the X.25 protocol suite), Minitel servers, and inexpensive terminals with built-in 
low-speed modems. The Minitel became a huge success in 1984 when the French 
government gave away a free Minitel terminal to each French household that wanted 
one. Minitel sites included free sites—such as a telephone directory site—as well as 
private sites, which collected a usage-based fee from each user. At its peak in the mid 
1990s, it offered more than 20,000 services, ranging from home banking to special-
ized research databases. The Minitel was in a large proportion of French homes 10 
years before most Americans had ever heard of the Internet.
1.7.4	The Internet Explosion: The 1990s
The 1990s were ushered in with a number of events that symbolized the continued 
evolution and the soon-to-arrive commercialization of the Internet. ARPAnet, the 
progenitor of the Internet, ceased to exist. In 1991, NSFNET lifted its restrictions on 
the use of NSFNET for commercial purposes. NSFNET itself would be decommis-
sioned in 1995, with Internet backbone traffic being carried by commercial Internet 
Service Providers.
The main event of the 1990s was to be the emergence of the World Wide Web 
application, which brought the Internet into the homes and businesses of millions 
of people worldwide. The Web served as a platform for enabling and deploying 
hundreds of new applications that we take for granted today, including search (e.g., 
Google and Bing) Internet commerce (e.g., Amazon and eBay) and social networks 
(e.g., Facebook).
The Web was invented at CERN by Tim Berners-Lee between 1989 and 1991 
[Berners-Lee 1989], based on ideas originating in earlier work on hypertext from the 
1940s by Vannevar Bush [Bush 1945] and since the 1960s by Ted Nelson [Xanadu 
2012]. Berners-Lee and his associates developed initial versions of HTML, HTTP, 
a Web server, and a browser—the four key components of the Web. Around the end 
of 1993 there were about two hundred Web servers in operation, this collection of 
servers being just a harbinger of what was about to come. At about this time sev-
eral researchers were developing Web browsers with GUI interfaces, including Marc 
Andreessen, who along with Jim Clark, formed Mosaic Communications, which 
later became Netscape Communications Corporation [Cusumano 1998; Quittner 
1998]. By 1995, university students were using Netscape browsers to surf the Web 
on a daily basis. At about this time companies—big and small—began to operate 
Web servers and transact commerce over the Web. In 1996, Microsoft started to 
92         Chapter 1    •    Computer Networks and the Internet
make browsers, which started the browser war between Netscape and Microsoft, 
which Microsoft won a few years later [Cusumano 1998].
The second half of the 1990s was a period of tremendous growth and innovation 
for the Internet, with major corporations and thousands of startups creating Internet 
products and services. By the end of the millennium the Internet was supporting 
hundreds of popular applications, including four killer applications:
•	 E-mail, including attachments and Web-accessible e-mail
•	 The Web, including Web browsing and Internet commerce
•	 Instant messaging, with contact lists
•	 Peer-to-peer file sharing of MP3s, pioneered by Napster
Interestingly, the first two killer applications came from the research community, 
whereas the last two were created by a few young entrepreneurs.
The period from 1995 to 2001 was a roller-coaster ride for the Internet in the 
financial markets. Before they were even profitable, hundreds of Internet startups 
made initial public offerings and started to be traded in a stock market. Many com-
panies were valued in the billions of dollars without having any significant revenue 
streams. The Internet stocks collapsed in 2000–2001, and many startups shut down. 
Nevertheless, a number of companies emerged as big winners in the Internet space, 
including Microsoft, Cisco, Yahoo, e-Bay, Google, and Amazon.
1.7.5	The New Millennium
Innovation in computer networking continues at a rapid pace. Advances are being 
made on all fronts, including deployments of faster routers and higher transmission 
speeds in both access networks and in network backbones. But the following devel-
opments merit special attention:
•	 Since the beginning of the millennium, we have been seeing aggressive deploy-
ment of broadband Internet access to homes—not only cable modems and DSL 
but also fiber to the home, as discussed in Section 1.2. This high-speed Internet 
access has set the stage for a wealth of video applications, including the distribu-
tion of user-generated video (for example, YouTube), on-demand streaming of 
movies and television shows (e.g., Netflix), and multi-person video conference 
(e.g., Skype, Facetime, and Google Hangouts).
•	 The increasing ubiquity of high-speed (54 Mbps and higher) public WiFi net-
works and medium-speed (tens of Mbps) Internet access via 4G cellular teleph-
ony networks is not only making it possible to remain constantly connected while 
on the move, but also enabling new location-specific applications such as Yelp, 
Tinder, Yik Yak, and Waz. The number of wireless devices connecting to the 
Internet surpassed the number of wired devices in 2011. This high-speed wireless 
1.8    •    Summary         93
access has set the stage for the rapid emergence of hand-held computers (iPhones, 
Androids, iPads, and so on), which enjoy constant and untethered access to the 
Internet.
•	 Online social networks—such as Facebook, Instagram, Twitter, and WeChat 
(hugely popular in China)—have created massive people networks on top of the 
Internet. Many of these social networks are extensively used for messaging as 
well as photo sharing. Many Internet users today “live” primarily within one or 
more social networks. Through their APIs, the online social networks create plat-
forms for new networked applications and distributed games.
•	 As discussed in Section 1.3.3, online service providers, such as Google and 
Microsoft, have deployed their own extensive private networks, which not only 
connect together their globally distributed data centers, but are used to bypass the 
Internet as much as possible by peering directly with lower-tier ISPs. As a result, 
Google provides search results and e-mail access almost instantaneously, as if 
their data centers were running within one’s own computer.
•	 Many Internet commerce companies are now running their applications in the 
“cloud”—such as in Amazon’s EC2, in Google’s Application Engine, or in 
Microsoft’s Azure. Many companies and universities have also migrated their 
Internet applications (e.g., e-mail and Web hosting) to the cloud. Cloud compa-
nies not only provide applications scalable computing and storage environments, 
but also provide the applications implicit access to their high-performance private 
networks.
1.8	 Summary
In this chapter we’ve covered a tremendous amount of material! We’ve looked at 
the various pieces of hardware and software that make up the Internet in particular 
and computer networks in general. We started at the edge of the network, looking at 
end systems and applications, and at the transport service provided to the applica-
tions running on the end systems. We also looked at the link-layer technologies and 
physical media typically found in the access network. We then dove deeper inside 
the network, into the network core, identifying packet switching and circuit switch-
ing as the two basic approaches for transporting data through a telecommunication 
network, and we examined the strengths and weaknesses of each approach. We also 
examined the structure of the global Internet, learning that the Internet is a network 
of networks. We saw that the Internet’s hierarchical structure, consisting of higher- 
and lower-tier ISPs, has allowed it to scale to include thousands of networks.
In the second part of this introductory chapter, we examined several topics cen-
tral to the field of computer networking. We first examined the causes of delay, 
throughput and packet loss in a packet-switched network. We developed simple 
94         Chapter 1    •    Computer Networks and the Internet
quantitative models for transmission, propagation, and queuing delays as well as 
for throughput; we’ll make extensive use of these delay models in the homework 
problems throughout this book. Next we examined protocol layering and service 
models, key architectural principles in networking that we will also refer back to 
throughout this book. We also surveyed some of the more prevalent security attacks 
in the Internet day. We finished our introduction to networking with a brief history 
of computer networking. The first chapter in itself constitutes a mini-course in com-
puter networking.
So, we have indeed covered a tremendous amount of ground in this first chapter! 
If you’re a bit overwhelmed, don’t worry. In the following chapters we’ll revisit all 
of these ideas, covering them in much more detail (that’s a promise, not a threat!). 
At this point, we hope you leave this chapter with a still-developing intuition for the 
pieces that make up a network, a still-developing command of the vocabulary of 
networking (don’t be shy about referring back to this chapter), and an ever-growing 
desire to learn more about networking. That’s the task ahead of us for the rest of this 
book.
Road-Mapping This Book
Before starting any trip, you should always glance at a road map in order to become 
familiar with the major roads and junctures that lie ahead. For the trip we are about 
to embark on, the ultimate destination is a deep understanding of the how, what, and 
why of computer networks. Our road map is the sequence of chapters of this book:
	1.	 Computer Networks and the Internet
	2.	 Application Layer
	3.	 Transport Layer
	4.	 Network Layer: Data Plane
	5.	 Network Layer: Control Plane
	6.	 The Link Layer and LANs
	7.	 Wireless and Mobile Networks
	8.	 Security in Computer Networks
	9.	 Multimedia Networking
Chapters 2 through 6 are the five core chapters of this book. You should notice 
that these chapters are organized around the top four layers of the five-layer Internet 
protocol. Further note that our journey will begin at the top of the Internet protocol 
stack, namely, the application layer, and will work its way downward. The rationale 
behind this top-down journey is that once we understand the applications, we can 
understand the network services needed to support these applications. We can then, 
in turn, examine the various ways in which such services might be implemented by 
a network architecture. Covering applications early thus provides motivation for the 
remainder of the text.
Homework Problems and Questions         95
The second half of the book—Chapters 7 through 9—zooms in on three 
enormously important (and somewhat independent) topics in modern computer 
networking. In Chapter 7, we examine wireless and mobile networks, includ-
ing wireless LANs (including WiFi and Bluetooth), Cellular telephony networks 
(including GSM, 3G, and 4G), and mobility (in both IP and GSM networks). 
 
Chapter 8, which addresses security in computer networks, first looks at the under-
pinnings of encryption and network security, and then we examine how the basic 
theory is being applied in a broad range of Internet contexts. The last chapter, which 
addresses multimedia networking, examines audio and video applications such as 
Internet phone, video conferencing, and streaming of stored media. We also look 
at how a packet-switched network can be designed to provide consistent quality of 
service to audio and video applications.
Homework Problems and Questions
Chapter 1 Review Questions
SECTION 1.1
	R1.	 What is the difference between a host and an end system? List several differ-
ent types of end systems. Is a Web server an end system?
	R2.	 Describe the protocol that might be used by two people having a telephonic 
conversation to initiate and end the conversation.
	R3.	 Why are standards important for protocols?
SECTION 1.2
	R4.	 List six access technologies. Classify each one as home access, enterprise 
access, or wide-area wireless access.
	R5.	 Is HFC transmission rate dedicated or shared among users? Are collisions 
possible in a downstream HFC channel? Why or why not?
	R6.	 What access network technologies would be most suitable for providing 
internet access in rural areas?
	R7.	 Dial-up modems and DSL both use the telephone line (a twisted-pair copper cable) 
as their transmission medium. Why then is DSL much faster than dial-up access?
	R8.	 What are some of the physical media that Ethernet can run over?
	R9.	 Dial-up modems, HFC, DSL and FTTH are all used for residential access. 
For each of these access technologies, provide a range of ­
transmission rates 
and comment on whether the transmission rate is shared or dedicated.
	
R10.	 Describe the different wireless technologies you use during the day and their 
characteristics. If you have a choice between multiple technologies, why do 
you prefer one over another?
96         Chapter 1    •    Computer Networks and the Internet
SECTION 1.3
	
R11.	 Suppose there is exactly one packet switch between a sending host and a 
receiving host. The transmission rates between the sending host and the 
switch and between the switch and the receiving host are R1 and R2, respec-
tively. Assuming that the switch uses store-and-forward packet switching, 
what is the total end-to-end delay to send a packet of length L? (Ignore queu-
ing, propagation delay, and processing delay.)
	
R12.	 What advantage does a circuit-switched network have over a packet-switched 
network? What advantages does TDM have over FDM in a circuit-switched 
network?
	
R13.	 Suppose users share a 2 Mbps link. Also suppose each user transmits contin-
uously at 1 Mbps when transmitting, but each user transmits only 20 percent 
of the time. (See the discussion of statistical multiplexing in Section 1.3.)
a.	 When circuit switching is used, how many users can be supported?
b.	 For the remainder of this problem, suppose packet switching is used. Why 
will there be essentially no queuing delay before the link if two or fewer 
users transmit at the same time? Why will there be a queuing delay if 
three users transmit at the same time?
c.	 Find the probability that a given user is transmitting.
d.	 Suppose now there are three users. Find the probability that at any given 
time, all three users are transmitting simultaneously. Find the fraction of 
time during which the queue grows.
	
R14.	 Why will two ISPs at the same level of the hierarchy often peer with each 
other? How does an IXP earn money?
	
R15.	 Why is a content provider considered a different Internet entity today? How 
does a content provider connect to other ISPs? Why?
SECTION 1.4
	
R16.	 Consider sending a packet from a source host to a destination host over a 
fixed route. List the delay components in the end-to-end delay. Which of 
these delays are constant and which are variable?
	
R17.	 Visit the Transmission Versus Propagation Delay applet at the companion 
Web site. Among the rates, propagation delay, and packet sizes available, find 
a combination for which the sender finishes transmitting before the first bit of 
the packet reaches the receiver. Find another combination for which the first 
bit of the packet reaches the receiver before the sender finishes transmitting.
	
R18.	 A user can directly connect to a server through either long-range wireless 
or a twisted-pair cable for transmitting a 1500-bytes file. The transmission 
rates of the wireless and wired media are 2 and 100 Mbps, respectively. 
Assume that the propagation speed in air is 3 3 108 m/s, while the speed in 
Homework Problems and Questions         97
the twisted pair is 2 3 108 m/s. If the user is located 1 km away from the 
server, what is the nodal delay when using each of the two technologies?
	
R19.	 Suppose Host A wants to send a large file to Host B. The path from Host A to Host 
B has three links, of rates R1 = 500 kbps, R2 = 2 Mbps, and R3 = 1 Mbps.
a.	 Assuming no other traffic in the network, what is the throughput for the 
file transfer?
b.	 Suppose the file is 4 million bytes. Dividing the file size by the through-
put, roughly how long will it take to transfer the file to Host B?
c.	 Repeat (a) and (b), but now with R2 reduced to 100 kbps.
	
R20.	 Suppose end system A wants to send a large file to end system B. At a very 
high level, describe how end system A creates packets from the file. When 
one of these packets arrives to a router, what information in the packet does 
the router use to determine the link onto which the packet is forwarded? 
Why is packet switching in the Internet analogous to driving from one city to 
another and asking directions along the way?
	
R21.	 Visit the Queuing and Loss applet at the companion Web site. What is the 
maximum emission rate and the minimum transmission rate? With those 
rates, what is the traffic intensity? Run the applet with these rates and deter-
mine how long it takes for packet loss to occur. Then repeat the experiment 
a second time and determine again how long it takes for packet loss to occur. 
Are the values different? Why or why not?
SECTION 1.5
	
R22.	 If two end-systems are connected through multiple routers and the data-link 
level between them ensures reliable data delivery, is a transport protocol offer-
ing reliable data delivery between these two end-systems necessary? Why?
	
R23.	 What are the five layers in the Internet protocol stack? What are the principal 
responsibilities of each of these layers?
	
R24.	 What do encapsulation and de-encapsulation mean? Why are they needed in 
a layered protocol stack?
	
R25.	 Which layers in the Internet protocol stack does a router process? Which lay-
ers does a link-layer switch process? Which layers does a host process?
SECTION 1.6
	
R26.	 You are in a university classroom and you want to spy on what websites your 
classmates are visiting with their laptops during the course lecture. If they all con-
nect to the Internet through the university’s WiFi network, what could you do?
	
R27.	 Describe how a botnet can be created and how it can be used for a DDoS attack.
	
R28.	 Suppose Alice and Bob are sending packets to each other over a computer 
network. Suppose Trudy positions herself in the network so that she can 
capture all the packets sent by Alice and send whatever she wants to Bob; she 
can also capture all the packets sent by Bob and send whatever she wants to 
Alice. List some of the malicious things Trudy can do from this position.
98         Chapter 1    •    Computer Networks and the Internet
Problems
	 P1.	 Design and describe an application-level protocol to be used between an 
automatic teller machine and a bank’s centralized computer. Your protocol 
should allow a user’s card and password to be verified, the account bal-
ance (which is maintained at the centralized computer) to be queried, and an 
account withdrawal to be made (that is, money disbursed to the user). Your 
protocol entities should be able to handle the all-too-common case in which 
there is not enough money in the account to cover the withdrawal. Specify 
your protocol by listing the messages exchanged and the action taken by the 
automatic teller machine or the bank’s centralized computer on transmission 
and receipt of messages. Sketch the operation of your protocol for the case of 
a simple withdrawal with no errors, using a diagram similar to that in Figure 1.2.  
Explicitly state the assumptions made by your protocol about the underlying 
end-to-end transport service.
	 P2.	 Equation 1.1 gives a formula for the end-to-end delay of sending one packet 
of length L over N links of transmission rate R. Generalize this formula for 
sending P such packets back-to-back over the N links.
	 P3.	 Consider an application that transmits data at a steady rate (for example, the 
sender generates an N-bit unit of data every k time units, where k is small 
and fixed). Also, when such an application starts, it will continue running 
for a relatively long period of time. Answer the following questions, briefly 
justifying your answer:
a.	 Would a packet-switched network or a circuit-switched network be more 
appropriate for this application? Why?
b.	 Suppose that a packet-switched network is used and the only traffic in 
this network comes from such applications as described above. Further-
more, assume that the sum of the application data rates is less than the 
capacities of each and every link. Is some form of congestion control 
needed? Why?
	 P4.	 Consider the circuit-switched network in Figure 1.13. Recall that there are 
4 circuits on each link. Label the four switches A, B, C, and D, going in the 
clockwise direction.
a.	 What is the maximum number of simultaneous connections that can be in 
progress at any one time in this network?
b.	 Suppose that all connections are between switches A and C. What is the 
maximum number of simultaneous connections that can be in progress?
c.	 Suppose we want to make four connections between switches A and C, 
and another four connections between switches B and D. Can we  
route these calls through the four links to accommodate all eight 
­
connections?
Problems         99
	 P5.	 Review the car-caravan analogy in Section 1.4. Assume a propagation speed 
of 100 km/hour.
a.	 Suppose the caravan travels 150 km, beginning in front of one tollbooth, 
passing through a second tollbooth, and finishing just after a third toll-
booth. What is the end-to-end delay?
b.	 Repeat (a), now assuming that there are eight cars in the caravan instead 
of ten.
	 P6.	 This elementary problem begins to explore propagation delay and transmis-
sion delay, two central concepts in data networking. Consider two hosts, A 
and B, connected by a single link of rate R bps. Suppose that the two hosts 
are separated by m meters, and suppose the propagation speed along the link 
is s meters/sec. Host A is to send a packet of size L bits to Host B.
a.	 Express the propagation delay,  dprop, in terms of m and s.
b.	 Determine the transmission time of the packet,  dtrans, in terms of L and R.
c.	 Ignoring processing and queuing delays, obtain an expression for the end-
to-end delay.
d.	 Suppose Host A begins to transmit the packet at time t = 0. At time t = 
 dtrans, where is the last bit of the packet?
e.	 Suppose  dprop is greater than  dtrans. At time  t = dtrans, where is the first 
bit of the packet?
f.	 Suppose  dprop is less than  dtrans. At time t = dtrans, where is the first bit of 
the packet?
g.	 Suppose s = 2.5 # 108, L = 120 bits, and R = 56 kbps. Find the distance 
m so that  dprop equals  dtrans.
	 P7.	 In this problem, we consider sending real-time voice from Host A to Host B 
over a packet-switched network (VoIP). Host A converts analog voice to a 
digital 64 kbps bit stream on the fly. Host A then groups the bits into 56-byte 
packets. There is one link between Hosts A and B; its transmission rate is 
2 Mbps and its propagation delay is 10 msec. As soon as Host A gathers a 
packet, it sends it to Host B. As soon as Host B receives an entire packet, it 
converts the packet’s bits to an analog signal. How much time elapses from 
the time a bit is created (from the original analog signal at Host A) until the 
bit is decoded (as part of the analog signal at Host B)?
	 P8.	 Suppose users share a 3 Mbps link. Also suppose each user requires 150 kbps 
when transmitting, but each user transmits only 10 percent of the time. (See 
the discussion of packet switching versus circuit switching in Section 1.3.)
a.	 When circuit switching is used, how many users can be supported?
b.	 For the remainder of this problem, suppose packet switching is used. Find 
the probability that a given user is transmitting.
VideoNote
Exploring propagation 
delay and transmission 
delay
100         Chapter 1    •    Computer Networks and the Internet
c.	 Suppose there are 120 users. Find the probability that at any given time, 
exactly n users are transmitting simultaneously. (Hint: Use the binomial 
distribution.)
d.	 Find the probability that there are 21 or more users transmitting 
­
simultaneously.
	 P9.	 Consider the discussion in Section 1.3 of packet switching versus circuit switch-
ing in which an example is provided with a 1 Mbps link. Users are generating 
data at a rate of 100 kbps when busy, but are busy generating data only with 
probability p = 0.1. Suppose that the 1 Mbps link is replaced by a 1 Gbps link.
a.	 What is N, the maximum number of users that can be supported simulta-
neously under circuit switching?
b.	 Now consider packet switching and a user population of M users. Give a 
formula (in terms of p, M, N) for the probability that more than N users 
are sending data.
	
P10.	 Consider the network illustrated in Figure 1.16. Assume the two hosts on the 
left of the figure start transmitting packets of 1500 bytes at the same time 
towards Router B. Suppose the link rates between the hosts and Router  
A is 4-Mbps. One link has a 6-ms propagation delay and the other has a 2-ms 
propagation delay. Will queuing delay occur at Router A?
	
P11.	 Consider the scenario in Problem P10 again, but now assume the links 
between the hosts and Router A have different rates R1 and R2 byte/s in 
addition to different propagation delays d1 and d2. Assume the packet lengths 
for the two hosts are of L bytes. For what values of the propagation delay will 
no queuing delay occur at Router A?
	
P12.	 Consider a client and a server connected through one router. Assume the router 
can start transmitting an incoming packet after receiving its first h bytes instead 
of the whole packet. Suppose that the link rates are R byte/s and that the client 
transmits one packet with a size of L bytes to the server. What is the end-to-end 
delay? Assume the propagation, processing, and queuing delays are negligible. 
Generalize the previous result to a scenario where the client and the server are 
interconnected by N routers.
	
P13.	 (a) 
Suppose N packets arrive simultaneously to a link at which no packets 
are currently being transmitted or queued. Each packet is of length L and 
the link has transmission rate R. What is the average queuing delay for 
the N packets?
Problems         101
(b) Now suppose that N such packets arrive to the link every LN/R seconds. 
What is the average queuing delay of a packet?
	
P14.	 Consider the queuing delay in a router buffer. Let I denote traffic intensity; 
that is, I = La/R. Suppose that the queuing delay takes the form IL/R (1 - I) 
for I 6 1.
a.	 Provide a formula for the total delay, that is, the queuing delay plus the 
transmission delay.
b.	 Plot the total delay as a function of L 
/R.
	
P15.	 Let a denote the rate of packets arriving at a link in packets/sec, and let µ 
denote the link’s transmission rate in packets/sec. Based on the formula for 
the total delay (i.e., the queuing delay plus the transmission delay) derived 
in the previous problem, derive a formula for the total delay in terms of a 
and µ.
	
P16.	 Consider a router buffer preceding an outbound link. In this problem, you 
will use Little’s formula, a famous formula from queuing theory. Let N 
denote the average number of packets in the buffer plus the packet being 
transmitted. Let a denote the rate of packets arriving at the link. Let d denote 
the average total delay (i.e., the queuing delay plus the transmission delay) 
experienced by a packet. Little’s formula is N = a · d. Suppose that on aver-
age, the buffer contains 10 packets, and the average packet queuing delay 
is 10 msec. The link’s transmission rate is 100 packets/sec. Using Little’s 
formula, what is the average packet arrival rate, assuming there is no packet 
loss?
	
P17.	 Consider the network illustrated in Figure 1.12. Would Equation 1.2 hold in 
such a scenario? If so, under which conditions? If not, why? (Assume N is the 
number of links between a source and a destination in the figure.)
	
P18.	 Perform a Traceroute between source and destination on the same continent 
at three different hours of the day.
a.	 Find the average and standard deviation of the round-trip delays at each of 
the three hours.
b.	 Find the number of routers in the path at each of the three hours. Did the 
paths change during any of the hours?
c.	 Try to identify the number of ISP networks that the Traceroute packets 
pass through from source to destination. Routers with similar names and/
or similar IP addresses should be considered as part of the same ISP. In 
your experiments, do the largest delays occur at the peering interfaces 
between adjacent ISPs?
d.	 Repeat the above for a source and destination on different continents. 
Compare the intra-continent and inter-continent results.
VideoNote
Using Traceroute to 
discover network  
paths and measure 
network delay
102         Chapter 1    •    Computer Networks and the Internet
	
P19.	 (a) 
Visit the site www.traceroute.org and perform traceroutes from two dif-
ferent cities in France to the same destination host in the United States. 
How many links are the same in the two traceroutes? Is the transatlantic 
link the same?
(b) 
Repeat (a) but this time choose one city in France and another city in 
Germany.
(c) 
Pick a city in the United States, and perform traceroutes to two hosts, 
each in a different city in China. How many links are common in 
the two traceroutes? Do the two traceroutes diverge before reaching 
China?
	
P20.	 Consider the throughput example corresponding to Figure 1.20(b). Now 
suppose that there are M client-server pairs rather than 10. Denote Rs, Rc, 
and R for the rates of the server links, client links, and network link. Assume 
all other links have abundant capacity and that there is no other traffic in the 
network besides the traffic generated by the M client-server pairs. Derive a 
general expression for throughput in terms of Rs, Rc, R, and M.
	
P21.	 Assume a client and a server can connect through either network (a) or (b) in 
Figure 1.19. Assume that Ri 5 (Rc 1 Rs) / i, for i 5 1, 2, ..., N. In what case 
will network (a) have a higher throughput than network (b)?
	
P22.	 Consider Figure 1.19(b). Suppose that each link between the server and the 
client has a packet loss probability p, and the packet loss probabilities for 
these links are independent. What is the probability that a packet (sent by the 
server) is successfully received by the receiver? If a packet is lost in the path 
from the server to the client, then the server will re-transmit the packet. On 
average, how many times will the server re-transmit the packet in order for 
the client to successfully receive the packet?
	
P23.	 Consider Figure 1.19(a). Assume that we know the bottleneck link along the 
path from the server to the client is the first link with rate Rs bits/sec. Suppose 
we send a pair of packets back to back from the server to the client, and there 
is no other traffic on this path. Assume each packet of size L bits, and both 
links have the same propagation delay  dprop.
a.	 What is the packet inter-arrival time at the destination? That is, how much 
time elapses from when the last bit of the first packet arrives until the last 
bit of the second packet arrives?
b.	 Now assume that the second link is the bottleneck link (i.e., Rc 6 Rs). Is 
it possible that the second packet queues at the input queue of the second 
link? Explain. Now suppose that the server sends the second packet T 
seconds after sending the first packet. How large must T be to ensure no 
queuing before the second link? Explain.
Problems         103
	
P24.	 Consider a user who needs to transmit 1.5 gigabytes of data to a server. The 
user lives in a small town where only dial-up access is available. A bus visits 
the small town once a day from the closest city, located 150 km away, and stops 
in front of the user’s house. The bus has a 100-Mbps WiFi connection. It can 
collect data from users in rural areas and transfer them to the Internet through a 
1 Gbps link once it gets back to the city. Suppose the average speed of the bus is 
60 km/h. What is the fastest way the user can transfer the data to the server?
	
P25.	 Suppose two hosts, A and B, are separated by 20,000 kilometers and are con-
nected by a direct link of R = 2 Mbps. Suppose the propagation speed over 
the link is 2.5 # 108 meters/sec.
a.	 Calculate the bandwidth-delay product, R #  dprop.
b.	 Consider sending a file of 800,000 bits from Host A to Host B. Suppose 
the file is sent continuously as one large message. What is the maximum 
number of bits that will be in the link at any given time?
c.	 Provide an interpretation of the bandwidth-delay product.
d.	 What is the width (in meters) of a bit in the link? Is it longer than a 
­
football field?
e.	 Derive a general expression for the width of a bit in terms of the propaga-
tion speed s, the transmission rate R, and the length of the link m.
	
P26.	 Consider problem P25 but now with a link of R 5 1 Gbps. 
a.	 Calculate the bandwidth-delay product, R ? dprop.
b.	 Consider sending a file of 800,000 bits from Host A to Host B. Suppose 
the file is sent continuously as one big message. What is the maximum 
number of bits that will be in the link at any given time?
c.	 What is the width (in meters) of a bit in the link?
	
P27.	 Consider the scenario illustrated in Figure 1.19(a). Assume Rs is 20 Mbps, 
Rc is 10 Mbps, and the server is continuously sending traffic to the client. 
Also assume the router between the server and the client can buffer at most 
four messages. After how many messages sent by the server will packet loss 
starts occurring at the router?
	
P28.	 Generalize the result obtained in Problem P27 for the case where the router 
can buffer m messages.
	
P29.	 Suppose there is a 10-Mbps microwave link between a geostationary satellite 
and its base station on Earth. Every minute the satellite takes a digital photo and 
sends it to the base station. Assume a propagation speed of 2.4 # 108 meters/sec.
a.	 What is the propagation delay of the link?
b.	 What is the bandwidth-delay product, R #  dprop?
c.	 Let x denote the size of the photo. What is the minimum value of x for the 
microwave link to be continuously transmitting?
104         Chapter 1    •    Computer Networks and the Internet
	
P30.	 Consider the airline travel analogy in our discussion of layering in Section 1.5, 
and the addition of headers to protocol data units as they flow down the proto-
col stack. Is there an equivalent notion of header information that is added to 
passengers and baggage as they move down the airline protocol stack?
	
P31.	 In modern packet-switched networks, including the Internet, the source host 
segments long, application-layer messages (for example, an image or a music 
file) into smaller packets and sends the packets into the network. The receiver 
then reassembles the packets back into the original message. We refer to 
this process as message segmentation. Figure 1.27 illustrates the end-to-end 
transport of a message with and without message segmentation. Consider a 
message that is 8 # 106 bits long that is to be sent from source to destination 
in Figure 1.27. Suppose each link in the figure is 2 Mbps. Ignore propagation, 
queuing, and processing delays.
a.	 Consider sending the message from source to destination without message 
segmentation. How long does it take to move the message from the source 
host to the first packet switch? Keeping in mind that each switch uses 
store-and-forward packet switching, what is the total time to move the 
message from source host to destination host?
b.	 Now suppose that the message is segmented into 800 packets, with each 
packet being 10,000 bits long. How long does it take to move the first 
packet from source host to the first switch? When the first packet is being 
sent from the first switch to the second switch, the second packet is being 
sent from the source host to the first switch. At what time will the second 
packet be fully received at the first switch?
c.	 How long does it take to move the file from source host to destination 
host when message segmentation is used? Compare this result with your 
answer in part (a) and comment.
Figure 1.27  ♦  
End-to-end message transport: (a) without message 
­
segmentation; (b) with message segmentation
Source
a.
Packet switch
Packet switch
Destination
Message
Source
b.
Packet switch
Packet
Packet switch
Destination
Wireshark Lab         105
d.	 In addition to reducing delay, what are reasons to use message 
­
segmentation?
e.	 Discuss the drawbacks of message segmentation.
	
P32.	 Consider Problem P31 and assume that the propagation delay is 250 ms. Recal-
culate the total time needed to transfer the source data with and without segmen-
tation. Is segmentation more beneficial or less if there is propagation delay?
	
P33.	 Consider sending a large file of F bits from Host A to Host B. There are three 
links (and two switches) between A and B, and the links are uncongested 
(that is, no queuing delays). Host A segments the file into segments of S bits 
each and adds 80 bits of header to each segment, forming packets of L = 80 
+ S bits. Each link has a transmission rate of R bps. Find the value of S that 
minimizes the delay of moving the file from Host A to Host B. Disregard 
propagation delay.
	
P34.	 Early versions of TCP combined functions for both forwarding and reliable 
delivery. How are these TCP variants located in the ISO/OSI protocol stack? 
Why were forwarding functions later separated from TCP? What were the 
consequences?
Wireshark Lab
“Tell me and I forget. Show me and I remember. Involve me and I understand.”
Chinese proverb
One’s understanding of network protocols can often be greatly deepened by seeing 
them in action and by playing around with them—observing the sequence of mes-
sages exchanged between two protocol entities, delving into the details of protocol 
operation, causing protocols to perform certain actions, and observing these actions 
and their consequences. This can be done in simulated scenarios or in a real net-
work environment such as the Internet. The Java applets at the textbook Web site 
take the first approach. In the Wireshark labs, we’ll take the latter approach. You’ll 
run network applications in various scenarios using a computer on your desk, at 
home, or in a lab. You’ll observe the network protocols in your computer, interacting 
and exchanging messages with protocol entities executing elsewhere in the Inter-
net. Thus, you and your computer will be an integral part of these live labs. You’ll 
observe—and you’ll learn—by doing.
The basic tool for observing the messages exchanged between executing pro-
tocol entities is called a packet sniffer. As the name suggests, a packet sniffer pas-
sively copies (sniffs) messages being sent from and received by your computer; it 
also displays the contents of the various protocol fields of these captured messages. 
A screenshot of the Wireshark packet sniffer is shown in Figure 1.28. Wireshark 
is a free packet sniffer that runs on Windows, Linux/Unix, and Mac computers. 
106         Chapter 1    •    Computer Networks and the Internet
Throughout the textbook, you will find Wireshark labs that allow you to explore 
a number of the protocols studied in the chapter. In this first Wireshark lab, you’ll 
obtain and install a copy of Wireshark, access a Web site, and capture and examine 
the protocol messages being exchanged between your Web browser and the Web 
server.
You can find full details about this first Wireshark lab (including instructions about 
how to obtain and install Wireshark) at the Web site http://www.pearsonglobaleditions 
.com/kurose.
Figure 1.28  ♦  
A Wireshark screenshot (Wireshark screenshot reprinted  
by permission of the Wireshark Foundation.)
Command
menus
Listing of
captured
packets
Details of
selected
packet
header
Packet
contents in
hexadecimal
and ASCII
107
What made you decide to specialize in networking/Internet technology?
As a PhD student at MIT in 1959, I looked around and found that most of my classmates 
were doing research in the area of information theory and coding theory. At MIT, there was 
the great researcher, Claude Shannon, who had launched these fields and had solved most 
of the important problems already. The research problems that were left were hard and of 
lesser consequence. So I decided to launch out in a new area that no one else had yet con-
ceived of. Remember that at MIT I was surrounded by lots of computers, and it was clear to 
me that soon these machines would need to communicate with each other. At the time, there 
was no effective way for them to do so, so I decided to develop the technology that would 
permit efficient and reliable data networks to be created.
What was your first job in the computer industry? What did it entail?
I went to the evening session at CCNY from 1951 to 1957 for my bachelor’s degree 
in electrical engineering. During the day, I worked first as a technician and then as an 
engineer at a small, industrial electronics firm called Photobell. While there, I introduced 
digital technology to their product line. Essentially, we were using photoelectric devices 
to detect the presence of certain items (boxes, people, etc.) and the use of a circuit known 
then as a bistable multivibrator was just the kind of technology we needed to bring 
digital processing into this field of detection. These circuits happen to be the building 
blocks for computers, and have come to be known as flip-flops or switches in today’s 
vernacular.
What was going through your mind when you sent the first host-to-host message (from 
UCLA to the Stanford Research Institute)?
Frankly, we had no idea of the importance of that event. We had not prepared a special 
message of historic significance, as did so many inventors of the past (Samuel Morse with 
“What hath God wrought.” or Alexander Graham Bell with “Watson, come here! I want 
you.” or Neal Amstrong with “That’s one small step for a man, one giant leap for mankind.”) 
 
Those guys were smart! They understood media and public relations. All we wanted to do 
was to login to the SRI computer. So we typed the “L”, which was correctly received, we 
typed the “o” which was received, and then we typed the “g” which caused the SRI host 
Leonard Kleinrock
Leonard Kleinrock is a professor of computer science at the University 
of California, Los Angeles. In 1969, his computer at UCLA became 
the first node of the Internet. His creation of packet-switching prin-
ciples in 1961 became the technology behind the Internet. He 
received his B.E.E. from the City College of New York (CCNY) and 
his masters and PhD in electrical engineering from MIT.
AN INTERVIEW WITH…
108
computer to crash! So, it turned out that our message was the shortest and perhaps the most 
prophetic message ever, namely “Lo!” as in “Lo and behold!”
Earlier that year, I was quoted in a UCLA press release saying that once the network 
was up and running, it would be possible to gain access to computer utilities from our 
homes and offices as easily as we gain access to electricity and telephone connectivity. So 
my vision at that time was that the Internet would be ubiquitous, always on, always avail-
able, anyone with any device could connect from any location, and it would be invisible. 
However, I never anticipated that my 99-year-old mother would use the Internet—and 
indeed she did!
What is your vision for the future of networking?
The easy part of the vision is to predict the infrastructure itself. I anticipate that we see con-
siderable deployment of nomadic computing, mobile devices, and smart spaces. Indeed, the 
availability of lightweight, inexpensive, high-performance, portable computing, and com-
munication devices (plus the ubiquity of the Internet) has enabled us to become nomads. 
Nomadic computing refers to the technology that enables end users who travel from place to 
place to gain access to Internet services in a transparent fashion, no matter where they travel 
and no matter what device they carry or gain access to. The harder part of the vision is to 
predict the applications and services, which have consistently surprised us in dramatic ways 
(e-mail, search technologies, the World Wide Web, blogs, social networks, user generation, 
and sharing of music, photos, and videos, etc.). We are on the verge of a new class of sur-
prising and innovative mobile applications delivered to our hand-held devices.
The next step will enable us to move out from the netherworld of cyberspace to the 
physical world of smart spaces. Our environments (desks, walls, vehicles, watches, belts, 
and so on) will come alive with technology, through actuators, sensors, logic, processing, 
storage, cameras, microphones, speakers, displays, and communication. This embedded 
technology will allow our environment to provide the IP services we want. When I walk 
into a room, the room will know I entered. I will be able to communicate with my environ-
ment naturally, as in spoken English; my requests will generate replies that present Web 
pages to me from wall displays, through my eyeglasses, as speech, holograms, and so forth.
Looking a bit further out, I see a networking future that includes the following addi-
tional key components. I see intelligent software agents deployed across the network whose 
function it is to mine data, act on that data, observe trends, and carry out tasks dynamically 
and adaptively. I see considerably more network traffic generated not so much by humans, 
but by these embedded devices and these intelligent software agents. I see large collec-
tions of self-organizing systems controlling this vast, fast network. I see huge amounts of 
information flashing across this network instantaneously with this information undergoing 
enormous processing and filtering. The Internet will essentially be a pervasive global nerv-
ous system. I see all these things and more as we move headlong through the twenty-first 
century.
109
What people have inspired you professionally?
By far, it was Claude Shannon from MIT, a brilliant researcher who had the ability to relate 
his mathematical ideas to the physical world in highly intuitive ways. He was on my PhD 
thesis committee.
Do you have any advice for students entering the networking/Internet field?
The Internet and all that it enables is a vast new frontier, full of amazing challenges. There 
is room for great innovation. Don’t be constrained by today’s technology. Reach out and 
imagine what could be and then make it happen.
This page intentionally left blank
111
Network applications are the raisons d’être of a computer network—if we couldn’t 
 
conceive of any useful applications, there wouldn’t be any need for networking infra-
structure and protocols to support them. Since the Internet’s inception, numerous useful 
and entertaining applications have indeed been created. These applications have been the 
driving force behind the Internet’s success, motivating people in homes, schools, govern-
ments, and businesses to make the Internet an integral part of their daily activities.
Internet applications include the classic text-based applications that became pop-
ular in the 1970s and 1980s: text e-mail, remote access to computers, file transfers, and 
 
newsgroups. They include the killer application of the mid-1990s, the World Wide 
Web, encompassing Web surfing, search, and electronic commerce. They include 
instant messaging and P2P file sharing, the two killer applications introduced at the 
end of the millennium. In the new millennium, new and highly compelling applica-
tions continue to emerge, including voice over IP and video conferencing such as 
Skype, Facetime, and Google Hangouts; user generated video such as YouTube and 
movies on demand such as Netflix; multiplayer online games such as Second Life 
and World of Warcraft. During this same period, we have seen the emergence of a 
new generation of social networking applications—such as Facebook, Instagram, 
Twitter, and WeChat—which have created engaging human networks on top of the 
Internet’s network or routers and communication links. And most recently, along 
with the arrival of the smartphone, there has been a profusion of location based 
mobile apps, including popular check-in, dating, and road-traffic forecasting apps 
(such as Yelp, Tinder, Waz, and Yik Yak). Clearly, there has been no slowing down 
of new and exciting Internet applications. Perhaps some of the readers of this text 
will create the next generation of killer Internet applications!
2
CHAPTER
Application 
Layer
112         Chapter 2    •    Application Layer
In this chapter we study the conceptual and implementation aspects of network 
applications. We begin by defining key application-layer concepts, including net-
work services required by applications, clients and servers, processes, and transport-
layer interfaces. We examine several network applications in detail, including the Web, 
e-mail, DNS, peer-to-peer (P2P) file distribution, and video streaming. (Chapter 9 will 
further examine multimedia applications, including streaming video and VoIP.) We 
then cover network application development, over both TCP and UDP. In particular, 
we study the socket interface and walk through some simple client-server applica-
tions in Python. We also provide several fun and interesting socket programming 
assignments at the end of the chapter.
The application layer is a particularly good place to start our study of protocols. 
It’s familiar ground. We’re acquainted with many of the applications that rely on 
the protocols we’ll study. It will give us a good feel for what protocols are all about 
and will introduce us to many of the same issues that we’ll see again when we study 
transport, network, and link layer protocols.
2.1	 Principles of Network Applications
Suppose you have an idea for a new network application. Perhaps this application 
will be a great service to humanity, or will please your professor, or will bring you 
great wealth, or will simply be fun to develop. Whatever the motivation may be, let’s 
now examine how you transform the idea into a real-world network application.
At the core of network application development is writing programs that run on 
different end systems and communicate with each other over the network. For exam-
ple, in the Web application there are two distinct programs that communicate with 
each other: the browser program running in the user’s host (desktop, laptop, tablet, 
smartphone, and so on); and the Web server program running in the Web server host. 
As another example, in a P2P file-sharing system there is a program in each host that 
participates in the file-sharing community. In this case, the programs in the various 
hosts may be similar or identical.
Thus, when developing your new application, you need to write software that 
will run on multiple end systems. This software could be written, for example, in 
C, Java, or Python. Importantly, you do not need to write software that runs on net-
work-core devices, such as routers or link-layer switches. Even if you wanted to 
write application software for these network-core devices, you wouldn’t be able to 
do so. As we learned in Chapter 1, and as shown earlier in Figure 1.24, network-core 
devices do not function at the application layer but instead function at lower layers—
specifically at the network layer and below. This basic design—namely, confining 
application software to the end systems—as shown in Figure 2.1, has facilitated the 
rapid development and deployment of a vast array of network applications.
2.1    •    Principles of Network Applications         113
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Home Network
Transport
Network
Link
Physical
Application
Transport
Network
Link
Application
Physical
Transport
Network
Link
Physical
Application
Figure 2.1  ♦  
Communication for a network application takes place  
between end systems at the application layer
114         Chapter 2    •    Application Layer
2.1.1 Network Application Architectures
Before diving into software coding, you should have a broad architectural plan for 
your application. Keep in mind that an application’s architecture is distinctly differ-
ent from the network architecture (e.g., the five-layer Internet architecture discussed 
in Chapter 1). From the application developer’s perspective, the network architec-
ture is fixed and provides a specific set of services to applications. The application 
 
architecture, on the other hand, is designed by the application developer and dic-
tates how the application is structured over the various end systems. In choosing 
the application architecture, an application developer will likely draw on one of the 
two predominant architectural paradigms used in modern network applications: the 
client-server architecture or the peer-to-peer (P2P) architecture.
In a client-server architecture, there is an always-on host, called the server, 
which services requests from many other hosts, called clients. A classic example 
is the Web application for which an always-on Web server services requests from 
browsers running on client hosts. When a Web server receives a request for an object 
from a client host, it responds by sending the requested object to the client host. 
Note that with the client-server architecture, clients do not directly communicate 
with each other; for example, in the Web application, two browsers do not directly 
communicate. Another characteristic of the client-server architecture is that the 
server has a fixed, well-known address, called an IP address (which we’ll discuss 
soon). Because the server has a fixed, well-known address, and because the server is 
always on, a client can always contact the server by sending a packet to the server’s 
IP address. Some of the better-known applications with a client-server architecture 
include the Web, FTP, Telnet, and e-mail. The client-server architecture is shown in 
Figure 2.2(a).
Often in a client-server application, a single-server host is incapable of keep-
ing up with all the requests from clients. For example, a popular social-networking 
site can quickly become overwhelmed if it has only one server handling all of its 
requests. For this reason, a data center, housing a large number of hosts, is often 
used to create a powerful virtual server. The most popular Internet services—such as 
search engines (e.g., Google, Bing, Baidu), Internet commerce (e.g., Amazon, eBay, 
Alibaba), Web-based e-mail (e.g., Gmail and Yahoo Mail), social networking (e.g., 
Facebook, Instagram, Twitter, and WeChat)—employ one or more data centers. As 
discussed in Section 1.3.3, Google has 30 to 50 data centers distributed around the 
world, which collectively handle search, YouTube, Gmail, and other services. A 
data center can have hundreds of thousands of servers, which must be powered and 
maintained. Additionally, the service providers must pay recurring interconnection 
and bandwidth costs for sending data from their data centers.
In a P2P architecture, there is minimal (or no) reliance on dedicated servers in 
data centers. Instead the application exploits direct communication between pairs of 
intermittently connected hosts, called peers. The peers are not owned by the service 
provider, but are instead desktops and laptops controlled by users, with most of the 
2.1    •    Principles of Network Applications         115
peers residing in homes, universities, and offices. Because the peers communicate 
without passing through a dedicated server, the architecture is called peer-to-peer. 
Many of today’s most popular and traffic-intensive applications are based on P2P 
architectures. These applications include file sharing (e.g., BitTorrent), peer-assisted 
download acceleration (e.g., Xunlei), and Internet telephony and video conference 
(e.g., Skype). The P2P architecture is illustrated in Figure 2.2(b). We mention that 
some applications have hybrid architectures, combining both client-server and P2P 
elements. For example, for many instant messaging applications, servers are used to 
track the IP addresses of users, but user-to-user messages are sent directly between 
user hosts (without passing through intermediate servers).
One of the most compelling features of P2P architectures is their self- 
scalability. For example, in a P2P file-sharing application, although each peer gener-
ates workload by requesting files, each peer also adds service capacity to the system 
by distributing files to other peers. P2P architectures are also cost effective, since 
they normally don’t require significant server infrastructure and server bandwidth 
a. Client-server architecture
b. Peer-to-peer architecture
Figure 2.2  ♦  (a) Client-server architecture; (b) P2P architecture
116         Chapter 2    •    Application Layer
(in contrast with clients-server designs with datacenters). However, P2P applica-
tions face challenges of security, performance, and reliability due to their highly 
 
decentralized structure.
2.1.2 Processes Communicating
Before building your network application, you also need a basic understanding of 
how the programs, running in multiple end systems, communicate with each other. 
In the jargon of operating systems, it is not actually programs but processes that 
communicate. A process can be thought of as a program that is running within an end 
system. When processes are running on the same end system, they can communicate 
with each other with interprocess communication, using rules that are governed by 
the end system’s operating system. But in this book we are not particularly interested 
in how processes in the same host communicate, but instead in how processes run-
ning on different hosts (with potentially different operating systems) communicate.
Processes on two different end systems communicate with each other by 
exchanging messages across the computer network. A sending process creates and 
sends messages into the network; a receiving process receives these messages and 
possibly responds by sending messages back. Figure 2.1 illustrates that processes 
communicating with each other reside in the application layer of the five-layer pro-
tocol stack.
Client and Server Processes
A network application consists of pairs of processes that send messages to each 
other over a network. For example, in the Web application a client browser process 
exchanges messages with a Web server process. In a P2P file-sharing system, a file 
is transferred from a process in one peer to a process in another peer. For each pair of 
communicating processes, we typically label one of the two processes as the client and 
the other process as the server. With the Web, a browser is a client process and a Web 
server is a server process. With P2P file sharing, the peer that is downloading the file 
is labeled as the client, and the peer that is uploading the file is labeled as the server.
You may have observed that in some applications, such as in P2P file sharing, 
a process can be both a client and a server. Indeed, a process in a P2P file-sharing 
system can both upload and download files. Nevertheless, in the context of any given 
communication session between a pair of processes, we can still label one process 
as the client and the other process as the server. We define the client and server pro-
cesses as follows:
In the context of a communication session between a pair of processes, the pro-
cess that initiates the communication (that is, initially contacts the other process 
at the beginning of the session) is labeled as the client. The process that waits to 
be contacted to begin the session is the server.
2.1    •    Principles of Network Applications         117
In the Web, a browser process initializes contact with a Web server process; 
hence the browser process is the client and the Web server process is the server. In 
P2P file sharing, when Peer A asks Peer B to send a specific file, Peer A is the cli-
ent and Peer B is the server in the context of this specific communication session. 
When there’s no confusion, we’ll sometimes also use the terminology “client side 
and server side of an application.” At the end of this chapter, we’ll step through sim-
ple code for both the client and server sides of network applications.
The Interface Between the Process and the Computer Network
As noted above, most applications consist of pairs of communicating processes, with 
the two processes in each pair sending messages to each other. Any message sent 
from one process to another must go through the underlying network. A process 
sends messages into, and receives messages from, the network through a software 
interface called a socket. Let’s consider an analogy to help us understand processes 
and sockets. A process is analogous to a house and its socket is analogous to its door. 
When a process wants to send a message to another process on another host, it shoves 
the message out its door (socket). This sending process assumes that there is a trans-
portation infrastructure on the other side of its door that will transport the message to 
the door of the destination process. Once the message arrives at the destination host, 
the message passes through the receiving process’s door (socket), and the receiving 
process then acts on the message.
Figure 2.3 illustrates socket communication between two processes that com-
municate over the Internet. (Figure 2.3 assumes that the underlying transport proto-
col used by the processes is the Internet’s TCP protocol.) As shown in this figure, a 
socket is the interface between the application layer and the transport layer within 
a host. It is also referred to as the Application Programming Interface (API) 
between the application and the network, since the socket is the programming inter-
face with which network applications are built. The application developer has con-
trol of everything on the application-layer side of the socket but has little control of 
the transport-layer side of the socket. The only control that the application developer 
has on the transport-layer side is (1) the choice of transport protocol and (2) perhaps 
the ability to fix a few transport-layer parameters such as maximum buffer and maxi-
mum segment sizes (to be covered in Chapter 3). Once the application developer 
chooses a transport protocol (if a choice is available), the application is built using 
the transport-layer services provided by that protocol. We’ll explore sockets in some 
detail in Section 2.7.
Addressing Processes
In order to send postal mail to a particular destination, the destination needs to have 
an address. Similarly, in order for a process running on one host to send packets to 
a process running on another host, the receiving process needs to have an address. 
118         Chapter 2    •    Application Layer
To identify the receiving process, two pieces of information need to be specified: (1) 
the address of the host and (2) an identifier that specifies the receiving process in the 
destination host.
In the Internet, the host is identified by its IP address. We’ll discuss IP addresses 
in great detail in Chapter 4. For now, all we need to know is that an IP address is a 
32-bit quantity that we can think of as uniquely identifying the host. In addition to 
knowing the address of the host to which a message is destined, the sending process 
must also identify the receiving process (more specifically, the receiving socket) 
running in the host. This information is needed because in general a host could be 
running many network applications. A destination port number serves this purpose. 
Popular applications have been assigned specific port numbers. For example, a Web 
server is identified by port number 80. A mail server process (using the SMTP proto-
col) is identified by port number 25. A list of well-known port numbers for all Inter-
net standard protocols can be found at www.iana.org. We’ll examine port numbers 
in detail in Chapter 3.
2.1.3 Transport Services Available to Applications
Recall that a socket is the interface between the application process and the trans-
port-layer protocol. The application at the sending side pushes messages through the 
socket. At the other side of the socket, the transport-layer protocol has the responsi-
bility of getting the messages to the socket of the receiving process.
Many networks, including the Internet, provide more than one transport-layer 
protocol. When you develop an application, you must choose one of the available 
Process
Host or
server
Host or
server
Controlled
by application
developer
Controlled
by application
developer
Process
TCP with
buffers,
variables
Internet
Controlled
by operating
system
Controlled
by operating
system
TCP with
buffers,
variables
Socket
Socket
Figure 2.3  ♦  
Application processes, sockets, and underlying transport  
protocol
2.1    •    Principles of Network Applications         119
transport-layer protocols. How do you make this choice? Most likely, you would 
study the services provided by the available transport-layer protocols, and then pick 
the protocol with the services that best match your application’s needs. The situation 
is similar to choosing either train or airplane transport for travel between two cities. 
You have to choose one or the other, and each transportation mode offers different 
services. (For example, the train offers downtown pickup and drop-off, whereas the 
plane offers shorter travel time.)
What are the services that a transport-layer protocol can offer to applications 
invoking it? We can broadly classify the possible services along four dimensions: 
reliable data transfer, throughput, timing, and security.
Reliable Data Transfer
As discussed in Chapter 1, packets can get lost within a computer network. For 
example, a packet can overflow a buffer in a router, or can be discarded by a host or 
router after having some of its bits corrupted. For many applications—such as elec-
tronic mail, file transfer, remote host access, Web document transfers, and financial 
applications—data loss can have devastating consequences (in the latter case, for 
either the bank or the customer!). Thus, to support these applications, something has 
to be done to guarantee that the data sent by one end of the application is delivered 
correctly and completely to the other end of the application. If a protocol provides 
such a guaranteed data delivery service, it is said to provide reliable data transfer. 
One important service that a transport-layer protocol can potentially provide to an 
application is process-to-process reliable data transfer. When a transport protocol 
provides this service, the sending process can just pass its data into the socket and 
know with complete confidence that the data will arrive without errors at the receiv-
ing process.
When a transport-layer protocol doesn’t provide reliable data transfer, some of 
the data sent by the sending process may never arrive at the receiving process. This 
may be acceptable for loss-tolerant applications, most notably multimedia applica-
tions such as conversational audio/video that can tolerate some amount of data loss. 
In these multimedia applications, lost data might result in a small glitch in the audio/
video—not a crucial impairment.
Throughput
In Chapter 1 we introduced the concept of available throughput, which, in the 
context of a communication session between two processes along a network path, 
is the rate at which the sending process can deliver bits to the receiving process. 
Because other sessions will be sharing the bandwidth along the network path, and 
because these other sessions will be coming and going, the available throughput 
can fluctuate with time. These observations lead to another natural service that a 
transport-layer protocol could provide, namely, guaranteed available throughput at 
120         Chapter 2    •    Application Layer
some specified rate. With such a service, the application could request a guaranteed 
throughput of r bits/sec, and the transport protocol would then ensure that the avail-
able throughput is always at least r bits/sec. Such a guaranteed throughput service 
would appeal to many applications. For example, if an Internet telephony applica-
tion encodes voice at 32 kbps, it needs to send data into the network and have data 
delivered to the receiving application at this rate. If the transport protocol cannot 
provide this throughput, the application would need to encode at a lower rate (and 
receive enough throughput to sustain this lower coding rate) or may have to give up, 
since receiving, say, half of the needed throughput is of little or no use to this Inter-
net telephony application. Applications that have throughput requirements are said 
to be bandwidth-sensitive applications. Many current multimedia applications are 
bandwidth sensitive, although some multimedia applications may use adaptive cod-
ing techniques to encode digitized voice or video at a rate that matches the currently 
available throughput.
While bandwidth-sensitive applications have specific throughput requirements, 
elastic applications can make use of as much, or as little, throughput as happens to 
be available. Electronic mail, file transfer, and Web transfers are all elastic applica-
tions. Of course, the more throughput, the better. There’san adage that says that one 
cannot be too rich, too thin, or have too much throughput!
Timing
A transport-layer protocol can also provide timing guarantees. As with throughput 
guarantees, timing guarantees can come in many shapes and forms. An example 
guarantee might be that every bit that the sender pumps into the socket arrives at the 
receiver’s socket no more than 100 msec later. Such a service would be appealing to 
interactive real-time applications, such as Internet telephony, virtual environments, 
teleconferencing, and multiplayer games, all of which require tight timing constraints 
on data delivery in order to be effective. (See Chapter 9, [Gauthier 1999; Ramjee 
1994].) Long delays in Internet telephony, for example, tend to result in unnatural 
pauses in the conversation; in a multiplayer game or virtual interactive environment, 
a long delay between taking an action and seeing the response from the environment 
(for example, from another player at the end of an end-to-end connection) makes the 
application feel less realistic. For non-real-time applications, lower delay is always 
preferable to higher delay, but no tight constraint is placed on the end-to-end delays.
Security
Finally, a transport protocol can provide an application with one or more security 
services. For example, in the sending host, a transport protocol can encrypt all data 
transmitted by the sending process, and in the receiving host, the transport-layer pro-
tocol can decrypt the data before delivering the data to the receiving process. Such a 
service would provide confidentiality between the two processes, even if the data is 
2.1    •    Principles of Network Applications         121
somehow observed between sending and receiving processes. A transport protocol 
can also provide other security services in addition to confidentiality, including data 
integrity and end-point authentication, topics that we’ll cover in detail in Chapter 8.
2.1.4 Transport Services Provided by the Internet
Up until this point, we have been considering transport services that a computer net-
work could provide in general. Let’s now get more specific and examine the type of 
transport services provided by the Internet. The Internet (and, more generally, TCP/
IP networks) makes two transport protocols available to applications, UDP and TCP. 
When you (as an application developer) create a new network application for the 
Internet, one of the first decisions you have to make is whether to use UDP or TCP. 
Each of these protocols offers a different set of services to the invoking applications. 
Figure 2.4 shows the service requirements for some selected applications.
TCP Services
The TCP service model includes a connection-oriented service and a reliable data 
transfer service. When an application invokes TCP as its transport protocol, the 
application receives both of these services from TCP.
•	 Connection-oriented service. TCP has the client and server exchange transport-
layer control information with each other before the application-level mes-
sages begin to flow. This so-called handshaking procedure alerts the client 
and server, allowing them to prepare for an onslaught of packets. After the 
handshaking phase, a TCP connection is said to exist between the sockets 
Application
Data Loss
Throughput
Time-Sensitive
File transfer/download
No loss
Elastic
No
E-mail
No loss
Elastic
No
Web documents
No loss
Elastic (few kbps)
No
Internet telephony/
Video conferencing
Loss-tolerant
Audio: few kbps–1Mbps
Video: 10 kbps–5 Mbps
Yes: 100s of msec
Streaming stored 
Loss-tolerant
Same as above
Yes: few seconds
audio/video
Interactive games
Loss-tolerant
Few kbps–10 kbps
Yes: 100s of msec
Smartphone messaging 
No loss 
Elastic 
Yes and no
Figure 2.4  ♦  Requirements of selected network applications
122         Chapter 2    •    Application Layer
of the two processes. The connection is a full-duplex connection in that the two 
processes can send messages to each other over the connection at the same time. 
When the application finishes sending messages, it must tear down the connec-
tion. In Chapter 3 we’ll discuss connection-oriented service in detail and examine 
how it is implemented.
•	 Reliable data transfer service. The communicating processes can rely on TCP to 
deliver all data sent without error and in the proper order. When one side of the 
application passes a stream of bytes into a socket, it can count on TCP to deliver the 
same stream of bytes to the receiving socket, with no missing or duplicate bytes.
TCP also includes a congestion-control mechanism, a service for the general 
welfare of the Internet rather than for the direct benefit of the communicating pro-
cesses. The TCP congestion-control mechanism throttles a sending process (client or 
server) when the network is congested between sender and receiver. As we will see 
SECURING TCP
Neither TCP nor UDP provides any encryption—the data that the sending process 
passes into its socket is the same data that travels over the network to the destina-
tion process. So, for example, if the sending process sends a password in cleartext 
(i.e., unencrypted) into its socket, the cleartext password will travel over all the links 
between sender and receiver, potentially getting sniffed and discovered at any of 
the intervening links. Because privacy and other security issues have become critical 
for many applications, the Internet community has developed an enhancement for 
TCP, called Secure Sockets Layer (SSL). TCP-enhanced-with-SSL not only does 
everything that traditional TCP does but also provides critical process-to-process 
security services, including encryption, data integrity, and end-point authentication. 
We emphasize that SSL is not a third Internet transport protocol, on the same level as 
TCP and UDP, but instead is an enhancement of TCP, with the enhancements being 
implemented in the application layer. In particular, if an application wants to use 
the services of SSL, it needs to include SSL code (existing, highly optimized libraries 
and classes) in both the client and server sides of the application. SSL has its own 
socket API that is similar to the traditional TCP socket API. When an application uses 
SSL, the sending process passes cleartext data to the SSL socket; SSL in the sending 
host then encrypts the data and passes the encrypted data to the TCP socket. The 
encrypted data travels over the Internet to the TCP socket in the receiving process. 
The receiving socket passes the encrypted data to SSL, which decrypts the data. 
Finally, SSL passes the cleartext data through its SSL socket to the receiving process. 
We’ll cover SSL in some detail in Chapter 8.
FOCUS ON SECURITY
2.1    •    Principles of Network Applications         123
in Chapter 3, TCP congestion control also attempts to limit each TCP connection to 
its fair share of network bandwidth.
UDP Services
UDP is a no-frills, lightweight transport protocol, providing minimal services. UDP 
is connectionless, so there is no handshaking before the two processes start to com-
municate. UDP provides an unreliable data transfer service—that is, when a process 
sends a message into a UDP socket, UDP provides no guarantee that the message 
will ever reach the receiving process. Furthermore, messages that do arrive at the 
receiving process may arrive out of order.
UDP does not include a congestion-control mechanism, so the sending side of 
UDP can pump data into the layer below (the network layer) at any rate it pleases. 
(Note, however, that the actual end-to-end throughput may be less than this rate due 
to the limited transmission capacity of intervening links or due to congestion).
Services Not Provided by Internet Transport Protocols
We have organized transport protocol services along four dimensions: reliable data 
transfer, throughput, timing, and security. Which of these services are provided 
by TCP and UDP? We have already noted that TCP provides reliable end-to-end 
data transfer. And we also know that TCP can be easily enhanced at the application 
layer with SSL to provide security services. But in our brief description of TCP and 
UDP, conspicuously missing was any mention of throughput or timing guarantees— 
services not provided by today’s Internet transport protocols. Does this mean that 
time-sensitive applications such as Internet telephony cannot run in today’s Internet? 
The answer is clearly no—the Internet has been hosting time-sensitive applications 
for many years. These applications often work fairly well because they have been 
designed to cope, to the greatest extent possible, with this lack of guarantee. We’ll 
investigate several of these design tricks in Chapter 9. Nevertheless, clever design 
has its limitations when delay is excessive, or the end-to-end throughput is limited. 
In summary, today’s Internet can often provide satisfactory service to time-sensitive 
applications, but it cannot provide any timing or throughput guarantees.
Figure 2.5 indicates the transport protocols used by some popular Internet appli-
cations. We see that e-mail, remote terminal access, the Web, and file transfer all 
use TCP. These applications have chosen TCP primarily because TCP provides reli-
able data transfer, guaranteeing that all data will eventually get to its destination. 
Because Internet telephony applications (such as Skype) can often tolerate some loss 
but require a minimal rate to be effective, developers of Internet telephony applica-
tions usually prefer to run their applications over UDP, thereby circumventing TCP’s 
congestion control mechanism and packet overheads. But because many firewalls 
are configured to block (most types of) UDP traffic, Internet telephony applications 
often are designed to use TCP as a backup if UDP communication fails.
124         Chapter 2    •    Application Layer
2.1.5 Application-Layer Protocols
We have just learned that network processes communicate with each other by sending 
messages into sockets. But how are these messages structured? What are the meanings 
of the various fields in the messages? When do the processes send the messages? These 
questions bring us into the realm of application-layer protocols. An application-layer 
protocol defines how an application’s processes, running on different end systems, 
pass messages to each other. In particular, an application-layer protocol defines:
•	 The types of messages exchanged, for example, request messages and response 
messages
•	 The syntax of the various message types, such as the fields in the message and 
how the fields are delineated
•	 The semantics of the fields, that is, the meaning of the information in the fields
•	 Rules for determining when and how a process sends messages and responds to 
messages
Some application-layer protocols are specified in RFCs and are therefore in the 
public domain. For example, the Web’s application-layer protocol, HTTP (the 
HyperText Transfer Protocol [RFC 2616]), is available as an RFC. If a browser 
developer follows the rules of the HTTP RFC, the browser will be able to retrieve 
Web pages from any Web server that has also followed the rules of the HTTP RFC. 
Many other application-layer protocols are proprietary and intentionally not avail-
able in the public domain. For example, Skype uses proprietary application-layer 
protocols.
Application
Application-Layer Protocol
Underlying Transport Protocol
Electronic mail
Remote terminal access
Web
File transfer
Streaming multimedia
Internet telephony
SMTP [RFC 5321]
Telnet [RFC 854]
HTTP [RFC 2616]
FTP [RFC 959]
HTTP (e.g., YouTube)
SIP [RFC 3261], RTP [RFC 3550], or proprietary
(e.g., Skype)
TCP
TCP
TCP
TCP
TCP
UDP or TCP
Figure 2.5  ♦  
Popular Internet applications, their application-layer  
protocols, and their underlying transport protocols
2.1    •    Principles of Network Applications         125
It is important to distinguish between network applications and application-layer 
protocols. An application-layer protocol is only one piece of a network application 
(albeit, a very important piece of the application from our point of view!). Let’s look 
at a couple of examples. The Web is a client-server application that allows users 
to obtain documents from Web servers on demand. The Web application consists 
of many components, including a standard for document formats (that is, HTML), 
Web browsers (for example, Firefox and Microsoft Internet Explorer), Web servers 
(for example, Apache and Microsoft servers), and an application-layer protocol. The 
Web’s application-layer protocol, HTTP, defines the format and sequence of mes-
sages exchanged between browser and Web server. Thus, HTTP is only one piece 
(albeit, an important piece) of the Web application. As another example, an Internet 
e-mail application also has many components, including mail servers that house user 
mailboxes; mail clients (such as Microsoft Outlook) that allow users to read and 
create messages; a standard for defining the structure of an e-mail message; and 
application-layer protocols that define how messages are passed between servers, 
how messages are passed between servers and mail clients, and how the contents 
of message headers are to be interpreted. The principal application-layer protocol 
for electronic mail is SMTP (Simple Mail Transfer Protocol) [RFC 5321]. Thus, 
 
e-mail’s principal application-layer protocol, SMTP, is only one piece (albeit an 
important piece) of the e-mail application.
2.1.6 Network Applications Covered in This Book
New public domain and proprietary Internet applications are being developed 
every day. Rather than covering a large number of Internet applications in an 
encyclopedic manner, we have chosen to focus on a small number of applications 
that are both pervasive and important. In this chapter we discuss five important 
applications: the Web, electronic mail, directory service video streaming, and 
P2P applications. We first discuss the Web, not only because it is an enormously 
popular application, but also because its application-layer protocol, HTTP, is 
straightforward and easy to understand. We then discuss electronic mail, the 
Internet’s first killer application. E-mail is more complex than the Web in the 
sense that it makes use of not one but several application-layer protocols. After 
e-mail, we cover DNS, which provides a directory service for the Internet. Most 
users do not interact with DNS directly; instead, users invoke DNS indirectly 
through other applications (including the Web, file transfer, and electronic mail). 
DNS illustrates nicely how a piece of core network functionality (network-name 
to network-address translation) can be implemented at the application layer in 
the Internet. We then discuss P2P file sharing applications, and complete our 
application study by discussing video streaming on demand, including dis-
tributing stored video over content distribution networks. In Chapter 9, we’ll 
cover multimedia applications in more depth, including voice over IP and video 
 
conferencing.
126         Chapter 2    •    Application Layer
2.2	 The Web and HTTP
Until the early 1990s the Internet was used primarily by researchers, academics, 
and university students to log in to remote hosts, to transfer files from local hosts 
to remote hosts and vice versa, to receive and send news, and to receive and send 
electronic mail. Although these applications were (and continue to be) extremely 
useful, the Internet was essentially unknown outside of the academic and research 
communities. Then, in the early 1990s, a major new application arrived on the 
scene—the World Wide Web [Berners-Lee 1994]. The Web was the first Internet 
application that caught the general public’s eye. It dramatically changed, and con-
tinues to change, how people interact inside and outside their work environments. 
It elevated the Internet from just one of many data networks to essentially the one 
and only data network.
Perhaps what appeals the most to users is that the Web operates on demand. 
Users receive what they want, when they want it. This is unlike traditional broadcast 
radio and television, which force users to tune in when the content provider makes 
the content available. In addition to being available on demand, the Web has many 
other wonderful features that people love and cherish. It is enormously easy for any 
individual to make information available over the Web—everyone can become a 
publisher at extremely low cost. Hyperlinks and search engines help us navigate 
through an ocean of information. Photos and videos stimulate our senses. Forms, 
JavaScript, Java applets, and many other devices enable us to interact with pages and 
sites. And the Web and its protocols serve as a platform for YouTube, Web-based 
e-mail (such as Gmail), and most mobile Internet applications, including Instagram 
and Google Maps.
2.2.1 Overview of HTTP
The HyperText Transfer Protocol (HTTP), the Web’s application-layer protocol, 
is at the heart of the Web. It is defined in [RFC 1945] and [RFC 2616]. HTTP is 
implemented in two programs: a client program and a server program. The client 
program and server program, executing on different end systems, talk to each other 
by exchanging HTTP messages. HTTP defines the structure of these messages and 
how the client and server exchange the messages. Before explaining HTTP in detail, 
we should review some Web terminology.
A Web page (also called a document) consists of objects. An object is 
simply a file—such as an HTML file, a JPEG image, a Java applet, or a video 
 
clip—that is addressable by a single URL. Most Web pages consist of a base 
HTML file and several referenced objects. For example, if a Web page con-
tains HTML text and five JPEG images, then the Web page has six objects: the 
base HTML file plus the five images. The base HTML file references the other 
objects in the page with the objects’ URLs. Each URL has two components: the 
2.2    •    The Web and HTTP         127
hostname of the server that houses the object and the object’s path name. For 
example, the URL
http://www.someSchool.edu/someDepartment/picture.gif
has www.someSchool.edu for a hostname and /someDepartment/picture.
gif for a path name. Because Web browsers (such as Internet Explorer and Firefox) 
implement the client side of HTTP, in the context of the Web, we will use the words 
browser and client interchangeably. Web servers, which implement the server side 
of HTTP, house Web objects, each addressable by a URL. Popular Web servers 
include Apache and Microsoft Internet Information Server.
HTTP defines how Web clients request Web pages from Web servers and how 
servers transfer Web pages to clients. We discuss the interaction between client 
and server in detail later, but the general idea is illustrated in Figure 2.6. When a 
 
user requests a Web page (for example, clicks on a hyperlink), the browser sends 
HTTP request messages for the objects in the page to the server. The server receives 
the requests and responds with HTTP response messages that contain the objects.
HTTP uses TCP as its underlying transport protocol (rather than running on top 
of UDP). The HTTP client first initiates a TCP connection with the server. Once the 
connection is established, the browser and the server processes access TCP through 
their socket interfaces. As described in Section 2.1, on the client side the socket inter-
face is the door between the client process and the TCP connection; on the server side 
it is the door between the server process and the TCP connection. The client sends 
HTTP request messages into its socket interface and receives HTTP response mes-
sages from its socket interface. Similarly, the HTTP server receives request messages 
HTTP request
HTTP response
HTTP response
HTTP request
PC running
Internet Explorer
Android smartphone
running Google Chrome
Server running
Apache Web server
Figure 2.6  ♦  HTTP request-response behavior
128         Chapter 2    •    Application Layer
from its socket interface and sends response messages into its socket interface. Once 
the client sends a message into its socket interface, the message is out of the client’s 
hands and is “in the hands” of TCP. Recall from Section 2.1 that TCP provides a 
reliable data transfer service to HTTP. This implies that each HTTP request message 
sent by a client process eventually arrives intact at the server; similarly, each HTTP 
response message sent by the server process eventually arrives intact at the client. 
Here we see one of the great advantages of a layered architecture—HTTP need not 
worry about lost data or the details of how TCP recovers from loss or reordering of 
data within the network. That is the job of TCP and the protocols in the lower layers 
of the protocol stack.
It is important to note that the server sends requested files to clients without 
storing any state information about the client. If a particular client asks for the same 
object twice in a period of a few seconds, the server does not respond by saying that 
it just served the object to the client; instead, the server resends the object, as it has 
completely forgotten what it did earlier. Because an HTTP server maintains no infor-
mation about the clients, HTTP is said to be a stateless protocol. We also remark 
that the Web uses the client-server application architecture, as described in Section 
2.1. A Web server is always on, with a fixed IP address, and it services requests from 
potentially millions of different browsers.
2.2.2 Non-Persistent and Persistent Connections
In many Internet applications, the client and server communicate for an extended 
period of time, with the client making a series of requests and the server respond-
ing to each of the requests. Depending on the application and on how the applica-
tion is being used, the series of requests may be made back-to-back, periodically 
at regular intervals, or intermittently. When this client-server interaction is 
taking place over TCP, the application developer needs to make an important 
 
decision—should each request/response pair be sent over a separate TCP connec-
tion, or should all of the requests and their corresponding responses be sent over 
the same TCP connection? In the former approach, the application is said to use 
 
non-persistent connections; and in the latter approach, persistent connections. 
To gain a deep understanding of this design issue, let’s examine the advantages 
and disadvantages of persistent connections in the context of a specific applica-
tion, namely, HTTP, which can use both non-persistent connections and per-
sistent connections. Although HTTP uses persistent connections in its default 
mode, HTTP clients and servers can be configured to use non-persistent connec-
tions instead.
HTTP with Non-Persistent Connections
Let’s walk through the steps of transferring a Web page from server to client for the 
case of non-persistent connections. Let’s suppose the page consists of a base HTML 
2.2    •    The Web and HTTP         129
file and 10 JPEG images, and that all 11 of these objects reside on the same server. 
Further suppose the URL for the base HTML file is
http://www.someSchool.edu/someDepartment/home.index
Here is what happens:
	1.	 The HTTP client process initiates a TCP connection to the server www 
.someSchool.edu on port number 80, which is the default port number for 
HTTP. Associated with the TCP connection, there will be a socket at the client 
and a socket at the server.
	2.	 The HTTP client sends an HTTP request message to the server via its socket. 
The request message includes the path name /someDepartment/home 
.index. (We will discuss HTTP messages in some detail below.)
	3.	 The HTTP server process receives the request message via its socket, retrieves 
the object /someDepartment/home.index from its storage (RAM or 
disk), encapsulates the object in an HTTP response message, and sends the 
response message to the client via its socket.
	4.	 The HTTP server process tells TCP to close the TCP connection. (But TCP 
doesn’t actually terminate the connection until it knows for sure that the client 
has received the response message intact.)
	5.	 The HTTP client receives the response message. The TCP connection termi-
nates. The message indicates that the encapsulated object is an HTML file. The 
client extracts the file from the response message, examines the HTML file, and 
finds references to the 10 JPEG objects.
	6.	 The first four steps are then repeated for each of the referenced JPEG objects.
As the browser receives the Web page, it displays the page to the user. Two dif-
ferent browsers may interpret (that is, display to the user) a Web page in somewhat 
different ways. HTTP has nothing to do with how a Web page is interpreted by a cli-
ent. The HTTP specifications ([RFC 1945] and [RFC 2616]) define only the commu-
nication protocol between the client HTTP program and the server HTTP program.
The steps above illustrate the use of non-persistent connections, where each TCP 
connection is closed after the server sends the object—the connection does not per-
sist for other objects. Note that each TCP connection transports exactly one request 
message and one response message. Thus, in this example, when a user requests the 
Web page, 11 TCP connections are generated.
In the steps described above, we were intentionally vague about whether the 
 
client obtains the 10 JPEGs over 10 serial TCP connections, or whether some of the 
JPEGs are obtained over parallel TCP connections. Indeed, users can configure modern 
browsers to control the degree of parallelism. In their default modes, most browsers open 
5 to 10 parallel TCP connections, and each of these connections handles one request-
response transaction. If the user prefers, the maximum number of parallel connections 
130         Chapter 2    •    Application Layer
can be set to one, in which case the 10 connections are established serially. As we’ll see 
in the next chapter, the use of parallel connections shortens the response time.
Before continuing, let’s do a back-of-the-envelope calculation to estimate the 
amount of time that elapses from when a client requests the base HTML file until 
the entire file is received by the client. To this end, we define the round-trip time 
(RTT), which is the time it takes for a small packet to travel from client to server 
and then back to the client. The RTT includes packet-propagation delays, packet-
queuing delays in intermediate routers and switches, and packet-processing delays. 
(These delays were discussed in Section 1.4.) Now consider what happens when 
a user clicks on a hyperlink. As shown in Figure 2.7, this causes the browser to 
initiate a TCP connection between the browser and the Web server; this involves 
a “three-way handshake”—the client sends a small TCP segment to the server, the 
server acknowledges and responds with a small TCP segment, and, finally, the cli-
ent acknowledges back to the server. The first two parts of the three-way handshake 
take one RTT. After completing the first two parts of the handshake, the client sends 
the HTTP request message combined with the third part of the three-way handshake 
(the acknowledgment) into the TCP connection. Once the request message arrives at 
 
Time
at client
Time
at server
Initiate TCP
connection
RTT
Request ﬁle
RTT
Entire ﬁle received
Time to transmit ﬁle
Figure 2.7  ♦  
Back-of-the-envelope calculation for the time needed  
to request and receive an HTML file
2.2    •    The Web and HTTP         131
the server, the server sends the HTML file into the TCP connection. This HTTP 
request/response eats up another RTT. Thus, roughly, the total response time is two 
RTTs plus the transmission time at the server of the HTML file.
HTTP with Persistent Connections
Non-persistent connections have some shortcomings. First, a brand-new connection 
must be established and maintained for each requested object. For each of these 
connections, TCP buffers must be allocated and TCP variables must be kept in both 
the client and server. This can place a significant burden on the Web server, which 
may be serving requests from hundreds of different clients simultaneously. Second, 
as we just described, each object suffers a delivery delay of two RTTs—one RTT to 
establish the TCP connection and one RTT to request and receive an object.
With HTTP 1.1 persistent connections, the server leaves the TCP connection 
open after sending a response. Subsequent requests and responses between the same 
client and server can be sent over the same connection. In particular, an entire Web 
page (in the example above, the base HTML file and the 10 images) can be sent over 
a single persistent TCP connection. Moreover, multiple Web pages residing on the 
same server can be sent from the server to the same client over a single persistent 
TCP connection. These requests for objects can be made back-to-back, without wait-
ing for replies to pending requests (pipelining). Typically, the HTTP server closes 
a connection when it isn’t used for a certain time (a configurable timeout interval). 
When the server receives the back-to-back requests, it sends the objects back-to-
back. The default mode of HTTP uses persistent connections with pipelining. Most 
recently, HTTP/2 [RFC 7540] builds on HTTP 1.1 by allowing multiple requests 
and replies to be interleaved in the same connection, and a mechanism for prioritiz-
ing HTTP message requests and replies within this connection. We’ll quantitatively 
compare the performance of non-persistent and persistent connections in the home-
work problems of Chapters 2 and 3. You are also encouraged to see [Heidemann 
1997; Nielsen 1997; RFC 7540].
2.2.3 HTTP Message Format
The HTTP specifications [RFC 1945; RFC 2616; RFC 7540] include the definitions 
of the HTTP message formats. There are two types of HTTP messages, request mes-
sages and response messages, both of which are discussed below.
HTTP Request Message
Below we provide a typical HTTP request message:
GET /somedir/page.html HTTP/1.1
Host: www.someschool.edu
132         Chapter 2    •    Application Layer
Connection: close
User-agent: Mozilla/5.0
Accept-language: fr
We can learn a lot by taking a close look at this simple request message. First of 
all, we see that the message is written in ordinary ASCII text, so that your ordinary 
computer-literate human being can read it. Second, we see that the message consists 
of five lines, each followed by a carriage return and a line feed. The last line is fol-
lowed by an additional carriage return and line feed. Although this particular request 
message has five lines, a request message can have many more lines or as few as 
one line. The first line of an HTTP request message is called the request line; the 
subsequent lines are called the header lines. The request line has three fields: the 
method field, the URL field, and the HTTP version field. The method field can take 
on several different values, including GET, POST, HEAD, PUT, and DELETE. 
The great majority of HTTP request messages use the GET method. The GET method 
is used when the browser requests an object, with the requested object identified in 
the URL field. In this example, the browser is requesting the object /somedir/
page.html. The version is self-explanatory; in this example, the browser imple-
ments version HTTP/1.1.
Now let’s look at the header lines in the example. The header line Host: www 
.someschool.edu specifies the host on which the object resides. You might 
think that this header line is unnecessary, as there is already a TCP connection in 
place to the host. But, as we’ll see in Section 2.2.5, the information provided by the 
host header line is required by Web proxy caches. By including the Connection: 
close header line, the browser is telling the server that it doesn’t want to bother 
with persistent connections; it wants the server to close the connection after sending 
the requested object. The User-agent: header line specifies the user agent, that 
is, the browser type that is making the request to the server. Here the user agent is 
Mozilla/5.0, a Firefox browser. This header line is useful because the server can actu-
ally send different versions of the same object to different types of user agents. (Each 
of the versions is addressed by the same URL.) Finally, the Accept-language: 
header indicates that the user prefers to receive a French version of the object, if such 
an object exists on the server; otherwise, the server should send its default version. 
The Accept-language: header is just one of many content negotiation headers 
available in HTTP.
Having looked at an example, let’s now look at the general format of a request 
message, as shown in Figure 2.8. We see that the general format closely follows our 
earlier example. You may have noticed, however, that after the header lines (and the 
additional carriage return and line feed) there is an “entity body.” The entity body 
is empty with the GET method, but is used with the POST method. An HTTP client 
often uses the POST method when the user fills out a form—for example, when a 
user provides search words to a search engine. With a POST message, the user is still 
requesting a Web page from the server, but the specific contents of the Web page 
2.2    •    The Web and HTTP         133
depend on what the user entered into the form fields. If the value of the method field 
is POST, then the entity body contains what the user entered into the form fields.
We would be remiss if we didn’t mention that a request generated with a form 
does not necessarily use the POST method. Instead, HTML forms often use the GET 
method and include the inputted data (in the form fields) in the requested URL. For 
example, if a form uses the GET method, has two fields, and the inputs to the two 
fields are monkeys and bananas, then the URL will have the structure www.
somesite.com/animalsearch?monkeys&bananas. In your day-to-day 
Web surfing, you have probably noticed extended URLs of this sort.
The HEAD method is similar to the GET method. When a server receives a 
request with the HEAD method, it responds with an HTTP message but it leaves out 
the requested object. Application developers often use the HEAD method for debug-
ging. The PUT method is often used in conjunction with Web publishing tools. It 
allows a user to upload an object to a specific path (directory) on a specific Web 
server. The PUT method is also used by applications that need to upload objects 
to Web servers. The DELETE method allows a user, or an application, to delete an 
object on a Web server.
HTTP Response Message
Below we provide a typical HTTP response message. This response message could 
be the response to the example request message just discussed.
HTTP/1.1 200 OK
Connection: close
Date: Tue, 18 Aug 2015 15:44:04 GMT
method
sp
sp
cr
lf
cr
lf
header ﬁeld name:
Header lines
Blank line
Entity body
Request line
value
sp
cr
lf
cr
lf
header ﬁeld name:
value
sp
URL
Version
Figure 2.8  ♦  General format of an HTTP request message
134         Chapter 2    •    Application Layer
Server: Apache/2.2.3 (CentOS)
Last-Modified: Tue, 18 Aug 2015 15:11:03 GMT
Content-Length: 6821
Content-Type: text/html 
(data data data data data ...)
Let’s take a careful look at this response message. It has three sections: an initial 
status line, six header lines, and then the entity body. The entity body is the meat 
of the message—it contains the requested object itself (represented by data data 
data data data ...). The status line has three fields: the protocol version 
field, a status code, and a corresponding status message. In this example, the status 
line indicates that the server is using HTTP/1.1 and that everything is OK (that is, the 
server has found, and is sending, the requested object).
Now let’s look at the header lines. The server uses the Connection: close 
header line to tell the client that it is going to close the TCP connection after sending 
the message. The Date: header line indicates the time and date when the HTTP 
response was created and sent by the server. Note that this is not the time when 
the object was created or last modified; it is the time when the server retrieves the 
object from its file system, inserts the object into the response message, and sends the 
response message. The Server: header line indicates that the message was gener-
ated by an Apache Web server; it is analogous to the User-agent: header line in 
the HTTP request message. The Last-Modified: header line indicates the time 
and date when the object was created or last modified. The Last-Modified: 
header, which we will soon cover in more detail, is critical for object caching, both 
in the local client and in network cache servers (also known as proxy servers). The 
Content-Length: header line indicates the number of bytes in the object being 
sent. The Content-Type: header line indicates that the object in the entity body 
is HTML text. (The object type is officially indicated by the Content-Type: 
header and not by the file extension.)
Having looked at an example, let’s now examine the general format of a response 
message, which is shown in Figure 2.9. This general format of the response message 
matches the previous example of a response message. Let’s say a few additional 
words about status codes and their phrases. The status code and associated phrase 
indicate the result of the request. Some common status codes and associated phrases 
include:
•	 200 OK: Request succeeded and the information is returned in the response.
•	 301 Moved Permanently: Requested object has been permanently moved; 
the new URL is specified in Location: header of the response message. The 
client software will automatically retrieve the new URL.
•	 400 Bad Request: This is a generic error code indicating that the request 
could not be understood by the server.
2.2    •    The Web and HTTP         135
•	 404 Not Found: The requested document does not exist on this server.
•	 505 HTTP Version Not Supported: The requested HTTP protocol ver-
sion is not supported by the server.
How would you like to see a real HTTP response message? This is highly rec-
ommended and very easy to do! First Telnet into your favorite Web server. Then 
type in a one-line request message for some object that is housed on the server. For 
example, if you have access to a command prompt, type:
telnet gaia.cs.umass.edu 80 
GET /kurose_ross/interactive/index.php HTTP/1.1
Host: gaia.cs.umass.edu
(Press the carriage return twice after typing the last line.) This opens a TCP con-
nection to port 80 of the host gaia.cs.umass.edu and then sends the HTTP 
request message. You should see a response message that includes the base HTML 
file for the interactive homework problems for this textbook. If you’d rather just see 
the HTTP message lines and not receive the object itself, replace GET with HEAD.
In this section we discussed a number of header lines that can be used within 
HTTP request and response messages. The HTTP specification defines many, 
many more header lines that can be inserted by browsers, Web servers, and net-
work cache servers. We have covered only a small number of the totality of header 
lines. We’ll cover a few more below and another small number when we discuss 
network Web caching in Section 2.2.5. A highly readable and comprehensive 
version
sp
sp
cr
lf
cr
lf
header ﬁeld name:
Header lines
Blank line
Entity body
Status line
value
cr
sp
sp
lf
cr
lf
header ﬁeld name:
value
status code
phrase
Figure 2.9  ♦  General format of an HTTP response message
VideoNote
Using Wireshark to 
investigate the HTTP 
protocol
136         Chapter 2    •    Application Layer
discussion of the HTTP protocol, including its headers and status codes, is given 
in [Krishnamurthy 2001].
How does a browser decide which header lines to include in a request mes-
sage? How does a Web server decide which header lines to include in a response 
message? A browser will generate header lines as a function of the browser type 
and version (for example, an HTTP/1.0 browser will not generate any 1.1 header 
lines), the user configuration of the browser (for example, preferred language), and 
whether the browser currently has a cached, but possibly out-of-date, version of the 
object. Web servers behave similarly: There are different products, versions, and 
configurations, all of which influence which header lines are included in response 
messages.
2.2.4 User-Server Interaction: Cookies
We mentioned above that an HTTP server is stateless. This simplifies server design 
and has permitted engineers to develop high-performance Web servers that can han-
dle thousands of simultaneous TCP connections. However, it is often desirable for 
a Web site to identify users, either because the server wishes to restrict user access 
or because it wants to serve content as a function of the user identity. For these pur-
poses, HTTP uses cookies. Cookies, defined in [RFC 6265], allow sites to keep track 
of users. Most major commercial Web sites use cookies today.
As shown in Figure 2.10, cookie technology has four components: (1) a cookie 
header line in the HTTP response message; (2) a cookie header line in the HTTP 
request message; (3) a cookie file kept on the user’s end system and managed by 
the user’s browser; and (4) a back-end database at the Web site. Using Figure 2.10, 
let’s walk through an example of how cookies work. Suppose Susan, who always 
accesses the Web using Internet Explorer from her home PC, contacts Amazon.com 
for the first time. Let us suppose that in the past she has already visited the eBay site. 
When the request comes into the Amazon Web server, the server creates a unique 
identification number and creates an entry in its back-end database that is indexed 
by the identification number. The Amazon Web server then responds to Susan’s 
browser, including in the HTTP response a Set-cookie: header, which contains 
the identification number. For example, the header line might be:
Set-cookie: 1678
When Susan’s browser receives the HTTP response message, it sees the 
 
Set-cookie: header. The browser then appends a line to the special cookie file 
that it manages. This line includes the hostname of the server and the identification 
number in the Set-cookie: header. Note that the cookie file already has an entry 
for eBay, since Susan has visited that site in the past. As Susan continues to browse 
the Amazon site, each time she requests a Web page, her browser consults her cookie 
file, extracts her identification number for this site, and puts a cookie header line that 
2.2    •    The Web and HTTP         137
includes the identification number in the HTTP request. Specifically, each of her 
HTTP requests to the Amazon server includes the header line:
Cookie: 1678
In this manner, the Amazon server is able to track Susan’s activity at the Amazon 
site. Although the Amazon Web site does not necessarily know Susan’s name, it 
knows exactly which pages user 1678 visited, in which order, and at what times! 
Client host
Server host
usual http request msg
usual http response
Set-cookie: 1678
usual http request msg
cookie: 1678
usual http response msg
usual http request msg
cookie: 1678
usual http response msg
Time
One week later
ebay: 8734
Server creates
ID 1678 for user
Time
Cookie ﬁle
Key:
amazon: 1678
ebay: 8734
amazon: 1678
ebay: 8734
Cookie-speciﬁc
action
access
access
entry in backend
database
Cookie-speciﬁc
action
Figure 2.10  ♦  Keeping user state with cookies
138         Chapter 2    •    Application Layer
Amazon uses cookies to provide its shopping cart service—Amazon can maintain a 
list of all of Susan’s intended purchases, so that she can pay for them collectively at 
the end of the session.
If Susan returns to Amazon’s site, say, one week later, her browser will con-
tinue to put the header line Cookie: 1678 in the request messages. Amazon also 
recommends products to Susan based on Web pages she has visited at Amazon in 
the past. If Susan also registers herself with Amazon—providing full name, e-mail 
address, postal address, and credit card information—Amazon can then include this 
information in its database, thereby associating Susan’s name with her identifica-
tion number (and all of the pages she has visited at the site in the past!). This is how 
 
Amazon and other e-commerce sites provide “one-click shopping”—when Susan 
chooses to purchase an item during a subsequent visit, she doesn’t need to re-enter 
her name, credit card number, or address.
From this discussion we see that cookies can be used to identify a user. The first 
time a user visits a site, the user can provide a user identification (possibly his or her 
name). During the subsequent sessions, the browser passes a cookie header to the 
server, thereby identifying the user to the server. Cookies can thus be used to create 
a user session layer on top of stateless HTTP. For example, when a user logs in to 
a Web-based e-mail application (such as Hotmail), the browser sends cookie infor-
mation to the server, permitting the server to identify the user throughout the user’s 
session with the application.
Although cookies often simplify the Internet shopping experience for the user, 
they are controversial because they can also be considered as an invasion of privacy. 
As we just saw, using a combination of cookies and user-supplied account informa-
tion, a Web site can learn a lot about a user and potentially sell this information to a 
third party. Cookie Central [Cookie Central 2016] includes extensive information on 
the cookie controversy.
2.2.5 Web Caching
A Web cache—also called a proxy server—is a network entity that satisfies HTTP 
requests on the behalf of an origin Web server. The Web cache has its own disk 
storage and keeps copies of recently requested objects in this storage. As shown in 
 
Figure 2.11, a user’s browser can be configured so that all of the user’s HTTP requests 
are first directed to the Web cache. Once a browser is configured, each browser request 
for an object is first directed to the Web cache. As an example, suppose a browser 
is requesting the object http://www.someschool.edu/campus.gif. 
 
Here is what happens:
	1.	 The browser establishes a TCP connection to the Web cache and sends an HTTP 
request for the object to the Web cache.
	2.	 The Web cache checks to see if it has a copy of the object stored locally. If it 
does, the Web cache returns the object within an HTTP response message to the 
client browser.
2.2    •    The Web and HTTP         139
	3.	 If the Web cache does not have the object, the Web cache opens a TCP connec-
tion to the origin server, that is, to www.someschool.edu. The Web cache 
then sends an HTTP request for the object into the cache-to-server TCP connec-
tion. After receiving this request, the origin server sends the object within an 
HTTP response to the Web cache.
	4.	 When the Web cache receives the object, it stores a copy in its local storage and 
sends a copy, within an HTTP response message, to the client browser (over the 
existing TCP connection between the client browser and the Web cache).
Note that a cache is both a server and a client at the same time. When it receives 
requests from and sends responses to a browser, it is a server. When it sends requests 
to and receives responses from an origin server, it is a client.
Typically a Web cache is purchased and installed by an ISP. For example, a uni-
versity might install a cache on its campus network and configure all of the campus 
browsers to point to the cache. Or a major residential ISP (such as Comcast) might 
install one or more caches in its network and preconfigure its shipped browsers to 
point to the installed caches.
Web caching has seen deployment in the Internet for two reasons. First, a Web 
cache can substantially reduce the response time for a client request, particularly if 
the bottleneck bandwidth between the client and the origin server is much less than 
the bottleneck bandwidth between the client and the cache. If there is a high-speed 
connection between the client and the cache, as there often is, and if the cache has 
the requested object, then the cache will be able to deliver the object rapidly to the 
client. Second, as we will soon illustrate with an example, Web caches can sub-
stantially reduce traffic on an institution’s access link to the Internet. By reducing 
traffic, the institution (for example, a company or a university) does not have to 
HTTP request
HTTP response
HTTP request
HTTP response
HTTP request
HTTP response
HTTP request
HTTP response
Client
Origin
server
Origin
server
Client
Proxy
server
Figure 2.11  ♦  Clients requesting objects through a Web cache
140         Chapter 2    •    Application Layer
upgrade bandwidth as quickly, thereby reducing costs. Furthermore, Web caches 
can substantially reduce Web traffic in the Internet as a whole, thereby improving 
performance for all applications.
To gain a deeper understanding of the benefits of caches, let’s consider an exam-
ple in the context of Figure 2.12. This figure shows two networks—the institutional 
network and the rest of the public Internet. The institutional network is a high-speed 
LAN. A router in the institutional network and a router in the Internet are connected 
by a 15 Mbps link. The origin servers are attached to the Internet but are located all 
over the globe. Suppose that the average object size is 1 Mbits and that the average 
request rate from the institution’s browsers to the origin servers is 15 requests per 
second. Suppose that the HTTP request messages are negligibly small and thus cre-
ate no traffic in the networks or in the access link (from institutional router to Internet 
router). Also suppose that the amount of time it takes from when the router on the 
Internet side of the access link in Figure 2.12 forwards an HTTP request (within an 
IP datagram) until it receives the response (typically within many IP datagrams) is 
two seconds on average. Informally, we refer to this last delay as the “Internet delay.”
Public Internet
Institutional network
15 Mbps access link
100 Mbps LAN
Origin servers
Figure 2.12  ♦  Bottleneck between an institutional network and the Internet
2.2    •    The Web and HTTP         141
The total response time—that is, the time from the browser’s request of an 
object until its receipt of the object—is the sum of the LAN delay, the access delay 
(that is, the delay between the two routers), and the Internet delay. Let’s now do 
a very crude calculation to estimate this delay. The traffic intensity on the LAN 
 
(see Section 1.4.2) is
(15 requests/sec) # (1 Mbits/request)/(100 Mbps) = 0.15
whereas the traffic intensity on the access link (from the Internet router to institution 
router) is
(15 requests/sec) # (1 Mbits/request)/(15 Mbps) = 1
A traffic intensity of 0.15 on a LAN typically results in, at most, tens of millisec-
onds of delay; hence, we can neglect the LAN delay. However, as discussed in 
Section 1.4.2, as the traffic intensity approaches 1 (as is the case of the access link 
in Figure 2.12), the delay on a link becomes very large and grows without bound. 
Thus, the average response time to satisfy requests is going to be on the order of 
minutes, if not more, which is unacceptable for the institution’s users. Clearly 
something must be done.
One possible solution is to increase the access rate from 15 Mbps to, say, 100 
Mbps. This will lower the traffic intensity on the access link to 0.15, which translates 
to negligible delays between the two routers. In this case, the total response time 
will roughly be two seconds, that is, the Internet delay. But this solution also means 
that the institution must upgrade its access link from 15 Mbps to 100 Mbps, a costly 
proposition.
Now consider the alternative solution of not upgrading the access link but 
instead installing a Web cache in the institutional network. This solution is illustrated 
in Figure 2.13. Hit rates—the fraction of requests that are satisfied by a cache— 
typically range from 0.2 to 0.7 in practice. For illustrative purposes, let’s suppose 
that the cache provides a hit rate of 0.4 for this institution. Because the clients and 
the cache are connected to the same high-speed LAN, 40 percent of the requests will 
be satisfied almost immediately, say, within 10 milliseconds, by the cache. Neverthe-
less, the remaining 60 percent of the requests still need to be satisfied by the origin 
servers. But with only 60 percent of the requested objects passing through the access 
link, the traffic intensity on the access link is reduced from 1.0 to 0.6. Typically, a 
traffic intensity less than 0.8 corresponds to a small delay, say, tens of milliseconds, 
on a 15 Mbps link. This delay is negligible compared with the two-second Internet 
delay. Given these considerations, average delay therefore is
0.4 # (0.01 seconds) + 0.6 # (2.01 seconds)
which is just slightly greater than 1.2 seconds. Thus, this second solution provides an 
even lower response time than the first solution, and it doesn’t require the institution 
142         Chapter 2    •    Application Layer
to upgrade its link to the Internet. The institution does, of course, have to purchase 
and install a Web cache. But this cost is low—many caches use public-domain soft-
ware that runs on inexpensive PCs.
Through the use of Content Distribution Networks (CDNs), Web caches are 
increasingly playing an important role in the Internet. A CDN company installs many 
geographically distributed caches throughout the Internet, thereby localizing much of 
the traffic. There are shared CDNs (such as Akamai and Limelight) and dedicated CDNs 
(such as Google and Netflix). We will discuss CDNs in more detail in Section 2.6.
The Conditional GET
Although caching can reduce user-perceived response times, it introduces a new 
problem—the copy of an object residing in the cache may be stale. In other words, 
the object housed in the Web server may have been modified since the copy was 
cached at the client. Fortunately, HTTP has a mechanism that allows a cache to 
Public Internet
Institutional network
15 Mbps access link
Institutional
cache
100 Mbps LAN
Origin servers
Figure 2.13  ♦  Adding a cache to the institutional network
2.2    •    The Web and HTTP         143
verify that its objects are up to date. This mechanism is called the conditional 
GET. An HTTP request message is a so-called conditional GET message if (1) 
the request message uses the GET method and (2) the request message includes an 
 
If-Modified-Since: header line.
To illustrate how the conditional GET operates, let’s walk through an example. 
First, on the behalf of a requesting browser, a proxy cache sends a request message 
to a Web server:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
Second, the Web server sends a response message with the requested object to the 
cache:
HTTP/1.1 200 OK
Date: Sat, 3 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)
Last-Modified: Wed, 9 Sep 2015 09:23:24
Content-Type: image/gif 
(data data data data data ...)
The cache forwards the object to the requesting browser but also caches the object 
locally. Importantly, the cache also stores the last-modified date along with the 
object. Third, one week later, another browser requests the same object via the cache, 
and the object is still in the cache. Since this object may have been modified at the 
Web server in the past week, the cache performs an up-to-date check by issuing a 
conditional GET. Specifically, the cache sends:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
If-modified-since: Wed, 9 Sep 2015 09:23:24
Note that the value of the If-modified-since: header line is exactly equal 
to the value of the Last-Modified: header line that was sent by the server one 
week ago. This conditional GET is telling the server to send the object only if the 
object has been modified since the specified date. Suppose the object has not been 
modified since 9 Sep 2015 09:23:24. Then, fourth, the Web server sends a response 
message to the cache:
HTTP/1.1 304 Not Modified
Date: Sat, 10 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix) 
(empty entity body)
144         Chapter 2    •    Application Layer
We see that in response to the conditional GET, the Web server still sends a 
response message but does not include the requested object in the response message. 
 
Including the requested object would only waste bandwidth and increase user- 
perceived response time, particularly if the object is large. Note that this last response 
message has 304 Not Modified in the status line, which tells the cache that it 
can go ahead and forward its (the proxy cache’s) cached copy of the object to the 
requesting browser.
This ends our discussion of HTTP, the first Internet protocol (an application-
layer protocol) that we’ve studied in detail. We’ve seen the format of HTTP mes-
sages and the actions taken by the Web client and server as these messages are 
sent and received. We’ve also studied a bit of the Web’s application infrastructure, 
including caches, cookies, and back-end databases, all of which are tied in some way 
to the HTTP protocol.
2.3	 Electronic Mail in the Internet
Electronic mail has been around since the beginning of the Internet. It was the most 
popular application when the Internet was in its infancy [Segaller 1998], and has 
become more elaborate and powerful over the years. It remains one of the Internet’s 
most important and utilized applications.
As with ordinary postal mail, e-mail is an asynchronous communication 
medium—people send and read messages when it is convenient for them, without 
having to coordinate with other people’s schedules. In contrast with postal mail, 
electronic mail is fast, easy to distribute, and inexpensive. Modern e-mail has 
many powerful features, including messages with attachments, hyperlinks, HTML- 
formatted text, and embedded photos.
In this section, we examine the application-layer protocols that are at the heart 
of Internet e-mail. But before we jump into an in-depth discussion of these protocols, 
let’s take a high-level view of the Internet mail system and its key components.
Figure 2.14 presents a high-level view of the Internet mail system. We see from 
this diagram that it has three major components: user agents, mail servers, and the 
Simple Mail Transfer Protocol (SMTP). We now describe each of these compo-
nents in the context of a sender, Alice, sending an e-mail message to a recipient, 
Bob. User agents allow users to read, reply to, forward, save, and compose mes-
sages. Microsoft Outlook and Apple Mail are examples of user agents for e-mail. 
When Alice is finished composing her message, her user agent sends the message to 
her mail server, where the message is placed in the mail server’s outgoing message 
queue. When Bob wants to read a message, his user agent retrieves the message from 
his mailbox in his mail server.
Mail servers form the core of the e-mail infrastructure. Each recipient, such as 
Bob, has a mailbox located in one of the mail servers. Bob’s mailbox manages and 
2.3    •    Electronic Mail in the Internet         145
maintains the messages that have been sent to him. A typical message starts its jour-
ney in the sender’s user agent, travels to the sender’s mail server, and travels to the 
recipient’s mail server, where it is deposited in the recipient’s mailbox. When Bob 
wants to access the messages in his mailbox, the mail server containing his mailbox 
authenticates Bob (with usernames and passwords). Alice’s mail server must also 
deal with failures in Bob’s mail server. If Alice’s server cannot deliver mail to Bob’s 
server, Alice’s server holds the message in a message queue and attempts to transfer 
the message later. Reattempts are often done every 30 minutes or so; if there is no 
success after several days, the server removes the message and notifies the sender 
(Alice) with an e-mail message.
SMTP is the principal application-layer protocol for Internet electronic mail. It 
uses the reliable data transfer service of TCP to transfer mail from the sender’s mail 
server to the recipient’s mail server. As with most application-layer protocols, SMTP 
Outgoing
message queue 
Key:
User mailbox
SMTP
User agent
User agent
User agent
User agent
User agent
User agent
Mail server
Mail server
Mail server
SMTP
SMTP
Figure 2.14  ♦  A high-level view of the Internet e-mail system
146         Chapter 2    •    Application Layer
has two sides: a client side, which executes on the sender’s mail server, and a server 
side, which executes on the recipient’s mail server. Both the client and server sides of 
SMTP run on every mail server. When a mail server sends mail to other mail servers, 
it acts as an SMTP client. When a mail server receives mail from other mail servers, 
it acts as an SMTP server.
2.3.1 SMTP
SMTP, defined in RFC 5321, is at the heart of Internet electronic mail. As men-
tioned above, SMTP transfers messages from senders’ mail servers to the recipients’ 
mail servers. SMTP is much older than HTTP. (The original SMTP RFC dates back 
to 1982, and SMTP was around long before that.) Although SMTP has numerous 
wonderful qualities, as evidenced by its ubiquity in the Internet, it is nevertheless 
a legacy technology that possesses certain archaic characteristics. For example, it 
restricts the body (not just the headers) of all mail messages to simple 7-bit ASCII. 
This restriction made sense in the early 1980s when transmission capacity was scarce 
and no one was e-mailing large attachments or large image, audio, or video files. But 
today, in the multimedia era, the 7-bit ASCII restriction is a bit of a pain—it requires 
binary multimedia data to be encoded to ASCII before being sent over SMTP; and it 
requires the corresponding ASCII message to be decoded back to binary after SMTP 
transport. Recall from Section 2.2 that HTTP does not require multimedia data to be 
ASCII encoded before transfer.
To illustrate the basic operation of SMTP, let’s walk through a common sce-
nario. Suppose Alice wants to send Bob a simple ASCII message.
	1.	 Alice invokes her user agent for e-mail, provides Bob’s e-mail address (for 
example, bob@someschool.edu), composes a message, and instructs the 
user agent to send the message.
	2.	 Alice’s user agent sends the message to her mail server, where it is placed in a 
message queue.
	3.	 The client side of SMTP, running on Alice’s mail server, sees the message in the 
message queue. It opens a TCP connection to an SMTP server, running on Bob’s 
mail server.
	4.	 After some initial SMTP handshaking, the SMTP client sends Alice’s message 
into the TCP connection.
	5.	 At Bob’s mail server, the server side of SMTP receives the message. Bob’s mail 
server then places the message in Bob’s mailbox.
	6.	 Bob invokes his user agent to read the message at his convenience.
The scenario is summarized in Figure 2.15.
It is important to observe that SMTP does not normally use intermediate mail serv-
ers for sending mail, even when the two mail servers are located at opposite ends of 
the world. If Alice’s server is in Hong Kong and Bob’s server is in St. Louis, the TCP 
2.3    •    Electronic Mail in the Internet         147
connection is a direct connection between the Hong Kong and St. Louis servers. In 
particular, if Bob’s mail server is down, the message remains in Alice’s mail server and 
waits for a new attempt—the message does not get placed in some intermediate mail 
server.
Let’s now take a closer look at how SMTP transfers a message from a sending mail 
server to a receiving mail server. We will see that the SMTP protocol has many simi-
larities with protocols that are used for face-to-face human interaction. First, the client 
SMTP (running on the sending mail server host) has TCP establish a connection to port 
25 at the server SMTP (running on the receiving mail server host). If the server is down, 
the client tries again later. Once this connection is established, the server and client per-
form some application-layer handshaking—just as humans often introduce themselves 
before transferring information from one to another, SMTP clients and servers intro-
duce themselves before transferring information. During this SMTP handshaking phase, 
 
the SMTP client indicates the e-mail address of the sender (the person who generated 
the message) and the e-mail address of the recipient. Once the SMTP client and server 
have introduced themselves to each other, the client sends the message. SMTP can 
count on the reliable data transfer service of TCP to get the message to the server with-
out errors. The client then repeats this process over the same TCP connection if it has 
other messages to send to the server; otherwise, it instructs TCP to close the connection.
Let’s next take a look at an example transcript of messages exchanged between an 
SMTP client (C) and an SMTP server (S). The hostname of the client is crepes.fr 
 
and the hostname of the server is hamburger.edu. The ASCII text lines prefaced 
with C: are exactly the lines the client sends into its TCP socket, and the ASCII text 
lines prefaced with S: are exactly the lines the server sends into its TCP socket. The 
following transcript begins as soon as the TCP connection is established.
S:  220 hamburger.edu
C:  HELO crepes.fr
SMTP
Alice’s
mail server
Bob’s
mail server
Alice’s
agent
Bob’s
agent
1
2
4
6
5
Message queue 
Key:
User mailbox
3
Figure 2.15  ♦  Alice sends a message to Bob
148         Chapter 2    •    Application Layer
S:  250 Hello crepes.fr, pleased to meet you
C:  MAIL FROM: <alice@crepes.fr>
S:  250 alice@crepes.fr ... Sender ok
C:  RCPT TO: <bob@hamburger.edu>
S:  250 bob@hamburger.edu ... Recipient ok
C:  DATA
S:  354 Enter mail, end with ”.” on a line by itself
C:  Do you like ketchup?
C:  How about pickles?
C:  .
S:  250 Message accepted for delivery
C:  QUIT
S:  221 hamburger.edu closing connection
In the example above, the client sends a message (“Do you like ketchup? 
How about pickles?”) from mail server crepes.fr to mail server 
 
hamburger.edu. As part of the dialogue, the client issued five commands: 
HELO (an abbreviation for HELLO), MAIL FROM, RCPT TO, DATA, and QUIT. 
These commands are self-explanatory. The client also sends a line consisting of a 
single period, which indicates the end of the message to the server. (In ASCII jar-
gon, each message ends with CRLF.CRLF, where CR and LF stand for carriage 
return and line feed, respectively.) The server issues replies to each command, 
with each reply having a reply code and some (optional) English-language expla-
nation. We mention here that SMTP uses persistent connections: If the sending 
mail server has several messages to send to the same receiving mail server, it can 
send all of the messages over the same TCP connection. For each message, the 
client begins the process with a new MAIL FROM: crepes.fr, designates the 
end of message with an isolated period, and issues QUIT only after all messages 
have been sent.
It is highly recommended that you use Telnet to carry out a direct dialogue with 
an SMTP server. To do this, issue
telnet serverName 25
where serverName is the name of a local mail server. When you do this, you are 
simply establishing a TCP connection between your local host and the mail server. 
After typing this line, you should immediately receive the 220 reply from the 
server. Then issue the SMTP commands HELO, MAIL FROM, RCPT TO, DATA, 
CRLF.CRLF, and QUIT at the appropriate times. It is also highly recommended 
that you do Programming Assignment 3 at the end of this chapter. In that assign-
ment, you’ll build a simple user agent that implements the client side of SMTP. It 
will allow you to send an e-mail message to an arbitrary recipient via a local mail 
server.
2.3    •    Electronic Mail in the Internet         149
2.3.2 Comparison with HTTP
Let’s now briefly compare SMTP with HTTP. Both protocols are used to transfer 
files from one host to another: HTTP transfers files (also called objects) from a Web 
server to a Web client (typically a browser); SMTP transfers files (that is, e-mail 
messages) from one mail server to another mail server. When transferring the files, 
both persistent HTTP and SMTP use persistent connections. Thus, the two protocols 
have common characteristics. However, there are important differences. First, HTTP 
is mainly a pull protocol—someone loads information on a Web server and users 
use HTTP to pull the information from the server at their convenience. In particular, 
the TCP connection is initiated by the machine that wants to receive the file. On the 
other hand, SMTP is primarily a push protocol—the sending mail server pushes 
 
the file to the receiving mail server. In particular, the TCP connection is initiated by 
the machine that wants to send the file.
A second difference, which we alluded to earlier, is that SMTP requires each 
message, including the body of each message, to be in 7-bit ASCII format. If the 
message contains characters that are not 7-bit ASCII (for example, French characters 
with accents) or contains binary data (such as an image file), then the message has to 
be encoded into 7-bit ASCII. HTTP data does not impose this restriction.
A third important difference concerns how a document consisting of text 
and images (along with possibly other media types) is handled. As we learned in 
 
Section 2.2, HTTP encapsulates each object in its own HTTP response message. 
SMTP places all of the message’s objects into one message.
2.3.3 Mail Message Formats
When Alice writes an ordinary snail-mail letter to Bob, she may include all kinds 
of peripheral header information at the top of the letter, such as Bob’s address, her 
own return address, and the date. Similarly, when an e-mail message is sent from 
one person to another, a header containing peripheral information precedes the 
body of the message itself. This peripheral information is contained in a series of 
header lines, which are defined in RFC 5322. The header lines and the body of the 
message are separated by a blank line (that is, by CRLF). RFC 5322 specifies the 
exact format for mail header lines as well as their semantic interpretations. As with 
HTTP, each header line contains readable text, consisting of a keyword followed 
by a colon followed by a value. Some of the keywords are required and others are 
optional. Every header must have a From: header line and a To: header line; 
a header may include a Subject: header line as well as other optional header 
lines. It is important to note that these header lines are different from the SMTP 
commands we studied in Section 2.4.1 (even though they contain some common 
words such as “from” and “to”). The commands in that section were part of the 
SMTP handshaking protocol; the header lines examined in this section are part of 
the mail message itself.
150         Chapter 2    •    Application Layer
A typical message header looks like this:
From: alice@crepes.fr
To: bob@hamburger.edu
Subject: Searching for the meaning of life.
After the message header, a blank line follows; then the message body (in ASCII) 
follows. You should use Telnet to send a message to a mail server that contains 
some header lines, including the Subject: header line. To do this, issue telnet 
serverName 25, as discussed in Section 2.4.1.
2.3.4 Mail Access Protocols
Once SMTP delivers the message from Alice’s mail server to Bob’s mail server, 
the message is placed in Bob’s mailbox. Throughout this discussion we have tacitly 
assumed that Bob reads his mail by logging onto the server host and then executing a 
mail reader that runs on that host. Up until the early 1990s this was the standard way 
of doing things. But today, mail access uses a client-server architecture—the typical 
user reads e-mail with a client that executes on the user’s end system, for example, 
on an office PC, a laptop, or a smartphone. By executing a mail client on a local PC, 
users enjoy a rich set of features, including the ability to view multimedia messages 
and attachments.
Given that Bob (the recipient) executes his user agent on his local PC, it is natural 
to consider placing a mail server on his local PC as well. With this approach, Alice’s 
mail server would dialogue directly with Bob’s PC. There is a problem with this 
approach, however. Recall that a mail server manages mailboxes and runs the client 
and server sides of SMTP. If Bob’s mail server were to reside on his local PC, then 
Bob’s PC would have to remain always on, and connected to the Internet, in order to 
receive new mail, which can arrive at any time. This is impractical for many Internet 
users. Instead, a typical user runs a user agent on the local PC but accesses its mailbox 
stored on an always-on shared mail server. This mail server is shared with other users 
and is typically maintained by the user’s ISP (for example, university or company).
Now let’s consider the path an e-mail message takes when it is sent from Alice 
to Bob. We just learned that at some point along the path the e-mail message needs 
to be deposited in Bob’s mail server. This could be done simply by having Alice’s 
user agent send the message directly to Bob’s mail server. And this could be done 
with SMTP—indeed, SMTP has been designed for pushing e-mail from one host to 
another. However, typically the sender’s user agent does not dialogue directly with 
the recipient’s mail server. Instead, as shown in Figure 2.16, Alice’s user agent uses 
SMTP to push the e-mail message into her mail server, then Alice’s mail server uses 
SMTP (as an SMTP client) to relay the e-mail message to Bob’s mail server. Why 
the two-step procedure? Primarily because without relaying through Alice’s mail 
server, Alice’s user agent doesn’t have any recourse to an unreachable destination 
2.3    •    Electronic Mail in the Internet         151
mail server. By having Alice first deposit the e-mail in her own mail server, Alice’s 
mail server can repeatedly try to send the message to Bob’s mail server, say every 
30 minutes, until Bob’s mail server becomes operational. (And if Alice’s mail server 
is down, then she has the recourse of complaining to her system administrator!) The 
SMTP RFC defines how the SMTP commands can be used to relay a message across 
multiple SMTP servers.
But there is still one missing piece to the puzzle! How does a recipient like Bob, 
running a user agent on his local PC, obtain his messages, which are sitting in a mail 
server within Bob’s ISP? Note that Bob’s user agent can’t use SMTP to obtain the 
messages because obtaining the messages is a pull operation, whereas SMTP is a 
push protocol. The puzzle is completed by introducing a special mail access protocol 
that transfers messages from Bob’s mail server to his local PC. There are currently a 
number of popular mail access protocols, including Post Office Protocol—Version 
3 (POP3), Internet Mail Access Protocol (IMAP), and HTTP.
Figure 2.16 provides a summary of the protocols that are used for Internet mail: 
SMTP is used to transfer mail from the sender’s mail server to the recipient’s mail 
server; SMTP is also used to transfer mail from the sender’s user agent to the send-
er’s mail server. A mail access protocol, such as POP3, is used to transfer mail from 
the recipient’s mail server to the recipient’s user agent.
POP3
POP3 is an extremely simple mail access protocol. It is defined in [RFC 1939], 
which is short and quite readable. Because the protocol is so simple, its functionality 
is rather limited. POP3 begins when the user agent (the client) opens a TCP connec-
tion to the mail server (the server) on port 110. With the TCP connection established, 
POP3 progresses through three phases: authorization, transaction, and update. Dur-
ing the first phase, authorization, the user agent sends a username and a password 
(in the clear) to authenticate the user. During the second phase, transaction, the user 
agent retrieves messages; also during this phase, the user agent can mark messages 
for deletion, remove deletion marks, and obtain mail statistics. The third phase, 
update, occurs after the client has issued the quit command, ending the POP3 ses-
sion; at this time, the mail server deletes the messages that were marked for deletion.
SMTP
Alice’s
mail server
Bob’s
mail server
Alice’s
agent
Bob’s
agent
SMTP
POP3,
IMAP, or
HTTP
Figure 2.16  ♦  E-mail protocols and their communicating entities
152         Chapter 2    •    Application Layer
In a POP3 transaction, the user agent issues commands, and the server responds 
to each command with a reply. There are two possible responses: +OK (sometimes 
followed by server-to-client data), used by the server to indicate that the previous 
command was fine; and -ERR, used by the server to indicate that something was 
wrong with the previous command.
The authorization phase has two principal commands: user <username> and 
pass <password>. To illustrate these two commands, we suggest that you Telnet 
directly into a POP3 server, using port 110, and issue these commands. Suppose that 
mailServer is the name of your mail server. You will see something like:
telnet mailServer 110
+OK POP3 server ready
user bob
+OK
pass hungry
+OK user successfully logged on
If you misspell a command, the POP3 server will reply with an -ERR message.
Now let’s take a look at the transaction phase. A user agent using POP3 can 
often be configured (by the user) to “download and delete” or to “download and 
keep.” The sequence of commands issued by a POP3 user agent depends on which 
of these two modes the user agent is operating in. In the download-and-delete mode, 
the user agent will issue the list, retr, and dele commands. As an example, 
suppose the user has two messages in his or her mailbox. In the dialogue below, C: 
(standing for client) is the user agent and S: (standing for server) is the mail server. 
The transaction will look something like:
C: list
S: 1 498
S: 2 912
S: .
C: retr 1
S: (blah blah ...
S: .................
S: ..........blah)
S: .
C: dele 1
C: retr 2
S: (blah blah ...
S: .................
S: ..........blah)
S: .
C: dele 2
2.3    •    Electronic Mail in the Internet         153
C: quit
S: +OK POP3 server signing off
The user agent first asks the mail server to list the size of each of the stored messages. 
The user agent then retrieves and deletes each message from the server. Note that 
after the authorization phase, the user agent employed only four commands: list, 
retr, dele, and quit. The syntax for these commands is defined in RFC 1939. 
After processing the quit command, the POP3 server enters the update phase and 
removes messages 1 and 2 from the mailbox.
A problem with this download-and-delete mode is that the recipient, Bob, may 
be nomadic and may want to access his mail messages from multiple machines, for 
example, his office PC, his home PC, and his portable computer. The download- and-
delete mode partitions Bob’s mail messages over these three machines; in particular, 
if Bob first reads a message on his office PC, he will not be able to reread the mes-
sage from his portable at home later in the evening. In the download-and-keep mode, 
the user agent leaves the messages on the mail server after downloading them. In this 
case, Bob can reread messages from different machines; he can access a message 
from work and access it again later in the week from home.
During a POP3 session between a user agent and the mail server, the POP3 
server maintains some state information; in particular, it keeps track of which user 
messages have been marked deleted. However, the POP3 server does not carry state 
information across POP3 sessions. This lack of state information across sessions 
greatly simplifies the implementation of a POP3 server.
IMAP
With POP3 access, once Bob has downloaded his messages to the local machine, he 
can create mail folders and move the downloaded messages into the folders. Bob can 
then delete messages, move messages across folders, and search for messages (by 
sender name or subject). But this paradigm—namely, folders and messages in the 
local machine—poses a problem for the nomadic user, who would prefer to maintain 
a folder hierarchy on a remote server that can be accessed from any computer. This 
is not possible with POP3—the POP3 protocol does not provide any means for a user 
to create remote folders and assign messages to folders.
To solve this and other problems, the IMAP protocol, defined in [RFC 3501], 
was invented. Like POP3, IMAP is a mail access protocol. It has many more features 
than POP3, but it is also significantly more complex. (And thus the client and server 
side implementations are significantly more complex.)
An IMAP server will associate each message with a folder; when a message first 
arrives at the server, it is associated with the recipient’s INBOX folder. The recipient 
can then move the message into a new, user-created folder, read the message, delete 
the message, and so on. The IMAP protocol provides commands to allow users to 
create folders and move messages from one folder to another. IMAP also provides 
154         Chapter 2    •    Application Layer
commands that allow users to search remote folders for messages matching specific 
criteria. Note that, unlike POP3, an IMAP server maintains user state information 
across IMAP sessions—for example, the names of the folders and which messages 
are associated with which folders.
Another important feature of IMAP is that it has commands that permit a user 
agent to obtain components of messages. For example, a user agent can obtain just 
the message header of a message or just one part of a multipart MIME message. This 
feature is useful when there is a low-bandwidth connection (for example, a slow-speed 
modem link) between the user agent and its mail server. With a low-bandwidth connec-
tion, the user may not want to download all of the messages in its mailbox, particularly 
avoiding long messages that might contain, for example, an audio or video clip.
Web-Based E-Mail
More and more users today are sending and accessing their e-mail through their Web 
browsers. Hotmail introduced Web-based access in the mid 1990s. Now Web-based 
e-mail is also provided by Google, Yahoo!, as well as just about every major univer-
sity and corporation. With this service, the user agent is an ordinary Web browser, 
and the user communicates with its remote mailbox via HTTP. When a recipient, 
such as Bob, wants to access a message in his mailbox, the e-mail message is sent 
from Bob’s mail server to Bob’s browser using the HTTP protocol rather than the 
POP3 or IMAP protocol. When a sender, such as Alice, wants to send an e-mail 
message, the e-mail message is sent from her browser to her mail server over HTTP 
rather than over SMTP. Alice’s mail server, however, still sends messages to, and 
receives messages from, other mail servers using SMTP.
2.4	 DNS—The Internet’s Directory Service
We human beings can be identified in many ways. For example, we can be identified 
by the names that appear on our birth certificates. We can be identified by our social 
security numbers. We can be identified by our driver’s license numbers. Although 
each of these identifiers can be used to identify people, within a given context one 
identifier may be more appropriate than another. For example, the computers at the 
IRS (the infamous tax-collecting agency in the United States) prefer to use fixed-
length social security numbers rather than birth certificate names. On the other hand, 
ordinary people prefer the more mnemonic birth certificate names rather than social 
security numbers. (Indeed, can you imagine saying, “Hi. My name is 132-67-9875. 
Please meet my husband, 178-87-1146.”)
Just as humans can be identified in many ways, so too can Internet hosts. One 
identifier for a host is its hostname. Hostnames—such as www.facebook.com, 
www.google.com, gaia.cs.umass.edu—are mnemonic and are therefore 
2.4    •    DNS—The Internet’s Directory Service         155
appreciated by humans. However, hostnames provide little, if any, information about 
the location within the Internet of the host. (A hostname such as www.eurecom.
fr, which ends with the country code .fr, tells us that the host is probably in 
France, but doesn’t say much more.) Furthermore, because hostnames can consist of 
variable-length alphanumeric characters, they would be difficult to process by rout-
ers. For these reasons, hosts are also identified by so-called IP addresses.
We discuss IP addresses in some detail in Chapter 4, but it is useful to say a 
few brief words about them now. An IP address consists of four bytes and has a 
rigid hierarchical structure. An IP address looks like 121.7.106.83, where each 
period separates one of the bytes expressed in decimal notation from 0 to 255. An IP 
address is hierarchical because as we scan the address from left to right, we obtain 
more and more specific information about where the host is located in the Internet 
(that is, within which network, in the network of networks). Similarly, when we scan 
a postal address from bottom to top, we obtain more and more specific information 
about where the addressee is located.
2.4.1 Services Provided by DNS
We have just seen that there are two ways to identify a host—by a hostname and 
by an IP address. People prefer the more mnemonic hostname identifier, while 
routers prefer fixed-length, hierarchically structured IP addresses. In order to rec-
oncile these preferences, we need a directory service that translates hostnames to 
IP addresses. This is the main task of the Internet’s domain name system (DNS). 
The DNS is (1) a distributed database implemented in a hierarchy of DNS servers, 
 
and (2) an application-layer protocol that allows hosts to query the distributed 
database. The DNS servers are often UNIX machines running the Berkeley Inter-
net Name Domain (BIND) software [BIND 2016]. The DNS protocol runs over 
UDP and uses port 53.
DNS is commonly employed by other application-layer protocols—including 
HTTP and SMTP to translate user-supplied hostnames to IP addresses. As an exam-
ple, consider what happens when a browser (that is, an HTTP client), running on 
some user’s host, requests the URL www.someschool.edu/index.html. In 
order for the user’s host to be able to send an HTTP request message to the Web 
server www.someschool.edu, the user’s host must first obtain the IP address of 
www.someschool.edu. This is done as follows.
	1.	 The same user machine runs the client side of the DNS application.
	2.	 The browser extracts the hostname, www.someschool.edu, from the URL 
and passes the hostname to the client side of the DNS application.
	3.	 The DNS client sends a query containing the hostname to a DNS server.
	4.	 The DNS client eventually receives a reply, which includes the IP address for 
the hostname.
	5.	 Once the browser receives the IP address from DNS, it can initiate a TCP con-
nection to the HTTP server process located at port 80 at that IP address.
156         Chapter 2    •    Application Layer
We see from this example that DNS adds an additional delay—sometimes 
 
substantial—to the Internet applications that use it. Fortunately, as we discuss below, 
the desired IP address is often cached in a “nearby” DNS server, which helps to 
reduce DNS network traffic as well as the average DNS delay.
DNS provides a few other important services in addition to translating host-
names to IP addresses:
•	 Host aliasing. A host with a complicated hostname can have one or more 
 
alias names. For example, a hostname such as relay1.west-coast 
.enterprise.com could have, say, two aliases such as enterprise.com  
and www.enterprise.com. In this case, the hostname relay1 
.west-coast.enterprise.com is said to be a canonical hostname. Alias 
hostnames, when present, are typically more mnemonic than canonical host-
names. DNS can be invoked by an application to obtain the canonical hostname 
for a supplied alias hostname as well as the IP address of the host.
•	 Mail server aliasing. For obvious reasons, it is highly desirable that e-mail 
addresses be mnemonic. For example, if Bob has an account with Yahoo Mail, 
Bob’s e-mail address might be as simple as bob@yahoo.mail. However, the 
hostname of the Yahoo mail server is more complicated and much less mnemonic 
than simply yahoo.com (for example, the canonical hostname might be some-
thing like relay1.west-coast.yahoo.com). DNS can be invoked by a 
mail application to obtain the canonical hostname for a supplied alias hostname 
as well as the IP address of the host. In fact, the MX record (see below) permits a 
company’s mail server and Web server to have identical (aliased) hostnames; for 
example, a company’s Web server and mail server can both be called enter-
prise.com.
•	 Load distribution. DNS is also used to perform load distribution among repli-
cated servers, such as replicated Web servers. Busy sites, such as cnn.com, are 
replicated over multiple servers, with each server running on a different end sys-
tem and each having a different IP address. For replicated Web servers, a set of IP 
addresses is thus associated with one canonical hostname. The DNS database con-
tains this set of IP addresses. When clients make a DNS query for a name mapped 
to a set of addresses, the server responds with the entire set of IP addresses, but 
rotates the ordering of the addresses within each reply. Because a client typically 
sends its HTTP request message to the IP address that is listed first in the set, DNS 
rotation distributes the traffic among the replicated servers. DNS rotation is also 
used for e-mail so that multiple mail servers can have the same alias name. Also, 
content distribution companies such as Akamai have used DNS in more sophisti-
cated ways [Dilley 2002] to provide Web content distribution (see Section 2.6.3).
The DNS is specified in RFC 1034 and RFC 1035, and updated in several addi-
tional RFCs. It is a complex system, and we only touch upon key aspects of its 
2.4    •    DNS—The Internet’s Directory Service         157
operation here. The interested reader is referred to these RFCs and the book by Albitz 
and Liu [Albitz 1993]; see also the retrospective paper [Mockapetris 1988], which 
provides a nice description of the what and why of DNS, and [Mockapetris 2005].
2.4.2 Overview of How DNS Works
We now present a high-level overview of how DNS works. Our discussion will focus 
on the hostname-to-IP-address translation service.
Suppose that some application (such as a Web browser or a mail reader) running 
in a user’s host needs to translate a hostname to an IP address. The application will 
invoke the client side of DNS, specifying the hostname that needs to be translated. 
(On many UNIX-based machines, gethostbyname() is the function call that 
an application calls in order to perform the translation.) DNS in the user’s host then 
takes over, sending a query message into the network. All DNS query and reply mes-
sages are sent within UDP datagrams to port 53. After a delay, ranging from millisec-
onds to seconds, DNS in the user’s host receives a DNS reply message that provides 
the desired mapping. This mapping is then passed to the invoking application. Thus, 
from the perspective of the invoking application in the user’s host, DNS is a black 
box providing a simple, straightforward translation service. But in fact, the black box 
that implements the service is complex, consisting of a large number of DNS servers 
distributed around the globe, as well as an application-layer protocol that specifies 
how the DNS servers and querying hosts communicate.
A simple design for DNS would have one DNS server that contains all the map-
pings. In this centralized design, clients simply direct all queries to the single DNS 
server, and the DNS server responds directly to the querying clients. Although the 
DNS: CRITICAL NETWORK FUNCTIONS VIA THE CLIENT-SERVER PARADIGM
Like HTTP, FTP, and SMTP, the DNS protocol is an application-layer protocol since it (1) 
runs between communicating end systems using the client-server paradigm and (2) relies 
on an underlying end-to-end transport protocol to transfer DNS messages between com-
municating end systems. In another sense, however, the role of the DNS is quite different 
from Web, file transfer, and e-mail applications. Unlike these applications, the DNS is 
not an application with which a user directly interacts. Instead, the DNS provides a core 
Internet function—namely, translating hostnames to their underlying IP addresses, for user 
applications and other software in the Internet. We noted in Section 1.2 that much of the 
complexity in the Internet architecture is located at the “edges” of the network. The DNS, 
which implements the critical name-to-address translation process using clients and servers 
located at the edge of the network, is yet another example of that design philosophy.
PRINCIPLES IN PRACTICE
158         Chapter 2    •    Application Layer
simplicity of this design is attractive, it is inappropriate for today’s Internet, with its 
vast (and growing) number of hosts. The problems with a centralized design include:
•	 A single point of failure. If the DNS server crashes, so does the entire Internet!
•	 Traffic volume. A single DNS server would have to handle all DNS queries (for 
all the HTTP requests and e-mail messages generated from hundreds of millions 
of hosts).
•	 Distant centralized database. A single DNS server cannot be “close to” all the 
querying clients. If we put the single DNS server in New York City, then all que-
ries from Australia must travel to the other side of the globe, perhaps over slow 
and congested links. This can lead to significant delays.
•	 Maintenance. The single DNS server would have to keep records for all Internet 
hosts. Not only would this centralized database be huge, but it would have to be 
updated frequently to account for every new host.
In summary, a centralized database in a single DNS server simply doesn’t scale. 
Consequently, the DNS is distributed by design. In fact, the DNS is a wonderful 
example of how a distributed database can be implemented in the Internet.
A Distributed, Hierarchical Database
In order to deal with the issue of scale, the DNS uses a large number of servers, 
organized in a hierarchical fashion and distributed around the world. No single DNS 
server has all of the mappings for all of the hosts in the Internet. Instead, the map-
pings are distributed across the DNS servers. To a first approximation, there are three 
classes of DNS servers—root DNS servers, top-level domain (TLD) DNS servers, 
and authoritative DNS servers—organized in a hierarchy as shown in Figure 2.17. 
To understand how these three classes of servers interact, suppose a DNS client 
wants to determine the IP address for the hostname www.amazon.com. To a first 
edu DNS servers
org DNS servers
com DNS servers
nyu.edu
DNS servers
facebook.com
DNS servers
amazon.com
DNS servers
pbs.org
DNS servers
umass.edu
DNS servers
Root DNS servers
Figure 2.17  ♦  Portion of the hierarchy of DNS servers
2.4    •    DNS—The Internet’s Directory Service         159
approximation, the following events will take place. The client first contacts one of 
the root servers, which returns IP addresses for TLD servers for the top-level domain 
com. The client then contacts one of these TLD servers, which returns the IP address 
of an authoritative server for amazon.com. Finally, the client contacts one of the 
authoritative servers for amazon.com, which returns the IP address for the host-
name www.amazon.com. We’ll soon examine this DNS lookup process in more 
detail. But let’s first take a closer look at these three classes of DNS servers:
•	 Root DNS servers. There are over 400 root name servers scattered all over the 
world. Figure 2.18 shows the countries that have root names servers, with coun-
tries having more than ten darkly shaded. These root name servers are managed 
by 13 different organizations. The full list of root name servers, along with the 
organizations that manage them and their IP addresses can be found at [Root 
Servers 2016]. Root name servers provide the IP addresses of the TLD servers.
•	 Top-level domain (TLD) servers. For each of the top-level domains — top-level 
domains such as com, org, net, edu, and gov, and all of the country top-level 
domains such as uk, fr, ca, and jp — there is TLD server (or server cluster). The 
company Verisign Global Registry Services maintains the TLD servers for the 
com top-level domain, and the company Educause maintains the TLD servers for 
the edu top-level domain. The network infrastructure supporting a TLD can be 
large and complex; see [Osterweil 2012] for a nice overview of the Verisign net-
work. See [TLD list 2016] for a list of all top-level domains. TLD servers provide 
the IP addresses for authoritative DNS servers.
0 Servers
1–10 Servers
11+ Servers
Key:
Figure 2.18  ♦  DNS root servers in 2016
160         Chapter 2    •    Application Layer
•	 Authoritative DNS servers. Every organization with publicly accessible hosts 
(such as Web servers and mail servers) on the Internet must provide publicly 
accessible DNS records that map the names of those hosts to IP addresses. An 
organization’s authoritative DNS server houses these DNS records. An organi-
zation can choose to implement its own authoritative DNS server to hold these 
records; alternatively, the organization can pay to have these records stored in an 
authoritative DNS server of some service provider. Most universities and large 
companies implement and maintain their own primary and secondary (backup) 
authoritative DNS server.
The root, TLD, and authoritative DNS servers all belong to the hierarchy of 
DNS servers, as shown in Figure 2.17. There is another important type of DNS server 
called the local DNS server. A local DNS server does not strictly belong to the hier-
archy of servers but is nevertheless central to the DNS architecture. Each ISP—such 
as a residential ISP or an institutional ISP—has a local DNS server (also called a 
default name server). When a host connects to an ISP, the ISP provides the host with 
the IP addresses of one or more of its local DNS servers (typically through DHCP, 
which is discussed in Chapter 4). You can easily determine the IP address of your 
local DNS server by accessing network status windows in Windows or UNIX. A 
host’s local DNS server is typically “close to” the host. For an institutional ISP, the 
local DNS server may be on the same LAN as the host; for a residential ISP, it is 
typically separated from the host by no more than a few routers. When a host makes 
a DNS query, the query is sent to the local DNS server, which acts a proxy, forward-
ing the query into the DNS server hierarchy, as we’ll discuss in more detail below.
Let’s take a look at a simple example. Suppose the host cse.nyu.edu desires 
the IP address of gaia.cs.umass.edu. Also suppose that NYU’s ocal DNS 
server for cse.nyu.edu is called dns.nyu.edu and that an authoritative DNS 
server for gaia.cs.umass.edu is called dns.umass.edu. As shown in Fig-
ure 2.19, the host cse.nyu.edu first sends a DNS query message to its local DNS 
server, dns.nyu.edu. The query message contains the hostname to be translated, 
namely, gaia.cs.umass.edu. The local DNS server forwards the query mes-
sage to a root DNS server. The root DNS server takes note of the edu suffix and 
returns to the local DNS server a list of IP addresses for TLD servers responsible 
for edu. The local DNS server then resends the query message to one of these TLD 
servers. The TLD server takes note of the umass.edu suffix and responds with 
the IP address of the authoritative DNS server for the University of Massachusetts, 
namely, dns.umass.edu. Finally, the local DNS server resends the query mes-
sage directly to dns.umass.edu, which responds with the IP address of gaia 
.cs.umass.edu. Note that in this example, in order to obtain the mapping for one 
hostname, eight DNS messages were sent: four query messages and four reply mes-
sages! We’ll soon see how DNS caching reduces this query traffic.
Our previous example assumed that the TLD server knows the authoritative 
DNS server for the hostname. In general this not always true. Instead, the TLD server 
2.4    •    DNS—The Internet’s Directory Service         161
may know only of an intermediate DNS server, which in turn knows the authorita-
tive DNS server for the hostname. For example, suppose again that the University of 
Massachusetts has a DNS server for the university, called dns.umass.edu. Also 
suppose that each of the departments at the University of Massachusetts has its own 
DNS server, and that each departmental DNS server is authoritative for all hosts in 
the department. In this case, when the intermediate DNS server, dns.umass.edu, 
receives a query for a host with a hostname ending with cs.umass.edu, it returns 
to dns.nyu.edu the IP address of dns.cs.umass.edu, which is authoritative 
for all hostnames ending with cs.umass.edu. The local DNS server dns.nyu 
.edu then sends the query to the authoritative DNS server, which returns the desired 
mapping to the local DNS server, which in turn returns the mapping to the requesting 
host. In this case, a total of 10 DNS messages are sent!
The example shown in Figure 2.19 makes use of both recursive queries 
and iterative queries. The query sent from cse.nyu.edu to dns.nyu.edu 
Requesting host
cse.nyu.edu
Local DNS server
TLD DNS server
dns.nyu.edu
Root DNS server
1
8
2
7
4
5
3
6
Authoritative DNS server
dns.umass.edu
gaia.cs.umass.edu
Figure 2.19  ♦  Interaction of the various DNS servers
162         Chapter 2    •    Application Layer
is a recursive query, since the query asks dns.nyu.edu to obtain the mapping 
on its behalf. But the subsequent three queries are iterative since all of the replies 
are directly returned to dns.nyu.edu. In theory, any DNS query can be itera-
tive or recursive. For example, Figure 2.20 shows a DNS query chain for which all 
 
of the queries are recursive. In practice, the queries typically follow the pattern in 
Figure 2.19: The query from the requesting host to the local DNS server is recursive, 
and the remaining queries are iterative.
DNS Caching
Our discussion thus far has ignored DNS caching, a critically important feature 
of the DNS system. In truth, DNS extensively exploits DNS caching in order to 
improve the delay performance and to reduce the number of DNS messages 
 
Requesting host
cse.nyu.edu
Local DNS server
TLD DNS server
dns.nyu.edu
Root DNS server
1
8
5
4
2
7
Authoritative DNS server
dns.umass.edu
gaia.cs.umass.edu
6
3
Figure 2.20  ♦  Recursive queries in DNS
2.4    •    DNS—The Internet’s Directory Service         163
ricocheting around the Internet. The idea behind DNS caching is very simple. In a 
query chain, when a DNS server receives a DNS reply (containing, for example, a 
mapping from a hostname to an IP address), it can cache the mapping in its local 
memory. For example, in Figure 2.19, each time the local DNS server dns.nyu.edu 
receives a reply from some DNS server, it can cache any of the information con-
tained in the reply. If a hostname/IP address pair is cached in a DNS server and 
another query arrives to the DNS server for the same hostname, the DNS server 
can provide the desired IP address, even if it is not authoritative for the hostname. 
Because hosts and mappings between hostnames and IP addresses are by no means 
permanent, DNS servers discard cached information after a period of time (often 
set to two days).
As an example, suppose that a host apricot.nyu.edu queries dns 
.nyu.edu for the IP address for the hostname cnn.com. Furthermore, ­
suppose 
that a few hours later, another NYU host, say, kiwi.nyu.edu, also queries 
dns.nyu.edu with the same hostname. Because of caching, the local DNS 
server will be able to immediately return the IP address of cnn.com to this 
second requesting host without having to query any other DNS servers. A local 
DNS server can also cache the IP addresses of TLD servers, thereby allowing 
the local DNS server to bypass the root DNS servers in a query chain. In fact, 
because of caching, root servers are bypassed for all but a very small fraction of 
DNS queries.
2.4.3 DNS Records and Messages
The DNS servers that together implement the DNS distributed database store 
resource records (RRs), including RRs that provide hostname-to-IP address map-
pings. Each DNS reply message carries one or more resource records. In this and 
the following subsection, we provide a brief overview of DNS resource records and 
messages; more details can be found in [Albitz 1993] or in the DNS RFCs [RFC 
1034; RFC 1035].
A resource record is a four-tuple that contains the following fields:
(Name, Value, Type, TTL)
TTL is the time to live of the resource record; it determines when a resource should 
be removed from a cache. In the example records given below, we ignore the TTL 
field. The meaning of Name and Value depend on Type:
•	 If Type=A, then Name is a hostname and Value is the IP address for the host-
name. Thus, a Type A record provides the standard hostname-to-IP address map-
ping. As an example, (relay1.bar.foo.com, 145.37.93.126, A) is 
a Type A record.
164         Chapter 2    •    Application Layer
•	 If Type=NS, then Name is a domain (such as foo.com) and Value is the host-
name of an authoritative DNS server that knows how to obtain the IP addresses 
for hosts in the domain. This record is used to route DNS queries further along in 
the query chain. As an example, (foo.com, dns.foo.com, NS) is a Type 
NS record.
•	 If Type=CNAME, then Value is a canonical hostname for the alias hostname 
Name. This record can provide querying hosts the canonical name for a host-
name. As an example, (foo.com, relay1.bar.foo.com, CNAME) is a 
CNAME record.
•	 If Type=MX, then Value is the canonical name of a mail server that has an alias 
hostname Name. As an example, (foo.com, mail.bar.foo.com, MX) 
is an MX record. MX records allow the hostnames of mail servers to have simple 
aliases. Note that by using the MX record, a company can have the same aliased 
name for its mail server and for one of its other servers (such as its Web server). 
To obtain the canonical name for the mail server, a DNS client would query for 
an MX record; to obtain the canonical name for the other server, the DNS client 
would query for the CNAME record.
If a DNS server is authoritative for a particular hostname, then the DNS server 
will contain a Type A record for the hostname. (Even if the DNS server is not author-
itative, it may contain a Type A record in its cache.) If a server is not authoritative for 
a hostname, then the server will contain a Type NS record for the domain that includes 
the hostname; it will also contain a Type A record that provides the IP address of 
the DNS server in the Value field of the NS record. As an example, suppose an 
edu TLD server is not authoritative for the host gaia.cs.umass.edu. Then this 
server will contain a record for a domain that includes the host gaia.cs.umass 
.edu, for example, (umass.edu, 
dns.umass.edu, 
NS). The edu TLD server 
would also contain a Type A record, which maps the DNS server dns.umass.edu 
to an IP address, for example, (dns.umass.edu, 128.119.40.111, A).
DNS Messages
Earlier in this section, we referred to DNS query and reply messages. These are the 
only two kinds of DNS messages. Furthermore, both query and reply messages have 
the same format, as shown in Figure 2.21.The semantics of the various fields in a 
DNS message are as follows:
•	 The first 12 bytes is the header section, which has a number of fields. The first 
field is a 16-bit number that identifies the query. This identifier is copied into the 
reply message to a query, allowing the client to match received replies with sent 
queries. There are a number of flags in the flag field. A 1-bit query/reply flag indi-
cates whether the message is a query (0) or a reply (1). A 1-bit authoritative flag is 
2.4    •    DNS—The Internet’s Directory Service         165
set in a reply message when a DNS server is an authoritative server for a queried 
name. A 1-bit recursion-desired flag is set when a client (host or DNS server) 
desires that the DNS server perform recursion when it doesn’t have the record. A 
1-bit recursion-available field is set in a reply if the DNS server supports recur-
sion. In the header, there are also four number-of fields. These fields indicate the 
number of occurrences of the four types of data sections that follow the header.
•	 The question section contains information about the query that is being made. 
This section includes (1) a name field that contains the name that is being que-
ried, and (2) a type field that indicates the type of question being asked about the 
name—for example, a host address associated with a name (Type A) or the mail 
server for a name (Type MX).
•	 In a reply from a DNS server, the answer section contains the resource records for 
the name that was originally queried. Recall that in each resource record there is the 
Type (for example, A, NS, CNAME, and MX), the Value, and the TTL. A reply can 
return multiple RRs in the answer, since a hostname can have multiple IP addresses 
(for example, for replicated Web servers, as discussed earlier in this section).
•	 The authority section contains records of other authoritative servers.
•	 The additional section contains other helpful records. For example, the answer 
field in a reply to an MX query contains a resource record providing the canoni-
cal hostname of a mail server. The additional section contains a Type A record 
providing the IP address for the canonical hostname of the mail server.
Identiﬁcation
Number of questions
Number of authority RRs
Name, type ﬁelds for
a query
12 bytes
RRs in response to query
Records for
authoritative servers
Additional “helpful”
info that may be used
Flags
Number of answer RRs
Number of additional RRs
Authority
(variable number of resource records)
Additional information
(variable number of resource records)
Answers
(variable number of resource records)
Questions
(variable number of questions)
Figure 2.21  ♦  DNS message format
166         Chapter 2    •    Application Layer
How would you like to send a DNS query message directly from the host you’re 
working on to some DNS server? This can easily be done with the nslookup program, 
which is available from most Windows and UNIX platforms. For example, from a Win-
dows host, open the Command Prompt and invoke the nslookup program by simply typ-
ing “nslookup.” After invoking nslookup, you can send a DNS query to any DNS server 
(root, TLD, or authoritative). After receiving the reply message from the DNS server, 
nslookup will display the records included in the reply (in a human-readable format). As 
an alternative to running nslookup from your own host, you can visit one of many Web 
sites that allow you to remotely employ nslookup. (Just type “nslookup” into a search 
engine and you’ll be brought to one of these sites.) The DNS Wireshark lab at the end of 
this chapter will allow you to explore the DNS in much more detail.
Inserting Records into the DNS Database
The discussion above focused on how records are retrieved from the DNS database. 
You might be wondering how records get into the database in the first place. Let’s 
look at how this is done in the context of a specific example. Suppose you have 
just created an exciting new startup company called Network Utopia. The first thing 
you’ll surely want to do is register the domain name networkutopia.com at 
a registrar. A registrar is a commercial entity that verifies the uniqueness of the 
domain name, enters the domain name into the DNS database (as discussed below), 
and collects a small fee from you for its services. Prior to 1999, a single registrar, 
Network Solutions, had a monopoly on domain name registration for com, net, 
and org domains. But now there are many registrars competing for customers, and 
the Internet Corporation for Assigned Names and Numbers (ICANN) accredits the 
various registrars. A complete list of accredited registrars is available at http://
www.internic.net.
When you register the domain name networkutopia.com with some reg-
istrar, you also need to provide the registrar with the names and IP addresses of 
your primary and secondary authoritative DNS servers. Suppose the names and IP 
addresses are dns1.networkutopia.com, dns2.networkutopia.com, 
212.2.212.1, and 212.212.212.2. For each of these two authoritative DNS 
servers, the registrar would then make sure that a Type NS and a Type A record are 
entered into the TLD com servers. Specifically, for the primary authoritative server 
for networkutopia.com, the registrar would insert the following two resource 
records into the DNS system:
(networkutopia.com, dns1.networkutopia.com, NS)
(dns1.networkutopia.com, 212.212.212.1, A)
You’ll also have to make sure that the Type A resource record for your Web server 
www.networkutopia.com and the Type MX resource record for your mail 
server mail.networkutopia.com are entered into your authoritative DNS 
2.4    •    DNS—The Internet’s Directory Service         167
DNS VULNERABILITIES
We have seen that DNS is a critical component of the Internet infrastructure, with 
many important services—including the Web and e-mail—simply incapable of func-
tioning without it. We therefore naturally ask, how can DNS be attacked? Is DNS a 
sitting duck, waiting to be knocked out of service, while taking most Internet applica-
tions down with it?
The first type of attack that comes to mind is a DDoS bandwidth-flooding attack 
(see Section 1.6) against DNS servers. For example, an attacker could attempt to 
send to each DNS root server a deluge of packets, so many that the majority of 
legitimate DNS queries never get answered. Such a large-scale DDoS attack against 
DNS root servers actually took place on October 21, 2002. In this attack, the attack-
ers leveraged a botnet to send truck loads of ICMP ping messages to each of the 
13 DNS root IP addresses. (ICMP messages are discussed in Section 5.6. For now, 
it suffices to know that ICMP packets are special types of IP datagrams.) Fortunately, 
this large-scale attack caused minimal damage, having little or no impact on users’ 
Internet experience. The attackers did succeed at directing a deluge of packets at the 
root servers. But many of the DNS root servers were protected by packet filters, con-
figured to always block all ICMP ping messages directed at the root servers. These 
protected servers were thus spared and functioned as normal. Furthermore, most local 
DNS servers cache the IP addresses of top-level-domain servers, allowing the query 
process to often bypass the DNS root servers.
A potentially more effective DDoS attack against DNS would be send a deluge of 
DNS queries to top-level-domain servers, for example, to all the top-level-domain serv-
ers that handle the .com domain. It would be harder to filter DNS queries directed 
to DNS servers; and top-level-domain servers are not as easily bypassed as are root 
servers. But the severity of such an attack would be partially mitigated by caching in 
local DNS servers.
DNS could potentially be attacked in other ways. In a man-in-the-middle attack, 
the attacker intercepts queries from hosts and returns bogus replies. In the DNS poi-
soning attack, the attacker sends bogus replies to a DNS server, tricking the server 
into accepting bogus records into its cache. Either of these attacks could be used, 
for example, to redirect an unsuspecting Web user to the attacker’s Web site. These 
attacks, however, are difficult to implement, as they require intercepting packets or 
throttling servers [Skoudis 2006].
In summary, DNS has demonstrated itself to be surprisingly robust against attacks. 
To date, there hasn’t been an attack that has successfully impeded the DNS service. 
FOCUS ON SECURITY
168         Chapter 2    •    Application Layer
servers. (Until recently, the contents of each DNS server were configured statically, 
for example, from a configuration file created by a system manager. More recently, 
an UPDATE option has been added to the DNS protocol to allow data to be dynami-
cally added or deleted from the database via DNS messages. [RFC 2136] and [RFC 
3007] specify DNS dynamic updates.)
Once all of these steps are completed, people will be able to visit your Web site 
and send e-mail to the employees at your company. Let’s conclude our discussion of 
DNS by verifying that this statement is true. This verification also helps to solidify 
what we have learned about DNS. Suppose Alice in Australia wants to view the 
Web page www.networkutopia.com. As discussed earlier, her host will first 
send a DNS query to her local DNS server. The local DNS server will then contact a 
TLD com server. (The local DNS server will also have to contact a root DNS server 
if the address of a TLD com server is not cached.) This TLD server contains the 
Type NS and Type A resource records listed above, because the registrar had these 
resource records inserted into all of the TLD com servers. The TLD com server 
sends a reply to Alice’s local DNS server, with the reply containing the two resource 
records. The local DNS server then sends a DNS query to 212.212.212.1, ask-
ing for the Type A record corresponding to www.networkutopia.com. This 
record provides the IP address of the desired Web server, say, 212.212.71.4, 
which the local DNS server passes back to Alice’s host. Alice’s browser can now 
initiate a TCP connection to the host 212.212.71.4 and send an HTTP request 
over the connection. Whew! There’s a lot more going on than what meets the eye 
when one surfs the Web!
2.5	 Peer-to-Peer File Distribution
The applications described in this chapter thus far—including the Web, e-mail, and 
DNS—all employ client-server architectures with significant reliance on always-on 
infrastructure servers. Recall from Section 2.1.1 that with a P2P architecture, there 
is minimal (or no) reliance on always-on infrastructure servers. Instead, pairs of 
intermittently connected hosts, called peers, communicate directly with each other. 
The peers are not owned by a service provider, but are instead desktops and laptops 
controlled by users.
In this section we consider a very natural P2P application, namely, distributing 
a large file from a single server to a large number of hosts (called peers). The file 
might be a new version of the Linux operating system, a software patch for an existing 
operating system or application, an MP3 music file, or an MPEG video file. In client-
server file distribution, the server must send a copy of the file to each of the peers—
placing an enormous burden on the server and consuming a large amount of server 
bandwidth. In P2P file distribution, each peer can redistribute any portion of the 
2.5    •    Peer-to-Peer File Distribution         169
file it has received to any other peers, thereby assisting the server in the distribution 
process. As of 2016, the most popular P2P file distribution protocol is BitTorrent. 
Originally developed by Bram Cohen, there are now many different independent Bit-
Torrent clients conforming to the BitTorrent protocol, just as there are a number of 
Web browser clients that conform to the HTTP protocol. In this subsection, we first 
examine the self-scalability of P2P architectures in the context of file distribution. 
We then describe BitTorrent in some detail, highlighting its most important charac-
teristics and features.
Scalability of P2P Architectures
To compare client-server architectures with peer-to-peer architectures, and illustrate 
the inherent self-scalability of P2P, we now consider a simple quantitative model 
for distributing a file to a fixed set of peers for both architecture types. As shown in 
Figure 2.22, the server and the peers are connected to the Internet with access links. 
Denote the upload rate of the server’s access link by us, the upload rate of the ith 
peer’s access link by ui, and the download rate of the ith peer’s access link by di. Also 
denote the size of the file to be distributed (in bits) by F and the number of peers that 
want to obtain a copy of the file by N. The distribution time is the time it takes to get 
Internet
File: F
Server
us
u1
u2
u3
d1
d2
d3
u4
u5
u6
d4
d5
d6
uN
dN
Figure 2.22  ♦  An illustrative file distribution problem
170         Chapter 2    •    Application Layer
a copy of the file to all N peers. In our analysis of the distribution time below, for both 
client-server and P2P architectures, we make the simplifying (and generally accurate 
[Akella 2003]) assumption that the Internet core has abundant bandwidth, implying 
that all of the bottlenecks are in access networks. We also suppose that the server 
and clients are not participating in any other network applications, so that all of their 
upload and download access bandwidth can be fully devoted to distributing this file.
Let’s first determine the distribution time for the client-server architecture, 
which we denote by Dcs. In the client-server architecture, none of the peers aids in 
distributing the file. We make the following observations:
•	 The server must transmit one copy of the file to each of the N peers. Thus the 
server must transmit NF bits. Since the server’s upload rate is us, the time to dis-
tribute the file must be at least NF/us.
•	 Let dmin denote the download rate of the peer with the lowest download rate, that 
is, dmin = min5d1, dp, . . . , dN6. The peer with the lowest download rate cannot 
obtain all F bits of the file in less than F/dmin seconds. Thus the minimum distri-
bution time is at least F/dmin.
Putting these two observations together, we obtain
Dcs Ú maxb NF
us
 , F
dmin
r.
This provides a lower bound on the minimum distribution time for the client-server 
architecture. In the homework problems you will be asked to show that the server can 
schedule its transmissions so that the lower bound is actually achieved. So let’s take 
this lower bound provided above as the actual distribution time, that is,
	
Dcs = maxb NF
us
, F
dmin
r
(2.1)
We see from Equation 2.1 that for N large enough, the client-server distribution time 
is given by NF/us. Thus, the distribution time increases linearly with the number of 
peers N. So, for example, if the number of peers from one week to the next increases 
a thousand-fold from a thousand to a million, the time required to distribute the file 
to all peers increases by 1,000.
Let’s now go through a similar analysis for the P2P architecture, where each peer 
can assist the server in distributing the file. In particular, when a peer receives some 
file data, it can use its own upload capacity to redistribute the data to other peers. Cal-
culating the distribution time for the P2P architecture is somewhat more complicated 
than for the client-server architecture, since the distribution time depends on how 
each peer distributes portions of the file to the other peers. Nevertheless, a simple 
2.5    •    Peer-to-Peer File Distribution         171
expression for the minimal distribution time can be obtained [Kumar 2006]. To this 
end, we first make the following observations:
•	 At the beginning of the distribution, only the server has the file. To get this file 
into the community of peers, the server must send each bit of the file at least once 
into its access link. Thus, the minimum distribution time is at least F/us. (Unlike 
the client-server scheme, a bit sent once by the server may not have to be sent by 
the server again, as the peers may redistribute the bit among themselves.)
•	 As with the client-server architecture, the peer with the lowest download rate 
cannot obtain all F bits of the file in less than F/dmin seconds. Thus the minimum 
distribution time is at least F/dmin.
•	 Finally, observe that the total upload capacity of the system as a whole is equal 
to the upload rate of the server plus the upload rates of each of the individual 
peers, that is, utotal = us + u1 + g + uN. The system must deliver (upload) F 
bits to each of the N peers, thus delivering a total of NF bits. This cannot be done 
at a rate faster than utotal. Thus, the minimum distribution time is also at least 
NF/(us + u1 + g + uN).
Putting these three observations together, we obtain the minimum distribution 
time for P2P, denoted by DP2P.
	
DP2P Ú max c
F
us
, F
dmin
, 
NF
us + a
N
i=1
ui
s
(2.2)
Equation 2.2 provides a lower bound for the minimum distribution time for the P2P 
architecture. It turns out that if we imagine that each peer can redistribute a bit as 
soon as it receives the bit, then there is a redistribution scheme that actually achieves 
this lower bound [Kumar 2006]. (We will prove a special case of this result in the 
homework.) In reality, where chunks of the file are redistributed rather than indi-
vidual bits, Equation 2.2 serves as a good approximation of the actual minimum 
distribution time. Thus, let’s take the lower bound provided by Equation 2.2 as the 
actual minimum distribution time, that is,
	
DP2P = max c
F
us
, F
dmin
, 
NF
us + a
N
i=1
ui
s
(2.3)
Figure 2.23 compares the minimum distribution time for the client-server and 
P2P architectures assuming that all peers have the same upload rate u. In Figure 2.23, 
we have set F/u = 1 hour, us = 10u, and dmin Ú us. Thus, a peer can transmit the 
entire file in one hour, the server transmission rate is 10 times the peer upload rate, 
172         Chapter 2    •    Application Layer
and (for simplicity) the peer download rates are set large enough so as not to have 
an effect. We see from Figure 2.23 that for the client-server architecture, the distri-
bution time increases linearly and without bound as the number of peers increases. 
However, for the P2P architecture, the minimal distribution time is not only always 
less than the distribution time of the client-server architecture; it is also less than one 
hour for any number of peers N. Thus, applications with the P2P architecture can be 
self-scaling. This scalability is a direct consequence of peers being redistributors as 
well as consumers of bits.
BitTorrent
BitTorrent is a popular P2P protocol for file distribution [Chao 2011]. In BitTorrent 
lingo, the collection of all peers participating in the distribution of a particular file is 
called a torrent. Peers in a torrent download equal-size chunks of the file from one 
another, with a typical chunk size of 256 kbytes. When a peer first joins a torrent, it 
has no chunks. Over time it accumulates more and more chunks. While it downloads 
chunks it also uploads chunks to other peers. Once a peer has acquired the entire 
file, it may (selfishly) leave the torrent, or (altruistically) remain in the torrent and 
continue to upload chunks to other peers. Also, any peer may leave the torrent at any 
time with only a subset of chunks, and later rejoin the torrent.
Let’s now take a closer look at how BitTorrent operates. Since BitTorrent is 
a rather complicated protocol and system, we’ll only describe its most important 
mechanisms, sweeping some of the details under the rug; this will allow us to see 
the forest through the trees. Each torrent has an infrastructure node called a tracker. 
0
5
10
15
20
25
30
0
N
Minimum distributioin tiime
35
0.5
1.5
2.5
1.0
3.0
2.0
3.5
Client-Server
P2P
Figure 2.23  ♦  Distribution time for P2P and client-server architectures
2.5    •    Peer-to-Peer File Distribution         173
When a peer joins a torrent, it registers itself with the tracker and periodically informs 
the tracker that it is still in the torrent. In this manner, the tracker keeps track of the 
peers that are participating in the torrent. A given torrent may have fewer than ten or 
more than a thousand peers participating at any instant of time.
As shown in Figure 2.24, when a new peer, Alice, joins the torrent, the tracker 
randomly selects a subset of peers (for concreteness, say 50) from the set of partici-
pating peers, and sends the IP addresses of these 50 peers to Alice. Possessing this 
list of peers, Alice attempts to establish concurrent TCP connections with all the 
peers on this list. Let’s call all the peers with which Alice succeeds in establishing a 
TCP connection “neighboring peers.” (In Figure 2.24, Alice is shown to have only 
three neighboring peers. Normally, she would have many more.) As time evolves, 
some of these peers may leave and other peers (outside the initial 50) may attempt to 
establish TCP connections with Alice. So a peer’s neighboring peers will fluctuate 
over time.
At any given time, each peer will have a subset of chunks from the file, with dif-
ferent peers having different subsets. Periodically, Alice will ask each of her neigh-
boring peers (over the TCP connections) for the list of the chunks they have. If Alice 
has L different neighbors, she will obtain L lists of chunks. With this knowledge, 
Tracker
Trading chunks
Peer
Obtain
list of
peers
Alice
Figure 2.24  ♦  File distribution with BitTorrent
174         Chapter 2    •    Application Layer
Alice will issue requests (again over the TCP connections) for chunks she currently 
does not have.
So at any given instant of time, Alice will have a subset of chunks and will know 
which chunks her neighbors have. With this information, Alice will have two impor-
tant decisions to make. First, which chunks should she request first from her neigh-
bors? And second, to which of her neighbors should she send requested chunks? In 
deciding which chunks to request, Alice uses a technique called rarest first. The 
idea is to determine, from among the chunks she does not have, the chunks that are 
the rarest among her neighbors (that is, the chunks that have the fewest repeated cop-
ies among her neighbors) and then request those rarest chunks first. In this manner, 
the rarest chunks get more quickly redistributed, aiming to (roughly) equalize the 
numbers of copies of each chunk in the torrent.
To determine which requests she responds to, BitTorrent uses a clever trading 
algorithm. The basic idea is that Alice gives priority to the neighbors that are cur-
rently supplying her data at the highest rate. Specifically, for each of her neighbors, 
Alice continually measures the rate at which she receives bits and determines the 
four peers that are feeding her bits at the highest rate. She then reciprocates by send-
ing chunks to these same four peers. Every 10 seconds, she recalculates the rates 
and possibly modifies the set of four peers. In BitTorrent lingo, these four peers are 
said to be unchoked. Importantly, every 30 seconds, she also picks one additional 
neighbor at random and sends it chunks. Let’s call the randomly chosen peer Bob. 
In BitTorrent lingo, Bob is said to be optimistically unchoked. Because Alice is 
sending data to Bob, she may become one of Bob’s top four uploaders, in which case 
Bob would start to send data to Alice. If the rate at which Bob sends data to Alice 
is high enough, Bob could then, in turn, become one of Alice’s top four uploaders. 
In other words, every 30 seconds, Alice will randomly choose a new trading partner 
and initiate trading with that partner. If the two peers are satisfied with the trading, 
they will put each other in their top four lists and continue trading with each other 
until one of the peers finds a better partner. The effect is that peers capable of upload-
ing at compatible rates tend to find each other. The random neighbor selection also 
allows new peers to get chunks, so that they can have something to trade. All other 
neighboring peers besides these five peers (four “top” peers and one probing peer) 
are “choked,” that is, they do not receive any chunks from Alice. BitTorrent has 
a number of interesting mechanisms that are not discussed here, including pieces 
(mini-chunks), pipelining, random first selection, endgame mode, and anti-snubbing 
[Cohen 2003].
The incentive mechanism for trading just described is often referred to as tit-for-
tat [Cohen 2003]. It has been shown that this incentive scheme can be circumvented 
[Liogkas 2006; Locher 2006; Piatek 2007]. Nevertheless, the BitTorrent ecosystem 
is wildly successful, with millions of simultaneous peers actively sharing files in 
hundreds of thousands of torrents. If BitTorrent had been designed without tit-for-tat 
(or a variant), but otherwise exactly the same, BitTorrent would likely not even exist 
now, as the majority of the users would have been freeriders [Saroiu 2002].
2.6    •    Video Streaming and Content Distribution Networks         175
We close our discussion on P2P by briefly mentioning another application of P2P, 
namely, Distributed Hast Table (DHT). A distributed hash table is a simple database, 
with the database records being distributed over the peers in a P2P system. DHTs have 
been widely implemented (e.g., in BitTorrent) and have been the subject of extensive 
research. An overview is provided in a Video Note in the companion website. 
2.6	 Video Streaming and Content Distribution 
Networks
Streaming prerecorded video now accounts for the majority of the traffic in residen-
tial ISPs in North America. In particular, the Netflix and YouTube services alone 
consumed a whopping 37% and 16%, respectively, of residential ISP traffic in 2015 
[Sandvine 2015]. In this section we will provide an overview of how popular video 
streaming services are implemented in today’s Internet. We will see they are imple-
mented using application-level protocols and servers that function in some ways like 
a cache. In Chapter 9, devoted to multimedia networking, we will further examine 
Internet video as well as other Internet multimedia services.
2.6.1 Internet Video
In streaming stored video applications, the underlying medium is prerecorded video, 
such as a movie, a television show, a prerecorded sporting event, or a prerecorded 
user-generated video (such as those commonly seen on YouTube). These prere-
corded videos are placed on servers, and users send requests to the servers to view 
the videos on demand. Many Internet companies today provide streaming video, 
including, Netflix, YouTube (Google), Amazon, and Youku.
But before launching into a discussion of video streaming, we should first get 
a quick feel for the video medium itself. A video is a sequence of images, typi-
cally being displayed at a constant rate, for example, at 24 or 30 images per second. 
An uncompressed, digitally encoded image consists of an array of pixels, with each 
pixel encoded into a number of bits to represent luminance and color. An important 
characteristic of video is that it can be compressed, thereby trading off video quality 
with bit rate. Today’s off-the-shelf compression algorithms can compress a video to 
essentially any bit rate desired. Of course, the higher the bit rate, the better the image 
quality and the better the overall user viewing experience.
From a networking perspective, perhaps the most salient characteristic of video 
is its high bit rate. Compressed Internet video typically ranges from 100 kbps for 
low-quality video to over 3 Mbps for streaming high-definition movies; 4K stream-
ing envisions a bitrate of more than 10 Mbps. This can translate to huge amount of 
traffic and storage, particularly for high-end video. For example, a single 2 Mbps 
VideoNote
Walking though 
distributed hash tables
176         Chapter 2    •    Application Layer
video with a duration of 67 minutes will consume 1 gigabyte of storage and traffic. 
By far, the most important performance measure for streaming video is average end-
to-end throughput. In order to provide continuous playout, the network must provide 
an average throughput to the streaming application that is at least as large as the bit 
rate of the compressed video.
We can also use compression to create multiple versions of the same video, each 
at a different quality level. For example, we can use compression to create, say, three 
versions of the same video, at rates of 300 kbps, 1 Mbps, and 3 Mbps. Users can then 
decide which version they want to watch as a function of their current available band-
width. Users with high-speed Internet connections might choose the 3 Mbps version; 
users watching the video over 3G with a smartphone might choose the 300 kbps version.
2.6.2 HTTP Streaming and DASH
In HTTP streaming, the video is simply stored at an HTTP server as an ordinary 
file with a specific URL. When a user wants to see the video, the client establishes 
a TCP connection with the server and issues an HTTP GET request for that URL. 
The server then sends the video file, within an HTTP response message, as quickly 
as the underlying network protocols and traffic conditions will allow. On the client 
side, the bytes are collected in a client application buffer. Once the number of bytes 
in this buffer exceeds a predetermined threshold, the client application begins play-
back—specifically, the streaming video application periodically grabs video frames 
from the client application buffer, decompresses the frames, and displays them on 
the user’s screen. Thus, the video streaming application is displaying video as it is 
receiving and buffering frames corresponding to latter parts of the video.
Although HTTP streaming, as described in the previous paragraph, has been 
extensively deployed in practice (for example, by YouTube since its inception), it has 
a major shortcoming: All clients receive the same encoding of the video, despite the 
large variations in the amount of bandwidth available to a client, both across different 
clients and also over time for the same client. This has led to the development of a new 
type of HTTP-based streaming, often referred to as Dynamic Adaptive Streaming 
over HTTP (DASH). In DASH, the video is encoded into several different versions, 
with each version having a different bit rate and, correspondingly, a different quality 
level. The client dynamically requests chunks of video segments of a few seconds in 
length. When the amount of available bandwidth is high, the client naturally selects 
chunks from a high-rate version; and when the available bandwidth is low, it naturally 
selects from a low-rate version. The client selects different chunks one at a time with 
HTTP GET request messages [Akhshabi 2011].
DASH allows clients with different Internet access rates to stream in video at 
different encoding rates. Clients with low-speed 3G connections can receive a low 
bit-rate (and low-quality) version, and clients with fiber connections can receive a 
high-quality version. DASH also allows a client to adapt to the available bandwidth 
if the available end-to-end bandwidth changes during the session. This feature is 
2.6    •    Video Streaming and Content Distribution Networks         177
particularly important for mobile users, who typically see their bandwidth availabil-
ity fluctuate as they move with respect to the base stations.
With DASH, each video version is stored in the HTTP server, each with a differ-
ent URL. The HTTP server also has a manifest file, which provides a URL for each 
version along with its bit rate. The client first requests the manifest file and learns 
about the various versions. The client then selects one chunk at a time by specifying a 
URL and a byte range in an HTTP GET request message for each chunk. While down-
loading chunks, the client also measures the received bandwidth and runs a rate deter-
mination algorithm to select the chunk to request next. Naturally, if the client has a lot 
of video buffered and if the measured receive bandwidth is high, it will choose a chunk 
from a high-bitrate version. And naturally if the client has little video buffered and the 
measured received bandwidth is low, it will choose a chunk from a low-bitrate version. 
DASH therefore allows the client to freely switch among different quality levels.
2.6.3 Content Distribution Networks
Today, many Internet video companies are distributing on-demand multi-Mbps 
streams to millions of users on a daily basis. YouTube, for example, with a library 
of hundreds of millions of videos, distributes hundreds of millions of video streams 
to users around the world every day. Streaming all this traffic to locations all over 
the world while providing continuous playout and high interactivity is clearly a chal-
lenging task.
For an Internet video company, perhaps the most straightforward approach to 
providing streaming video service is to build a single massive data center, store all 
of its videos in the data center, and stream the videos directly from the data center 
to clients worldwide. But there are three major problems with this approach. First, if 
the client is far from the data center, server-to-client packets will cross many com-
munication links and likely pass through many ISPs, with some of the ISPs possibly 
located on different continents. If one of these links provides a throughput that is less 
than the video consumption rate, the end-to-end throughput will also be below the 
consumption rate, resulting in annoying freezing delays for the user. (Recall from 
Chapter 1 that the end-to-end throughput of a stream is governed by the throughput 
at the bottleneck link.) The likelihood of this happening increases as the number of 
links in the end-to-end path increases. A second drawback is that a popular video will 
likely be sent many times over the same communication links. Not only does this 
waste network bandwidth, but the Internet video company itself will be paying its 
provider ISP (connected to the data center) for sending the same bytes into the Inter-
net over and over again. A third problem with this solution is that a single data center 
represents a single point of failure—if the data center or its links to the Internet goes 
down, it would not be able to distribute any video streams.
In order to meet the challenge of distributing massive amounts of video data 
to users distributed around the world, almost all major video-streaming companies 
make use of Content Distribution Networks (CDNs). A CDN manages servers in 
178         Chapter 2    •    Application Layer
multiple geographically distributed locations, stores copies of the videos (and other 
types of Web content, including documents, images, and audio) in its servers, and 
attempts to direct each user request to a CDN location that will provide the best user 
experience. The CDN may be a private CDN, that is, owned by the content provider 
itself; for example, Google’s CDN distributes YouTube videos and other types of 
content. The CDN may alternatively be a third-party CDN that distributes content 
on behalf of multiple content providers; Akamai, Limelight and Level-3 all operate 
third-party CDNs. A very readable overview of modern CDNs is [Leighton 2009; 
Nygren 2010].
CDNs typically adopt one of two different server placement philosophies 
[Huang 2008]:
•	 Enter Deep. One philosophy, pioneered by Akamai, is to enter deep into the 
access networks of Internet Service Providers, by deploying server clusters in 
access ISPs all over the world. (Access networks are described in Section 1.3.) 
Akamai takes this approach with clusters in approximately 1,700 locations. The 
goal is to get close to end users, thereby improving user-perceived delay and 
throughput by decreasing the number of links and routers between the end user 
and the CDN server from which it receives content. Because of this highly dis-
tributed design, the task of maintaining and managing the clusters becomes chal-
lenging.
•	 Bring Home. A second design philosophy, taken by Limelight and many other 
CDN companies, is to bring the ISPs home by building large clusters at a smaller 
number (for example, tens) of sites. Instead of getting inside the access ISPs, 
these CDNs typically place their clusters in Internet Exchange Points (IXPs) (see 
Section 1.3). Compared with the enter-deep design philosophy, the bring-home 
design typically results in lower maintenance and management overhead, pos-
sibly at the expense of higher delay and lower throughput to end users.
Once its clusters are in place, the CDN replicates content across its clusters. The 
CDN may not want to place a copy of every video in each cluster, since some videos 
are rarely viewed or are only popular in some countries. In fact, many CDNs do not 
push videos to their clusters but instead use a simple pull strategy: If a client requests 
a video from a cluster that is not storing the video, then the cluster retrieves the 
video (from a central repository or from another cluster) and stores a copy locally 
while streaming the video to the client at the same time. Similar Web caching (see 
Section 2.2.5), when a cluster’s storage becomes full, it removes videos that are not 
frequently requested.
CDN Operation
Having identified the two major approaches toward deploying a CDN, let’s now dive 
down into the nuts and bolts of how a CDN operates. When a browser in a user’s 
2.6    •    Video Streaming and Content Distribution Networks         179
host is instructed to retrieve a specific video (identified by a URL), the CDN must 
intercept the request so that it can (1) determine a suitable CDN server cluster for that 
client at that time, and (2) redirect the client’s request to a server in that cluster. We’ll 
shortly discuss how a CDN can determine a suitable cluster. But first let’s examine 
the mechanics behind intercepting and redirecting a request.
Most CDNs take advantage of DNS to intercept and redirect requests; an inter-
esting discussion of such a use of the DNS is [Vixie 2009]. Let’s consider a simple 
GOOGLE’S NETWORK INFRASTRUCTURE
To support its vast array of cloud services—including search, Gmail, calendar, 
YouTube video, maps, documents, and social networks—Google has deployed an 
extensive private network and CDN infrastructure. Google’s CDN infrastructure has 
three tiers of server clusters:
• 

Fourteen “mega data centers,” with eight in North America, four in Europe, and 
two in Asia [Google Locations 2016], with each data center having on the order 
of 100,000 servers. These mega data centers are responsible for serving dynamic 
(and often personalized) content, including search results and Gmail messages.
• 

An estimated 50 clusters in IXPs scattered throughout the world, with each cluster 
consisting on the order of 100–500 servers [Adhikari 2011a]. These clusters are 
responsible for serving static content, including YouTube videos [Adhikari 2011a].
• 

Many hundreds of “enter-deep” clusters located within an access ISP. Here a cluster 
typically consists of tens of servers within a single rack. These enter-deep ­
servers 
perform TCP splitting (see Section 3.7) and serve static content [Chen 2011], 
including the static portions of Web pages that embody search results.
All of these data centers and cluster locations are networked together with 
Google’s own private network. When a user makes a search query, often the query 
is first sent over the local ISP to a nearby enter-deep cache, from where the static 
content is retrieved; while providing the static content to the client, the nearby cache 
also forwards the query over Google’s private network to one of the mega data cent-
ers, from where the personalized search results are retrieved. For a YouTube video, 
the video itself may come from one of the bring-home caches, whereas portions of 
the Web page surrounding the video may come from the nearby enter-deep cache, 
and the advertisements surrounding the video come from the data centers. In sum-
mary, except for the local ISPs, the Google cloud services are largely provided by a 
network infrastructure that is independent of the public Internet.
CASE STUDY
180         Chapter 2    •    Application Layer
example to illustrate how the DNS is typically involved. Suppose a content provider, 
NetCinema, employs the third-party CDN company, KingCDN, to distribute its vid-
eos to its customers. On the NetCinema Web pages, each of its videos is assigned a 
URL that includes the string “video” and a unique identifier for the video itself; for 
example, Transformers 7 might be assigned http://video.netcinema.com/6Y7B23V. 
Six steps then occur, as shown in Figure 2.25:
	1.	 The user visits the Web page at NetCinema.
	2.	 When the user clicks on the link http://video.netcinema.com/6Y7B23V, the 
user’s host sends a DNS query for video.netcinema.com.
	3.	 The user’s Local DNS Server (LDNS) relays the DNS query to an authoritative 
DNS server for NetCinema, which observes the string “video” in the host-
name video.netcinema.com. To “hand over” the DNS query to KingCDN, 
instead of returning an IP address, the NetCinema authoritative DNS server 
returns to the LDNS a hostname in the KingCDN’s domain, for example, 
a1105.kingcdn.com.
	4.	 From this point on, the DNS query enters into KingCDN’s private DNS infra-
structure. The user’s LDNS then sends a second query, now for a1105.kingcdn.
com, and KingCDN’s DNS system eventually returns the IP addresses of a 
KingCDN content server to the LDNS. It is thus here, within the KingCDN’s 
DNS system, that the CDN server from which the client will receive its content 
is specified.
Local
DNS server
NetCinema authoritative
 DNS server
www.NetCinema.com
KingCDN authoritative
server
KingCDN content
distribution server
2
5
6
3
1
4
Figure 2.25  ♦  DNS redirects a user’s request to a CDN server
2.6    •    Video Streaming and Content Distribution Networks         181
	5.	 The LDNS forwards the IP address of the content-serving CDN node to the 
user’s host.
	6.	 Once the client receives the IP address for a KingCDN content server, it estab-
lishes a direct TCP connection with the server at that IP address and issues an 
HTTP GET request for the video. If DASH is used, the server will first send to 
the client a manifest file with a list of URLs, one for each version of the video, 
and the client will dynamically select chunks from the different versions.
Cluster Selection Strategies
At the core of any CDN deployment is a cluster selection strategy, that is, a mecha-
nism for dynamically directing clients to a server cluster or a data center within the 
CDN. As we just saw, the CDN learns the IP address of the client’s LDNS server 
via the client’s DNS lookup. After learning this IP address, the CDN needs to select 
an appropriate cluster based on this IP address. CDNs generally employ proprietary 
cluster selection strategies. We now briefly survey a few approaches, each of which 
has its own advantages and disadvantages.
One simple strategy is to assign the client to the cluster that is geographically clos-
est. Using commercial geo-location databases (such as Quova [Quova 2016] and Max-
Mind [MaxMind 2016]), each LDNS IP address is mapped to a geographic location. 
When a DNS request is received from a particular LDNS, the CDN chooses the geo-
graphically closest cluster, that is, the cluster that is the fewest kilometers from the LDNS 
“as the bird flies.” Such a solution can work reasonably well for a large fraction of the cli-
ents [Agarwal 2009]. However, for some clients, the solution may perform poorly, since 
the geographically closest cluster may not be the closest cluster in terms of the length 
or number of hops of the network path. Furthermore, a problem inherent with all DNS-
based approaches is that some end-users are configured to use remotely located LDNSs 
[Shaikh 2001; Mao 2002], in which case the LDNS location may be far from the client’s 
location. Moreover, this simple strategy ignores the variation in delay and available band-
width over time of Internet paths, always assigning the same cluster to a particular client.
In order to determine the best cluster for a client based on the current traffic 
conditions, CDNs can instead perform periodic real-time measurements of delay 
and loss performance between their clusters and clients. For instance, a CDN can 
have each of its clusters periodically send probes (for example, ping messages or 
DNS queries) to all of the LDNSs around the world. One drawback of this approach 
is that many LDNSs are configured to not respond to such probes.
2.6.4 Case Studies: Netflix, YouTube, and Kankan
We conclude our discussion of streaming stored video by taking a look at three 
highly successful large-scale deployments: Netflix, YouTube, and Kankan. We’ll 
see that each of these systems take a very different approach, yet employ many of the 
underlying principles discussed in this section.
182         Chapter 2    •    Application Layer
Netflix
Generating 37% of the downstream traffic in residential ISPs in North America in 
2015, Netflix has become the leading service provider for online movies and TV series 
in the United States [Sandvine 2015]. As we discuss below, Netflix video distribution 
has two major components: the Amazon cloud and its own private CDN infrastructure.
Netflix has a Web site that handles numerous functions, including user registra-
tion and login, billing, movie catalogue for browsing and searching, and a movie 
recommendation system. As shown in Figure 2.26, this Web site (and its associated 
backend databases) run entirely on Amazon servers in the Amazon cloud. Addition-
ally, the Amazon cloud handles the following critical functions:
•	 Content ingestion. Before Netflix can distribute a movie to its customers, it must 
first ingest and process the movie. Netflix receives studio master versions of 
movies and uploads them to hosts in the Amazon cloud.
•	 Content processing. The machines in the Amazon cloud create many different 
formats for each movie, suitable for a diverse array of client video players run-
ning on desktop computers, smartphones, and game consoles connected to televi-
sions. A different version is created for each of these formats and at multiple bit 
rates, allowing for adaptive streaming over HTTP using DASH.
•	 Uploading versions to its CDN. Once all of the versions of a movie have been 
created, the hosts in the Amazon cloud upload the versions to its CDN.
Amazon Cloud
CDN server
CDN server
Upload
versions
to CDNs
CDN server
Client
Manifest 
ﬁle
Video
chunks
(DASH)
Figure 2.26  ♦  Netflix video streaming platform
2.6    •    Video Streaming and Content Distribution Networks         183
When Netflix first rolled out its video streaming service in 2007, it employed 
three third-party CDN companies to distribute its video content. Netflix has since 
created its own private CDN, from which it now streams all of its videos. (Netflix 
still uses Akamai to distribute its Web pages, however.) To create its own CDN, Net-
flix has installed server racks both in IXPs and within residential ISPs themselves. 
Netflix currently has server racks in over 50 IXP locations; see [Netflix Open Con-
nect 2016] for a current list of IXPs housing Netflix racks. There are also hundreds 
of ISP locations housing Netflix racks; also see [Netflix Open Connect 2016], where 
Netflix provides to potential ISP partners instructions about installing a (free) Net-
flix rack for their networks. Each server in the rack has several 10 Gbps Ethernet 
ports and over 100 terabytes of storage. The number of servers in a rack varies: IXP 
installations often have tens of servers and contain the entire Netflix streaming video 
library, including multiple versions of the videos to support DASH; local IXPs may 
only have one server and contain only the most popular videos. Netflix does not 
use pull-caching (Section 2.2.5) to populate its CDN servers in the IXPs and ISPs. 
Instead, Netflix distributes by pushing the videos to its CDN servers during off-peak 
hours. For those locations that cannot hold the entire library, Netflix pushes only 
the most popular videos, which are determined on a day-to-day basis. The Netflix 
CDN design is described in some detail in the YouTube videos [Netflix Video 1] and 
[Netflix Video 2].
Having described the components of the Netflix architecture, let’s take a closer 
look at the interaction between the client and the various servers that are involved in 
movie delivery. As indicated earlier, the Web pages for browsing the Netflix video 
library are served from servers in the Amazon cloud. When a user selects a movie to 
play, the Netflix software, running in the Amazon cloud, first determines which of 
its CDN servers have copies of the movie. Among the servers that have the movie, 
the software then determines the “best” server for that client request. If the client is 
using a residential ISP that has a Netflix CDN server rack installed in that ISP, and 
this rack has a copy of the requested movie, then a server in this rack is typically 
selected. If not, a server at a nearby IXP is typically selected.
Once Netflix determines the CDN server that is to deliver the content, it sends 
the client the IP address of the specific server as well as a manifest file, which has 
the URLs for the different versions of the requested movie. The client and that CDN 
server then directly interact using a proprietary version of DASH. Specifically, 
as described in Section 2.6.2, the client uses the byte-range header in HTTP GET 
request messages, to request chunks from the different versions of the movie. Netflix 
uses chunks that are approximately four-seconds long [Adhikari 2012]. While the 
chunks are being downloaded, the client measures the received throughput and runs 
a rate-determination algorithm to determine the quality of the next chunk to request.
Netflix embodies many of the key principles discussed earlier in this section, 
including adaptive streaming and CDN distribution. However, because Netflix uses 
its own private CDN, which distributes only video (and not Web pages), Netflix 
has been able to simplify and tailor its CDN design. In particular, Netflix does not 
184         Chapter 2    •    Application Layer
need to employ DNS redirect, as discussed in Section 2.6.3, to connect a particular 
client to a CDN server; instead, the Netflix software (running in the Amazon cloud) 
directly tells the client to use a particular CDN server. Furthermore, the Netflix CDN 
uses push caching rather than pull caching (Section 2.2.5): content is pushed into the 
servers at scheduled times at off-peak hours, rather than dynamically during cache 
misses.
YouTube
With 300 hours of video uploaded to YouTube every minute and several billion 
video views per day [YouTube 2016], YouTube is indisputably the world’s largest 
video-sharing site. YouTube began its service in April 2005 and was acquired by 
Google in November 2006. Although the Google/YouTube design and protocols are 
proprietary, through several independent measurement efforts we can gain a basic 
understanding about how YouTube operates [Zink 2009; Torres 2011; Adhikari 
2011a]. As with Netflix, YouTube makes extensive use of CDN technology to dis-
tribute its videos [Torres 2011]. Similar to Netflix, Google uses its own private CDN 
to distribute YouTube videos, and has installed server clusters in many hundreds 
of different IXP and ISP locations. From these locations and directly from its huge 
data centers, Google distributes YouTube videos [Adhikari 2011a]. Unlike Netflix, 
however, Google uses pull caching, as described in Section 2.2.5, and DNS redirect, 
as described in Section 2.6.3. Most of the time, Google’s cluster-selection strategy 
directs the client to the cluster for which the RTT between client and cluster is the 
lowest; however, in order to balance the load across clusters, sometimes the client is 
directed (via DNS) to a more distant cluster [Torres 2011].
YouTube employs HTTP streaming, often making a small number of differ-
ent versions available for a video, each with a different bit rate and corresponding 
quality level. YouTube does not employ adaptive streaming (such as DASH), but 
instead requires the user to manually select a version. In order to save bandwidth and 
server resources that would be wasted by repositioning or early termination, You-
Tube uses the HTTP byte range request to limit the flow of transmitted data after a 
target amount of video is prefetched.
Several million videos are uploaded to YouTube every day. Not only are You-
Tube videos streamed from server to client over HTTP, but YouTube uploaders also 
upload their videos from client to server over HTTP. YouTube processes each video 
it receives, converting it to a YouTube video format and creating multiple versions 
at different bit rates. This processing takes place entirely within Google data centers. 
(See the case study on Google’s network infrastructure in Section 2.6.3.)
Kankan
We just saw that dedicated servers, operated by private CDNs, stream Netflix and 
YouTube videos to clients. Netflix and YouTube have to pay not only for the server 
2.7    •    Socket Programming: Creating Network Applications         185
hardware but also for the bandwidth the servers use to distribute the videos. Given 
the scale of these services and the amount of bandwidth they are consuming, such a 
CDN deployment can be costly.
We conclude this section by describing an entirely different approach for provid-
ing video on demand over the Internet at a large scale—one that allows the service 
provider to significantly reduce its infrastructure and bandwidth costs. As you might 
suspect, this approach uses P2P delivery instead of (or along with) client-server 
delivery. Since 2011, Kankan (owned and operated by Xunlei) has been deploying 
P2P video delivery with great success, with tens of millions of users every month 
[Zhang 2015].
At a high level, P2P video streaming is very similar to BitTorrent file download-
ing. When a peer wants to see a video, it contacts a tracker to discover other peers in 
the system that have a copy of that video. This requesting peer then requests chunks 
of the video in parallel from the other peers that have the video. Different from 
downloading with BitTorrent, however, requests are preferentially made for chunks 
that are to be played back in the near future in order to ensure continuous playback 
[Dhungel 2012].
Recently, Kankan has migrated to a hybrid CDN-P2P streaming system 
[Zhang 2015]. Specifically, Kankan now deploys a few hundred servers within 
China and pushes video content to these servers. This Kankan CDN plays a major 
role in the start-up stage of video streaming. In most cases, the client requests the 
beginning of the content from CDN servers, and in parallel requests content from 
peers. When the total P2P traffic is sufficient for video playback, the client will 
cease streaming from the CDN and only stream from peers. But if the P2P stream-
ing traffic becomes insufficient, the client will restart CDN connections and return 
to the mode of hybrid CDN-P2P streaming. In this manner, Kankan can ensure 
short initial start-up delays while minimally relying on costly infrastructure servers 
and bandwidth.
2.7	 Socket Programming: Creating Network 
Applications
Now that we’ve looked at a number of important network applications, let’s explore 
how network application programs are actually created. Recall from Section 2.1 that 
a typical network application consists of a pair of programs—a client program and 
a server program—residing in two different end systems. When these two programs 
are executed, a client process and a server process are created, and these processes 
communicate with each other by reading from, and writing to, sockets. When creat-
ing a network application, the developer’s main task is therefore to write the code for 
both the client and server programs.
186         Chapter 2    •    Application Layer
There are two types of network applications. One type is an implementation 
whose operation is specified in a protocol standard, such as an RFC or some other 
standards document; such an application is sometimes referred to as “open,” since 
the rules specifying its operation are known to all. For such an implementation, the 
client and server programs must conform to the rules dictated by the RFC. For exam-
ple, the client program could be an implementation of the client side of the HTTP 
protocol, described in Section 2.2 and precisely defined in RFC 2616; similarly, 
the server program could be an implementation of the HTTP server protocol, also 
precisely defined in RFC 2616. If one developer writes code for the client program 
and another developer writes code for the server program, and both developers care-
fully follow the rules of the RFC, then the two programs will be able to interoper-
ate. Indeed, many of today’s network applications involve communication between 
client and server programs that have been created by independent developers—for 
example, a Google Chrome browser communicating with an Apache Web server, or 
a BitTorrent client communicating with BitTorrent tracker.
The other type of network application is a proprietary network application. In 
this case the client and server programs employ an application-layer protocol that has 
not been openly published in an RFC or elsewhere. A single developer (or develop-
ment team) creates both the client and server programs, and the developer has com-
plete control over what goes in the code. But because the code does not implement 
an open protocol, other independent developers will not be able to develop code that 
interoperates with the application.
In this section, we’ll examine the key issues in developing a client-server appli-
cation, and we’ll “get our hands dirty” by looking at code that implements a very 
simple client-server application. During the development phase, one of the first deci-
sions the developer must make is whether the application is to run over TCP or over 
UDP. Recall that TCP is connection oriented and provides a reliable byte-stream 
channel through which data flows between two end systems. UDP is connectionless 
and sends independent packets of data from one end system to the other, without any 
guarantees about delivery. Recall also that when a client or server program imple-
ments a protocol defined by an RFC, it should use the well-known port number 
associated with the protocol; conversely, when developing a proprietary application, 
the developer must be careful to avoid using such well-known port numbers. (Port 
numbers were briefly discussed in Section 2.1. They are covered in more detail in 
Chapter 3.)
We introduce UDP and TCP socket programming by way of a simple UDP 
application and a simple TCP application. We present the simple UDP and TCP 
applications in Python 3. We could have written the code in Java, C, or C++, but we 
chose Python mostly because Python clearly exposes the key socket concepts. With 
Python there are fewer lines of code, and each line can be explained to the novice 
programmer without difficulty. But there’s no need to be frightened if you are not 
familiar with Python. You should be able to easily follow the code if you have expe-
rience programming in Java, C, or C++.
2.7    •    Socket Programming: Creating Network Applications         187
If you are interested in client-server programming with Java, you are encour-
aged to see the Companion Website for this textbook; in fact, you can find there 
all the examples in this section (and associated labs) in Java. For readers who are 
interested in client-server programming in C, there are several good references avail-
able [Donahoo 2001; Stevens 1997; Frost 1994; Kurose 1996]; our Python examples 
below have a similar look and feel to C.
2.7.1 Socket Programming with UDP
In this subsection, we’ll write simple client-server programs that use UDP; in the 
following section, we’ll write similar programs that use TCP.
Recall from Section 2.1 that processes running on different machines communi-
cate with each other by sending messages into sockets. We said that each process is 
analogous to a house and the process’s socket is analogous to a door. The application 
resides on one side of the door in the house; the transport-layer protocol resides on 
the other side of the door in the outside world. The application developer has control 
of everything on the application-layer side of the socket; however, it has little control 
of the transport-layer side.
Now let’s take a closer look at the interaction between two communicating pro-
cesses that use UDP sockets. Before the sending process can push a packet of data 
out the socket door, when using UDP, it must first attach a destination address to 
the packet. After the packet passes through the sender’s socket, the Internet will use 
this destination address to route the packet through the Internet to the socket in the 
receiving process. When the packet arrives at the receiving socket, the receiving 
process will retrieve the packet through the socket, and then inspect the packet’s 
contents and take appropriate action.
So you may be now wondering, what goes into the destination address that 
is attached to the packet? As you might expect, the destination host’s IP address 
is part of the destination address. By including the destination IP address in the 
packet, the routers in the Internet will be able to route the packet through the 
Internet to the destination host. But because a host may be running many net-
work application processes, each with one or more sockets, it is also necessary 
to identify the particular socket in the destination host. When a socket is created, 
an identifier, called a port number, is assigned to it. So, as you might expect, 
the packet’s destination address also includes the socket’s port number. In sum-
mary, the sending process attaches to the packet a destination address, which con-
sists of the destination host’s IP address and the destination socket’s port number. 
 
Moreover, as we shall soon see, the sender’s source address—consisting of the 
IP address of the source host and the port number of the source socket—are also 
attached to the packet. However, attaching the source address to the packet is typi-
cally not done by the UDP application code; instead it is automatically done by the 
underlying operating system.
188         Chapter 2    •    Application Layer
We’ll use the following simple client-server application to demonstrate socket 
programming for both UDP and TCP:
	1.	 The client reads a line of characters (data) from its keyboard and sends the data 
to the server.
	2.	 The server receives the data and converts the characters to uppercase.
	3.	 The server sends the modified data to the client.
	4.	 The client receives the modified data and displays the line on its screen.
Figure 2.27 highlights the main socket-related activity of the client and server that 
communicate over the UDP transport service.
Now let’s get our hands dirty and take a look at the client-server program 
pair for a UDP implementation of this simple application. We also provide a 
detailed, line-by-line analysis after each program. We’ll begin with the UDP cli-
ent, which will send a simple application-level message to the server. In order for 
Create  socket, port=x:
Server
serverSocket =
socket(AF_INET,SOCK_DGRAM)
(Running on serverIP)
Client
Read UDP segment from
serverSocket
Write reply to
specifying client address,
port number
serverSocket
Create datagram with serverIP
and port=x;
send datagram via
clientSocket
Create socket:
clientSocket =
socket(AF_INET,SOCK_DGRAM)
Read datagram from
clientSocket
Close
clientSocket
Figure 2.27  ♦  The client-server application using UDP
2.7    •    Socket Programming: Creating Network Applications         189
the server to be able to receive and reply to the client’s message, it must be ready 
and running—that is, it must be running as a process before the client sends its 
message.
The client program is called UDPClient.py, and the server program is called 
UDPServer.py. In order to emphasize the key issues, we intentionally provide code 
that is minimal. “Good code” would certainly have a few more auxiliary lines, in 
particular for handling error cases. For this application, we have arbitrarily chosen 
12000 for the server port number.
UDPClient.py
Here is the code for the client side of the application:
from socket import *
serverName = ’hostname’
serverPort = 12000
clientSocket = socket(AF_INET, SOCK_DGRAM)
message = raw_input(’Input lowercase sentence:’)
clientSocket.sendto(message.encode(),(serverName, serverPort))
modifiedMessage, serverAddress = clientSocket.recvfrom(2048)
print(modifiedMessage.decode())
clientSocket.close()
Now let’s take a look at the various lines of code in UDPClient.py.
from socket import *
The socket module forms the basis of all network communications in Python. By 
including this line, we will be able to create sockets within our program.
serverName = ’hostname’
serverPort = 12000
The first line sets the variable serverName to the string ‘hostname’. Here, we pro-
vide a string containing either the IP address of the server (e.g., “128.138.32.126”) 
or the hostname of the server (e.g., “cis.poly.edu”). If we use the hostname, then a 
DNS lookup will automatically be performed to get the IP address.) The second line 
sets the integer variable serverPort to 12000.
clientSocket = socket(AF_INET, SOCK_DGRAM)
190         Chapter 2    •    Application Layer
This line creates the client’s socket, called clientSocket. The first param-
eter indicates the address family; in particular, AF_INET indicates that the 
underlying network is using IPv4. (Do not worry about this now—we will dis-
cuss IPv4 in Chapter 4.) The second parameter indicates that the socket is of 
type SOCK_DGRAM, which means it is a UDP socket (rather than a TCP socket). 
Note that we are not specifying the port number of the client socket when we 
create it; we are instead letting the operating system do this for us. Now that the 
client process’s door has been created, we will want to create a message to send 
through the door.
message = raw_input(’Input lowercase sentence:’)
raw_input() is a built-in function in Python. When this command is executed, 
the user at the client is prompted with the words “Input lowercase sentence:” The 
user then uses her keyboard to input a line, which is put into the variable message. 
Now that we have a socket and a message, we will want to send the message through 
the socket to the destination host.
clientSocket.sendto(message.encode(),(serverName, serverPort))
In the above line, we first convert the message from string type to byte type, as we 
need to send bytes into a socket; this is done with the encode() method. The 
 
method sendto() attaches the destination address (serverName, serverPort) 
 
to the message and sends the resulting packet into the process’s socket, 
 
clientSocket. (As mentioned earlier, the source address is also attached to 
the packet, although this is done automatically rather than explicitly by the code.) 
 
Sending a client-to-server message via a UDP socket is that simple! After sending 
the packet, the client waits to receive data from the server.
modifiedMessage, serverAddress = clientSocket.recvfrom(2048)
With the above line, when a packet arrives from the Internet at the client’s socket, the 
packet’s data is put into the variable modifiedMessage and the packet’s source 
address is put into the variable serverAddress. The variable serverAddress 
contains both the server’s IP address and the server’s port number. The program 
UDPClient doesn’t actually need this server address information, since it already 
knows the server address from the outset; but this line of Python provides the server 
address nevertheless. The method recvfrom also takes the buffer size 2048 as 
input. (This buffer size works for most purposes.)
print(modifiedMessage.decode())
2.7    •    Socket Programming: Creating Network Applications         191
This line prints out modifiedMessage on the user’s display, after converting the mes-
sage from bytes to string. It should be the original line that the user typed, but now 
capitalized.
clientSocket.close()
This line closes the socket. The process then terminates.
UDPServer.py
Let’s now take a look at the server side of the application:
from socket import *
serverPort = 12000
serverSocket = socket(AF_INET, SOCK_DGRAM)
serverSocket.bind((’’, serverPort))
print(”The server is ready to receive”)
while True:
    message, clientAddress = serverSocket.recvfrom(2048)
    modifiedMessage = message.decode().upper()
    serverSocket.sendto(modifiedMessage.encode(), clientAddress)
Note that the beginning of UDPServer is similar to UDPClient. It also imports the 
socket module, also sets the integer variable serverPort to 12000, and also 
 
creates a socket of type SOCK_DGRAM (a UDP socket). The first line of code that is 
significantly different from UDPClient is:
serverSocket.bind((’’, serverPort))
The above line binds (that is, assigns) the port number 12000 to the server’s socket. 
Thus in UDPServer, the code (written by the application developer) is explicitly 
assigning a port number to the socket. In this manner, when anyone sends a packet to 
port 12000 at the IP address of the server, that packet will be directed to this socket. 
UDPServer then enters a while loop; the while loop will allow UDPServer to receive 
and process packets from clients indefinitely. In the while loop, UDPServer waits for 
a packet to arrive.
message, clientAddress = serverSocket.recvfrom(2048)
This line of code is similar to what we saw in UDPClient. When a packet arrives 
at the server’s socket, the packet’s data is put into the variable message and the 
192         Chapter 2    •    Application Layer
packet’s source address is put into the variable clientAddress. The variable 
­
clientAddress contains both the client’s IP address and the client’s port number. 
Here, UDPServer will make use of this address information, as it provides a return 
address, similar to the return address with ordinary postal mail. With this source 
address information, the server now knows to where it should direct its reply.
modifiedMessage = message.decode().upper()
This line is the heart of our simple application. It takes the line sent by the client and, 
after converting the message to a string, uses the method upper() to capitalize it.
serverSocket.sendto(modifiedMessage.encode(), clientAddress)
This last line attaches the client’s address (IP address and port number) to the capital-
ized message (after converting the string to bytes), and sends the resulting packet into 
the server’s socket. (As mentioned earlier, the server address is also attached to the 
 
packet, although this is done automatically rather than explicitly by the code.) The 
Internet will then deliver the packet to this client address. After the server sends 
 
the packet, it remains in the while loop, waiting for another UDP packet to arrive 
(from any client running on any host).
To test the pair of programs, you run UDPClient.py on one host and UDPServer.
py on another host. Be sure to include the proper hostname or IP address of the server 
in UDPClient.py. Next, you execute UDPServer.py, the compiled server program, in 
the server host. This creates a process in the server that idles until it is contacted by 
some client. Then you execute UDPClient.py, the compiled client program, in the 
client. This creates a process in the client. Finally, to use the application at the client, 
you type a sentence followed by a carriage return.
To develop your own UDP client-server application, you can begin by slightly 
modifying the client or server programs. For example, instead of converting all 
the letters to uppercase, the server could count the number of times the letter s 
appears and return this number. Or you can modify the client so that after receiv-
ing a capitalized sentence, the user can continue to send more sentences to the 
server.
2.7.2 Socket Programming with TCP
Unlike UDP, TCP is a connection-oriented protocol. This means that before the cli-
ent and server can start to send data to each other, they first need to handshake and 
establish a TCP connection. One end of the TCP connection is attached to the client 
socket and the other end is attached to a server socket. When creating the TCP con-
nection, we associate with it the client socket address (IP address and port number) 
and the server socket address (IP address and port number). With the TCP connec-
tion established, when one side wants to send data to the other side, it just drops the 
2.7    •    Socket Programming: Creating Network Applications         193
data into the TCP connection via its socket. This is different from UDP, for which 
the server must attach a destination address to the packet before dropping it into the 
socket.
Now let’s take a closer look at the interaction of client and server programs 
in TCP. The client has the job of initiating contact with the server. In order for the 
server to be able to react to the client’s initial contact, the server has to be ready. This 
implies two things. First, as in the case of UDP, the TCP server must be running as 
a process before the client attempts to initiate contact. Second, the server program 
must have a special door—more precisely, a special socket—that welcomes some 
initial contact from a client process running on an arbitrary host. Using our house/
door analogy for a process/socket, we will sometimes refer to the client’s initial con-
tact as “knocking on the welcoming door.”
With the server process running, the client process can initiate a TCP connection 
to the server. This is done in the client program by creating a TCP socket. When the 
client creates its TCP socket, it specifies the address of the welcoming socket in the 
server, namely, the IP address of the server host and the port number of the socket. 
After creating its socket, the client initiates a three-way handshake and establishes a 
TCP connection with the server. The three-way handshake, which takes place within 
the transport layer, is completely invisible to the client and server programs.
During the three-way handshake, the client process knocks on the welcom-
ing door of the server process. When the server “hears” the knocking, it creates a 
new door—more precisely, a new socket that is dedicated to that particular ­
client. 
In our example below, the welcoming door is a TCP socket object that we call 
­
serverSocket; the newly created socket dedicated to the client making the con-
nection is called connectionSocket. Students who are encountering TCP sock-
ets for the first time sometimes confuse the welcoming socket (which is the initial 
point of contact for all clients wanting to communicate with the server), and each 
newly created server-side connection socket that is subsequently created for com-
municating with each client.
From the application’s perspective, the client’s socket and the server’s con-
nection socket are directly connected by a pipe. As shown in Figure 2.28, the cli-
ent process can send arbitrary bytes into its socket, and TCP guarantees that the 
server process will receive (through the connection socket) each byte in the order 
sent. TCP thus provides a reliable service between the client and server processes. 
Furthermore, just as people can go in and out the same door, the client process 
not only sends bytes into but also receives bytes from its socket; similarly, the 
server process not only receives bytes from but also sends bytes into its connec-
tion socket.
We use the same simple client-server application to demonstrate socket pro-
gramming with TCP: The client sends one line of data to the server, the server 
capitalizes the line and sends it back to the client. Figure 2.29 highlights the main 
socket-related activity of the client and server that communicate over the TCP trans-
port service.
194         Chapter 2    •    Application Layer
TCPClient.py
Here is the code for the client side of the application:
from socket import *
serverName = ’servername’
serverPort = 12000
clientSocket = socket(AF_INET, SOCK_STREAM)
clientSocket.connect((serverName,serverPort))
sentence = raw_input(’Input lowercase sentence:’)
clientSocket.send(sentence.encode())
modifiedSentence = clientSocket.recv(1024)
print(’From Server: ’, modifiedSentence.decode()) 
clientSocket.close()
Let’s now take a look at the various lines in the code that differ significantly from the 
UDP implementation. The first such line is the creation of the client socket.
clientSocket = socket(AF_INET, SOCK_STREAM)
This line creates the client’s socket, called clientSocket. The first parameter 
again indicates that the underlying network is using IPv4. The second parameter 
Client process
Server process
Client
socket
Welcoming
socket
Three-way handshake
Connection
socket
bytes
bytes
Figure 2.28  ♦  The TCPServer process has two sockets
2.7    •    Socket Programming: Creating Network Applications         195
indicates that the socket is of type SOCK_STREAM, which means it is a TCP socket 
(rather than a UDP socket). Note that we are again not specifying the port number of 
the client socket when we create it; we are instead letting the operating system do this 
for us. Now the next line of code is very different from what we saw in UDPClient:
clientSocket.connect((serverName,serverPort))
Recall that before the client can send data to the server (or vice versa) using a TCP 
socket, a TCP connection must first be established between the client and server. The 
Close
connectionSocket
Write reply to
connectionSocket
Read request from
connectionSocket
Create  socket, port=x,
for incoming request:
Server
serverSocket =
socket()
Wait for incoming
connection request:
connectionSocket =
serverSocket.accept()
(Running on serverIP)
Client
TCP
connection setup
Create socket, connect
to serverIP, port=x:
clientSocket =
socket()
Read reply from
clientSocket
Send request using
clientSocket
Close
clientSocket
Figure 2.29  ♦  The client-server application using TCP
196         Chapter 2    •    Application Layer
above line initiates the TCP connection between the client and server. The parameter 
of the connect() method is the address of the server side of the connection. After 
this line of code is executed, the three-way handshake is performed and a TCP con-
nection is established between the client and server.
sentence = raw_input(’Input lowercase sentence:’)
As with UDPClient, the above obtains a sentence from the user. The string 
 
sentence continues to gather characters until the user ends the line by typing a 
carriage return. The next line of code is also very different from UDPClient:
clientSocket.send(sentence.encode())
The above line sends the sentence through the client’s socket and into the TCP 
connection. Note that the program does not explicitly create a packet and attach the 
destination address to the packet, as was the case with UDP sockets. Instead the cli-
ent program simply drops the bytes in the string sentence into the TCP connec-
tion. The client then waits to receive bytes from the server.
modifiedSentence = clientSocket.recv(2048)
When characters arrive from the server, they get placed into the string 
 
modifiedSentence. Characters continue to accumulate in modifiedSen-
tence until the line ends with a carriage return character. After printing the capital-
ized sentence, we close the client’s socket:
clientSocket.close()
This last line closes the socket and, hence, closes the TCP connection between the 
client and the server. It causes TCP in the client to send a TCP message to TCP in 
the server (see Section 3.5).
TCPServer.py
Now let’s take a look at the server program.
from socket import *
serverPort = 12000
serverSocket = socket(AF_INET,SOCK_STREAM)
serverSocket.bind((’’,serverPort))
serverSocket.listen(1)
print(’The server is ready to receive’)
2.7    •    Socket Programming: Creating Network Applications         197
while True:
    connectionSocket, addr = serverSocket.accept()
    sentence = connectionSocket.recv(1024).decode()
    capitalizedSentence = sentence.upper()
    connectionSocket.send(capitalizedSentence.encode()) 
    connectionSocket.close()
Let’s now take a look at the lines that differ significantly from UDPServer and TCP-
Client. As with TCPClient, the server creates a TCP socket with:
serverSocket=socket(AF_INET,SOCK_STREAM)
Similar to UDPServer, we associate the server port number, serverPort, with 
this socket:
serverSocket.bind((’’,serverPort))
But with TCP, serverSocket will be our welcoming socket. After establish-
ing this welcoming door, we will wait and listen for some client to knock on the 
door:
serverSocket.listen(1)
This line has the server listen for TCP connection requests from the client. The 
parameter specifies the maximum number of queued connections (at least 1).
connectionSocket, addr = serverSocket.accept()
When a client knocks on this door, the program invokes the accept() method for 
serverSocket, which creates a new socket in the server, called ­
connectionSocket, 
dedicated to this particular client. The client and server then complete the hand-
shaking, creating a TCP connection between the client’s clientSocket and the 
server’s connectionSocket. With the TCP connection established, the client 
and server can now send bytes to each other over the connection. With TCP, all bytes 
sent from one side not are not only guaranteed to arrive at the other side but also 
guaranteed arrive in order.
connectionSocket.close()
In this program, after sending the modified sentence to the client, we close the con-
nection socket. But since serverSocket remains open, another client can now 
knock on the door and send the server a sentence to modify.
198         Chapter 2    •    Application Layer
This completes our discussion of socket programming in TCP. You are encour-
aged to run the two programs in two separate hosts, and also to modify them to 
achieve slightly different goals. You should compare the UDP program pair with the 
TCP program pair and see how they differ. You should also do many of the socket 
programming assignments described at the ends of Chapter 2, 4, and 9. Finally, we 
hope someday, after mastering these and more advanced socket programs, you will 
write your own popular network application, become very rich and famous, and 
remember the authors of this textbook!
2.8	 Summary
In this chapter, we’ve studied the conceptual and the implementation aspects of 
network applications. We’ve learned about the ubiquitous client-server architecture 
adopted by many Internet applications and seen its use in the HTTP, SMTP, POP3, 
and DNS protocols. We’ve studied these important application-level protocols, 
and their corresponding associated applications (the Web, file transfer, e-mail, and 
DNS) in some detail. We’ve learned about the P2P architecture and how it is used 
in many applications. We’ve also learned about streaming video, and how modern 
video distribution systems leverage CDNs. We’ve examined how the socket API 
can be used to build network applications. We’ve walked through the use of sock-
ets for connection-oriented (TCP) and connectionless (UDP) end-to-end transport 
services. The first step in our journey down the layered network architecture is now 
complete!
At the very beginning of this book, in Section 1.1, we gave a rather vague, bare-
bones definition of a protocol: “the format and the order of messages exchanged 
between two or more communicating entities, as well as the actions taken on the 
transmission and/or receipt of a message or other event.” The material in this chapter, 
and in particular our detailed study of the HTTP, SMTP, POP3, and DNS protocols, 
has now added considerable substance to this definition. Protocols are a key concept 
in networking; our study of application protocols has now given us the opportunity 
to develop a more intuitive feel for what protocols are all about.
In Section 2.1, we described the service models that TCP and UDP offer to 
applications that invoke them. We took an even closer look at these service models 
when we developed simple applications that run over TCP and UDP in Section 2.7. 
However, we have said little about how TCP and UDP provide these service models. 
For example, we know that TCP provides a reliable data service, but we haven’t said 
yet how it does so. In the next chapter we’ll take a careful look at not only the what, 
but also the how and why of transport protocols.
Equipped with knowledge about Internet application structure and application-
level protocols, we’re now ready to head further down the protocol stack and exam-
ine the transport layer in Chapter 3.
Homework Problems and Questions         199
Homework Problems and Questions
Chapter 2 Review Questions
SECTION 2.1
	R1.	 List five nonproprietary Internet applications and the application-layer proto-
cols that they use.
	R2.	 What is the difference between network architecture and application architecture?
	R3.	 For a communication session between a pair of processes, which process is 
the client and which is the server?
	R4.	 Why are the terms client and server still used in peer-to-peer applications?
	R5.	 What information is used by a process running on one host to identify a pro-
cess running on another host?
	R6.	 What is the role of HTTP in a network application? What other components 
are needed to complete a Web application?
	R7.	 Referring to Figure 2.4, we see that none of the applications listed in Figure 
2.4 requires both no data loss and timing. Can you conceive of an application 
that requires no data loss and that is also highly time-sensitive?
	R8.	 List the four broad classes of services that a transport protocol can provide. 
For each of the service classes, indicate if either UDP or TCP (or both) pro-
vides such a service.
	R9.	 Recall that TCP can be enhanced with SSL to provide process-to-process 
security services, including encryption. Does SSL operate at the transport 
layer or the application layer? If the application developer wants TCP to be 
enhanced with SSL, what does the developer have to do?
SECTION 2.2–2.5
	
R10.	 What is meant by a handshaking protocol?
	
R11.	 What does a stateless protocol mean? Is IMAP stateless? What about SMTP?
	
R12.	 How can websites keep track of users? Do they always need to use cookies?
	
R13.	 Describe how Web caching can reduce the delay in receiving a requested 
object. Will Web caching reduce the delay for all objects requested by a user 
or for only some of the objects? Why?
	
R14.	 Telnet into a Web server and send a multiline request message. Include in 
the request message the If-modified-since: header line to force a 
response message with the 304 Not Modified status code.
	
R15.	 Are there any constraints on the format of the HTTP body? What about the 
email message body sent with SMTP? How can arbitrary data be transmitted 
over SMTP?
200         Chapter 2    •    Application Layer
	
R16.	 Suppose Alice, with a Web-based e-mail account (such as Hotmail or Gmail), 
sends a message to Bob, who accesses his mail from his mail server using 
POP3. Discuss how the message gets from Alice’s host to Bob’s host. Be 
sure to list the series of application-layer protocols that are used to move the 
message between the two hosts.
	
R17.	 Print out the header of an e-mail message you have recently received. How 
many Received: header lines are there? Analyze each of the header lines 
in the message.
	
R18.	 Assume you have multiple devices, and you connect to your email provider 
using POP3. You retrieve messages with the “download and keep” strategy 
from multiple devices. Can your email client tell if you have already read the 
message in this scenario?
	
R19.	 Why are MX records needed? Would it not be enough to use a CNAME 
record? (Assume the email client looks up email addresses through a Type 
A query and that the target host only runs an email server.)
	
R20.	 What is the difference between recursive and iterative DNS queries?
SECTION 2.5
	
R21.	 Under what circumstances is file downloading through P2P much faster 
than through a centralized client-server approach? Justify your answer using 
Equation 2.2.
	
R22.	 Consider a new peer Alice that joins BitTorrent without possessing any chunks. 
Without any chunks, she cannot become a top-four uploader for any of the other 
peers, since she has nothing to upload. How then will Alice get her first chunk?
	
R23.	 Assume a BitTorrent tracker suddenly becomes unavailable. What are its 
consequences? Can files still be downloaded?
SECTION 2.6
	
R24.	 CDNs typically adopt one of two different server placement philosophies. 
Name and briefly describe them.
	
R25.	 Besides network-related considerations such as delay, loss, and bandwidth 
performance, there are other important factors that go into designing a CDN 
server selection strategy. What are they?
SECTION 2.7
	
R26.	 In Section 2.7, the UDP server described needed only one socket, whereas 
the TCP server needed two sockets. Why? If the TCP server were to support 
n simultaneous connections, each from a different client host, how many 
sockets would the TCP server need?
Problems         201
	
R27.	 For the client-server application over TCP described in Section 2.7, why 
must the server program be executed before the client program? For the 
client-server application over UDP, why may the client program be executed 
before the server program?
Problems
	 P1.	 True or false?
a.	 A user requests a Web page that consists of some text and three images. 
For this page, the client will send one request message and receive four 
response messages.
b.	 Two distinct Web pages (for example, www.mit.edu/research 
.html and www.mit.edu/students.html) can be sent over the 
same persistent connection.
c.	 With nonpersistent connections between browser and origin server, it is 
possible for a single TCP segment to carry two distinct HTTP request 
messages.
d.	 The Date: header in the HTTP response message indicates when the 
object in the response was last modified.
e.	 HTTP response messages never have an empty message body.
	 P2.	 SMS, iMessage, and WhatsApp are all smartphone real-time messaging 
systems. After doing some research on the Internet, for each of these systems 
write one paragraph about the protocols they use. Then write a paragraph 
explaining how they differ.
	 P3.	 Assume you open a browser and enter http://yourbusiness.com/
about.html in the address bar. What happens until the webpage is dis-
played? Provide details about the protocol(s) used and a high-level description 
of the messages exchanged.
	 P4.	 Consider the following string of ASCII characters that were captured by 
Wireshark when the browser sent an HTTP GET message (i.e., this is the 
actual content of an HTTP GET message). The characters <cr><lf> are 
carriage return and line-feed characters (that is, the italized character string 
<cr> in the text below represents the single carriage-return character that was 
contained at that point in the HTTP header). Answer the following questions, 
indicating where in the HTTP GET message below you find the answer.
GET /cs453/index.html HTTP/1.1<cr><lf>Host: gai
a.cs.umass.edu<cr><lf>User-Agent: Mozilla/5.0 (
Windows;U; Windows NT 5.1; en-US; rv:1.7.2) Gec
ko/20040804 Netscape/7.2 (ax) <cr><lf>Accept:ex
t/xml, application/xml, application/xhtml+xml, text
/html;q=0.9, text/plain;q=0.8,image/png,*/*;q=0.5
202         Chapter 2    •    Application Layer
<cr><lf>Accept-Language: en-us,en;q=0.5<cr><lf>Accept-
Encoding: zip,deflate<cr><lf>Accept-Charset: ISO
-8859-1,utf-8;q=0.7,*;q=0.7<cr><lf>Keep-Alive: 300<cr>
<lf>Connection:keep-alive<cr><lf><cr><lf>
a.	 What is the URL of the document requested by the browser?
b.	 What version of HTTP is the browser running?
c.	 Does the browser request a non-persistent or a persistent connection?
d.	 What is the IP address of the host on which the browser is running?
e.	 What type of browser initiates this message? Why is the browser type 
needed in an HTTP request message?
	 P5.	 The text below shows the reply sent from the server in response to the HTTP 
GET message in the question above. Answer the following questions, indicat-
ing where in the message below you find the answer.
HTTP/1.1 200 OK<cr><lf>Date: Tue, 07 Mar 2008
12:39:45GMT<cr><lf>Server: Apache/2.0.52 (Fedora)
<cr><lf>Last-Modified: Sat, 10 Dec2005 18:27:46 
GMT<cr><lf>ETag: ”526c3-f22-a88a4c80”<cr><lf>Accept- 
Ranges: bytes<cr><lf>Content-Length: 3874<cr><lf> 
Keep-Alive: timeout=max=100<cr><lf>Connection:
Keep-Alive<cr><lf>Content-Type: text/html; charset= 
ISO-8859-1<cr><lf><cr><lf><!doctype html public ”- 
//w3c//dtd html 4.0transitional//en”><lf><html><lf> 
<head><lf> <meta http-equiv=”Content-Type”  
content=”text/html; charset=iso-8859-1”><lf> <meta
name=”GENERATOR” content=”Mozilla/4.79 [en] (Windows NT
5.0; U) Netscape]”><lf> <title>CMPSCI 453 / 591 /  
NTU-ST550ASpring 2005 homepage</title><lf></head><lf> 
<much more document text following here (not shown)>
a.	 Was the server able to successfully find the document or not? What time 
was the document reply provided?
b.	 When was the document last modified?
c.	 How many bytes are there in the document being returned?
d.	 What are the first 5 bytes of the document being returned? Did the server 
agree to a persistent connection?
	 P6.	 Obtain the HTTP/1.1 specification (RFC 2616). Answer the following  
questions:
a.	 Explain the mechanism used for signaling between the client and server 
to indicate that a persistent connection is being closed. Can the client, the 
server, or both signal the close of a connection?
Problems         203
b.	 What encryption services are provided by HTTP?
c.	 Can a client open three or more simultaneous connections with a given 
server?
d.	 Either a server or a client may close a transport connection between them 
if either one detects the connection has been idle for some time. Is it 
possible that one side starts closing a connection while the other side is 
transmitting data via this connection? Explain.
	 P7.	 Assume that the RTT between a client and the local DNS server is TTl,  
while the RTT between the local DNS server and other DNS servers is RTTr . 
Assume that no DNS server performs caching.
a.	 What is the total response time for the scenario illustrated in Figure 2.19?
b.	 What is the total response time for the scenario illustrated in Figure 2.20?
c.	 Assume now that the DNS record for the requested name is cached 
at the local DNS server. What is the total response time for the two 
scenarios?
	 P8.	 Referring to Problem P7, suppose the HTML file references eight very small 
objects on the same server. Neglecting transmission times, how much time 
elapses with
a.	 Non-persistent HTTP with no parallel TCP connections?
b.	 Non-persistent HTTP with the browser configured for 5 parallel  
connections?
c.	 Persistent HTTP?
	 P9.	 Consider Figure 2.12, for which there is an institutional network connected to 
the Internet. Suppose that the average object size is 850,000 bits and that the 
average request rate from the institution’s browsers to the origin servers is 16 
requests per second. Also suppose that the amount of time it takes from when 
the router on the Internet side of the access link forwards an HTTP request 
until it receives the response is three seconds on average (see Section 2.2.5). 
Model the total average response time as the sum of the average access delay 
(that is, the delay from Internet router to institution router) and the average 
Internet delay. For the average access delay, use ∆/(1 - ∆b), where ∆ is 
the average time required to send an object over the access link and b is the 
arrival rate of objects to the access link.
a.	 Find the total average response time.
b.	 Now suppose a cache is installed in the institutional LAN. Suppose the 
miss rate is 0.4. Find the total response time.
	
P10.	 Assume you request a webpage consisting of one document and five images. 
The document size is 1 kbyte, all images have the same size of 50 kbytes, the 
download rate is 1 Mbps, and the RTT is 100 ms. How long does it take to 
204         Chapter 2    •    Application Layer
obtain the whole webpage under the following conditions? (Assume no DNS 
name query is needed and the impact of the request line and the headers in 
the HTTP messages is negligible).
a.	 Nonpersistent HTTP with serial connections.
b.	 Nonpersistent HTTP with two parallel connections.
c.	 Nonpersistent HTTP with six parallel connections.
d.	 Persistent HTTP with one connection.
	
P11.	 Generalize the results obtained for the first and the last scenario in the previ-
ous problem to a document size of Ld bytes, N images with size of Li bytes 
(for 0 # i , N), a rate of R byte/s and an RTT of RTTavg.
	
P12.	 Write a simple TCP program for a server that accepts lines of input from a 
client and prints the lines onto the server’s standard output. (You can do this 
by modifying the TCPServer.py program in the text.) Compile and execute 
your program. On any other machine that contains a Web browser, set the 
proxy server in the browser to the host that is running your server program; 
also configure the port number appropriately. Your browser should now send 
its GET request messages to your server, and your server should display the 
messages on its standard output. Use this platform to determine whether your 
browser generates conditional GET messages for objects that are locally 
cached.
	
P13.	 Describe a few scenarios in which mail access protocols are not needed.
	
P14.	 Why does an SMTP server retry to transmit a message even though TCP is 
used to connect with the destination?
	
P15.	 Read RFC 5321 for SMTP. What does MTA stand for? Consider the follow-
ing received spam e-mail (modified from a real spam e-mail). Assuming only 
the originator of this spam e-mail is malicious and all other hosts are honest, 
identify the malacious host that has generated this spam e-mail.
From - Fri Nov 07 13:41:30 2008
Return-Path: <tennis5@pp33head.com>
Received: from barmail.cs.umass.edu (barmail.cs.umass.
edu
Problems         205
[128.119.240.3]) by cs.umass.edu (8.13.1/8.12.6) for
<hg@cs.umass.edu>; Fri, 7 Nov 2008 13:27:10 -0500
Received: from asusus-4b96 (localhost [127.0.0.1]) by
barmail.cs.umass.edu (Spam Firewall) for <hg@cs.umass.
edu>; Fri, 7
Nov 2008 13:27:07 -0500 (EST)
Received: from asusus-4b96 ([58.88.21.177]) by barmail.
cs.umass.edu
for <hg@cs.umass.edu>; Fri, 07 Nov 2008 13:27:07 -0500 
(EST)
Received: from [58.88.21.177] by inbnd55.exchangeddd.
com; Sat, 8
Nov 2008 01:27:07 +0700
From: ”Jonny” <tennis5@pp33head.com>
To: <hg@cs.umass.edu>
 
Subject: How to secure your savings
	
P16.	 Read the DNS SRV RFC, RFC 2782. What is the purpose of the SRV 
record?
	
P17.	 Consider accessing your e-mail with POP3.
a.	 Suppose you have configured your POP mail client to operate in the 
download-and-delete mode. Complete the following transaction:
C: list
S: 1 498
S: 2 912
S: .
C: retr 1
S: blah blah ...
S: ..........blah
S: .
?
?
b.	 Suppose you have configured your POP mail client to operate in the 
download-and-keep mode. Complete the following transaction:
C: list
S: 1 498
S: 2 912
S: .
C: retr 1
206         Chapter 2    •    Application Layer
S: blah blah ...
S: ..........blah
S: .
? 
?
c.	 Suppose you have configured your POP mail client to operate in the down-
load-and-keep mode. Using your transcript in part (b), suppose you retrieve 
messages 1 and 2, exit POP, and then five minutes later you again access POP 
to retrieve new e-mail. Suppose that in the five-minute interval no new mes-
sages have been sent to you. Provide a transcript of this second POP session.
	
P18.	 a.	 What is a whois database?
b.	 Use various whois databases on the Internet to obtain the names of two 
DNS servers. Indicate which whois databases you used.
c.	 Use nslookup on your local host to send DNS queries to three DNS serv-
ers: your local DNS server and the two DNS servers you found in part (b). 
Try querying for Type A, NS, and MX reports. Summarize your findings.
d.	 Use nslookup to find a Web server that has multiple IP addresses. Does 
the Web server of your institution (school or company) have multiple IP 
addresses?
e.	 Use the ARIN whois database to determine the IP address range used by 
your university.
f.	 Describe how an attacker can use whois databases and the nslookup tool 
to perform reconnaissance on an institution before launching an attack.
g.	 Discuss why whois databases should be publicly available.
	
P19.	 In this problem, we use the useful dig tool available on Unix and Linux hosts to 
explore the hierarchy of DNS servers. Recall that in Figure 2.19, a DNS server 
in the DNS hierarchy delegates a DNS query to a DNS server lower in the 
hierarchy, by sending back to the DNS client the name of that lower-level DNS 
server. First read the man page for dig, and then answer the following questions.
a.	 Starting with a root DNS server (from one of the root servers [a-m].
root-servers.net), initiate a sequence of queries for the IP address for your 
department’s Web server by using dig. Show the list of the names of DNS 
servers in the delegation chain in answering your query.
b.	 Repeat part (a) for several popular Web sites, such as google.com, yahoo 
.com, or amazon.com.
	
P20.	 Consider the scenarios illustrated in Figures 2.12 and 2.13. Assume the rate 
of the institutional network is Rl and that of the bottleneck link is Rb. Suppose 
there are N clients requesting a file of size L with HTTP at the same time. 
For what values of Rl would the file transfer takes less time when a proxy is 
installed at the institutional network? (Assume the RTT between a client and 
any other host in the institutional network is negligible.)
Problems         207
	
P21.	 Suppose that your department has a local DNS server for all computers in the 
department. You are an ordinary user (i.e., not a network/system administra-
tor). Can you determine if an external Web site was likely accessed from a 
computer in your department a couple of seconds ago? Explain.
	
P22.	 Consider distributing a file of F = 15 Gbits to N peers. The server has 
an upload rate of us = 30 Mbps, and each peer has a download rate of 
di = 2 Mbps and an upload rate of u. For N = 10, 100, and 1,000 and 
u = 300 Kbps, 700 Kbps, and 2 Mbps, prepare a chart giving the minimum 
distribution time for each of the combinations of N and u for both client-
server distribution and P2P distribution.
	
P23.	 Consider distributing a file of F bits to N peers using a client-server archi-
tecture. Assume a fluid model where the server can simultaneously transmit 
to multiple peers, transmitting to each peer at different rates, as long as the 
combined rate does not exceed us.
a.	 Suppose that us/N … dmin. Specify a distribution scheme that has a distri-
bution time of NF/us.
b.	 Suppose that us/N Ú dmin. Specify a distribution scheme that has a distri-
bution time of F/dmin.
c.	 Conclude that the minimum distribution time is in general given by 
max5NF/us, F/dmin6.
	
P24.	 Consider distributing a file of F bits to N peers using a P2P architecture. 
Assume a fluid model. For simplicity assume that dmin is very large, so that 
peer download bandwidth is never a bottleneck.
a.	 Suppose that us … (us + u1 + . . . + uN)/N. Specify a distribution 
scheme that has a distribution time of F/us.
b.	 Suppose that us Ú (us + u1 + . . . + uN)/N. Specify a distribution 
scheme that has a distribution time of NF/(us + u1 + . . . +  uN).
c.	 Conclude that the minimum distribution time is in general given by 
max5F/us, NF/(us + u1 + . . . + uN)6.
	
P25.	 Suppose Bob joins a BitTorrent torrent, but he does not want to upload any 
data to any other peers (so called free-riding).
a.	 Bob claims that he can receive a complete copy of the file that is shared 
by the swarm. Is Bob’s claim possible? Why or why not?
b.	 Bob further claims that he can further make his “free-riding” more 
efficient by using a collection of multiple computers (with distinct IP 
addresses) in the computer lab in his department. How can he do that?
208         Chapter 2    •    Application Layer
	
P26.	 Consider a DASH system for which there are N video versions (at N different 
rates and qualities) and N audio versions (at N different rates and qualities). 
Suppose we want to allow the player to choose at any time any of the N video 
versions and any of the N audio versions.
a.	 If we create files so that the audio is mixed in with the video, so server 
sends only one media stream at given time, how many files will the server 
need to store (each a different URL)?
b.	 If the server instead sends the audio and video streams separately and has the 
client synchronize the streams, how many files will the server need to store?
	
P27.	 Install and compile the Python programs TCPClient and UDPClient on one 
host and TCPServer and UDPServer on another host.
a.	 Suppose you run TCPClient before you run TCPServer. What happens? 
Why?
b.	 Suppose you run UDPClient before you run UDPServer. What happens? 
Why?
c.	 What happens if you use different port numbers for the client and server 
sides?
	
P28.	 Suppose that in UDPClient.py, after we create the socket, we add the line:
clientSocket.bind((’’, 5432))
Will it become necessary to change UDPServer.py? What are the port num-
bers for the sockets in UDPClient and UDPServer? What were they before 
making this change?
	
P29.	 Can you configure your browser to open multiple simultaneous connections 
to a Web site? What are the advantages and disadvantages of having a large 
number of simultaneous TCP connections?
	
P30.	 We have seen that Internet TCP sockets treat the data being sent as a 
byte stream but UDP sockets recognize message boundaries. What are 
one advantage and one disadvantage of byte-oriented API versus having 
the API explicitly recognize and preserve application-defined message 
boundaries?
	
P31.	 What is the server placement strategy adopted by Netflix for its CDN? How 
is content replicated at the different servers?
Socket Programming Assignments
The Companion Website includes six socket programming assignments. The 
first four assignments are summarized below. The fifth assignment makes use 
of the ICMP protocol and is summarized at the end of Chapter 5. The sixth 
Socket Programming Assignments         209
assignment employs multimedia protocols and is summarized at the end of 
 
Chapter 9. It is highly recommended that students complete several, if not all, of 
these assignments. Students can find full details of these assignments, as well as 
important snippets of the Python code, at the Web site www.pearsonglobaleditions 
.com/kurose.
Assignment 1: Web Server
In this assignment, you will develop a simple Web server in Python that is capable of 
processing only one request. Specifically, your Web server will (i) create a connec-
tion socket when contacted by a client (browser); (ii) receive the HTTP request from 
this connection; (iii) parse the request to determine the specific file being requested; 
(iv) get the requested file from the server’s file system; (v) create an HTTP response 
message consisting of the requested file preceded by header lines; and (vi) send the 
response over the TCP connection to the requesting browser. If a browser requests 
a file that is not present in your server, your server should return a “404 Not Found” 
error message.
In the Companion Website, we provide the skeleton code for your server. Your 
job is to complete the code, run your server, and then test your server by sending 
requests from browsers running on different hosts. If you run your server on a host 
that already has a Web server running on it, then you should use a different port than 
port 80 for your Web server.
Assignment 2: UDP Pinger
In this programming assignment, you will write a client ping program in Python. 
Your client will send a simple ping message to a server, receive a corresponding 
pong message back from the server, and determine the delay between when the client 
sent the ping message and received the pong message. This delay is called the Round 
Trip Time (RTT). The functionality provided by the client and server is similar to the 
functionality provided by standard ping program available in modern operating sys-
tems. However, standard ping programs use the Internet Control Message Protocol 
(ICMP) (which we will study in Chapter 5). Here we will create a nonstandard (but 
simple!) UDP-based ping program.
Your ping program is to send 10 ping messages to the target server over UDP. 
For each message, your client is to determine and print the RTT when the corre-
sponding pong message is returned. Because UDP is an unreliable protocol, a packet 
sent by the client or server may be lost. For this reason, the client cannot wait indefi-
nitely for a reply to a ping message. You should have the client wait up to one second 
for a reply from the server; if no reply is received, the client should assume that the 
packet was lost and print a message accordingly.
In this assignment, you will be given the complete code for the server (available 
in the Companion Website). Your job is to write the client code, which will be very 
210         Chapter 2    •    Application Layer
similar to the server code. It is recommended that you first study carefully the server 
code. You can then write your client code, liberally cutting and pasting lines from 
the server code.
Assignment 3: Mail Client
The goal of this programming assignment is to create a simple mail client that sends 
e-mail to any recipient. Your client will need to establish a TCP connection with 
a mail server (e.g., a Google mail server), dialogue with the mail server using the 
SMTP protocol, send an e-mail message to a recipient (e.g., your friend) via the mail 
server, and finally close the TCP connection with the mail server.
For this assignment, the Companion Website provides the skeleton code for 
your client. Your job is to complete the code and test your client by sending e-mail 
to different user accounts. You may also try sending through different servers (for 
example, through a Google mail server and through your university mail server).
Assignment 4: Multi-Threaded Web Proxy
In this assignment, you will develop a Web proxy. When your proxy receives an 
HTTP request for an object from a browser, it generates a new HTTP request for 
the same object and sends it to the origin server. When the proxy receives the cor-
responding HTTP response with the object from the origin server, it creates a new 
HTTP response, including the object, and sends it to the client. This proxy will be 
multi-threaded, so that it will be able to handle multiple requests at the same time.
For this assignment, the Companion Website provides the skeleton code for the 
proxy server. Your job is to complete the code, and then test it by having different 
browsers request Web objects via your proxy.
Wireshark Lab: HTTP
Having gotten our feet wet with the Wireshark packet sniffer in Lab 1, we’re now 
ready to use Wireshark to investigate protocols in operation. In this lab, we’ll explore 
several aspects of the HTTP protocol: the basic GET/reply interaction, HTTP message 
formats, retrieving large HTML files, retrieving HTML files with embedded URLs, 
persistent and non-persistent connections, and HTTP authentication and security.
As is the case with all Wireshark labs, the full description of this lab is available 
at this book’s Web site, www.pearsonglobaleditions.com/kurose.
Wireshark Lab: DNS         211
Wireshark Lab: DNS
In this lab, we take a closer look at the client side of the DNS, the protocol that 
translates Internet hostnames to IP addresses. Recall from Section 2.5 that the cli-
ent’s role in the DNS is relatively simple—a client sends a query to its local DNS 
server and receives a response back. Much can go on under the covers, invisible to 
the DNS clients, as the hierarchical DNS servers communicate with each other to 
either recursively or iteratively resolve the client’s DNS query. From the DNS cli-
ent’s standpoint, however, the protocol is quite simple—a query is formulated to the 
local DNS server and a response is received from that server. We observe DNS in 
action in this lab.
As is the case with all Wireshark labs, the full description of this lab is available 
at this book’s Web site, www.pearsonglobaleditions.com/kurose.
212
AN INTERVIEW WITH...
Marc Andreessen
Marc Andreessen is the co-creator of Mosaic, the Web browser 
that popularized the World Wide Web in 1993. Mosaic had 
a clean, easily understood interface and was the first browser to 
display images in-line with text. In 1994, Marc Andreessen and 
Jim Clark founded Netscape, whose browser was by far the most 
popular browser through the mid-1990s. Netscape also devel-
oped the Secure Sockets Layer (SSL) protocol and many Internet 
server products, including mail servers and SSL-based Web serv-
ers. He is now a co-founder and general partner of venture capital 
firm Andreessen Horowitz, overseeing portfolio development with 
holdings that include Facebook, Foursquare, Groupon, Jawbone, 
Twitter, and Zynga. He serves on numerous boards, including Bump, 
eBay, Glam Media, Facebook, and Hewlett-Packard. He holds a 
BS in Computer Science from the University of Illinois at Urbana-
Champaign.
How did you become interested in computing? Did you always know that you wanted to 
work in information technology?
The video game and personal computing revolutions hit right when I was growing up—
personal computing was the new technology frontier in the late 70’s and early 80’s. And 
it wasn’t just Apple and the IBM PC, but hundreds of new companies like Commodore 
and Atari as well. I taught myself to program out of a book called “Instant Freeze-Dried 
BASIC” at age 10, and got my first computer (a TRS-80 Color Computer—look it up!) 
at age 12.
Please describe one or two of the most exciting projects you have worked on during your 
career. What were the biggest challenges?
Undoubtedly the most exciting project was the original Mosaic web browser in ’92–’93—
and the biggest challenge was getting anyone to take it seriously back then. At the time, 
everyone thought the interactive future would be delivered as “interactive television” by 
huge companies, not as the Internet by startups.
213
What excites you about the future of networking and the Internet? What are your biggest 
concerns?
The most exciting thing is the huge unexplored frontier of applications and services that 
programmers and entrepreneurs are able to explore—the Internet has unleashed creativity 
at a level that I don’t think we’ve ever seen before. My biggest concern is the principle of 
unintended consequences—we don’t always know the implications of what we do, such as 
the Internet being used by governments to run a new level of surveillance on citizens.
Is there anything in particular students should be aware of as Web technology advances?
The rate of change—the most important thing to learn is how to learn—how to flexibly 
adapt to changes in the specific technologies, and how to keep an open mind on the new 
opportunities and possibilities as you move through your career.
What people inspired you professionally?
Vannevar Bush, Ted Nelson, Doug Engelbart, Nolan Bushnell, Bill Hewlett and Dave 
Packard, Ken Olsen, Steve Jobs, Steve Wozniak, Andy Grove, Grace Hopper, Hedy Lamarr, 
Alan Turing, Richard Stallman.
What are your recommendations for students who want to pursue careers in computing 
and information technology?
Go as deep as you possibly can on understanding how technology is created, and then com-
plement with learning how business works.
Can technology solve the world’s problems?
No, but we advance the standard of living of people through economic growth, and most 
economic growth throughout history has come from technology—so that’s as good as it 
gets.
This page intentionally left blank
215
Residing between the application and network layers, the transport layer is a central 
piece of the layered network architecture. It has the critical role of providing com-
munication services directly to the application processes running on different hosts. 
The pedagogic approach we take in this chapter is to alternate between discussions of 
transport-layer principles and discussions of how these principles are implemented 
in existing protocols; as usual, particular emphasis will be given to Internet proto-
cols, in particular the TCP and UDP transport-layer protocols.
We’ll begin by discussing the relationship between the transport and network 
layers. This sets the stage for examining the first critical function of the transport 
layer—extending the network layer’s delivery service between two end systems to 
a delivery service between two application-layer processes running on the end sys-
tems. We’ll illustrate this function in our coverage of the Internet’s connectionless 
transport protocol, UDP.
We’ll then return to principles and confront one of the most fundamental prob-
lems in computer networking—how two entities can communicate reliably over a 
medium that may lose and corrupt data. Through a series of increasingly complicated 
(and realistic!) scenarios, we’ll build up an array of techniques that transport proto-
cols use to solve this problem. We’ll then show how these principles are embodied 
in TCP, the Internet’s connection-oriented transport protocol.
We’ll next move on to a second fundamentally important problem in 
 
networking—controlling the transmission rate of transport-layer entities in order to 
avoid, or recover from, congestion within the network. We’ll consider the causes 
and consequences of congestion, as well as commonly used congestion-control 
3
CHAPTER
Transport 
Layer
216         Chapter 3    •    Transport Layer
techniques. After obtaining a solid understanding of the issues behind congestion 
control, we’ll study TCP’s approach to congestion control.
3.1	 Introduction and Transport-Layer Services
In the previous two chapters we touched on the role of the transport layer and the 
services that it provides. Let’s quickly review what we have already learned about 
the transport layer.
A transport-layer protocol provides for logical communication between 
application processes running on different hosts. By logical communication, we 
mean that from an application’s perspective, it is as if the hosts running the pro-
cesses were directly connected; in reality, the hosts may be on opposite sides of the 
planet, connected via numerous routers and a wide range of link types. Application 
processes use the logical communication provided by the transport layer to send 
messages to each other, free from the worry of the details of the physical infra-
structure used to carry these messages. Figure 3.1 illustrates the notion of logical 
communication.
As shown in Figure 3.1, transport-layer protocols are implemented in the end 
systems but not in network routers. On the sending side, the transport layer converts 
the application-layer messages it receives from a sending application process into 
transport-layer packets, known as transport-layer segments in Internet terminology. 
This is done by (possibly) breaking the application messages into smaller chunks 
and adding a transport-layer header to each chunk to create the transport-layer seg-
ment. The transport layer then passes the segment to the network layer at the send-
ing end system, where the segment is encapsulated within a network-layer packet (a 
datagram) and sent to the destination. It’s important to note that network routers act 
only on the network-layer fields of the datagram; that is, they do not examine the 
fields of the transport-layer segment encapsulated with the datagram. On the receiv-
ing side, the network layer extracts the transport-layer segment from the datagram 
and passes the segment up to the transport layer. The transport layer then processes 
the received segment, making the data in the segment available to the receiving 
application.
More than one transport-layer protocol may be available to network applications. 
For example, the Internet has two protocols—TCP and UDP. Each of these protocols 
provides a different set of transport-layer services to the invoking application.
3.1.1 Relationship Between Transport and Network Layers
Recall that the transport layer lies just above the network layer in the protocol 
stack. Whereas a transport-layer protocol provides logical communication between 
3.1    •    Introduction and Transport-Layer Services         217
Network
Data link
Physical
Application
Transport
Network
Data link
Physical
Network
Data link
Physical
Network
Data link
Physical
Application
Transport
Network
Data link
Physical
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Home Network
Logical end-to-end transport
Network
Data link
Physical
Network
Data link
Physical
Figure 3.1  ♦  
The transport layer provides logical rather than physical  
communication between application processes
218         Chapter 3    •    Transport Layer
processes running on different hosts, a network-layer protocol provides logical- 
communication between hosts. This distinction is subtle but important. Let’s exam-
ine this distinction with the aid of a household analogy.
Consider two houses, one on the East Coast and the other on the West Coast, 
with each house being home to a dozen kids. The kids in the East Coast household 
are cousins of the kids in the West Coast household. The kids in the two households 
love to write to each other—each kid writes each cousin every week, with each letter 
delivered by the traditional postal service in a separate envelope. Thus, each house-
hold sends 144 letters to the other household every week. (These kids would save a lot 
of money if they had e-mail!) In each of the households there is one kid—Ann in the 
West Coast house and Bill in the East Coast house—responsible for mail collection 
 
and mail distribution. Each week Ann visits all her brothers and sisters, collects the 
mail, and gives the mail to a postal-service mail carrier, who makes daily visits to 
the house. When letters arrive at the West Coast house, Ann also has the job of dis-
tributing the mail to her brothers and sisters. Bill has a similar job on the East Coast.
In this example, the postal service provides logical communication between the 
two houses—the postal service moves mail from house to house, not from person to 
person. On the other hand, Ann and Bill provide logical communication among the 
cousins—Ann and Bill pick up mail from, and deliver mail to, their brothers and sis-
ters. Note that from the cousins’ perspective, Ann and Bill are the mail service, even 
though Ann and Bill are only a part (the end-system part) of the end-to-end delivery 
process. This household example serves as a nice analogy for explaining how the 
transport layer relates to the network layer:
application messages = letters in envelopes
processes = cousins
hosts (also called end systems) = houses
transport-layer protocol = Ann and Bill
network-layer protocol = postal service (including mail carriers)
Continuing with this analogy, note that Ann and Bill do all their work within 
their respective homes; they are not involved, for example, in sorting mail in 
any intermediate mail center or in moving mail from one mail center to another. 
 
Similarly, transport-layer protocols live in the end systems. Within an end system, a 
transport protocol moves messages from application processes to the network edge 
(that is, the network layer) and vice versa, but it doesn’t have any say about how the 
messages are moved within the network core. In fact, as illustrated in Figure 3.1, 
intermediate routers neither act on, nor recognize, any information that the transport 
layer may have added to the application messages.
Continuing with our family saga, suppose now that when Ann and Bill go on 
vacation, another cousin pair—say, Susan and Harvey—substitute for them and pro-
vide the household-internal collection and delivery of mail. Unfortunately for the 
two families, Susan and Harvey do not do the collection and delivery in exactly 
3.1    •    Introduction and Transport-Layer Services         219
the same way as Ann and Bill. Being younger kids, Susan and Harvey pick up and 
drop off the mail less frequently and occasionally lose letters (which are sometimes 
chewed up by the family dog). Thus, the cousin-pair Susan and Harvey do not pro-
vide the same set of services (that is, the same service model) as Ann and Bill. In 
an analogous manner, a computer network may make available multiple transport 
protocols, with each protocol offering a different service model to applications.
The possible services that Ann and Bill can provide are clearly constrained by 
the possible services that the postal service provides. For example, if the postal ser-
vice doesn’t provide a maximum bound on how long it can take to deliver mail 
between the two houses (for example, three days), then there is no way that Ann and 
Bill can guarantee a maximum delay for mail delivery between any of the cousin 
pairs. In a similar manner, the services that a transport protocol can provide are often 
constrained by the service model of the underlying network-layer protocol. If the 
network-layer protocol cannot provide delay or bandwidth guarantees for transport-
layer segments sent between hosts, then the transport-layer protocol cannot provide 
delay or bandwidth guarantees for application messages sent between processes.
Nevertheless, certain services can be offered by a transport protocol even when 
the underlying network protocol doesn’t offer the corresponding service at the net-
work layer. For example, as we’ll see in this chapter, a transport protocol can offer 
reliable data transfer service to an application even when the underlying network 
protocol is unreliable, that is, even when the network protocol loses, garbles, or 
duplicates packets. As another example (which we’ll explore in Chapter 8 when we 
discuss network security), a transport protocol can use encryption to guarantee that 
application messages are not read by intruders, even when the network layer cannot 
guarantee the confidentiality of transport-layer segments.
3.1.2 Overview of the Transport Layer in the Internet
Recall that the Internet makes two distinct transport-layer protocols available to the 
application layer. One of these protocols is UDP (User Datagram Protocol), which 
provides an unreliable, connectionless service to the invoking application. The sec-
ond of these protocols is TCP (Transmission Control Protocol), which provides a 
reliable, connection-oriented service to the invoking application. When designing a 
network application, the application developer must specify one of these two trans-
port protocols. As we saw in Section 2.7, the application developer selects between 
UDP and TCP when creating sockets.
To simplify terminology, we refer to the transport-layer packet as a segment. We 
mention, however, that the Internet literature (for example, the RFCs) also refers to 
the transport-layer packet for TCP as a segment but often refers to the packet for UDP 
as a datagram. But this same Internet literature also uses the term datagram for the 
network-layer packet! For an introductory book on computer networking such as this, 
we believe that it is less confusing to refer to both TCP and UDP packets as segments, 
and reserve the term datagram for the network-layer packet.
220         Chapter 3    •    Transport Layer
Before proceeding with our brief introduction of UDP and TCP, it will be useful 
to say a few words about the Internet’s network layer. (We’ll learn about the network 
layer in detail in Chapters 4 and 5.) The Internet’s network-layer protocol has a 
name—IP, for Internet Protocol. IP provides logical communication between hosts. 
The IP service model is a best-effort delivery service. This means that IP makes 
its “best effort” to deliver segments between communicating hosts, but it makes no 
guarantees. In particular, it does not guarantee segment delivery, it does not guaran-
tee orderly delivery of segments, and it does not guarantee the integrity of the data 
in the segments. For these reasons, IP is said to be an unreliable service. We also 
mention here that every host has at least one network-layer address, a so-called IP 
address. We’ll examine IP addressing in detail in Chapter 4; for this chapter we need 
only keep in mind that each host has an IP address.
Having taken a glimpse at the IP service model, let’s now summarize the service 
models provided by UDP and TCP. The most fundamental responsibility of UDP 
and TCP is to extend IP’s delivery service between two end systems to a delivery 
service between two processes running on the end systems. Extending host-to-host 
delivery to process-to-process delivery is called transport-layer multiplexing and 
 
demultiplexing. We’ll discuss transport-layer multiplexing and demultiplexing in 
the next section. UDP and TCP also provide integrity checking by including error-
detection fields in their segments’ headers. These two minimal transport-layer 
services—process-to-process data delivery and error checking—are the only two 
services that UDP provides! In particular, like IP, UDP is an unreliable service—it 
does not guarantee that data sent by one process will arrive intact (or at all!) to the 
destination process. UDP is discussed in detail in Section 3.3.
TCP, on the other hand, offers several additional services to applications. First 
and foremost, it provides reliable data transfer. Using flow control, sequence 
numbers, acknowledgments, and timers (techniques we’ll explore in detail in this 
chapter), TCP ensures that data is delivered from sending process to receiving pro-
cess, correctly and in order. TCP thus converts IP’s unreliable service between end 
systems into a reliable data transport service between processes. TCP also provides 
congestion control. Congestion control is not so much a service provided to the 
invoking application as it is a service for the Internet as a whole, a service for the 
general good. Loosely speaking, TCP congestion control prevents any one TCP con-
nection from swamping the links and routers between communicating hosts with 
an excessive amount of traffic. TCP strives to give each connection traversing a 
congested link an equal share of the link bandwidth. This is done by regulating the 
rate at which the sending sides of TCP connections can send traffic into the network. 
UDP traffic, on the other hand, is unregulated. An application using UDP transport 
can send at any rate it pleases, for as long as it pleases.
A protocol that provides reliable data transfer and congestion control is neces-
sarily complex. We’ll need several sections to cover the principles of reliable data 
transfer and congestion control, and additional sections to cover the TCP protocol 
itself. These topics are investigated in Sections 3.4 through 3.8. The approach taken 
3.2    •    Multiplexing and Demultiplexing         221
in this chapter is to alternate between basic principles and the TCP protocol. For 
example, we’ll first discuss reliable data transfer in a general setting and then discuss 
how TCP specifically provides reliable data transfer. Similarly, we’ll first discuss 
congestion control in a general setting and then discuss how TCP performs conges-
tion control. But before getting into all this good stuff, let’s first look at transport-
layer multiplexing and demultiplexing.
3.2	 Multiplexing and Demultiplexing
In this section, we discuss transport-layer multiplexing and demultiplexing, that 
is, extending the host-to-host delivery service provided by the network layer to a 
 
process-to-process delivery service for applications running on the hosts. In order to 
keep the discussion concrete, we’ll discuss this basic transport-layer service in the 
context of the Internet. We emphasize, however, that a multiplexing/demultiplexing 
service is needed for all computer networks.
At the destination host, the transport layer receives segments from the network 
layer just below. The transport layer has the responsibility of delivering the data in 
these segments to the appropriate application process running in the host. Let’s take 
a look at an example. Suppose you are sitting in front of your computer, and you are 
downloading Web pages while running one FTP session and two Telnet sessions. 
You therefore have four network application processes running—two Telnet pro-
cesses, one FTP process, and one HTTP process. When the transport layer in your 
computer receives data from the network layer below, it needs to direct the received 
data to one of these four processes. Let’s now examine how this is done.
First recall from Section 2.7 that a process (as part of a network application) 
can have one or more sockets, doors through which data passes from the network to 
the process and through which data passes from the process to the network. Thus, 
as shown in Figure 3.2, the transport layer in the receiving host does not actually 
deliver data directly to a process, but instead to an intermediary socket. Because at 
any given time there can be more than one socket in the receiving host, each socket 
has a unique identifier. The format of the identifier depends on whether the socket is 
a UDP or a TCP socket, as we’ll discuss shortly.
Now let’s consider how a receiving host directs an incoming transport-layer 
segment to the appropriate socket. Each transport-layer segment has a set of fields in 
the segment for this purpose. At the receiving end, the transport layer examines these 
fields to identify the receiving socket and then directs the segment to that socket. 
This job of delivering the data in a transport-layer segment to the correct socket is 
called demultiplexing. The job of gathering data chunks at the source host from 
different sockets, encapsulating each data chunk with header information (that will 
later be used in demultiplexing) to create segments, and passing the segments to the 
network layer is called multiplexing. Note that the transport layer in the middle host 
222         Chapter 3    •    Transport Layer
in Figure 3.2 must demultiplex segments arriving from the network layer below to 
either process P1 or P2 above; this is done by directing the arriving segment’s data to 
the corresponding process’s socket. The transport layer in the middle host must also 
gather outgoing data from these sockets, form transport-layer segments, and pass 
these segments down to the network layer. Although we have introduced multiplex-
ing and demultiplexing in the context of the Internet transport protocols, it’s impor-
tant to realize that they are concerns whenever a single protocol at one layer (at the 
transport layer or elsewhere) is used by multiple protocols at the next higher layer.
To illustrate the demultiplexing job, recall the household analogy in the previous 
section. Each of the kids is identified by his or her name. When Bill receives a batch 
of mail from the mail carrier, he performs a demultiplexing operation by observing 
to whom the letters are addressed and then hand delivering the mail to his brothers 
and sisters. Ann performs a multiplexing operation when she collects letters from her 
brothers and sisters and gives the collected mail to the mail person.
Now that we understand the roles of transport-layer multiplexing and demulti-
plexing, let us examine how it is actually done in a host. From the discussion above, 
we know that transport-layer multiplexing requires (1) that sockets have unique 
identifiers, and (2) that each segment have special fields that indicate the socket to 
which the segment is to be delivered. These special fields, illustrated in Figure 3.3, 
are the source port number field and the destination port number field. (The UDP 
and TCP segments have other fields as well, as discussed in the subsequent sections 
of this chapter.) Each port number is a 16-bit number, ranging from 0 to 65535. 
The port numbers ranging from 0 to 1023 are called well-known port numbers  
and are restricted, which means that they are reserved for use by well-known 
Network
Key:
Process
Socket
Data link
Physical
Transport
Application
Network
Application
Data link
Physical
Transport
Network
Data link
Physical
Transport
P3
P2
P1
P4
Application
Figure 3.2  ♦  Transport-layer multiplexing and demultiplexing
3.2    •    Multiplexing and Demultiplexing         223
application protocols such as HTTP (which uses port number 80) and FTP (which 
uses port number 21). The list of well-known port numbers is given in RFC 1700 
and is updated at http://www.iana.org [RFC 3232]. When we develop a new appli-
cation (such as the simple application developed in Section 2.7), we must assign the 
application a port number.
It should now be clear how the transport layer could implement the demultiplex-
ing service: Each socket in the host could be assigned a port number, and when 
a segment arrives at the host, the transport layer examines the destination port 
number in the segment and directs the segment to the corresponding socket. The 
segment’s data then passes through the socket into the attached process. As we’ll 
see, this is basically how UDP does it. However, we’ll also see that multiplexing/
demultiplexing in TCP is yet more subtle.
Connectionless Multiplexing and Demultiplexing
Recall from Section 2.7.1 that the Python program running in a host can create a 
UDP socket with the line
clientSocket = socket(AF_INET, SOCK_DGRAM)
When a UDP socket is created in this manner, the transport layer automatically 
assigns a port number to the socket. In particular, the transport layer assigns a port 
number in the range 1024 to 65535 that is currently not being used by any other UDP 
port in the host. Alternatively, we can add a line into our Python program after we 
create the socket to associate a specific port number (say, 19157) to this UDP socket 
via the socket bind() method:
clientSocket.bind((’’, 19157))
Source port #
32 bits
Dest. port #
Other header ﬁelds
Application
data
(message)
Figure 3.3  ♦  
Source and destination port-number fields in a transport-layer 
segment
224         Chapter 3    •    Transport Layer
If the application developer writing the code were implementing the server side of 
a “well-known protocol,” then the developer would have to assign the correspond-
ing well-known port number. Typically, the client side of the application lets the 
transport layer automatically (and transparently) assign the port number, whereas the 
server side of the application assigns a specific port number.
With port numbers assigned to UDP sockets, we can now precisely describe 
UDP multiplexing/demultiplexing. Suppose a process in Host A, with UDP port 
19157, wants to send a chunk of application data to a process with UDP port 46428 in 
Host B. The transport layer in Host A creates a transport-layer segment that includes 
the application data, the source port number (19157), the destination port number 
(46428), and two other values (which will be discussed later, but are unimportant for 
the current discussion). The transport layer then passes the resulting segment to the 
network layer. The network layer encapsulates the segment in an IP datagram and 
makes a best-effort attempt to deliver the segment to the receiving host. If the seg-
ment arrives at the receiving Host B, the transport layer at the receiving host exam-
ines the destination port number in the segment (46428) and delivers the segment 
to its socket identified by port 46428. Note that Host B could be running multiple 
processes, each with its own UDP socket and associated port number. As UDP seg-
ments arrive from the network, Host B directs (demultiplexes) each segment to the 
appropriate socket by examining the segment’s destination port number.
It is important to note that a UDP socket is fully identified by a two-tuple consist-
ing of a destination IP address and a destination port number. As a consequence, if 
two UDP segments have different source IP addresses and/or source port numbers, but 
have the same destination IP address and destination port number, then the two seg-
ments will be directed to the same destination process via the same destination socket.
You may be wondering now, what is the purpose of the source port number? 
As shown in Figure 3.4, in the A-to-B segment the source port number serves as 
part of a “return address”—when B wants to send a segment back to A, the destina-
tion port in the B-to-A segment will take its value from the source port value of the 
A-to-B segment. (The complete return address is A’s IP address and the source port 
number.) As an example, recall the UDP server program studied in Section 2.7. In 
UDPServer.py, the server uses the recvfrom() method to extract the client-
side (source) port number from the segment it receives from the client; it then sends 
a new segment to the client, with the extracted source port number serving as the 
destination port number in this new segment.
Connection-Oriented Multiplexing and Demultiplexing
In order to understand TCP demultiplexing, we have to take a close look at TCP 
sockets and TCP connection establishment. One subtle difference between a 
TCP socket and a UDP socket is that a TCP socket is identified by a four-tuple: 
(source IP address, source port number, destination IP address, destination port 
number). Thus, when a TCP segment arrives from the network to a host, the host 
uses all four values to direct (demultiplex) the segment to the appropriate socket. 
 
3.2    •    Multiplexing and Demultiplexing         225
In particular, and in contrast with UDP, two arriving TCP segments with differ-
ent source IP addresses or source port numbers will (with the exception of a TCP 
segment carrying the original connection-establishment request) be directed to two 
different sockets. To gain further insight, let’s reconsider the TCP client-server pro-
gramming example in Section 2.7.2:
•	 The TCP server application has a “welcoming socket,” that waits for connection-
establishment requests from TCP clients (see Figure 2.29) on port number 12000.
•	 The TCP client creates a socket and sends a connection establishment request 
segment with the lines:
	 clientSocket = socket(AF_INET, SOCK_STREAM)
	 	
clientSocket.connect((serverName,12000))
•	 A connection-establishment request is nothing more than a TCP segment with 
destination port number 12000 and a special connection-establishment bit set in 
the TCP header (discussed in Section 3.5). The segment also includes a source 
port number that was chosen by the client.
•	 When the host operating system of the computer running the server process 
receives the incoming connection-request segment with destination port 12000, 
it locates the server process that is waiting to accept a connection on port number 
12000. The server process then creates a new socket:
	 connectionSocket, addr = serverSocket.accept()
Host A
Client process
Socket
Server B
source port:
19157
dest. port:
46428
source port:
46428
dest. port:
19157
Figure 3.4  ♦  The inversion of source and destination port numbers
226         Chapter 3    •    Transport Layer
•	 Also, the transport layer at the server notes the following four values in the con-
nection-request segment: (1) the source port number in the segment, (2) the IP 
address of the source host, (3) the destination port number in the segment, and 
(4) its own IP address. The newly created connection socket is identified by these 
four values; all subsequently arriving segments whose source port, source IP 
address, destination port, and destination IP address match these four values will 
be demultiplexed to this socket. With the TCP connection now in place, the client 
and server can now send data to each other.
The server host may support many simultaneous TCP connection sockets, with 
each socket attached to a process, and with each socket identified by its own four-
tuple. When a TCP segment arrives at the host, all four fields (source IP address, 
source port, destination IP address, destination port) are used to direct (demultiplex) 
the segment to the appropriate socket.
Port Scanning
We’ve seen that a server process waits patiently on an open port for contact by a 
remote client. Some ports are reserved for well-known applications (e.g., Web, FTP, 
DNS, and SMTP servers); other ports are used by convention by popular applications 
(e.g., the Microsoft 2000 SQL server listens for requests on UDP port 1434). Thus, 
if we determine that a port is open on a host, we may be able to map that port to a 
specific application running on the host. This is very useful for system administrators, 
who are often interested in knowing which network applications are running on the 
hosts in their networks. But attackers, in order to “case the joint,” also want to know 
which ports are open on target hosts. If a host is found to be running an application 
with a known security flaw (e.g., a SQL server listening on port 1434 was subject to 
a buffer overflow, allowing a remote user to execute arbitrary code on the vulnerable 
host, a flaw exploited by the Slammer worm [CERT 2003–04]), then that host is ripe 
for attack.
Determining which applications are listening on which ports is a relatively easy 
task. Indeed there are a number of public domain programs, called port scanners, 
that do just that. Perhaps the most widely used of these is nmap, freely available at 
http://nmap.org and included in most Linux distributions. For TCP, nmap sequentially 
scans ports, looking for ports that are accepting TCP connections. For UDP, nmap 
again sequentially scans ports, looking for UDP ports that respond to transmitted UDP 
segments. In both cases, nmap returns a list of open, closed, or unreachable ports. 
A host running nmap can attempt to scan any target host anywhere in the Internet. 
We’ll revisit nmap in Section 3.5.6, when we discuss TCP connection management.
FOCUS ON SECURITY
3.2    •    Multiplexing and Demultiplexing         227
The situation is illustrated in Figure 3.5, in which Host C initiates two HTTP 
sessions to server B, and Host A initiates one HTTP session to B. Hosts A and C 
and server B each have their own unique IP address—A, C, and B, respectively. 
Host C assigns two different source port numbers (26145 and 7532) to its two HTTP 
connections. Because Host A is choosing source port numbers independently of C, 
it might also assign a source port of 26145 to its HTTP connection. But this is not 
a problem—server B will still be able to correctly demultiplex the two connections 
having the same source port number, since the two connections have different source 
IP addresses.
Web Servers and TCP
Before closing this discussion, it’s instructive to say a few additional words about 
Web servers and how they use port numbers. Consider a host running a Web server, 
such as an Apache Web server, on port 80. When clients (for example, browsers) 
send segments to the server, all segments will have destination port 80. In particular, 
both the initial connection-establishment segments and the segments carrying HTTP 
source port:
7532
dest. port:
80
source IP:
C
dest. IP:
B
source port:
26145
dest. port:
80
source IP:
C
dest. IP:
B
source port:
26145
dest. port:
80
source IP:
A
dest. IP:
B
Per-connection
HTTP
processes
Transport-
layer
demultiplexing
Web 
server B
Web client
host C
Web client
host A
Figure 3.5  ♦  
Two clients, using the same destination port number (80) to 
communicate with the same Web server application
228         Chapter 3    •    Transport Layer
request messages will have destination port 80. As we have just described, the server 
distinguishes the segments from the different clients using source IP addresses and 
source port numbers.
Figure 3.5 shows a Web server that spawns a new process for each connec-
tion. As shown in Figure 3.5, each of these processes has its own connection socket 
through which HTTP requests arrive and HTTP responses are sent. We mention, 
however, that there is not always a one-to-one correspondence between connection 
sockets and processes. In fact, today’s high-performing Web servers often use only 
one process, and create a new thread with a new connection socket for each new 
client connection. (A thread can be viewed as a lightweight subprocess.) If you did 
the first programming assignment in Chapter 2, you built a Web server that does just 
this. For such a server, at any given time there may be many connection sockets (with 
different identifiers) attached to the same process.
If the client and server are using persistent HTTP, then throughout the duration 
of the persistent connection the client and server exchange HTTP messages via the 
same server socket. However, if the client and server use non-persistent HTTP, then 
a new TCP connection is created and closed for every request/response, and hence 
a new socket is created and later closed for every request/response. This frequent 
creating and closing of sockets can severely impact the performance of a busy Web 
server (although a number of operating system tricks can be used to mitigate the 
problem). Readers interested in the operating system issues surrounding persistent 
and non-persistent HTTP are encouraged to see [Nielsen 1997; Nahum 2002].
Now that we’ve discussed transport-layer multiplexing and demultiplexing, let’s 
move on and discuss one of the Internet’s transport protocols, UDP. In the next sec-
tion we’ll see that UDP adds little more to the network-layer protocol than a multi-
plexing/demultiplexing service.
3.3	 Connectionless Transport: UDP
In this section, we’ll take a close look at UDP, how it works, and what it does. 
We encourage you to refer back to Section 2.1, which includes an overview of the 
UDP service model, and to Section 2.7.1, which discusses socket programming using 
UDP.
To motivate our discussion about UDP, suppose you were interested in design-
ing a no-frills, bare-bones transport protocol. How might you go about doing this? 
You might first consider using a vacuous transport protocol. In particular, on the 
sending side, you might consider taking the messages from the application process 
and passing them directly to the network layer; and on the receiving side, you might 
consider taking the messages arriving from the network layer and passing them 
directly to the application process. But as we learned in the previous section, we have 
3.3    •    Connectionless Transport: UDP         229
to do a little more than nothing! At the very least, the transport layer has to provide a 
multiplexing/demultiplexing service in order to pass data between the network layer 
and the correct application-level process.
UDP, defined in [RFC 768], does just about as little as a transport protocol can do. 
Aside from the multiplexing/demultiplexing function and some light error checking, it 
adds nothing to IP. In fact, if the application developer chooses UDP instead of TCP, 
then the application is almost directly talking with IP. UDP takes messages from the 
application process, attaches source and destination port number fields for the multi-
plexing/demultiplexing service, adds two other small fields, and passes the resulting 
segment to the network layer. The network layer encapsulates the transport-layer seg-
ment into an IP datagram and then makes a best-effort attempt to deliver the segment 
to the receiving host. If the segment arrives at the receiving host, UDP uses the destina-
tion port number to deliver the segment’s data to the correct application process. Note 
that with UDP there is no handshaking between sending and receiving transport-layer 
entities before sending a segment. For this reason, UDP is said to be connectionless.
DNS is an example of an application-layer protocol that typically uses UDP. 
When the DNS application in a host wants to make a query, it constructs a DNS query 
message and passes the message to UDP. Without performing any handshaking with 
the UDP entity running on the destination end system, the host-side UDP adds header 
fields to the message and passes the resulting segment to the network layer. The net-
work layer encapsulates the UDP segment into a datagram and sends the datagram to 
a name server. The DNS application at the querying host then waits for a reply to its 
query. If it doesn’t receive a reply (possibly because the underlying network lost the 
query or the reply), it might try resending the query, try sending the query to another 
name server, or inform the invoking application that it can’t get a reply.
Now you might be wondering why an application developer would ever choose 
to build an application over UDP rather than over TCP. Isn’t TCP always preferable, 
since TCP provides a reliable data transfer service, while UDP does not? The answer 
is no, as some applications are better suited for UDP for the following reasons:
•	 Finer application-level control over what data is sent, and when. Under UDP, as 
soon as an application process passes data to UDP, UDP will package the data 
inside a UDP segment and immediately pass the segment to the network layer. 
TCP, on the other hand, has a congestion-control mechanism that throttles the 
transport-layer TCP sender when one or more links between the source and des-
tination hosts become excessively congested. TCP will also continue to resend a 
segment until the receipt of the segment has been acknowledged by the destina-
tion, regardless of how long reliable delivery takes. Since real-time applications 
often require a minimum sending rate, do not want to overly delay segment trans-
mission, and can tolerate some data loss, TCP’s service model is not particularly 
well matched to these applications’ needs. As discussed below, these applications 
can use UDP and implement, as part of the application, any additional functional-
ity that is needed beyond UDP’s no-frills segment-delivery service.
230         Chapter 3    •    Transport Layer
•	 No connection establishment. As we’ll discuss later, TCP uses a three-way hand-
shake before it starts to transfer data. UDP just blasts away without any formal 
preliminaries. Thus UDP does not introduce any delay to establish a connection. 
This is probably the principal reason why DNS runs over UDP rather than TCP—
DNS would be much slower if it ran over TCP. HTTP uses TCP rather than UDP, 
since reliability is critical for Web pages with text. But, as we briefly discussed 
in Section 2.2, the TCP connection-establishment delay in HTTP is an important 
contributor to the delays associated with downloading Web documents. Indeed, 
the QUIC protocol (Quick UDP Internet Connection, [Iyengar 2015]), used in 
Google’s Chrome browser, uses UDP as its underlying transport protocol and 
implements reliability in an application-layer protocol on top of UDP.
•	 No connection state. TCP maintains connection state in the end systems. This 
connection state includes receive and send buffers, congestion-control param-
eters, and sequence and acknowledgment number parameters. We will see in 
 
Section 3.5 that this state information is needed to implement TCP’s reliable data 
transfer service and to provide congestion control. UDP, on the other hand, does 
not maintain connection state and does not track any of these parameters. For this 
reason, a server devoted to a particular application can typically support many 
more active clients when the application runs over UDP rather than TCP.
•	 Small packet header overhead. The TCP segment has 20 bytes of header over-
head in every segment, whereas UDP has only 8 bytes of overhead.
Figure 3.6 lists popular Internet applications and the transport protocols that 
they use. As we expect, e-mail, remote terminal access, the Web, and file transfer 
run over TCP—all these applications need the reliable data transfer service of TCP. 
 
Nevertheless, many important applications run over UDP rather than TCP. For example, 
UDP is used to carry network management (SNMP; see Section 5.7) data. UDP is 
preferred to TCP in this case, since network management applications must often 
run when the network is in a stressed state—precisely when reliable, congestion-
controlled data transfer is difficult to achieve. Also, as we mentioned earlier, DNS 
runs over UDP, thereby avoiding TCP’s connection-establishment delays.
As shown in Figure 3.6, both UDP and TCP are somtimes used today with multi-
media applications, such as Internet phone, real-time video conferencing, and stream-
ing of stored audio and video. We’ll take a close look at these applications in Chapter 9. 
 
We just mention now that all of these applications can tolerate a small amount of 
packet loss, so that reliable data transfer is not absolutely critical for the application’s 
success. Furthermore, real-time applications, like Internet phone and video confer-
encing, react very poorly to TCP’s congestion control. For these reasons, developers 
of multimedia applications may choose to run their applications over UDP instead 
of TCP. When packet loss rates are low, and with some organizations blocking UDP 
traffic for security reasons (see Chapter 8), TCP becomes an increasingly attractive 
protocol for streaming media transport.
3.3    •    Connectionless Transport: UDP         231
Although commonly done today, running multimedia applications over UDP is 
controversial. As we mentioned above, UDP has no congestion control. But conges-
tion control is needed to prevent the network from entering a congested state in which 
very little useful work is done. If everyone were to start streaming high-bit-rate video 
without using any congestion control, there would be so much packet overflow at 
routers that very few UDP packets would successfully traverse the source-to-desti-
nation path. Moreover, the high loss rates induced by the uncontrolled UDP senders 
would cause the TCP senders (which, as we’ll see, do decrease their sending rates in 
the face of congestion) to dramatically decrease their rates. Thus, the lack of conges-
tion control in UDP can result in high loss rates between a UDP sender and receiver, 
and the crowding out of TCP sessions—a potentially serious problem [Floyd 1999]. 
Many researchers have proposed new mechanisms to force all sources, including 
UDP sources, to perform adaptive congestion control [Mahdavi 1997; Floyd 2000; 
Kohler 2006: RFC 4340].
Before discussing the UDP segment structure, we mention that it is possible for 
an application to have reliable data transfer when using UDP. This can be done if 
reliability is built into the application itself (for example, by adding acknowledgment 
and retransmission mechanisms, such as those we’ll study in the next section). We 
mentioned earlier that the QUIC protocol [Iyengar 2015] used in Google’s Chrome 
browser implements reliability in an application-layer protocol on top of UDP. But 
this is a nontrivial task that would keep an application developer busy debugging for 
a long time. Nevertheless, building reliability directly into the application allows the 
Electronic mail
Remote terminal access
Web
File transfer
Remote ﬁle server
Streaming multimedia
Internet telephony
Network management
Name translation
SMTP
Telnet
HTTP
FTP
NFS
typically proprietary
typically proprietary
SNMP
DNS
TCP
TCP
TCP
TCP
Typically UDP
UDP or TCP
UDP or TCP
Typically UDP
Typically UDP
Application
Application-Layer
Protocol
Underlying Transport
Protocol
Figure 3.6  ♦  
Popular Internet applications and their underlying transport  
protocols
232         Chapter 3    •    Transport Layer
application to “have its cake and eat it too. That is, application processes can commu-
nicate reliably without being subjected to the transmission-rate constraints imposed 
by TCP’s congestion-control mechanism.
3.3.1 UDP Segment Structure
The UDP segment structure, shown in Figure 3.7, is defined in RFC 768. The applica-
tion data occupies the data field of the UDP segment. For example, for DNS, the data 
field contains either a query message or a response message. For a streaming audio 
application, audio samples fill the data field. The UDP header has only four fields, 
each consisting of two bytes. As discussed in the previous section, the port numbers 
allow the destination host to pass the application data to the correct process run-
ning on the destination end system (that is, to perform the demultiplexing function). 
 
The length field specifies the number of bytes in the UDP segment (header plus 
data). An explicit length value is needed since the size of the data field may differ 
from one UDP segment to the next. The checksum is used by the receiving host to 
check whether errors have been introduced into the segment. In truth, the check-
sum is also calculated over a few of the fields in the IP header in addition to the 
UDP segment. But we ignore this detail in order to see the forest through the trees. 
We’ll discuss the checksum calculation below. Basic principles of error detection are 
described in Section 6.2. The length field specifies the length of the UDP segment, 
including the header, in bytes.
3.3.2 UDP Checksum
The UDP checksum provides for error detection. That is, the checksum is used to 
determine whether bits within the UDP segment have been altered (for example, by 
noise in the links or while stored in a router) as it moved from source to destination. 
Source port #
32 bits
Dest. port #
Length
Checksum
Application
data
(message)
Figure 3.7  ♦  UDP segment structure
3.3    •    Connectionless Transport: UDP         233
UDP at the sender side performs the 1s complement of the sum of all the 16-bit 
words in the segment, with any overflow encountered during the sum being wrapped 
around. This result is put in the checksum field of the UDP segment. Here we give 
a simple example of the checksum calculation. You can find details about efficient 
implementation of the calculation in RFC 1071 and performance over real data in 
[Stone 1998; Stone 2000]. As an example, suppose that we have the following three 
16-bit words:
0110011001100000
0101010101010101
1000111100001100
The sum of first two of these 16-bit words is
0110011001100000
0101010101010101
1011101110110101
Adding the third word to the above sum gives
1011101110110101
1000111100001100
0100101011000010
Note that this last addition had overflow, which was wrapped around. The 1s 
complement is obtained by converting all the 0s to 1s and converting all the 1s to 
0s. Thus the 1s complement of the sum 0100101011000010 is 1011010100111101, 
which becomes the checksum. At the receiver, all four 16-bit words are added, 
including the checksum. If no errors are introduced into the packet, then clearly the 
sum at the receiver will be 1111111111111111. If one of the bits is a 0, then we know 
that errors have been introduced into the packet.
You may wonder why UDP provides a checksum in the first place, as many 
link-layer protocols (including the popular Ethernet protocol) also provide error 
checking. The reason is that there is no guarantee that all the links between source 
and destination provide error checking; that is, one of the links may use a link-layer 
protocol that does not provide error checking. Furthermore, even if segments are 
correctly transferred across a link, it’s possible that bit errors could be introduced 
when a segment is stored in a router’s memory. Given that neither link-by-link reli-
ability nor in-memory error detection is guaranteed, UDP must provide error detec-
tion at the transport layer, on an end-end basis, if the end-end data transfer service 
is to provide error detection. This is an example of the celebrated end-end principle 
in system design [Saltzer 1984], which states that since certain functionality (error 
detection, in this case) must be implemented on an end-end basis: “functions placed 
234         Chapter 3    •    Transport Layer
at the lower levels may be redundant or of little value when compared to the cost of 
providing them at the higher level.”
Because IP is supposed to run over just about any layer-2 protocol, it is useful 
for the transport layer to provide error checking as a safety measure. Although UDP 
provides error checking, it does not do anything to recover from an error. Some 
implementations of UDP simply discard the damaged segment; others pass the dam-
aged segment to the application with a warning.
That wraps up our discussion of UDP. We will soon see that TCP offers reli-
able data transfer to its applications as well as other services that UDP doesn’t offer. 
Naturally, TCP is also more complex than UDP. Before discussing TCP, however, 
it will be useful to step back and first discuss the underlying principles of reliable 
data transfer.
3.4	 Principles of Reliable Data Transfer
In this section, we consider the problem of reliable data transfer in a general context. 
This is appropriate since the problem of implementing reliable data transfer occurs 
not only at the transport layer, but also at the link layer and the application layer as 
well. The general problem is thus of central importance to networking. Indeed, if one 
had to identify a “top-ten” list of fundamentally important problems in all of net-
working, this would be a candidate to lead the list. In the next section we’ll examine 
TCP and show, in particular, that TCP exploits many of the principles that we are 
about to describe.
Figure 3.8 illustrates the framework for our study of reliable data transfer. The 
service abstraction provided to the upper-layer entities is that of a reliable channel 
through which data can be transferred. With a reliable channel, no transferred data 
bits are corrupted (flipped from 0 to 1, or vice versa) or lost, and all are delivered in 
the order in which they were sent. This is precisely the service model offered by TCP 
to the Internet applications that invoke it.
It is the responsibility of a reliable data transfer protocol to implement this 
service abstraction. This task is made difficult by the fact that the layer below the 
reliable data transfer protocol may be unreliable. For example, TCP is a reliable data 
transfer protocol that is implemented on top of an unreliable (IP) end-to-end network 
layer. More generally, the layer beneath the two reliably communicating end points 
might consist of a single physical link (as in the case of a link-level data transfer 
protocol) or a global internetwork (as in the case of a transport-level protocol). For 
our purposes, however, we can view this lower layer simply as an unreliable point-
to-point channel.
In this section, we will incrementally develop the sender and receiver sides of 
a reliable data transfer protocol, considering increasingly complex models of the 
underlying channel. For example, we’ll consider what protocol mechanisms are 
3.4    •    Principles of Reliable Data Transfer         235
needed when the underlying channel can corrupt bits or lose entire packets. One 
assumption we’ll adopt throughout our discussion here is that packets will be deliv-
ered in the order in which they were sent, with some packets possibly being lost; 
that is, the underlying channel will not reorder packets. Figure 3.8(b) illustrates the 
interfaces for our data transfer protocol. The sending side of the data transfer proto-
col will be invoked from above by a call to rdt_send(). It will pass the data to be 
delivered to the upper layer at the receiving side. (Here rdt stands for reliable data 
transfer protocol and _send indicates that the sending side of rdt is being called. 
The first step in developing any protocol is to choose a good name!) On the receiving 
side, rdt_rcv() will be called when a packet arrives from the receiving side of the 
channel. When the rdt protocol wants to deliver data to the upper layer, it will do 
so by calling deliver_data(). In the following we use the terminology “packet” 
rather than transport-layer “segment.” Because the theory developed in this section 
Reliable channel
Unreliable channel
rdt_send()
udt_send()
Sending
process
Receiver
process
deliver_data
Application
layer
Transport
layer
a. Provided service
Network
layer
Key:
Data
Packet
b. Service implementation
Reliable data
transfer protocol
(sending side)
Reliable data
transfer protocol
(receiving side)
rdt_rcv()
Figure 3.8  ♦  
Reliable data transfer: Service model and service  
implementation
236         Chapter 3    •    Transport Layer
applies to computer networks in general and not just to the Internet transport layer, 
the generic term “packet” is perhaps more appropriate here.
In this section we consider only the case of unidirectional data transfer, that is, 
data transfer from the sending to the receiving side. The case of reliable bidirectional 
(that is, full-duplex) data transfer is conceptually no more difficult but considerably 
more tedious to explain. Although we consider only unidirectional data transfer, it is 
important to note that the sending and receiving sides of our protocol will nonetheless 
need to transmit packets in both directions, as indicated in Figure 3.8. We will see 
shortly that, in addition to exchanging packets containing the data to be transferred, 
the sending and receiving sides of rdt will also need to exchange control packets 
back and forth. Both the send and receive sides of rdt send packets to the other side 
by a call to udt_send() (where udt stands for unreliable data transfer).
3.4.1 Building a Reliable Data Transfer Protocol
We now step through a series of protocols, each one becoming more complex, arriv-
ing at a flawless, reliable data transfer protocol.
Reliable Data Transfer over a Perfectly Reliable Channel: rdt1.0
We first consider the simplest case, in which the underlying channel is completely 
reliable. The protocol itself, which we’ll call rdt1.0, is trivial. The finite-state 
machine (FSM) definitions for the rdt1.0 sender and receiver are shown in 
Figure 3.9. The FSM in Figure 3.9(a) defines the operation of the sender, while 
the FSM in Figure 3.9(b) defines the operation of the receiver. It is important to 
note that there are separate FSMs for the sender and for the receiver. The sender 
and receiver FSMs in Figure 3.9 each have just one state. The arrows in the FSM 
description indicate the transition of the protocol from one state to another. (Since 
each FSM in Figure 3.9 has just one state, a transition is necessarily from the one 
state back to itself; we’ll see more complicated state diagrams shortly.) The event 
causing the transition is shown above the horizontal line labeling the transition, and 
the actions taken when the event occurs are shown below the horizontal line. When 
no action is taken on an event, or no event occurs and an action is taken, we’ll use 
the symbol Λ below or above the horizontal, respectively, to explicitly denote the 
lack of an action or event. The initial state of the FSM is indicated by the dashed 
arrow. Although the FSMs in Figure 3.9 have but one state, the FSMs we will see 
shortly have multiple states, so it will be important to identify the initial state of 
each FSM.
The sending side of rdt simply accepts data from the upper layer via the 
rdt_send(data) event, creates a packet containing the data (via the action 
make_pkt(data)) and sends the packet into the channel. In practice, the 
 
rdt_send(data) event would result from a procedure call (for example, to 
rdt_send()) by the upper-layer application.
3.4    •    Principles of Reliable Data Transfer         237
On the receiving side, rdt receives a packet from the underlying channel via 
the rdt_rcv(packet) event, removes the data from the packet (via the action 
extract (packet, data)) and passes the data up to the upper layer (via 
the action deliver_data(data)). In practice, the rdt_rcv(packet) event 
would result from a procedure call (for example, to rdt_rcv()) from the lower-
layer protocol.
In this simple protocol, there is no difference between a unit of data and a packet. 
Also, all packet flow is from the sender to receiver; with a perfectly reliable chan-
nel there is no need for the receiver side to provide any feedback to the sender since 
nothing can go wrong! Note that we have also assumed that the receiver is able to 
receive data as fast as the sender happens to send data. Thus, there is no need for the 
receiver to ask the sender to slow down!
Reliable Data Transfer over a Channel with Bit Errors: rdt2.0
A more realistic model of the underlying channel is one in which bits in a packet may 
be corrupted. Such bit errors typically occur in the physical components of a network 
as a packet is transmitted, propagates, or is buffered. We’ll continue to assume for 
the moment that all transmitted packets are received (although their bits may be cor-
rupted) in the order in which they were sent.
Before developing a protocol for reliably communicating over such a channel, 
first consider how people might deal with such a situation. Consider how you yourself 
Wait for
call from
above
a.  rdt1.0: sending side
rdt_send(data)
packet=make_pkt(data)
udt_send(packet)
Wait for
call from
below
b.  rdt1.0: receiving side
rdt_rcv(packet)
extract(packet,data)
deliver_data(data)
Figure 3.9  ♦  rdt1.0 – A protocol for a completely reliable channel
238         Chapter 3    •    Transport Layer
might dictate a long message over the phone. In a typical scenario, the message taker 
might say “OK” after each sentence has been heard, understood, and recorded. If the 
message taker hears a garbled sentence, you’re asked to repeat the garbled sentence. 
This message-dictation protocol uses both positive acknowledgments (“OK”) and 
negative acknowledgments (“Please repeat that.”). These control messages allow 
the receiver to let the sender know what has been received correctly, and what has 
been received in error and thus requires repeating. In a computer network setting, 
reliable data transfer protocols based on such retransmission are known as ARQ 
(Automatic Repeat reQuest) protocols.
Fundamentally, three additional protocol capabilities are required in ARQ pro-
tocols to handle the presence of bit errors:
•	 Error detection. First, a mechanism is needed to allow the receiver to detect when 
bit errors have occurred. Recall from the previous section that UDP uses the Inter-
net checksum field for exactly this purpose. In Chapter 6 we’ll examine error-
detection and -correction techniques in greater detail; these techniques allow the 
receiver to detect and possibly correct packet bit errors. For now, we need only 
know that these techniques require that extra bits (beyond the bits of original data 
to be transferred) be sent from the sender to the receiver; these bits will be gath-
ered into the packet checksum field of the rdt2.0 data packet.
•	 Receiver feedback. Since the sender and receiver are typically executing on dif-
ferent end systems, possibly separated by thousands of miles, the only way for 
the sender to learn of the receiver’s view of the world (in this case, whether or not 
a packet was received correctly) is for the receiver to provide explicit feedback 
to the sender. The positive (ACK) and negative (NAK) acknowledgment replies 
in the message-dictation scenario are examples of such feedback. Our rdt2.0 
protocol will similarly send ACK and NAK packets back from the receiver to 
the sender. In principle, these packets need only be one bit long; for example, a 0 
value could indicate a NAK and a value of 1 could indicate an ACK.
•	 Retransmission. A packet that is received in error at the receiver will be retrans-
mitted by the sender.
Figure 3.10 shows the FSM representation of rdt2.0, a data transfer 
protocol employing error detection, positive acknowledgments, and negative 
acknowledgments.
The send side of rdt2.0 has two states. In the leftmost state, the send-side 
protocol is waiting for data to be passed down from the upper layer. When the 
rdt_send(data) event occurs, the sender will create a packet (sndpkt) con-
taining the data to be sent, along with a packet checksum (for example, as discussed 
in Section 3.3.2 for the case of a UDP segment), and then send the packet via the 
udt_send(sndpkt) operation. In the rightmost state, the sender protocol is wait-
ing for an ACK or a NAK packet from the receiver. If an ACK packet is received 
3.4    •    Principles of Reliable Data Transfer         239
(the notation rdt_rcv(rcvpkt) && isACK (rcvpkt) in Figure 3.10 cor-
responds to this event), the sender knows that the most recently transmitted packet 
has been received correctly and thus the protocol returns to the state of waiting for 
data from the upper layer. If a NAK is received, the protocol retransmits the last 
packet and waits for an ACK or NAK to be returned by the receiver in response to 
the retransmitted data packet. It is important to note that when the sender is in the 
wait-for-ACK-or-NAK state, it cannot get more data from the upper layer; that is, the 
rdt_send() event can not occur; that will happen only after the sender receives 
an ACK and leaves this state. Thus, the sender will not send a new piece of data until 
it is sure that the receiver has correctly received the current packet. Because of this 
behavior, protocols such as rdt2.0 are known as stop-and-wait protocols.
Wait for
call from
above
a.  rdt2.0: sending side
b.  rdt2.0: receiving side
rdt_rcv(rcvpkt) && corrupt(rcvpkt)
sndpkt=make_pkt(NAK)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && isNAK(rcvpkt)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && isACK(rcvpkt)

rdt_send(data)
sndpkt=make_pkt(data,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK)
udt_send(sndpkt)
Wait for
call from
below
Wait for
ACK or
NAK
Figure 3.10  ♦  rdt2.0 – A protocol for a channel with bit errors
240         Chapter 3    •    Transport Layer
The receiver-side FSM for rdt2.0 still has a single state. On packet arrival, 
the receiver replies with either an ACK or a NAK, depending on whether or not the 
received packet is corrupted. In Figure 3.10, the notation rdt_rcv(rcvpkt) && 
corrupt(rcvpkt) corresponds to the event in which a packet is received and is 
found to be in error.
Protocol rdt2.0 may look as if it works but, unfortunately, it has a fatal flaw. 
In particular, we haven’t accounted for the possibility that the ACK or NAK packet 
could be corrupted! (Before proceeding on, you should think about how this prob-
lem may be fixed.) Unfortunately, our slight oversight is not as innocuous as it may 
seem. Minimally, we will need to add checksum bits to ACK/NAK packets in order 
to detect such errors. The more difficult question is how the protocol should recover 
from errors in ACK or NAK packets. The difficulty here is that if an ACK or NAK 
is corrupted, the sender has no way of knowing whether or not the receiver has cor-
rectly received the last piece of transmitted data.
Consider three possibilities for handling corrupted ACKs or NAKs:
•	 For the first possibility, consider what a human might do in the message-dictation 
scenario. If the speaker didn’t understand the “OK” or “Please repeat that” reply 
from the receiver, the speaker would probably ask, “What did you say?” (thus 
introducing a new type of sender-to-receiver packet to our protocol). The receiver 
would then repeat the reply. But what if the speaker’s “What did you say?” is cor-
rupted? The receiver, having no idea whether the garbled sentence was part of the 
dictation or a request to repeat the last reply, would probably then respond with 
“What did you say?” And then, of course, that response might be garbled. Clearly, 
we’re heading down a difficult path.
•	 A second alternative is to add enough checksum bits to allow the sender not only 
to detect, but also to recover from, bit errors. This solves the immediate problem 
for a channel that can corrupt packets but not lose them.
•	 A third approach is for the sender simply to resend the current data packet when 
it receives a garbled ACK or NAK packet. This approach, however, introduces 
duplicate packets into the sender-to-receiver channel. The fundamental diffi-
culty with duplicate packets is that the receiver doesn’t know whether the ACK 
or NAK it last sent was received correctly at the sender. Thus, it cannot know a 
priori whether an arriving packet contains new data or is a retransmission!
A simple solution to this new problem (and one adopted in almost all exist-
ing data transfer protocols, including TCP) is to add a new field to the data packet 
and have the sender number its data packets by putting a sequence number into 
this field. The receiver then need only check this sequence number to determine 
whether or not the received packet is a retransmission. For this simple case of a 
stop-and-wait protocol, a 1-bit sequence number will suffice, since it will allow the 
receiver to know whether the sender is resending the previously transmitted packet 
3.4    •    Principles of Reliable Data Transfer         241
(the sequence number of the received packet has the same sequence number as the 
most recently received packet) or a new packet (the sequence number changes, mov-
ing “forward” in modulo-2 arithmetic). Since we are currently assuming a channel 
that does not lose packets, ACK and NAK packets do not themselves need to indicate 
the sequence number of the packet they are acknowledging. The sender knows that a 
received ACK or NAK packet (whether garbled or not) was generated in response to 
its most recently transmitted data packet.
Figures 3.11 and 3.12 show the FSM description for rdt2.1, our fixed version 
of rdt2.0. The rdt2.1 sender and receiver FSMs each now have twice as many 
states as before. This is because the protocol state must now reflect whether the 
packet currently being sent (by the sender) or expected (at the receiver) should have a 
sequence number of 0 or 1. Note that the actions in those states where a 0-numbered 
packet is being sent or expected are mirror images of those where a 1-numbered 
packet is being sent or expected; the only differences have to do with the handling 
of the sequence number.
Protocol rdt2.1 uses both positive and negative acknowledgments from the 
receiver to the sender. When an out-of-order packet is received, the receiver sends 
a positive acknowledgment for the packet it has received. When a corrupted packet 
Wait for
call 0 from
above
rdt_rcv(rcvpkt)&&
(corrupt(rcvpkt)||
isNAK(rcvpkt))
udt_send(sndpkt)
rdt_rcv(rcvpkt)&&
(corrupt(rcvpkt)||
isNAK(rcvpkt))
udt_send(sndpkt)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt)


rdt_send(data)
sndpkt=make_pkt(0,data,checksum)
udt_send(sndpkt)
rdt_send(data)
sndpkt=make_pkt(1,data,checksum)
udt_send(sndpkt)
Wait for
ACK or
NAK 0
Wait for
ACK or
NAK 1
Wait for
call 1 from
above
Figure 3.11  ♦  rdt2.1 sender
242         Chapter 3    •    Transport Layer
is received, the receiver sends a negative acknowledgment. We can accomplish the 
same effect as a NAK if, instead of sending a NAK, we send an ACK for the last 
correctly received packet. A sender that receives two ACKs for the same packet (that 
is, receives duplicate ACKs) knows that the receiver did not correctly receive the 
packet following the packet that is being ACKed twice. Our NAK-free reliable data 
transfer protocol for a channel with bit errors is rdt2.2, shown in Figures 3.13 and 
3.14. One subtle change between rtdt2.1 and rdt2.2 is that the receiver must 
now include the sequence number of the packet being acknowledged by an ACK 
message (this is done by including the ACK, 0 or ACK, 1 argument in make_pkt() 
in the receiver FSM), and the sender must now check the sequence number of the 
packet being acknowledged by a received ACK message (this is done by including 
the 0 or 1 argument in isACK() in the sender FSM).
Reliable Data Transfer over a Lossy Channel with Bit Errors: rdt3.0
Suppose now that in addition to corrupting bits, the underlying channel can lose 
packets as well, a not-uncommon event in today’s computer networks (including 
the Internet). Two additional concerns must now be addressed by the protocol: how 
to detect packet loss and what to do when packet loss occurs. The use of check-
summing, sequence numbers, ACK packets, and retransmissions—the techniques 
rdt_rcv(rcvpkt)&& notcorrupt
(rcvpkt)&&has_seq0(rcvpkt)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && corrupt(rcvpkt)
sndpkt=make_pkt(NAK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt)
 && corrupt(rcvpkt)
sndpkt=make_pkt(NAK,checksum)
udt_send(sndpkt)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq1(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt)&& notcorrupt(rcvpkt)
 && has_seq0(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
Wait for
0 from
below
Wait for
1 from
below
rdt_rcv(rcvpkt)&& notcorrupt
(rcvpkt)&&has_seq1(rcvpkt)
Figure 3.12  ♦  rdt2.1 receiver
3.4    •    Principles of Reliable Data Transfer         243
Wait for
call 0 from
above
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,1))
udt_send(sndpkt)
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,0))
udt_send(sndpkt)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,0)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,1)
rdt_send(data)
sndpkt=make_pkt(0,data,checksum)
udt_send(sndpkt)
rdt_send(data)
sndpkt=make_pkt(1,data,checksum)
udt_send(sndpkt)
Wait for
ACK 0
Wait for
ACK 1


Wait for
call 1 from
above
Figure 3.13  ♦  rdt2.2 sender
already developed in rdt2.2—will allow us to answer the latter concern. Handling 
the first concern will require adding a new protocol mechanism.
There are many possible approaches toward dealing with packet loss (several 
more of which are explored in the exercises at the end of the chapter). Here, we’ll 
put the burden of detecting and recovering from lost packets on the sender. Suppose 
that the sender transmits a data packet and either that packet, or the receiver’s ACK 
of that packet, gets lost. In either case, no reply is forthcoming at the sender from the 
receiver. If the sender is willing to wait long enough so that it is certain that a packet 
has been lost, it can simply retransmit the data packet. You should convince yourself 
that this protocol does indeed work.
But how long must the sender wait to be certain that something has been lost? 
The sender must clearly wait at least as long as a round-trip delay between the sender 
and receiver (which may include buffering at intermediate routers) plus whatever 
amount of time is needed to process a packet at the receiver. In many networks, this 
worst-case maximum delay is very difficult even to estimate, much less know with 
certainty. Moreover, the protocol should ideally recover from packet loss as soon as 
possible; waiting for a worst-case delay could mean a long wait until error recovery 
244         Chapter 3    •    Transport Layer
is initiated. The approach thus adopted in practice is for the sender to judiciously 
choose a time value such that packet loss is likely, although not guaranteed, to have 
happened. If an ACK is not received within this time, the packet is retransmitted. 
Note that if a packet experiences a particularly large delay, the sender may retrans-
mit the packet even though neither the data packet nor its ACK have been lost. This 
introduces the possibility of duplicate data packets in the sender-to-receiver chan-
nel. Happily, protocol rdt2.2 already has enough functionality (that is, sequence 
numbers) to handle the case of duplicate packets.
From the sender’s viewpoint, retransmission is a panacea. The sender does not 
know whether a data packet was lost, an ACK was lost, or if the packet or ACK was 
simply overly delayed. In all cases, the action is the same: retransmit. Implement-
ing a time-based retransmission mechanism requires a countdown timer that can 
interrupt the sender after a given amount of time has expired. The sender will thus 
need to be able to (1) start the timer each time a packet (either a first-time packet or 
a retransmission) is sent, (2) respond to a timer interrupt (taking appropriate actions), 
and (3) stop the timer.
Figure 3.15 shows the sender FSM for rdt3.0, a protocol that reliably transfers 
data over a channel that can corrupt or lose packets; in the homework problems, you’ll 
be asked to provide the receiver FSM for rdt3.0. Figure 3.16 shows how the pro-
tocol operates with no lost or delayed packets and how it handles lost data packets. In 
Figure 3.16, time moves forward from the top of the diagram toward the bottom of the 
Wait for
0 from
below
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq0(rcvpkt))
sndpkt=make_pkt(ACK,0,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq1(rcvpkt))
sndpkt=make_pkt(ACK,1,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq1(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,1,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq0(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,0,checksum)
udt_send(sndpkt)
Wait for
1 from
below
Figure 3.14  ♦  rdt2.2 receiver
3.4    •    Principles of Reliable Data Transfer         245
diagram; note that a receive time for a packet is necessarily later than the send time for 
a packet as a result of transmission and propagation delays. In Figures 3.16(b)–(d), the 
send-side brackets indicate the times at which a timer is set and later times out. Sev-
eral of the more subtle aspects of this protocol are explored in the exercises at the end 
of this chapter. Because packet sequence numbers alternate between 0 and 1, protocol 
rdt3.0 is sometimes known as the alternating-bit protocol.
We have now assembled the key elements of a data transfer protocol. Check-
sums, sequence numbers, timers, and positive and negative acknowledgment packets 
each play a crucial and necessary role in the operation of the protocol. We now have 
a working reliable data transfer protocol!
3.4.2 Pipelined Reliable Data Transfer Protocols
Protocol rdt3.0 is a functionally correct protocol, but it is unlikely that anyone 
would be happy with its performance, particularly in today’s high-speed networks. 
At the heart of rdt3.0’s performance problem is the fact that it is a stop-and-wait 
protocol.
Wait for
call 0 from
above
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,1))
timeout
udt_send(sndpkt)
start_timer
rdt_rcv(rcvpkt)

rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,0))
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,0)
stop_timer
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,1)
stop_timer
timeout
udt_send(sndpkt)
start_timer
rdt_send(data)
sndpkt=make_pkt(0,data,checksum)
udt_send(sndpkt)
start_timer
rdt_send(data)
sndpkt=make_pkt(1,data,checksum)
udt_send(sndpkt)
start_timer
Wait for
ACK 0
Wait for
ACK 1


Wait for
call 1 from
above
rdt_rcv(rcvpkt)

Figure 3.15  ♦  rdt3.0 sender
VideoNote
Developing a protocol 
and FSM representation 
for a simple application-
layer protocol
246         Chapter 3    •    Transport Layer
rcv pkt0
send ACK0
rcv pkt1
send ACK1
rcv pkt0
send ACK0
Sender
Receiver
a. Operation with no loss
pkt0
ACK0
pkt1
pkt0
ACK1
ACK0
(loss) X
b. Lost packet
rcv pkt0
send ACK0
rcv pkt1
send ACK1
c. Lost ACK
send pkt0
rcv ACK0
send pkt1
rcv ACK1
send pkt0
send pkt0
rcv ACK0
send pkt1
timeout
resend pkt1
rcv ACK1
send pkt0
rcv pkt0
send ACK0
rcv pkt1
(detect
duplicate)
send ACK1
send pkt0
rcv ACK0
send pkt1
rcv pkt0
send ACK0
timeout
resend pkt1
rcv pkt1
send ACK1
d. Premature timeout
rcv ACK1
send pkt0
rcv ACK1
do nothing
rcv pkt0
send ACK0
rcv pkt 1
(detect duplicate)
send ACK1
Sender
Receiver
Receiver
Sender
pkt0
ACK0
pkt1
ACK1
ACK1
ACK0
ACK1
ACK0
pkt1
pkt0
pkt0
pkt1
pkt1
pkt0
ACK1
ACK0
X (loss)
pkt1
rcv pkt0
send ACK0
send pkt0
rcv ACK0
send pkt1
timeout
resend pkt1
rcv ACK1
send pkt0
rcv pkt0
send ACK0
rcv pkt1
send ACK1
Sender
Receiver
pkt0
ACK0
pkt1
pkt0
ACK1
ACK0
Figure 3.16  ♦  Operation of rdt3.0, the alternating-bit protocol
3.4    •    Principles of Reliable Data Transfer         247
To appreciate the performance impact of this stop-and-wait behavior, consider 
an idealized case of two hosts, one located on the West Coast of the United States 
and the other located on the East Coast, as shown in Figure 3.17. The speed-of-light 
round-trip propagation delay between these two end systems, RTT, is approximately 
30 milliseconds. Suppose that they are connected by a channel with a transmission 
rate, R, of 1 Gbps (109 bits per second). With a packet size, L, of 1,000 bytes (8,000 
bits) per packet, including both header fields and data, the time needed to actually 
transmit the packet into the 1 Gbps link is
dtrans = L
R =
8000 bits>packet
109 bits/sec
= 8 microseconds
Figure 3.18(a) shows that with our stop-and-wait protocol, if the sender begins 
sending the packet at t = 0, then at t = L/R = 8 microseconds, the last bit enters 
the channel at the sender side. The packet then makes its 15-msec cross-country jour-
ney, with the last bit of the packet emerging at the receiver at t = RTT/2 + L/R =
15.008 msec. Assuming for simplicity that ACK packets are extremely small (so that 
we can ignore their transmission time) and that the receiver can send an ACK as soon 
as the last bit of a data packet is received, the ACK emerges back at the sender at 
t = RTT + L/R = 30.008 msec. At this point, the sender can now transmit the next 
message. Thus, in 30.008 msec, the sender was sending for only 0.008 msec. If we 
define the utilization of the sender (or the channel) as the fraction of time the sender 
is actually busy sending bits into the channel, the analysis in Figure 3.18(a) shows 
that the stop-and-wait protocol has a rather dismal sender utilization, Usender, of
Usender =
L>R
RTT + L>R =
.008
30.008 = 0.00027
Data packets
Data packet
ACK packets
a. A stop-and-wait protocol in operation
b. A pipelined protocol in operation
Figure 3.17  ♦  Stop-and-wait versus pipelined protocol
248         Chapter 3    •    Transport Layer
First bit of ﬁrst packet
transmitted, t = 0
Last bit of ﬁrst packet
transmitted, t = L/R
First bit of ﬁrst packet
transmitted, t = 0
Last bit of ﬁrst packet
transmitted, t = L/R
ACK arrives, send next packet,
t = RTT + L/R
a. Stop-and-wait operation
Sender
Receiver
RTT
First bit of ﬁrst packet arrives
Last bit of ﬁrst packet arrives, send ACK
First bit of ﬁrst packet arrives
Last bit of ﬁrst packet arrives, send ACK
ACK arrives, send next packet,
t = RTT + L/R
b. Pipelined operation
Sender
Receiver
RTT
Last bit of 2nd packet arrives, send ACK
Last bit of 3rd packet arrives, send ACK
Figure 3.18  ♦  Stop-and-wait and pipelined sending
3.4    •    Principles of Reliable Data Transfer         249
That is, the sender was busy only 2.7 hundredths of one percent of the time! 
Viewed another way, the sender was able to send only 1,000 bytes in 30.008 mil-
liseconds, an effective throughput of only 267 kbps—even though a 1 Gbps link 
was available! Imagine the unhappy network manager who just paid a fortune for 
a gigabit capacity link but manages to get a throughput of only 267 kilobits per 
second! This is a graphic example of how network protocols can limit the capabili-
ties provided by the underlying network hardware. Also, we have neglected lower- 
layer protocol-processing times at the sender and receiver, as well as the process-
ing and queuing delays that would occur at any intermediate routers between the 
sender and receiver. Including these effects would serve only to further increase the 
 
delay and further accentuate the poor performance.
The solution to this particular performance problem is simple: Rather than oper-
ate in a stop-and-wait manner, the sender is allowed to send multiple packets with-
out waiting for acknowledgments, as illustrated in Figure 3.17(b). Figure 3.18(b) 
shows that if the sender is allowed to transmit three packets before having to wait for 
acknowledgments, the utilization of the sender is essentially tripled. Since the many 
in-transit sender-to-receiver packets can be visualized as filling a pipeline, this tech-
nique is known as pipelining. Pipelining has the following consequences for reliable 
data transfer protocols:
•	 The range of sequence numbers must be increased, since each in-transit packet 
(not counting retransmissions) must have a unique sequence number and there 
may be multiple, in-transit, unacknowledged packets.
•	 The sender and receiver sides of the protocols may have to buffer more than one 
packet. Minimally, the sender will have to buffer packets that have been transmit-
ted but not yet acknowledged. Buffering of correctly received packets may also 
be needed at the receiver, as discussed below.
•	 The range of sequence numbers needed and the buffering requirements will 
depend on the manner in which a data transfer protocol responds to lost, cor-
rupted, and overly delayed packets. Two basic approaches toward pipelined error 
recovery can be identified: Go-Back-N and selective repeat.
3.4.3 Go-Back-N (GBN)
In a Go-Back-N (GBN) protocol, the sender is allowed to transmit multiple packets 
(when available) without waiting for an acknowledgment, but is constrained to have 
no more than some maximum allowable number, N, of unacknowledged packets in 
the pipeline. We describe the GBN protocol in some detail in this section. But before 
reading on, you are encouraged to play with the GBN applet (an awesome applet!) 
at the companion Web site.
Figure 3.19 shows the sender’s view of the range of sequence numbers in a GBN 
protocol. If we define base to be the sequence number of the oldest unacknowledged 
250         Chapter 3    •    Transport Layer
packet and nextseqnum to be the smallest unused sequence number (that is, the 
sequence number of the next packet to be sent), then four intervals in the range of 
sequence numbers can be identified. Sequence numbers in the interval [0,base-1] 
correspond to packets that have already been transmitted and acknowledged. The inter-
val [base,nextseqnum-1] corresponds to packets that have been sent but not 
yet acknowledged. Sequence numbers in the interval [nextseqnum,base+N-1] 
can be used for packets that can be sent immediately, should data arrive from the 
upper layer. Finally, sequence numbers greater than or equal to base+N cannot 
be used until an unacknowledged packet currently in the pipeline (specifically, the 
packet with sequence number base) has been acknowledged.
As suggested by Figure 3.19, the range of permissible sequence numbers for 
transmitted but not yet acknowledged packets can be viewed as a window of size N 
over the range of sequence numbers. As the protocol operates, this window slides 
forward over the sequence number space. For this reason, N is often referred to as the 
window size and the GBN protocol itself as a sliding-window protocol. You might 
be wondering why we would even limit the number of outstanding, unacknowledged 
packets to a value of N in the first place. Why not allow an unlimited number of such 
packets? We’ll see in Section 3.5 that flow control is one reason to impose a limit 
on the sender. We’ll examine another reason to do so in Section 3.7, when we study 
TCP congestion control.
In practice, a packet’s sequence number is carried in a fixed-length field in the 
packet header. If k is the number of bits in the packet sequence number field, the 
range of sequence numbers is thus [0,2k - 1]. With a finite range of sequence num-
bers, all arithmetic involving sequence numbers must then be done using modulo 2k 
arithmetic. (That is, the sequence number space can be thought of as a ring of size 
2k, where sequence number 2k - 1 is immediately followed by sequence number 0.) 
Recall that rdt3.0 had a 1-bit sequence number and a range of sequence numbers 
of [0,1]. Several of the problems at the end of this chapter explore the consequences 
of a finite range of sequence numbers. We will see in Section 3.5 that TCP has a 
32-bit sequence number field, where TCP sequence numbers count bytes in the byte 
stream rather than packets.
Figures 3.20 and 3.21 give an extended FSM description of the sender and 
receiver sides of an ACK-based, NAK-free, GBN protocol. We refer to this FSM 
base
nextseqnum
Window size
N
Key:
Already
ACK’d
Sent, not
yet ACK’d
Usable,
not yet sent
Not usable
Figure 3.19  ♦  Sender’s view of sequence numbers in Go-Back-N
3.4    •    Principles of Reliable Data Transfer         251
rdt_send(data)
if(nextseqnum<base+N){
   sndpkt[nextseqnum]=make_pkt(nextseqnum,data,checksum)
   udt_send(sndpkt[nextseqnum])
   if(base==nextseqnum)
      start_timer
   nextseqnum++
   }
else
   refuse_data(data)

rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
base=getacknum(rcvpkt)+1
If(base==nextseqnum)
   stop_timer
else
   start_timer
rdt_rcv(rcvpkt) && corrupt(rcvpkt)

base=1
nextseqnum=1
timeout
start_timer
udt_send(sndpkt[base])
udt_send(sndpkt[base+1])
...
udt_send(sndpkt[nextseqnum-1])
Wait
Figure 3.20  ♦  Extended FSM description of the GBN sender
rdt_rcv(rcvpkt)
  && notcorrupt(rcvpkt)
  && hasseqnum(rcvpkt,expectedseqnum)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(expectedseqnum,ACK,checksum)
udt_send(sndpkt)
expectedseqnum++

expectedseqnum=1
sndpkt=make_pkt(0,ACK,checksum)
default
udt_send(sndpkt)
Wait
Figure 3.21  ♦  Extended FSM description of the GBN receiver
252         Chapter 3    •    Transport Layer
description as an extended FSM because we have added variables (similar to 
programming-language variables) for base and nextseqnum, and added opera-
tions on these variables and conditional actions involving these variables. Note that 
the extended FSM specification is now beginning to look somewhat like a program-
ming-language specification. [Bochman 1984] provides an excellent survey of addi-
tional extensions to FSM techniques as well as other programming-language-based 
techniques for specifying protocols.
The GBN sender must respond to three types of events:
•	 Invocation from above. When rdt_send() is called from above, the sender 
first checks to see if the window is full, that is, whether there are N outstand-
ing, unacknowledged packets. If the window is not full, a packet is created and 
sent, and variables are appropriately updated. If the window is full, the sender 
simply returns the data back to the upper layer, an implicit indication that the 
window is full. The upper layer would presumably then have to try again later. 
In a real implementation, the sender would more likely have either buffered (but 
not immediately sent) this data, or would have a synchronization mechanism 
(for example, a semaphore or a flag) that would allow the upper layer to call 
 
rdt_send() only when the window is not full.
•	 Receipt of an ACK. In our GBN protocol, an acknowledgment for a packet with 
sequence number n will be taken to be a cumulative acknowledgment, indicat-
ing that all packets with a sequence number up to and including n have been 
correctly received at the receiver. We’ll come back to this issue shortly when we 
examine the receiver side of GBN.
•	 A timeout event. The protocol’s name, “Go-Back-N,” is derived from the sender’s 
behavior in the presence of lost or overly delayed packets. As in the stop-and-wait 
protocol, a timer will again be used to recover from lost data or acknowledgment 
packets. If a timeout occurs, the sender resends all packets that have been previ-
ously sent but that have not yet been acknowledged. Our sender in Figure 3.20 
uses only a single timer, which can be thought of as a timer for the oldest trans-
mitted but not yet acknowledged packet. If an ACK is received but there are still 
additional transmitted but not yet acknowledged packets, the timer is restarted. If 
there are no outstanding, unacknowledged packets, the timer is stopped.
The receiver’s actions in GBN are also simple. If a packet with sequence number 
n is received correctly and is in order (that is, the data last delivered to the upper layer 
came from a packet with sequence number n - 1), the receiver sends an ACK for 
packet n and delivers the data portion of the packet to the upper layer. In all other 
cases, the receiver discards the packet and resends an ACK for the most recently 
received in-order packet. Note that since packets are delivered one at a time to the 
upper layer, if packet k has been received and delivered, then all packets with a 
3.4    •    Principles of Reliable Data Transfer         253
sequence number lower than k have also been delivered. Thus, the use of cumulative 
acknowledgments is a natural choice for GBN.
In our GBN protocol, the receiver discards out-of-order packets. Although 
it may seem silly and wasteful to discard a correctly received (but out-of-order) 
packet, there is some justification for doing so. Recall that the receiver must 
deliver data in order to the upper layer. Suppose now that packet n is expected, but 
packet n + 1 arrives. Because data must be delivered in order, the receiver could 
buffer (save) packet n + 1 and then deliver this packet to the upper layer after it 
had later received and delivered packet n. However, if packet n is lost, both it and 
packet n + 1 will eventually be retransmitted as a result of the GBN retransmis-
sion rule at the sender. Thus, the receiver can simply discard packet n + 1. The 
advantage of this approach is the simplicity of receiver buffering—the receiver 
need not buffer any out-of-order packets. Thus, while the sender must maintain 
the upper and lower bounds of its window and the position of nextseqnum 
within this window, the only piece of information the receiver need maintain is 
the sequence number of the next in-order packet. This value is held in the variable 
expectedseqnum, shown in the receiver FSM in Figure 3.21. Of course, the 
disadvantage of throwing away a correctly received packet is that the subsequent 
retransmission of that packet might be lost or garbled and thus even more retrans-
missions would be required.
Figure 3.22 shows the operation of the GBN protocol for the case of a window 
size of four packets. Because of this window size limitation, the sender sends pack-
ets 0 through 3 but then must wait for one or more of these packets to be acknowl-
edged before proceeding. As each successive ACK (for example, ACK0 and ACK1) 
is received, the window slides forward and the sender can transmit one new packet 
(pkt4 and pkt5, respectively). On the receiver side, packet 2 is lost and thus packets 
3, 4, and 5 are found to be out of order and are discarded.
Before closing our discussion of GBN, it is worth noting that an implementa-
tion of this protocol in a protocol stack would likely have a structure similar to that 
of the extended FSM in Figure 3.20. The implementation would also likely be in 
the form of various procedures that implement the actions to be taken in response to 
the various events that can occur. In such event-based programming, the various 
procedures are called (invoked) either by other procedures in the protocol stack, or 
as the result of an interrupt. In the sender, these events would be (1) a call from the 
upper-layer entity to invoke rdt_send(), (2) a timer interrupt, and (3) a call from 
the lower layer to invoke rdt_rcv() when a packet arrives. The programming 
exercises at the end of this chapter will give you a chance to actually implement these 
routines in a simulated, but realistic, network setting.
We note here that the GBN protocol incorporates almost all of the techniques 
that we will encounter when we study the reliable data transfer components of TCP 
in Section 3.5. These techniques include the use of sequence numbers, cumulative 
acknowledgments, checksums, and a timeout/retransmit operation.
254         Chapter 3    •    Transport Layer
3.4.4 Selective Repeat (SR)
The GBN protocol allows the sender to potentially “fill the pipeline” in Figure 3.17 
with packets, thus avoiding the channel utilization problems we noted with stop-
and-wait protocols. There are, however, scenarios in which GBN itself suffers from 
performance problems. In particular, when the window size and bandwidth-delay 
product are both large, many packets can be in the pipeline. A single packet error 
can thus cause GBN to retransmit a large number of packets, many unnecessarily. 
As the probability of channel errors increases, the pipeline can become filled with 
these unnecessary retransmissions. Imagine, in our message-dictation scenario, that 
Sender
Receiver
 send pkt0
 send pkt1
 send pkt2
send pkt3
  
(wait)
 rcv ACK0
send pkt4
 rcv ACK1
send pkt5
send pkt2
send pkt3
send pkt4
send pkt5
pkt2 timeout
rcv pkt0
send ACK0
rcv pkt1
send ACK1
rcv pkt3, discard
send ACK1
rcv pkt4, discard
send ACK1
rcv pkt5, discard
send ACK1
rcv pkt2, deliver
send ACK2
rcv pkt3, deliver
send ACK3
X
(loss)
Figure 3.22  ♦  Go-Back-N in operation
3.4    •    Principles of Reliable Data Transfer         255
if every time a word was garbled, the surrounding 1,000 words (for example, a win-
dow size of 1,000 words) had to be repeated. The dictation would be slowed by all 
of the reiterated words.
As the name suggests, selective-repeat protocols avoid unnecessary retrans-
missions by having the sender retransmit only those packets that it suspects were 
received in error (that is, were lost or corrupted) at the receiver. This individual, 
as-needed, retransmission will require that the receiver individually acknowledge 
correctly received packets. A window size of N will again be used to limit the num-
ber of outstanding, unacknowledged packets in the pipeline. However, unlike GBN, 
the sender will have already received ACKs for some of the packets in the window. 
Figure 3.23 shows the SR sender’s view of the sequence number space. Figure 3.24 
details the various actions taken by the SR sender.
The SR receiver will acknowledge a correctly received packet whether or not it is 
in order. Out-of-order packets are buffered until any missing packets (that is, packets 
with lower sequence numbers) are received, at which point a batch of packets can be 
delivered in order to the upper layer. Figure 3.25 itemizes the various actions taken by 
the SR receiver. Figure 3.26 shows an example of SR operation in the presence of lost 
packets. Note that in Figure 3.26, the receiver initially buffers packets 3, 4, and 5, and 
delivers them together with packet 2 to the upper layer when packet 2 is finally received.
send_base
nextseqnum
Window size
N
Key:
Key:
Already
ACK’d
Sent, not
yet ACK’d
Usable,
not yet sent
Not usable
Out of order
(buffered) but
already ACK’d
Expected, not
yet received
Acceptable
(within
window)
Not usable
a. Sender view of sequence numbers
b. Receiver view of sequence numbers
rcv_base
Window size
N
Figure 3.23  ♦  
Selective-repeat (SR) sender and receiver views  
of sequence-number space
256         Chapter 3    •    Transport Layer
It is important to note that in Step 2 in Figure 3.25, the receiver reacknowledges 
(rather than ignores) already received packets with certain sequence numbers below 
the current window base. You should convince yourself that this reacknowledgment 
is indeed needed. Given the sender and receiver sequence number spaces in Figure 
3.23, for example, if there is no ACK for packet send_base propagating from the 
1. Data received from above. When data is received from above, the SR sender
checks the next available sequence number for the packet. If the sequence
number is within the sender’s window, the data is packetized and sent; other-
wise it is either bufered or returned to the upper layer for later transmission, 
as in GBN.
2. Timeout. Timers are again used to protect against lost packets. However, each
packet must now have its own logical timer, since only a single packet will 
be transmitted on timeout. A single hardware timer can be used to mimic the
operation of multiple logical timers [Varghese 1997].
3. ACK received. If an ACK is received, the SR sender marks that packet as 
having been received, provided it is in the window. If the packet’s sequence
number is equal to send_base, the window base is moved forward to the 
unacknowledged packet with the smallest sequence number. If the window
moves and there are untransmitted packets with sequence numbers that now
fall within the window, these packets are transmitted.
Figure 3.24  ♦  SR sender events and actions
1. Packet with sequence number in [rcv_base, rcv_base+N-1]is cor-
rectly received. In this case, the received packet falls within the receiver’s win-
dow and a selective ACK packet is returned to the sender. If the packet was not
previously received, it is buffered. If this packet has a sequence number equal to
the base of the receive window (rcv_base in Figure 3.22), then this packet,
and any previously buffered and consecutively numbered (beginning with
rcv_base) packets are delivered to the upper layer. The receive window is
then moved forward by the number of packets delivered to the upper layer. As
an example, consider Figure 3.26. When a packet with a sequence number of
rcv_base=2 is received, it and packets 3, 4, and 5 can be delivered to the
upper layer.
2. Packet with sequence number in [rcv_base-N, rcv_base-1]is cor-
rectly received. In this case, an ACK must be generated, even though this is a
packet that the receiver has previously acknowledged.
3. Otherwise. Ignore the packet.
Figure 3.25  ♦  SR receiver events and actions
3.4    •    Principles of Reliable Data Transfer         257
receiver to the sender, the sender will eventually retransmit packet send_base, 
even though it is clear (to us, not the sender!) that the receiver has already received 
that packet. If the receiver were not to acknowledge this packet, the sender’s win-
dow would never move forward! This example illustrates an important aspect of 
SR protocols (and many other protocols as well). The sender and receiver will not 
always have an identical view of what has been received correctly and what has not. 
For SR protocols, this means that the sender and receiver windows will not always 
coincide.
pkt0 rcvd, delivered, ACK0 sent
0 1 2 3 4 5 6 7 8 9
pkt1 rcvd, delivered, ACK1 sent
0 1 2 3 4 5 6 7 8 9
pkt3 rcvd, bufered, ACK3 sent
0 1 2 3 4 5 6 7 8 9
pkt4 rcvd, bufered, ACK4 sent
0 1 2 3 4 5 6 7 8 9
pkt5 rcvd; bufered, ACK5 sent
0 1 2 3 4 5 6 7 8 9
pkt2 rcvd, pkt2,pkt3,pkt4,pkt5
delivered, ACK2 sent
0 1 2 3 4 5 6 7 8 9
pkt0 sent
0 1 2 3 4 5 6 7 8 9
pkt1 sent
0 1 2 3 4 5 6 7 8 9
pkt2 sent
0 1 2 3 4 5 6 7 8 9
pkt3 sent, window full
0 1 2 3 4 5 6 7 8 9
ACK0 rcvd, pkt4 sent
0 1 2 3 4 5 6 7 8 9
ACK1 rcvd, pkt5 sent
0 1 2 3 4 5 6 7 8 9
pkt2 TIMEOUT, pkt2
resent
0 1 2 3 4 5 6 7 8 9
ACK3 rcvd, nothing sent
0 1 2 3 4 5 6 7 8 9
X
(loss)
Sender
Receiver
Figure 3.26  ♦  SR operation
258         Chapter 3    •    Transport Layer
The lack of synchronization between sender and receiver windows has impor-
tant consequences when we are faced with the reality of a finite range of sequence 
numbers. Consider what could happen, for example, with a finite range of four packet 
sequence numbers, 0, 1, 2, 3, and a window size of three. Suppose packets 0 through 
2 are transmitted and correctly received and acknowledged at the receiver. At this 
point, the receiver’s window is over the fourth, fifth, and sixth packets, which have 
sequence numbers 3, 0, and 1, respectively. Now consider two scenarios. In the first 
scenario, shown in Figure 3.27(a), the ACKs for the first three packets are lost and 
the sender retransmits these packets. The receiver thus next receives a packet with 
sequence number 0—a copy of the first packet sent.
In the second scenario, shown in Figure 3.27(b), the ACKs for the first three 
packets are all delivered correctly. The sender thus moves its window forward and 
sends the fourth, fifth, and sixth packets, with sequence numbers 3, 0, and 1, respec-
tively. The packet with sequence number 3 is lost, but the packet with sequence 
number 0 arrives—a packet containing new data.
Now consider the receiver’s viewpoint in Figure 3.27, which has a figurative 
curtain between the sender and the receiver, since the receiver cannot “see” the 
actions taken by the sender. All the receiver observes is the sequence of messages 
it receives from the channel and sends into the channel. As far as it is concerned, 
the two scenarios in Figure 3.27 are identical. There is no way of distinguishing the 
retransmission of the first packet from an original transmission of the fifth packet. 
Clearly, a window size that is 1 less than the size of the sequence number space 
won’t work. But how small must the window size be? A problem at the end of the 
chapter asks you to show that the window size must be less than or equal to half the 
size of the sequence number space for SR protocols.
At the companion Web site, you will find an applet that animates the operation 
of the SR protocol. Try performing the same experiments that you did with the GBN 
applet. Do the results agree with what you expect?
This completes our discussion of reliable data transfer protocols. We’ve covered 
a lot of ground and introduced numerous mechanisms that together provide for reli-
able data transfer. Table 3.1 summarizes these mechanisms. Now that we have seen 
all of these mechanisms in operation and can see the “big picture,” we encourage you 
to review this section again to see how these mechanisms were incrementally added 
to cover increasingly complex (and realistic) models of the channel connecting the 
sender and receiver, or to improve the performance of the protocols.
Let’s conclude our discussion of reliable data transfer protocols by consider-
ing one remaining assumption in our underlying channel model. Recall that we 
have assumed that packets cannot be reordered within the channel between the 
sender and receiver. This is generally a reasonable assumption when the sender and 
receiver are connected by a single physical wire. However, when the “channel” 
connecting the two is a network, packet reordering can occur. One manifestation of 
packet reordering is that old copies of a packet with a sequence or acknowledgment 
3.4    •    Principles of Reliable Data Transfer         259
pkt0
timeout
retransmit pkt0
0 1 2 3 0 1 2
pkt0
pkt1
pkt2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
ACK0
ACK1
ACK2
x
0 1 2 3 0 1 2
0 1 2 3 0 1 2
Sender window
(after receipt)
a.
b.
Receiver window
(after receipt)
receive packet
with seq number 0
0 1 2 3 0 1 2
pkt0
pkt1
pkt2
pkt3
pkt0
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
ACK0
ACK1
ACK2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
Sender window
(after receipt)
Receiver window
(after receipt)
receive packet
with seq number 0
0 1 2 3 0 1 2
x
x
x
Figure 3.27  ♦  
SR receiver dilemma with too-large windows: A new packet 
or a retransmission?
260         Chapter 3    •    Transport Layer
number of x can appear, even though neither the sender’s nor the receiver’s win-
dow contains x. With packet reordering, the channel can be thought of as essen-
tially buffering packets and spontaneously emitting these packets at any point in 
the future. Because sequence numbers may be reused, some care must be taken to 
guard against such duplicate packets. The approach taken in practice is to ensure 
that a sequence number is not reused until the sender is “sure” that any previously 
sent packets with sequence number x are no longer in the network. This is done 
by assuming that a packet cannot “live” in the network for longer than some fixed 
maximum amount of time. A maximum packet lifetime of approximately three 
minutes is assumed in the TCP extensions for high-speed networks [RFC 1323]. 
[Sunshine 1978] describes a method for using sequence numbers such that reorder-
ing problems can be completely avoided.
Table 3.1  ♦  Summary of reliable data transfer mechanisms and their use
Mechanism
Use, Comments
Checksum
Used to detect bit errors in a transmitted packet.
Timer
Used to timeout/retransmit a packet, possibly because the packet (or its ACK) 
was lost within the channel. Because timeouts can occur when a packet is delayed 
but not lost (premature timeout), or when a packet has been received by the 
receiver but the receiver-to-sender ACK has been lost, duplicate copies of a packet 
may be received by a receiver.
Sequence number
Used for sequential numbering of packets of data flowing from sender to receiver. 
Gaps in the sequence numbers of received packets allow the receiver to detect a 
lost packet. Packets with duplicate sequence numbers allow the receiver to detect 
duplicate copies of a packet.
Acknowledgment
Used by the receiver to tell the sender that a packet or set of packets has been 
received correctly. Acknowledgments will typically carry the sequence number of 
the packet or packets being acknowledged. Acknowledgments may be individual 
or cumulative, depending on the protocol.
Negative acknowledgment
Used by the receiver to tell the sender that a packet has not been received 
correctly. Negative acknowledgments will typically carry the sequence number  
of the packet that was not received correctly.
Window, pipelining
The sender may be restricted to sending only packets with sequence numbers that 
fall within a given range. By allowing multiple packets to be transmitted but not 
yet acknowledged, sender utilization can be increased over a stop-and-wait mode 
of operation. We’ll see shortly that the window size may be set on the basis of 
the receiver’s ability to receive and buffer messages, or the level of congestion in 
the network, or both.
3.5    •    Connection-Oriented Transport: TCP         261
3.5	 Connection-Oriented Transport: TCP
Now that we have covered the underlying principles of reliable data transfer, 
let’s turn to TCP—the Internet’s transport-layer, connection-oriented, reliable 
transport protocol. In this section, we’ll see that in order to provide reliable 
data transfer, TCP relies on many of the underlying principles discussed in 
the previous section, including error detection, retransmissions, cumulative 
acknowledgments, timers, and header fields for sequence and acknowledgment 
numbers. TCP is defined in RFC 793, RFC 1122, RFC 1323, RFC 2018, and 
RFC 2581.
3.5.1 The TCP Connection
TCP is said to be connection-oriented because before one application process can 
begin to send data to another, the two processes must first “handshake” with each 
other—that is, they must send some preliminary segments to each other to establish 
the parameters of the ensuing data transfer. As part of TCP connection establish-
ment, both sides of the connection will initialize many TCP state variables (many of 
which will be discussed in this section and in Section 3.7) associated with the TCP 
connection.
The TCP “connection” is not an end-to-end TDM or FDM circuit as in a circuit-
switched network. Instead, the “connection” is a logical one, with common state 
residing only in the TCPs in the two communicating end systems. Recall that because 
the TCP protocol runs only in the end systems and not in the intermediate network 
elements (routers and link-layer switches), the intermediate network elements do 
not maintain TCP connection state. In fact, the intermediate routers are completely 
oblivious to TCP connections; they see datagrams, not connections.
A TCP connection provides a full-duplex service: If there is a TCP con-
nection between Process A on one host and Process B on another host, then 
application-layer data can flow from Process A to Process B at the same time 
as application-layer data flows from Process B to Process A. A TCP connec-
tion is also always point-to-point, that is, between a single sender and a single 
receiver. So-called “multicasting” (see the online supplementary materials for 
this text)—the transfer of data from one sender to many receivers in a single 
send operation—is not possible with TCP. With TCP, two hosts are company 
and three are a crowd!
Let’s now take a look at how a TCP connection is established. Suppose a process 
running in one host wants to initiate a connection with another process in another 
host. Recall that the process that is initiating the connection is called the client 
 
process, while the other process is called the server process. The client application 
process first informs the client transport layer that it wants to establish a connection 
262         Chapter 3    •    Transport Layer
to a process in the server. Recall from Section 2.7.2, a Python client program does 
this by issuing the command
clientSocket.connect((serverName,serverPort))
where serverName is the name of the server and serverPort identifies the 
process on the server. TCP in the client then proceeds to establish a TCP connec-
tion with TCP in the server. At the end of this section we discuss in some detail the 
connection-establishment procedure. For now it suffices to know that the client first 
sends a special TCP segment; the server responds with a second special TCP seg-
ment; and finally the client responds again with a third special segment. The first 
two segments carry no payload, that is, no application-layer data; the third of these 
segments may carry a payload. Because three segments are sent between the two 
hosts, this connection-establishment procedure is often referred to as a three-way 
handshake.
Vinton Cerf, Robert Kahn, and TCP/IP
In the early 1970s, packet-switched networks began to proliferate, with the 
ARPAnet—the precursor of the Internet—being just one of many networks. Each of 
these networks had its own protocol. Two researchers, Vinton Cerf and Robert Kahn, 
recognized the importance of interconnecting these networks and invented a cross-
network protocol called TCP/IP, which stands for Transmission Control Protocol/
Internet Protocol. Although Cerf and Kahn began by seeing the protocol as a single 
entity, it was later split into its two parts, TCP and IP, which operated separately. 
Cerf and Kahn published a paper on TCP/IP in May 1974 in IEEE Transactions on 
Communications Technology [Cerf 1974].
The TCP/IP protocol, which is the bread and butter of today’s Internet, was 
devised before PCs, workstations, smartphones, and tablets, before the prolifera-
tion of Ethernet, cable, and DSL, WiFi, and other access network technologies, and 
before the Web, social media, and streaming video. Cerf and Kahn saw the need 
for a networking protocol that, on the one hand, provides broad support for yet-to-
be-defined applications and, on the other hand, allows arbitrary hosts and link-layer 
protocols to interoperate.
In 2004, Cerf and Kahn received the ACM’s Turing Award, considered the 
“Nobel Prize of Computing” for “pioneering work on internetworking, including the 
design and implementation of the Internet’s basic communications protocols, TCP/IP, 
and for inspired leadership in networking.”
Case History
3.5    •    Connection-Oriented Transport: TCP         263
Once a TCP connection is established, the two application processes can send 
data to each other. Let’s consider the sending of data from the client process to the 
server process. The client process passes a stream of data through the socket (the 
door of the process), as described in Section 2.7. Once the data passes through the 
door, the data is in the hands of TCP running in the client. As shown in Figure 3.28, 
TCP directs this data to the connection’s send buffer, which is one of the buffers that 
is set aside during the initial three-way handshake. From time to time, TCP will grab 
chunks of data from the send buffer and pass the data to the network layer. Interest-
ingly, the TCP specification [RFC 793] is very laid back about specifying when TCP 
should actually send buffered data, stating that TCP should “send that data in seg-
ments at its own convenience.” The maximum amount of data that can be grabbed 
and placed in a segment is limited by the maximum segment size (MSS). The MSS 
is typically set by first determining the length of the largest link-layer frame that 
can be sent by the local sending host (the so-called maximum transmission unit, 
MTU), and then setting the MSS to ensure that a TCP segment (when encapsulated 
in an IP datagram) plus the TCP/IP header length (typically 40 bytes) will fit into a 
single link-layer frame. Both Ethernet and PPP link-layer protocols have an MTU of 
1,500 bytes. Thus a typical value of MSS is 1460 bytes. Approaches have also been 
proposed for discovering the path MTU—the largest link-layer frame that can be sent 
on all links from source to destination [RFC 1191]—and setting the MSS based on 
the path MTU value. Note that the MSS is the maximum amount of application-layer 
data in the segment, not the maximum size of the TCP segment including headers. 
(This terminology is confusing, but we have to live with it, as it is well entrenched.)
TCP pairs each chunk of client data with a TCP header, thereby forming TCP 
segments. The segments are passed down to the network layer, where they are sepa-
rately encapsulated within network-layer IP datagrams. The IP datagrams are then 
sent into the network. When TCP receives a segment at the other end, the segment’s 
data is placed in the TCP connection’s receive buffer, as shown in Figure 3.28. The 
application reads the stream of data from this buffer. Each side of the connection has 
Process
writes data
Process
reads data
TCP
send
buffer
Socket
TCP
receive
buffer
Socket
Segment
Segment
Figure 3.28  ♦  TCP send and receive buffers
264         Chapter 3    •    Transport Layer
its own send buffer and its own receive buffer. (You can see the online flow-control 
applet at http://www.awl.com/kurose-ross, which provides an animation of the send 
and receive buffers.)
We see from this discussion that a TCP connection consists of buffers, variables, 
and a socket connection to a process in one host, and another set of buffers, vari-
ables, and a socket connection to a process in another host. As mentioned earlier, no 
buffers or variables are allocated to the connection in the network elements (routers, 
switches, and repeaters) between the hosts.
3.5.2 TCP Segment Structure
Having taken a brief look at the TCP connection, let’s examine the TCP segment 
structure. The TCP segment consists of header fields and a data field. The data field 
contains a chunk of application data. As mentioned above, the MSS limits the maxi-
mum size of a segment’s data field. When TCP sends a large file, such as an image 
as part of a Web page, it typically breaks the file into chunks of size MSS (except 
for the last chunk, which will often be less than the MSS). Interactive applications, 
however, often transmit data chunks that are smaller than the MSS; for example, with 
remote login applications like Telnet, the data field in the TCP segment is often only 
one byte. Because the TCP header is typically 20 bytes (12 bytes more than the UDP 
header), segments sent by Telnet may be only 21 bytes in length.
Figure 3.29 shows the structure of the TCP segment. As with UDP, the header 
includes source and destination port numbers, which are used for multiplexing/
demultiplexing data from/to upper-layer applications. Also, as with UDP, the header 
includes a checksum field. A TCP segment header also contains the following fields:
•	 The 32-bit sequence number field and the 32-bit acknowledgment number 
field are used by the TCP sender and receiver in implementing a reliable data 
transfer service, as discussed below.
•	 The 16-bit receive window field is used for flow control. We will see shortly that 
it is used to indicate the number of bytes that a receiver is willing to accept.
•	 The 4-bit header length field specifies the length of the TCP header in 32-bit 
words. The TCP header can be of variable length due to the TCP options field. 
(Typically, the options field is empty, so that the length of the typical TCP header 
is 20 bytes.)
•	 The optional and variable-length options field is used when a sender and receiver 
negotiate the maximum segment size (MSS) or as a window scaling factor for use 
in high-speed networks. A time-stamping option is also defined. See RFC 854 
and RFC 1323 for additional details.
•	 The flag field contains 6 bits. The ACK bit is used to indicate that the value 
carried in the acknowledgment field is valid; that is, the segment contains an 
acknowledgment for a segment that has been successfully received. The RST, 
3.5    •    Connection-Oriented Transport: TCP         265
SYN, and FIN bits are used for connection setup and teardown, as we will discuss 
at the end of this section. The CWR and ECE bits are used in explicit congestion 
notification, as discussed in Section 3.7.2. Setting the PSH bit indicates that the 
receiver should pass the data to the upper layer immediately. Finally, the URG bit 
is used to indicate that there is data in this segment that the sending-side upper-
layer entity has marked as “urgent.” The location of the last byte of this urgent 
data is indicated by the 16-bit urgent data pointer field. TCP must inform the 
receiving-side upper-layer entity when urgent data exists and pass it a pointer to 
the end of the urgent data. (In practice, the PSH, URG, and the urgent data pointer 
are not used. However, we mention these fields for completeness.)
Our experience as teachers is that our students sometimes find discussion of 
packet formats rather dry and perhaps a bit boring. For a fun and fanciful look at 
TCP header fields, particularly if you love Legos™ as we do, see [Pomeranz 2010].
Sequence Numbers and Acknowledgment Numbers
Two of the most important fields in the TCP segment header are the sequence number 
field and the acknowledgment number field. These fields are a critical part of TCP’s 
reliable data transfer service. But before discussing how these fields are used to pro-
vide reliable data transfer, let us first explain what exactly TCP puts in these fields.
Source port #
Internet checksum
Header
length
Unused
URG
ECE
CWR
ACK
PSH
RST
SYN
FIN
32 bits
Dest port #
Receive window
Urgent data pointer
Sequence number
Acknowledgment number
Options
Data
Figure 3.29  ♦  TCP segment structure
266         Chapter 3    •    Transport Layer
TCP views data as an unstructured, but ordered, stream of bytes. TCP’s use of 
sequence numbers reflects this view in that sequence numbers are over the stream 
of transmitted bytes and not over the series of transmitted segments. The sequence 
number for a segment is therefore the byte-stream number of the first byte in the 
segment. Let’s look at an example. Suppose that a process in Host A wants to send a 
stream of data to a process in Host B over a TCP connection. The TCP in Host A will 
implicitly number each byte in the data stream. Suppose that the data stream consists 
of a file consisting of 500,000 bytes, that the MSS is 1,000 bytes, and that the first 
byte of the data stream is numbered 0. As shown in Figure 3.30, TCP constructs 500 
segments out of the data stream. The first segment gets assigned sequence number 
0, the second segment gets assigned sequence number 1,000, the third segment gets 
assigned sequence number 2,000, and so on. Each sequence number is inserted in the 
sequence number field in the header of the appropriate TCP segment.
Now let’s consider acknowledgment numbers. These are a little trickier than 
sequence numbers. Recall that TCP is full-duplex, so that Host A may be receiving 
data from Host B while it sends data to Host B (as part of the same TCP connection). 
Each of the segments that arrive from Host B has a sequence number for the data 
flowing from B to A. The acknowledgment number that Host A puts in its segment 
is the sequence number of the next byte Host A is expecting from Host B. It is good 
to look at a few examples to understand what is going on here. Suppose that Host A 
has received all bytes numbered 0 through 535 from B and suppose that it is about 
to send a segment to Host B. Host A is waiting for byte 536 and all the subsequent 
bytes in Host B’s data stream. So Host A puts 536 in the acknowledgment number 
field of the segment it sends to B.
As another example, suppose that Host A has received one segment from Host 
B containing bytes 0 through 535 and another segment containing bytes 900 through 
1,000. For some reason Host A has not yet received bytes 536 through 899. In this 
example, Host A is still waiting for byte 536 (and beyond) in order to re-create B’s 
data stream. Thus, A’s next segment to B will contain 536 in the acknowledgment 
number field. Because TCP only acknowledges bytes up to the first missing byte in 
the stream, TCP is said to provide cumulative acknowledgments.
0
1
1,000
1,999
499,999
File
Data for 1st segment
Data for 2nd segment
Figure 3.30  ♦  Dividing file data into TCP segments
3.5    •    Connection-Oriented Transport: TCP         267
This last example also brings up an important but subtle issue. Host A received 
the third segment (bytes 900 through 1,000) before receiving the second segment 
(bytes 536 through 899). Thus, the third segment arrived out of order. The sub-
tle issue is: What does a host do when it receives out-of-order segments in a TCP 
connection? Interestingly, the TCP RFCs do not impose any rules here and leave 
the decision up to the programmers implementing a TCP implementation. There 
are basically two choices: either (1) the receiver immediately discards out-of-order 
segments (which, as we discussed earlier, can simplify receiver design), or (2) the 
receiver keeps the out-of-order bytes and waits for the missing bytes to fill in the 
gaps. Clearly, the latter choice is more efficient in terms of network bandwidth, and 
is the approach taken in practice.
In Figure 3.30, we assumed that the initial sequence number was zero. In truth, 
both sides of a TCP connection randomly choose an initial sequence number. This 
is done to minimize the possibility that a segment that is still present in the network 
from an earlier, already-terminated connection between two hosts is mistaken for a 
valid segment in a later connection between these same two hosts (which also happen 
to be using the same port numbers as the old connection) [Sunshine 1978].
Telnet: A Case Study for Sequence and Acknowledgment Numbers
Telnet, defined in RFC 854, is a popular application-layer protocol used for remote 
login. It runs over TCP and is designed to work between any pair of hosts. Unlike the 
bulk data transfer applications discussed in Chapter 2, Telnet is an interactive appli-
cation. We discuss a Telnet example here, as it nicely illustrates TCP sequence and 
acknowledgment numbers. We note that many users now prefer to use the SSH proto-
col rather than Telnet, since data sent in a Telnet connection (including passwords!) 
are not encrypted, making Telnet vulnerable to eavesdropping attacks (as discussed 
in Section 8.7).
Suppose Host A initiates a Telnet session with Host B. Because Host A initiates 
the session, it is labeled the client, and Host B is labeled the server. Each character 
typed by the user (at the client) will be sent to the remote host; the remote host will 
send back a copy of each character, which will be displayed on the Telnet user’s 
screen. This “echo back” is used to ensure that characters seen by the Telnet user 
have already been received and processed at the remote site. Each character thus 
traverses the network twice between the time the user hits the key and the time the 
character is displayed on the user’s monitor.
Now suppose the user types a single letter, ‘C,’ and then grabs a coffee. Let’s 
examine the TCP segments that are sent between the client and server. As shown 
in Figure 3.31, we suppose the starting sequence numbers are 42 and 79 for the cli-
ent and server, respectively. Recall that the sequence number of a segment is the 
sequence number of the first byte in the data field. Thus, the first segment sent from 
the client will have sequence number 42; the first segment sent from the server will 
have sequence number 79. Recall that the acknowledgment number is the sequence 
268         Chapter 3    •    Transport Layer
number of the next byte of data that the host is waiting for. After the TCP connec-
tion is established but before any data is sent, the client is waiting for byte 79 and the 
server is waiting for byte 42.
As shown in Figure 3.31, three segments are sent. The first segment is sent from 
the client to the server, containing the 1-byte ASCII representation of the letter ‘C’ in 
its data field. This first segment also has 42 in its sequence number field, as we just 
described. Also, because the client has not yet received any data from the server, this 
first segment will have 79 in its acknowledgment number field.
The second segment is sent from the server to the client. It serves a dual purpose. 
First it provides an acknowledgment of the data the server has received. By putting 
43 in the acknowledgment field, the server is telling the client that it has successfully 
received everything up through byte 42 and is now waiting for bytes 43 onward. The 
second purpose of this segment is to echo back the letter ‘C.’ Thus, the second seg-
ment has the ASCII representation of ‘C’ in its data field. This second segment has 
the sequence number 79, the initial sequence number of the server-to-client data flow 
of this TCP connection, as this is the very first byte of data that the server is send-
ing. Note that the acknowledgment for client-to-server data is carried in a segment 
Time
Time
Host A
Host B
User types
'C'
Seq=42, ACK=79, data='C'
Seq=79, ACK=43, data='C'
Seq=43, ACK=80
Host ACKs
receipt of 'C',
echoes back 'C'
Host ACKs
receipt of
echoed 'C'
Figure 3.31  ♦  
Sequence and acknowledgment numbers for a simple Telnet 
application over TCP
3.5    •    Connection-Oriented Transport: TCP         269
carrying server-to-client data; this acknowledgment is said to be piggybacked on the 
server-to-client data segment.
The third segment is sent from the client to the server. Its sole purpose is to 
acknowledge the data it has received from the server. (Recall that the second seg-
ment contained data—the letter ‘C’—from the server to the client.) This segment 
has an empty data field (that is, the acknowledgment is not being piggybacked with 
any client-to-server data). The segment has 80 in the acknowledgment number field 
because the client has received the stream of bytes up through byte sequence number 
79 and it is now waiting for bytes 80 onward. You might think it odd that this seg-
ment also has a sequence number since the segment contains no data. But because 
TCP has a sequence number field, the segment needs to have some sequence number.
3.5.3 Round-Trip Time Estimation and Timeout
TCP, like our rdt protocol in Section 3.4, uses a timeout/retransmit mechanism to 
recover from lost segments. Although this is conceptually simple, many subtle issues 
arise when we implement a timeout/retransmit mechanism in an actual protocol such 
as TCP. Perhaps the most obvious question is the length of the timeout intervals. 
Clearly, the timeout should be larger than the connection’s round-trip time (RTT), 
that is, the time from when a segment is sent until it is acknowledged. Otherwise, 
unnecessary retransmissions would be sent. But how much larger? How should the 
RTT be estimated in the first place? Should a timer be associated with each and 
every unacknowledged segment? So many questions! Our discussion in this section 
is based on the TCP work in [Jacobson 1988] and the current IETF recommendations 
for managing TCP timers [RFC 6298].
Estimating the Round-Trip Time
Let’s begin our study of TCP timer management by considering how TCP estimates 
the round-trip time between sender and receiver. This is accomplished as follows. 
The sample RTT, denoted SampleRTT, for a segment is the amount of time between 
when the segment is sent (that is, passed to IP) and when an acknowledgment for 
the segment is received. Instead of measuring a SampleRTT for every transmitted 
segment, most TCP implementations take only one SampleRTT measurement at 
a time. That is, at any point in time, the SampleRTT is being estimated for only 
one of the transmitted but currently unacknowledged segments, leading to a new 
value of SampleRTT approximately once every RTT. Also, TCP never computes a 
 
SampleRTT for a segment that has been retransmitted; it only measures 
 
SampleRTT for segments that have been transmitted once [Karn 1987]. (A problem 
at the end of the chapter asks you to consider why.)
Obviously, the SampleRTT values will fluctuate from segment to segment due 
to congestion in the routers and to the varying load on the end systems. Because of 
this fluctuation, any given SampleRTT value may be atypical. In order to estimate 
270         Chapter 3    •    Transport Layer
a typical RTT, it is therefore natural to take some sort of average of the SampleRTT 
values. TCP maintains an average, called EstimatedRTT, of the SampleRTT 
values. Upon obtaining a new SampleRTT, TCP updates EstimatedRTT accord-
ing to the following formula:
EstimatedRTT = (1 – α) # EstimatedRTT + α # SampleRTT
The formula above is written in the form of a programming-language state-
ment—the new value of EstimatedRTT is a weighted combination of the previous 
value of EstimatedRTT and the new value for SampleRTT. The recommended 
value of α is α = 0.125 (that is, 1/8) [RFC 6298], in which case the formula above 
becomes:
EstimatedRTT = 0.875 # EstimatedRTT + 0.125 # SampleRTT
Note that EstimatedRTT is a weighted average of the SampleRTT values. As 
discussed in a homework problem at the end of this chapter, this weighted average 
puts more weight on recent samples than on old samples. This is natural, as the more 
recent samples better reflect the current congestion in the network. In statistics, such 
an average is called an exponential weighted moving average (EWMA). The word 
“exponential” appears in EWMA because the weight of a given SampleRTT decays 
exponentially fast as the updates proceed. In the homework problems you will be 
asked to derive the exponential term in EstimatedRTT.
Figure 3.32 shows the SampleRTT values and EstimatedRTT for a value 
of α = 1/8 for a TCP connection between gaia.cs.umass.edu (in Amherst, 
Massachusetts) to fantasia.eurecom.fr (in the south of France). Clearly, 
the variations in the SampleRTT are smoothed out in the computation of the 
EstimatedRTT.
In addition to having an estimate of the RTT, it is also valuable to have a meas-
ure of the variability of the RTT. [RFC 6298] defines the RTT variation, DevRTT, 
as an estimate of how much SampleRTT typically deviates from EstimatedRTT:
DevRTT = (1 – β) # DevRTT + β # | SampleRTT – EstimatedRTT |
Note that DevRTT is an EWMA of the difference between SampleRTT and 
EstimatedRTT. If the SampleRTT values have little fluctuation, then DevRTT 
will be small; on the other hand, if there is a lot of fluctuation, DevRTT will be large. 
The recommended value of β is 0.25.
Setting and Managing the Retransmission Timeout Interval
Given values of EstimatedRTT and DevRTT, what value should be used for 
TCP’s timeout interval? Clearly, the interval should be greater than or equal to 
 
3.5    •    Connection-Oriented Transport: TCP         271
EstimatedRTT, or unnecessary retransmissions would be sent. But the timeout 
interval should not be too much larger than EstimatedRTT; otherwise, when a 
segment is lost, TCP would not quickly retransmit the segment, leading to large 
 
data transfer delays. It is therefore desirable to set the timeout equal to the 
 
EstimatedRTT plus some margin. The margin should be large when there is a lot 
of fluctuation in the SampleRTT values; it should be small when there is little fluc-
tuation. The value of DevRTT should thus come into play here. All of these consid-
erations are taken into account in TCP’s method for determining the retransmission 
timeout interval:
TimeoutInterval = EstimatedRTT + 4 # DevRTT
An initial TimeoutInterval value of 1 second is recommended [RFC 
6298]. Also, when a timeout occurs, the value of TimeoutInterval is doubled 
to avoid a premature timeout occurring for a subsequent segment that will soon be 
acknowledged. However, as soon as a segment is received and EstimatedRTT is 
updated, the TimeoutInterval is again computed using the formula above.
TCP provides reliable data transfer by using positive acknowledgments and timers in much 
the same way that we studied in Section 3.4. TCP acknowledges data that has been 
received correctly, and it then retransmits segments when segments or their corresponding 
acknowledgments are thought to be lost or corrupted. Certain versions of TCP also have 
an implicit NAK mechanism—with TCP’s fast retransmit mechanism, the receipt of three 
duplicate ACKs for a given segment serves as an implicit NAK for the following segment, 
triggering retransmission of that segment before timeout. TCP uses sequences of numbers to 
allow the receiver to identify lost or duplicate segments. Just as in the case of our reliable 
data transfer protocol, rdt3.0, TCP cannot itself tell for certain if a segment, or its ACK, is 
lost, corrupted, or overly delayed. At the sender, TCP’s response will be the same: retrans-
mit the segment in question.
TCP also uses pipelining, allowing the sender to have multiple transmitted but yet-to-
be-acknowledged segments outstanding at any given time. We saw earlier that pipelining 
can greatly improve a session’s throughput when the ratio of the segment size to round-
trip delay is small. The specific number of outstanding, unacknowledged segments that a 
sender can have is determined by TCP’s flow-control and congestion-control mechanisms. 
TCP flow control is discussed at the end of this section; TCP congestion control is discussed 
in Section 3.7. For the time being, we must simply be aware that the TCP sender uses 
pipelining.
Principles in Practice
272         Chapter 3    •    Transport Layer
3.5.4 Reliable Data Transfer
Recall that the Internet’s network-layer service (IP service) is unreliable. IP does 
not guarantee datagram delivery, does not guarantee in-order delivery of datagrams, 
and does not guarantee the integrity of the data in the datagrams. With IP service, 
datagrams can overflow router buffers and never reach their destination, datagrams 
can arrive out of order, and bits in the datagram can get corrupted (flipped from 0 to 
1 and vice versa). Because transport-layer segments are carried across the network 
by IP datagrams, transport-layer segments can suffer from these problems as well.
TCP creates a reliable data transfer service on top of IP’s unreliable best-
effort service. TCP’s reliable data transfer service ensures that the data stream that 
a process reads out of its TCP receive buffer is uncorrupted, without gaps, with-
out duplication, and in sequence; that is, the byte stream is exactly the same byte 
stream that was sent by the end system on the other side of the connection. How TCP 
provides a reliable data transfer involves many of the principles that we studied in 
 
Section 3.4.
In our earlier development of reliable data transfer techniques, it was conceptu-
ally easiest to assume that an individual timer is associated with each transmitted 
but not yet acknowledged segment. While this is great in theory, timer management 
can require considerable overhead. Thus, the recommended TCP timer management 
RTT (milliseconds)
150
200
250
300
350
100
1
8
15
22
29
36
43
50
Time (seconds)
Sample RTT
57
64
71
78
85
92
99
106
Estimated RTT
Figure 3.32  ♦  RTT samples and RTT estimates
3.5    •    Connection-Oriented Transport: TCP         273
procedures [RFC 6298] use only a single retransmission timer, even if there are mul-
tiple transmitted but not yet acknowledged segments. The TCP protocol described in 
this section follows this single-timer recommendation.
We will discuss how TCP provides reliable data transfer in two incremental 
steps. We first present a highly simplified description of a TCP sender that uses only 
timeouts to recover from lost segments; we then present a more complete description 
that uses duplicate acknowledgments in addition to timeouts. In the ensuing discus-
sion, we suppose that data is being sent in only one direction, from Host A to Host B, 
and that Host A is sending a large file.
Figure 3.33 presents a highly simplified description of a TCP sender. We see 
that there are three major events related to data transmission and retransmission 
in the TCP sender: data received from application above; timer timeout; and ACK 
/* Assume sender is not constrained by TCP ﬂow or congestion control, that data from above is less
than MSS in size, and that data transfer is in one direction only. */
NextSeqNum=InitialSeqNumber
SendBase=InitialSeqNumber
loop (forever) {
switch(event)
event: data received from application above
create TCP segment with sequence number NextSeqNum
if (timer currently not running)
start timer
pass segment to IP
NextSeqNum=NextSeqNum+length(data)
break;
event: timer timeout
retransmit not-yet-acknowledged segment with
smallest sequence number
start timer
break;
event: ACK received, with ACK ﬁeld value of y
if (y > SendBase) {
SendBase=y
if (there are currently any not-yet-acknowledged segments)
start timer
}
break;
} /* end of loop forever */
Figure 3.33  ♦  Simplified TCP sender
274         Chapter 3    •    Transport Layer
receipt. Upon the occurrence of the first major event, TCP receives data from the 
application, encapsulates the data in a segment, and passes the segment to IP. Note 
that each segment includes a sequence number that is the byte-stream number of 
the first data byte in the segment, as described in Section 3.5.2. Also note that if the 
timer is already not running for some other segment, TCP starts the timer when the 
segment is passed to IP. (It is helpful to think of the timer as being associated with 
the oldest unacknowledged segment.) The expiration interval for this timer is the 
TimeoutInterval, which is calculated from EstimatedRTT and DevRTT, as 
described in Section 3.5.3.
The second major event is the timeout. TCP responds to the timeout event by 
retransmitting the segment that caused the timeout. TCP then restarts the timer.
The third major event that must be handled by the TCP sender is the arrival of 
an acknowledgment segment (ACK) from the receiver (more specifically, a segment 
containing a valid ACK field value). On the occurrence of this event, TCP compares 
the ACK value y with its variable SendBase. The TCP state variable SendBase 
is the sequence number of the oldest unacknowledged byte. (Thus SendBase–1 is 
the sequence number of the last byte that is known to have been received correctly 
and in order at the receiver.) As indicated earlier, TCP uses cumulative acknowl-
edgments, so that y acknowledges the receipt of all bytes before byte number y. If 
 
y > SendBase, then the ACK is acknowledging one or more previously unac-
knowledged segments. Thus the sender updates its SendBase variable; it also 
restarts the timer if there currently are any not-yet-acknowledged segments.
A Few Interesting Scenarios
We have just described a highly simplified version of how TCP provides reliable data 
transfer. But even this highly simplified version has many subtleties. To get a good 
feeling for how this protocol works, let’s now walk through a few simple scenarios. 
Figure 3.34 depicts the first scenario, in which Host A sends one segment to Host B. 
Suppose that this segment has sequence number 92 and contains 8 bytes of data. After 
sending this segment, Host A waits for a segment from B with acknowledgment num-
ber 100. Although the segment from A is received at B, the acknowledgment from B 
to A gets lost. In this case, the timeout event occurs, and Host A retransmits the same 
segment. Of course, when Host B receives the retransmission, it observes from the 
sequence number that the segment contains data that has already been received. Thus, 
TCP in Host B will discard the bytes in the retransmitted segment.
In a second scenario, shown in Figure 3.35, Host A sends two segments back to 
back. The first segment has sequence number 92 and 8 bytes of data, and the second 
segment has sequence number 100 and 20 bytes of data. Suppose that both segments 
arrive intact at B, and B sends two separate acknowledgments for each of these seg-
ments. The first of these acknowledgments has acknowledgment number 100; the 
second has acknowledgment number 120. Suppose now that neither of the acknowl-
edgments arrives at Host A before the timeout. When the timeout event occurs, Host 
3.5    •    Connection-Oriented Transport: TCP         275
A resends the first segment with sequence number 92 and restarts the timer. As long 
as the ACK for the second segment arrives before the new timeout, the second seg-
ment will not be retransmitted.
In a third and final scenario, suppose Host A sends the two segments, exactly 
as in the second example. The acknowledgment of the first segment is lost in the 
network, but just before the timeout event, Host A receives an acknowledgment with 
acknowledgment number 120. Host A therefore knows that Host B has received 
everything up through byte 119; so Host A does not resend either of the two 
 
segments. This scenario is illustrated in Figure 3.36.
Doubling the Timeout Interval
We now discuss a few modifications that most TCP implementations employ. The 
first concerns the length of the timeout interval after a timer expiration. In this modi-
fication, whenever the timeout event occurs, TCP retransmits the not-yet-acknowl-
edged segment with the smallest sequence number, as described above. But each 
time TCP retransmits, it sets the next timeout interval to twice the previous value, 
Time
Time
Host A
Host B
Timeout
Seq=92, 8 bytes data
Seq=92, 8 bytes data
ACK=100
ACK=100
X
(loss)
Figure 3.34  ♦  Retransmission due to a lost acknowledgment
276         Chapter 3    •    Transport Layer
rather than deriving it from the last EstimatedRTT and DevRTT (as described 
in Section 3.5.3). For example, suppose TimeoutInterval associated with 
the oldest not yet acknowledged segment is .75 sec when the timer first expires. 
TCP will then retransmit this segment and set the new expiration time to 1.5 sec. If 
the timer expires again 1.5 sec later, TCP will again retransmit this segment, now 
setting the expiration time to 3.0 sec. Thus the intervals grow exponentially after 
each retransmission. However, whenever the timer is started after either of the two 
other events (that is, data received from application above, and ACK received), the 
TimeoutInterval is derived from the most recent values of EstimatedRTT 
and DevRTT.
This modification provides a limited form of congestion control. (More com-
prehensive forms of TCP congestion control will be studied in Section 3.7.) The 
timer expiration is most likely caused by congestion in the network, that is, too many 
packets arriving at one (or more) router queues in the path between the source and 
destination, causing packets to be dropped and/or long queuing delays. In times of 
congestion, if the sources continue to retransmit packets persistently, the congestion 
Time
Time
Host A
Host B
seq=92 timeout interval
Seq=92, 8 bytes data
Seq=100, 20 bytes data
ACK=100
ACK=120
ACK=120
seq=92 timeout interval
Seq=92, 8 bytes data
Figure 3.35  ♦  Segment 100 not retransmitted
3.5    •    Connection-Oriented Transport: TCP         277
may get worse. Instead, TCP acts more politely, with each sender retransmitting after 
longer and longer intervals. We will see that a similar idea is used by Ethernet when 
we study CSMA/CD in Chapter 6.
Fast Retransmit
One of the problems with timeout-triggered retransmissions is that the timeout period 
can be relatively long. When a segment is lost, this long timeout period forces the 
sender to delay resending the lost packet, thereby increasing the end-to-end delay. 
Fortunately, the sender can often detect packet loss well before the timeout event 
occurs by noting so-called duplicate ACKs. A duplicate ACK is an ACK that reac-
knowledges a segment for which the sender has already received an earlier acknowl-
edgment. To understand the sender’s response to a duplicate ACK, we must look at 
why the receiver sends a duplicate ACK in the first place. Table 3.2 summarizes the 
TCP receiver’s ACK generation policy [RFC 5681]. When a TCP receiver receives 
Time
Time
Host A
Host B
Seq=92 timeout interval
Seq=92, 8 bytes data
Seq=100,  20 bytes data
ACK=100
ACK=120
X
(loss)
Figure 3.36  ♦  
A cumulative acknowledgment avoids retransmission of the 
first segment
278         Chapter 3    •    Transport Layer
a segment with a sequence number that is larger than the next, expected, in-order 
sequence number, it detects a gap in the data stream—that is, a missing segment. 
This gap could be the result of lost or reordered segments within the network. Since 
TCP does not use negative acknowledgments, the receiver cannot send an explicit 
negative acknowledgment back to the sender. Instead, it simply reacknowledges 
(that is, generates a duplicate ACK for) the last in-order byte of data it has received. 
(Note that Table 3.2 allows for the case that the receiver does not discard out-of-
order segments.)
Because a sender often sends a large number of segments back to back, if one 
segment is lost, there will likely be many back-to-back duplicate ACKs. If the TCP 
sender receives three duplicate ACKs for the same data, it takes this as an indication 
that the segment following the segment that has been ACKed three times has been 
lost. (In the homework problems, we consider the question of why the sender waits 
for three duplicate ACKs, rather than just a single duplicate ACK.) In the case that 
three duplicate ACKs are received, the TCP sender performs a fast retransmit [RFC 
5681], retransmitting the missing segment before that segment’s timer expires. This 
is shown in Figure 3.37, where the second segment is lost, then retransmitted before 
its timer expires. For TCP with fast retransmit, the following code snippet replaces 
the ACK received event in Figure 3.33:
event: ACK received, with ACK field value of y
            if (y > SendBase) {
            SendBase=y
            if (there are currently any not yet
                       acknowledged segments)
               start timer
               }
Table 3.2  ♦  TCP ACK Generation Recommendation [RFC 5681]
Event
TCP Receiver Action
Arrival of in-order segment with expected sequence number. All  
data up to expected sequence number already acknowledged.
Delayed ACK. Wait up to 500 msec for arrival of another in-order segment.  
If next in-order segment does not arrive in this interval, send an ACK.
Arrival of in-order segment with expected sequence number. One  
other in-order segment waiting for ACK transmission.
One Immediately send single cumulative ACK, ACKing both in-order segments.
Arrival of out-of-order segment with higher-than-expected sequence 
number. Gap detected.
Immediately send duplicate ACK, indicating sequence number of next 
expected byte (which is the lower end of the gap).
Arrival of segment that partially or completely fills in gap in  
received data.
Immediately send ACK, provided that segment starts at the lower end  
of gap.
3.5    •    Connection-Oriented Transport: TCP         279
Host A
Host B
seq=100, 20 bytes of data
Timeout
Time
Time
X
seq=100, 20 bytes of data
seq=92, 8 bytes of data
seq=120, 15 bytes of data
seq=135, 6 bytes of data
seq=141, 16 bytes of data
ack=100
ack=100
ack=100
ack=100
Figure 3.37  ♦  
Fast retransmit: retransmitting the missing segment before 
the segment’s timer expires
            else {/* a duplicate ACK for already ACKed
                   segment */
               increment number of duplicate ACKs
                   received for y
               if (number of duplicate ACKS received
                   for y==3)
                   /* TCP fast retransmit */
                   resend segment with sequence number y
               }
           break;
We noted earlier that many subtle issues arise when a timeout/retransmit mech-
anism is implemented in an actual protocol such as TCP. The procedures above, 
which have evolved as a result of more than 20 years of experience with TCP timers, 
should convince you that this is indeed the case!
280         Chapter 3    •    Transport Layer
Go-Back-N or Selective Repeat?
Let us close our study of TCP’s error-recovery mechanism by considering the fol-
lowing question: Is TCP a GBN or an SR protocol? Recall that TCP acknowledg-
ments are cumulative and correctly received but out-of-order segments are not 
individually ACKed by the receiver. Consequently, as shown in Figure 3.33 (see 
also Figure 3.19), the TCP sender need only maintain the smallest sequence number 
of a transmitted but unacknowledged byte (SendBase) and the sequence number 
of the next byte to be sent (NextSeqNum). In this sense, TCP looks a lot like a 
GBN-style protocol. But there are some striking differences between TCP and Go-
Back-N. Many TCP implementations will buffer correctly received but out-of-order 
segments [Stevens 1994]. Consider also what happens when the sender sends a 
sequence of segments 1, 2, . . . , N, and all of the segments arrive in order without 
error at the receiver. Further suppose that the acknowledgment for packet n 6 N 
gets lost, but the remaining N - 1 acknowledgments arrive at the sender before 
their respective timeouts. In this example, GBN would retransmit not only packet n, 
but also all of the subsequent packets n + 1, n + 2, . . . , N. TCP, on the other hand, 
would retransmit at most one segment, namely, segment n. Moreover, TCP would 
not even retransmit segment n if the acknowledgment for segment n + 1 arrived 
before the timeout for segment n.
A proposed modification to TCP, the so-called selective acknowledgment  
[RFC 2018], allows a TCP receiver to acknowledge out-of-order segments selectively 
rather than just cumulatively acknowledging the last correctly received, in-order 
 
segment. When combined with selective retransmission—skipping the retransmis-
sion of segments that have already been selectively acknowledged by the receiver—
TCP looks a lot like our generic SR protocol. Thus, TCP’s error-recovery mechanism 
is probably best categorized as a hybrid of GBN and SR protocols.
3.5.5 Flow Control
Recall that the hosts on each side of a TCP connection set aside a receive buffer 
for the connection. When the TCP connection receives bytes that are correct and in 
sequence, it places the data in the receive buffer. The associated application process 
will read data from this buffer, but not necessarily at the instant the data arrives. 
Indeed, the receiving application may be busy with some other task and may not even 
attempt to read the data until long after it has arrived. If the application is relatively 
slow at reading the data, the sender can very easily overflow the connection’s receive 
buffer by sending too much data too quickly.
TCP provides a flow-control service to its applications to eliminate the pos-
sibility of the sender overflowing the receiver’s buffer. Flow control is thus a speed-
matching service—matching the rate at which the sender is sending against the rate 
at which the receiving application is reading. As noted earlier, a TCP sender can also 
be throttled due to congestion within the IP network; this form of sender control is 
3.5    •    Connection-Oriented Transport: TCP         281
referred to as congestion control, a topic we will explore in detail in Sections 3.6 
and 3.7. Even though the actions taken by flow and congestion control are similar 
(the throttling of the sender), they are obviously taken for very different reasons. 
Unfortunately, many authors use the terms interchangeably, and the savvy reader 
would be wise to distinguish between them. Let’s now discuss how TCP provides its 
flow-control service. In order to see the forest for the trees, we suppose throughout 
this section that the TCP implementation is such that the TCP receiver discards out-
of-order segments.
TCP provides flow control by having the sender maintain a variable called 
the receive window. Informally, the receive window is used to give the sender an 
idea of how much free buffer space is available at the receiver. Because TCP is 
full-duplex, the sender at each side of the connection maintains a distinct receive 
window. Let’s investigate the receive window in the context of a file transfer. Sup-
pose that Host A is sending a large file to Host B over a TCP connection. Host B 
allocates a receive buffer to this connection; denote its size by RcvBuffer. From 
time to time, the application process in Host B reads from the buffer. Define the 
following variables:
•	 LastByteRead: the number of the last byte in the data stream read from the 
buffer by the application process in B
•	 LastByteRcvd: the number of the last byte in the data stream that has arrived 
from the network and has been placed in the receive buffer at B
Because TCP is not permitted to overflow the allocated buffer, we must have
LastByteRcvd – LastByteRead … RcvBuffer
The receive window, denoted rwnd is set to the amount of spare room in the buffer:
rwnd = RcvBuffer – [LastByteRcvd – LastByteRead]
Because the spare room changes with time, rwnd is dynamic. The variable rwnd is 
illustrated in Figure 3.38.
How does the connection use the variable rwnd to provide the flow-control 
service? Host B tells Host A how much spare room it has in the connection buffer 
by placing its current value of rwnd in the receive window field of every segment it 
sends to A. Initially, Host B sets rwnd = RcvBuffer. Note that to pull this off, 
Host B must keep track of several connection-specific variables.
Host A in turn keeps track of two variables, LastByteSent and Last-
ByteAcked, which have obvious meanings. Note that the difference between these 
two variables, LastByteSent – LastByteAcked, is the amount of unac-
knowledged data that A has sent into the connection. By keeping the amount of 
unacknowledged data less than the value of rwnd, Host A is assured that it is not 
282         Chapter 3    •    Transport Layer
overflowing the receive buffer at Host B. Thus, Host A makes sure throughout the 
connection’s life that
LastByteSent – LastByteAcked … rwnd
There is one minor technical problem with this scheme. To see this, suppose Host 
B’s receive buffer becomes full so that rwnd = 0. After advertising rwnd = 0 to 
Host A, also suppose that B has nothing to send to A. Now consider what happens. 
As the application process at B empties the buffer, TCP does not send new seg-
ments with new rwnd values to Host A; indeed, TCP sends a segment to Host A 
only if it has data to send or if it has an acknowledgment to send. Therefore, Host 
A is never informed that some space has opened up in Host B’s receive buffer—
Host A is blocked and can transmit no more data! To solve this problem, the TCP 
specification requires Host A to continue to send segments with one data byte when 
B’s receive window is zero. These segments will be acknowledged by the receiver. 
Eventually the buffer will begin to empty and the acknowledgments will contain a 
nonzero rwnd value.
The online site at http://www.awl.com/kurose-ross for this book provides an 
interactive Java applet that illustrates the operation of the TCP receive window.
Having described TCP’s flow-control service, we briefly mention here that UDP 
does not provide flow control and consequently, segments may be lost at the receiver 
due to buffer overflow. For example, consider sending a series of UDP segments 
from a process on Host A to a process on Host B. For a typical UDP implementation, 
UDP will append the segments in a finite-sized buffer that “precedes” the corre-
sponding socket (that is, the door to the process). The process reads one entire seg-
ment at a time from the buffer. If the process does not read the segments fast enough 
from the buffer, the buffer will overflow and segments will get dropped.
Application
process 
Data
from IP
TCP data
in buffer
rwnd
RcvBufer
Spare room
Figure 3.38  ♦  
The receive window (rwnd) and the receive buffer 
(RcvBuffer)
3.5    •    Connection-Oriented Transport: TCP         283
3.5.6 TCP Connection Management
In this subsection we take a closer look at how a TCP connection is established and 
torn down. Although this topic may not seem particularly thrilling, it is important 
because TCP connection establishment can significantly add to perceived delays (for 
example, when surfing the Web). Furthermore, many of the most common network 
attacks—including the incredibly popular SYN flood attack—exploit vulnerabilities 
in TCP connection management. Let’s first take a look at how a TCP connection is 
established. Suppose a process running in one host (client) wants to initiate a con-
nection with another process in another host (server). The client application process 
first informs the client TCP that it wants to establish a connection to a process in the 
server. The TCP in the client then proceeds to establish a TCP connection with the 
TCP in the server in the following manner:
•	 Step 1. The client-side TCP first sends a special TCP segment to the server-side 
TCP. This special segment contains no application-layer data. But one of the flag 
bits in the segment’s header (see Figure 3.29), the SYN bit, is set to 1. For this 
reason, this special segment is referred to as a SYN segment. In addition, the cli-
ent randomly chooses an initial sequence number (client_isn) and puts this 
number in the sequence number field of the initial TCP SYN segment. This seg-
ment is encapsulated within an IP datagram and sent to the server. There has been 
considerable interest in properly randomizing the choice of the client_isn in 
order to avoid certain security attacks [CERT 2001–09].
•	 Step 2. Once the IP datagram containing the TCP SYN segment arrives at the 
server host (assuming it does arrive!), the server extracts the TCP SYN segment 
from the datagram, allocates the TCP buffers and variables to the connection, 
and sends a connection-granted segment to the client TCP. (We’ll see in Chapter 
8 that the allocation of these buffers and variables before completing the third 
step of the three-way handshake makes TCP vulnerable to a denial-of-service 
attack known as SYN flooding.) This connection-granted segment also contains 
no application-layer data. However, it does contain three important pieces of 
information in the segment header. First, the SYN bit is set to 1. Second, the 
acknowledgment field of the TCP segment header is set to client_isn+1. 
Finally, the server chooses its own initial sequence number (server_isn) and 
puts this value in the sequence number field of the TCP segment header. This 
connection-granted segment is saying, in effect, “I received your SYN packet to 
start a connection with your initial sequence number, client_isn. I agree to 
establish this connection. My own initial sequence number is server_isn.” 
The connection-granted segment is referred to as a SYNACK segment.
•	 Step 3. Upon receiving the SYNACK segment, the client also allocates buffers 
and variables to the connection. The client host then sends the server yet another 
segment; this last segment acknowledges the server’s connection-granted segment 
(the client does so by putting the value server_isn+1 in the acknowledgment 
284         Chapter 3    •    Transport Layer
field of the TCP segment header). The SYN bit is set to zero, since the connection 
is established. This third stage of the three-way handshake may carry client-to-
server data in the segment payload.
Once these three steps have been completed, the client and server hosts can send 
segments containing data to each other. In each of these future segments, the SYN 
bit will be set to zero. Note that in order to establish the connection, three packets 
are sent between the two hosts, as illustrated in Figure 3.39. For this reason, this 
connection-establishment procedure is often referred to as a three-way handshake. 
Several aspects of the TCP three-way handshake are explored in the homework prob-
lems (Why are initial sequence numbers needed? Why is a three-way handshake, 
as opposed to a two-way handshake, needed?). It’s interesting to note that a rock 
climber and a belayer (who is stationed below the rock climber and whose job it is 
to handle the climber’s safety rope) use a three-way-handshake communication pro-
tocol that is identical to TCP’s to ensure that both sides are ready before the climber 
begins ascent.
All good things must come to an end, and the same is true with a TCP connec-
tion. Either of the two processes participating in a TCP connection can end the con-
nection. When a connection ends, the “resources” (that is, the buffers and variables) 
Time
Time
Client host
Connection
request
Connection
granted
Server host
SYN=1, seq=client_isn
SYN=1, seq=server_isn,
ack=client_isn+1
SYN=0, seq=client_isn+1,
ack=server_isn+1
ACK
Figure 3.39  ♦  TCP three-way handshake: segment exchange
3.5    •    Connection-Oriented Transport: TCP         285
in the hosts are deallocated. As an example, suppose the client decides to close the 
connection, as shown in Figure 3.40. The client application process issues a close 
command. This causes the client TCP to send a special TCP segment to the server 
process. This special segment has a flag bit in the segment’s header, the FIN bit (see 
Figure 3.29), set to 1. When the server receives this segment, it sends the client an 
acknowledgment segment in return. The server then sends its own shutdown segment, 
which has the FIN bit set to 1. Finally, the client acknowledges the server’s shutdown 
segment. At this point, all the resources in the two hosts are now deallocated.
During the life of a TCP connection, the TCP protocol running in each host 
makes transitions through various TCP states. Figure 3.41 illustrates a typical 
sequence of TCP states that are visited by the client TCP. The client TCP begins in 
the CLOSED state. The application on the client side initiates a new TCP connec-
tion (by creating a Socket object in our Java examples as in the Python examples 
from Chapter 2). This causes TCP in the client to send a SYN segment to TCP in the 
server. After having sent the SYN segment, the client TCP enters the SYN_SENT 
state. While in the SYN_SENT state, the client TCP waits for a segment from the 
server TCP that includes an acknowledgment for the client’s previous segment and 
Time
Time
Client
Close
Close
Server
FIN
ACK
ACK
FIN
Closed
Timed wait
Figure 3.40  ♦  Closing a TCP connection
286         Chapter 3    •    Transport Layer
has the SYN bit set to 1. Having received such a segment, the client TCP enters the 
ESTABLISHED state. While in the ESTABLISHED state, the TCP client can send 
and receive TCP segments containing payload (that is, application-generated) data.
Suppose that the client application decides it wants to close the connection. (Note 
that the server could also choose to close the connection.) This causes the client TCP 
to send a TCP segment with the FIN bit set to 1 and to enter the FIN_WAIT_1 state. 
While in the FIN_WAIT_1 state, the client TCP waits for a TCP segment from the 
server with an acknowledgment. When it receives this segment, the client TCP enters 
the FIN_WAIT_2 state. While in the FIN_WAIT_2 state, the client waits for another 
segment from the server with the FIN bit set to 1; after receiving this segment, the client 
TCP acknowledges the server’s segment and enters the TIME_WAIT state. The TIME_
WAIT state lets the TCP client resend the final acknowledgment in case the ACK is 
lost. The time spent in the TIME_WAIT state is implementation-dependent, but typical 
values are 30 seconds, 1 minute, and 2 minutes. After the wait, the connection formally 
closes and all resources on the client side (including port numbers) are released.
Figure 3.42 illustrates the series of states typically visited by the server-side 
TCP, assuming the client begins connection teardown. The transitions are self-
explanatory. In these two state-transition diagrams, we have only shown how a TCP 
connection is normally established and shut down. We have not described what hap-
pens in certain pathological scenarios, for example, when both sides of a connection 
want to initiate or shut down at the same time. If you are interested in learning about 
CLOSED
SYN_SENT
ESTABLISHED
FIN_WAIT_1
FIN_WAIT_2
TIME_WAIT
Send SYN
Send FIN
Receive ACK, 
send nothing
Wait 30 seconds
Receive FIN, 
send ACK
Receive SYN & ACK, 
send ACK
Client application
initiates a TCP connection
Client application
initiates close connection
Figure 3.41  ♦  A typical sequence of TCP states visited by a client TCP
3.5    •    Connection-Oriented Transport: TCP         287
this and other advanced issues concerning TCP, you are encouraged to see Stevens’ 
comprehensive book [Stevens 1994].
Our discussion above has assumed that both the client and server are prepared 
to communicate, i.e., that the server is listening on the port to which the client sends 
its SYN segment. Let’s consider what happens when a host receives a TCP segment 
whose port numbers or source IP address do not match with any of the ongoing sock-
ets in the host. For example, suppose a host receives a TCP SYN packet with desti-
nation port 80, but the host is not accepting connections on port 80 (that is, it is not 
running a Web server on port 80). Then the host will send a special reset segment to 
the source. This TCP segment has the RST flag bit (see Section 3.5.2) set to 1. Thus, 
when a host sends a reset segment, it is telling the source “I don’t have a socket for 
that segment. Please do not resend the segment.” When a host receives a UDP packet 
whose destination port number doesn’t match with an ongoing UDP socket, the host 
sends a special ICMP datagram, as discussed in Chapter 5.
Now that we have a good understanding of TCP connection management, let’s 
revisit the nmap port-scanning tool and examine more closely how it works. To explore 
a specific TCP port, say port 6789, on a target host, nmap will send a TCP SYN seg-
ment with destination port 6789 to that host. There are three possible outcomes:
•	 The source host receives a TCP SYNACK segment from the target host. Since this 
means that an application is running with TCP port 6789 on the target post, nmap 
returns “open.”
CLOSED
LISTEN
SYN_RCVD
ESTABLISHED
CLOSE_WAIT
LAST_ACK
Receive FIN,
send ACK
Receive ACK, 
send nothing
Send FIN
Receive SYN 
send SYN & ACK
Server application
creates a listen socket
Receive ACK, 
send nothing
Figure 3.42  ♦  
A typical sequence of TCP states visited by a server-side TCP
288         Chapter 3    •    Transport Layer
The Syn Flood Attack
We’ve seen in our discussion of TCP’s three-way handshake that a server allocates 
and initializes connection variables and buffers in response to a received SYN. The 
server then sends a SYNACK in response, and awaits an ACK segment from the cli-
ent. If the client does not send an ACK to complete the third step of this 3-way hand-
shake, eventually (often after a minute or more) the server will terminate the half-open 
connection and reclaim the allocated resources.
This TCP connection management protocol sets the stage for a classic Denial of 
Service (DoS) attack known as the SYN flood attack. In this attack, the attacker(s) send 
a large number of TCP SYN segments, without completing the third handshake step. With 
this deluge of SYN segments, the server’s connection resources become exhausted as 
they are allocated (but never used!) for half-open connections; legitimate clients are then 
denied service. Such SYN flooding attacks were among the first documented DoS attacks 
[CERT SYN 1996]. Fortunately, an effective defense known as SYN cookies [RFC 
4987] are now deployed in most major operating systems. SYN cookies work as follows:
•	

When the server receives a SYN segment, it does not know if the segment is 
coming from a legitimate user or is part of a SYN flood attack. So, instead of 
creating a half-open TCP connection for this SYN, the server creates an initial 
TCP sequence number that is a complicated function (hash function) of source 
and destination IP addresses and port numbers of the SYN segment, as well as 
a secret number only known to the server. This carefully crafted initial sequence 
number is the so-called “cookie.” The server then sends the client a SYNACK 
packet with this special initial sequence number. Importantly, the server does not 
remember the cookie or any other state information corresponding to the SYN.
•	

A legitimate client will return an ACK segment. When the server receives this 
ACK, it must verify that the ACK corresponds to some SYN sent earlier. But how 
is this done if the server maintains no memory about SYN segments? As you may 
have guessed, it is done with the cookie. Recall that for a legitimate ACK, the 
value in the acknowledgment field is equal to the initial sequence number in the 
SYNACK (the cookie value in this case) plus one (see Figure 3.39). The server 
can then run the same hash function using the source and destination IP address 
and port numbers in the SYNACK (which are the same as in the original SYN) 
and the secret number. If the result of the function plus one is the same as the 
acknowledgment (cookie) value in the client’s SYNACK, the server concludes that 
the ACK corresponds to an earlier SYN segment and is hence valid. The server 
then creates a fully open connection along with a socket.
•	

On the other hand, if the client does not return an ACK segment, then the origi-
nal SYN has done no harm at the server, since the server hasn’t yet allocated 
any resources in response to the original bogus SYN.
FOCUS ON SECURITY
3.6    •    Principles of Congestion Control         289
•	 The source host receives a TCP RST segment from the target host. This means 
that the SYN segment reached the target host, but the target host is not running 
an application with TCP port 6789. But the attacker at least knows that the seg-
ments destined to the host at port 6789 are not blocked by any firewall on the path 
between source and target hosts. (Firewalls are discussed in Chapter 8.)
•	 The source receives nothing. This likely means that the SYN segment was blocked 
by an intervening firewall and never reached the target host.
Nmap is a powerful tool that can “case the joint” not only for open TCP ports, 
but also for open UDP ports, for firewalls and their configurations, and even for the 
versions of applications and operating systems. Most of this is done by manipulating 
TCP connection-management segments [Skoudis 2006]. You can download nmap 
from www.nmap.org.
This completes our introduction to error control and flow control in TCP. In 
Section 3.7 we’ll return to TCP and look at TCP congestion control in some depth. 
Before doing so, however, we first step back and examine congestion-control issues 
in a broader context.
3.6	 Principles of Congestion Control
In the previous sections, we examined both the general principles and specific TCP 
mechanisms used to provide for a reliable data transfer service in the face of packet 
loss. We mentioned earlier that, in practice, such loss typically results from the over-
flowing of router buffers as the network becomes congested. Packet retransmission 
thus treats a symptom of network congestion (the loss of a specific transport-layer 
segment) but does not treat the cause of network congestion—too many sources 
attempting to send data at too high a rate. To treat the cause of network congestion, 
mechanisms are needed to throttle senders in the face of network congestion.
In this section, we consider the problem of congestion control in a general con-
text, seeking to understand why congestion is a bad thing, how network congestion 
is manifested in the performance received by upper-layer applications, and various 
approaches that can be taken to avoid, or react to, network congestion. This more 
general study of congestion control is appropriate since, as with reliable data trans-
fer, it is high on our “top-ten” list of fundamentally important problems in network-
ing. The following section contains a detailed study of TCP’s congestion-control 
algorithm.
3.6.1 The Causes and the Costs of Congestion
Let’s begin our general study of congestion control by examining three increas-
ingly complex scenarios in which congestion occurs. In each case, we’ll look at why 
 
290         Chapter 3    •    Transport Layer
congestion occurs in the first place and at the cost of congestion (in terms of resources 
not fully utilized and poor performance received by the end systems). We’ll not (yet) 
focus on how to react to, or avoid, congestion but rather focus on the simpler issue of 
understanding what happens as hosts increase their transmission rate and the network 
becomes congested.
Scenario 1: Two Senders, a Router with Infinite Buffers
We begin by considering perhaps the simplest congestion scenario possible: Two 
hosts (A and B) each have a connection that shares a single hop between source and 
destination, as shown in Figure 3.43.
Let’s assume that the application in Host A is sending data into the connection 
(for example, passing data to the transport-level protocol via a socket) at an average 
rate of lin bytes/sec. These data are original in the sense that each unit of data is sent 
into the socket only once. The underlying transport-level protocol is a simple one. 
Data is encapsulated and sent; no error recovery (for example, retransmission), flow 
control, or congestion control is performed. Ignoring the additional overhead due 
to adding transport- and lower-layer header information, the rate at which Host A 
offers traffic to the router in this first scenario is thus lin bytes/sec. Host B operates 
in a similar manner, and we assume for simplicity that it too is sending at a rate of 
lin bytes/sec. Packets from Hosts A and B pass through a router and over a shared 
outgoing link of capacity R. The router has buffers that allow it to store incoming 
packets when the packet-arrival rate exceeds the outgoing link’s capacity. In this first 
scenario, we assume that the router has an infinite amount of buffer space.
Figure 3.44 plots the performance of Host A’s connection under this first sce-
nario. The left graph plots the per-connection throughput (number of bytes per 
Host B
Unlimited shared
output link buffers
in: original data
Host A
Host D
Host C
out 
Figure 3.43  ♦  
Congestion scenario 1: Two connections sharing a single 
hop with infinite buffers
3.6    •    Principles of Congestion Control         291
second at the receiver) as a function of the connection-sending rate. For a sending 
rate between 0 and R/2, the throughput at the receiver equals the sender’s sending 
rate—everything sent by the sender is received at the receiver with a finite delay. 
When the sending rate is above R/2, however, the throughput is only R/2. This upper 
limit on throughput is a consequence of the sharing of link capacity between two 
connections. The link simply cannot deliver packets to a receiver at a steady-state 
rate that exceeds R/2. No matter how high Hosts A and B set their sending rates, they 
will each never see a throughput higher than R/2.
Achieving a per-connection throughput of R/2 might actually appear to be a good 
thing, because the link is fully utilized in delivering packets to their destinations. The 
right-hand graph in Figure 3.44, however, shows the consequence of operating near link 
capacity. As the sending rate approaches R/2 (from the left), the average delay becomes 
larger and larger. When the sending rate exceeds R/2, the average number of queued 
packets in the router is unbounded, and the average delay between source and destina-
tion becomes infinite (assuming that the connections operate at these sending rates for 
an infinite period of time and there is an infinite amount of buffering available). Thus, 
while operating at an aggregate throughput of near R may be ideal from a throughput 
standpoint, it is far from ideal from a delay standpoint. Even in this (extremely) ideal-
ized scenario, we’ve already found one cost of a congested network—large queuing 
delays are experienced as the packet-arrival rate nears the link capacity.
Scenario 2: Two Senders and a Router with Finite Buffers
Let’s now slightly modify scenario 1 in the following two ways (see Figure 3.45). 
First, the amount of router buffering is assumed to be finite. A consequence of this 
real-world assumption is that packets will be dropped when arriving to an already-
full buffer. Second, we assume that each connection is reliable. If a packet containing 
R/2
R/2
Delay
R/2
in
in
out
a.
b.
Figure 3.44  ♦  
Congestion scenario 1: Throughput and delay as a function 
of host sending rate
292         Chapter 3    •    Transport Layer
a transport-level segment is dropped at the router, the sender will eventually retrans-
mit it. Because packets can be retransmitted, we must now be more careful with our 
use of the term sending rate. Specifically, let us again denote the rate at which the 
application sends original data into the socket by lin bytes/sec. The rate at which the 
transport layer sends segments (containing original data and retransmitted data) into 
the network will be denoted l′
in bytes/sec. l′
in is sometimes referred to as the offered 
load to the network.
The performance realized under scenario 2 will now depend strongly on how 
retransmission is performed. First, consider the unrealistic case that Host A is able 
to somehow (magically!) determine whether or not a buffer is free in the router and 
thus sends a packet only when a buffer is free. In this case, no loss would occur, lin 
would be equal to l′
in, and the throughput of the connection would be equal to lin. 
This case is shown in Figure 3.46(a). From a throughput standpoint, performance 
is ideal—everything that is sent is received. Note that the average host sending rate 
cannot exceed R/2 under this scenario, since packet loss is assumed never to occur.
Consider next the slightly more realistic case that the sender retransmits only 
when a packet is known for certain to be lost. (Again, this assumption is a bit of 
a stretch. However, it is possible that the sending host might set its timeout large 
enough to be virtually assured that a packet that has not been acknowledged has been 
lost.) In this case, the performance might look something like that shown in Figure 
3.46(b). To appreciate what is happening here, consider the case that the offered 
load, l′
in (the rate of original data transmission plus retransmissions), equals R/2. 
According to Figure 3.46(b), at this value of the offered load, the rate at which data 
Finite shared output
link buffers
Host B
Host A
Host D
Host C
out 
in: original data
’in: original data, plus
retransmitted data
Figure 3.45  ♦  
Scenario 2: Two hosts (with retransmissions) and a router 
with finite buffers
3.6    •    Principles of Congestion Control         293
are delivered to the receiver application is R/3. Thus, out of the 0.5R units of data 
transmitted, 0.333R bytes/sec (on average) are original data and 0.166R bytes/sec (on 
average) are retransmitted data. We see here another cost of a congested network—
the sender must perform retransmissions in order to compensate for dropped (lost) 
packets due to buffer overflow.
Finally, let us consider the case that the sender may time out prematurely and 
retransmit a packet that has been delayed in the queue but not yet lost. In this case, 
both the original data packet and the retransmission may reach the receiver. Of 
course, the receiver needs but one copy of this packet and will discard the retrans-
mission. In this case, the work done by the router in forwarding the retransmitted 
copy of the original packet was wasted, as the receiver will have already received 
the original copy of this packet. The router would have better used the link trans-
mission capacity to send a different packet instead. Here then is yet another cost of 
a congested network—unneeded retransmissions by the sender in the face of large 
delays may cause a router to use its link bandwidth to forward unneeded copies of a 
packet. Figure 3.46 (c) shows the throughput versus offered load when each packet 
is assumed to be forwarded (on average) twice by the router. Since each packet is 
forwarded twice, the throughput will have an asymptotic value of R/4 as the offered 
load approaches R/2.
Scenario 3: Four Senders, Routers with Finite Buffers, and  
Multihop Paths
In our final congestion scenario, four hosts transmit packets, each over overlap-
ping two-hop paths, as shown in Figure 3.47. We again assume that each host uses 
a timeout/retransmission mechanism to implement a reliable data transfer service, 
that all hosts have the same value of lin, and that all router links have capacity 
 
R bytes/sec.
R/2
R/2
R/2
out
a.
b.
R/2
out
R/3
R/2
R/2
out
R/4
c.
’in
’in
’in
Figure 3.46  ♦  Scenario 2 performance with finite buffers
294         Chapter 3    •    Transport Layer
Let’s consider the connection from Host A to Host C, passing through routers 
R1 and R2. The A–C connection shares router R1 with the D–B connection and 
shares router R2 with the B–D connection. For extremely small values of lin, buffer 
overflows are rare (as in congestion scenarios 1 and 2), and the throughput approxi-
mately equals the offered load. For slightly larger values of lin, the corresponding 
throughput is also larger, since more original data is being transmitted into the net-
work and delivered to the destination, and overflows are still rare. Thus, for small 
values of lin, an increase in lin results in an increase in lout.
Having considered the case of extremely low traffic, let’s next examine the case 
that lin (and hence l′
in) is extremely large. Consider router R2. The A–C traffic 
arriving to router R2 (which arrives at R2 after being forwarded from R1) can have 
an arrival rate at R2 that is at most R, the capacity of the link from R1 to R2, regard-
less of the value of lin. If l′
in is extremely large for all connections (including the 
 
Host B
Host A
R1
R4
R2
R3
Host C
Host D
Finite shared output
link buffers
in: original data
’in: original
data, plus
retransmitted
data
out
Figure 3.47  ♦  Four senders, routers with finite buffers, and multihop paths
3.6    •    Principles of Congestion Control         295
B–D connection), then the arrival rate of B–D traffic at R2 can be much larger than 
that of the A–C traffic. Because the A–C and B–D traffic must compete at router 
R2 for the limited amount of buffer space, the amount of A–C traffic that success-
fully gets through R2 (that is, is not lost due to buffer overflow) becomes smaller 
and smaller as the offered load from B–D gets larger and larger. In the limit, as the 
offered load approaches infinity, an empty buffer at R2 is immediately filled by a 
B–D packet, and the throughput of the A–C connection at R2 goes to zero. This, in 
turn, implies that the A–C end-to-end throughput goes to zero in the limit of heavy 
traffic. These considerations give rise to the offered load versus throughput tradeoff 
shown in Figure 3.48.
The reason for the eventual decrease in throughput with increasing offered 
load is evident when one considers the amount of wasted work done by the net-
work. In the high-traffic scenario outlined above, whenever a packet is dropped at 
a second-hop router, the work done by the first-hop router in forwarding a packet 
to the second-hop router ends up being “wasted.” The network would have been 
equally well off (more accurately, equally bad off) if the first router had simply 
discarded that packet and remained idle. More to the point, the transmission capac-
ity used at the first router to forward the packet to the second router could have 
been much more profitably used to transmit a different packet. (For example, when 
selecting a packet for transmission, it might be better for a router to give priority 
to packets that have already traversed some number of upstream routers.) So here 
we see yet another cost of dropping a packet due to congestion—when a packet 
is dropped along a path, the transmission capacity that was used at each of the 
upstream links to forward that packet to the point at which it is dropped ends up 
having been wasted.
R/2
out
’in
Figure 3.48  ♦  
Scenario 3 performance with finite buffers and multihop 
paths
296         Chapter 3    •    Transport Layer
3.6.2 Approaches to Congestion Control
In Section 3.7, we’ll examine TCP’s specific approach to congestion control in great 
detail. Here, we identify the two broad approaches to congestion control that are 
taken in practice and discuss specific network architectures and congestion-control 
protocols embodying these approaches.
At the highest level, we can distinguish among congestion-control approaches 
by whether the network layer provides explicit assistance to the transport layer for 
congestion-control purposes:
•	 End-to-end congestion control. In an end-to-end approach to congestion control, 
the network layer provides no explicit support to the transport layer for conges-
tion-control purposes. Even the presence of network congestion must be inferred 
by the end systems based only on observed network behavior (for example, packet 
loss and delay). We’ll see shortly in Section 3.7.1 that TCP takes this end-to-end 
approach toward congestion control, since the IP layer is not required to provide 
feedback to hosts regarding network congestion. TCP segment loss (as indicated 
by a timeout or the receipt of three duplicate acknowledgments) is taken as an 
indication of network congestion, and TCP decreases its window size accord-
ingly. We’ll also see a more recent proposal for TCP congestion control that 
uses increasing round-trip segment delay as an indicator of increased network 
 
congestion
•	 Network-assisted congestion control. With network-assisted congestion control, 
routers provide explicit feedback to the sender and/or receiver regarding the con-
gestion state of the network. This feedback may be as simple as a single bit indi-
cating congestion at a link – an approach taken in the early IBM SNA [Schwartz 
1982], DEC DECnet [Jain 1989; Ramakrishnan 1990] architectures, and ATM 
[Black 1995] network architectures. More sophisticated feedback is also possible. 
For example, in ATM Available Bite Rate (ABR) congestion control, a router 
informs the sender of the maximum host sending rate it (the router) can support 
on an outgoing link. As noted above, the Internet-default versions of IP and TCP 
adopt an end-to-end approach towards congestion control. We’ll see, however, 
in Section 3.7.2 that, more recently, IP and TCP may also optionally implement 
network-assisted congestion control.
For network-assisted congestion control, congestion information is typically 
fed back from the network to the sender in one of two ways, as shown in Figure 
3.49. Direct feedback may be sent from a network router to the sender. This form 
of notification typically takes the form of a choke packet (essentially saying, “I’m 
congested!”). The second and more common form of notification occurs when a 
router marks/updates a field in a packet flowing from sender to receiver to indicate 
congestion. Upon receipt of a marked packet, the receiver then notifies the sender of 
the congestion indication. This latter form of notification takes a full round-trip time.
3.7    •    TCP Congestion Control         297
3.7	 TCP Congestion Control
In this section we return to our study of TCP. As we learned in Section 3.5, TCP pro-
vides a reliable transport service between two processes running on different hosts. 
Another key component of TCP is its congestion-control mechanism. As indicated 
in the previous section, TCP must use end-to-end congestion control rather than net-
work-assisted congestion control, since the IP layer provides no explicit feedback to 
the end systems regarding network congestion.
The approach taken by TCP is to have each sender limit the rate at which it 
sends traffic into its connection as a function of perceived network congestion. If 
a TCP sender perceives that there is little congestion on the path between itself and 
the destination, then the TCP sender increases its send rate; if the sender perceives 
that there is congestion along the path, then the sender reduces its send rate. But this 
approach raises three questions. First, how does a TCP sender limit the rate at which 
it sends traffic into its connection? Second, how does a TCP sender perceive that 
there is congestion on the path between itself and the destination? And third, what 
algorithm should the sender use to change its send rate as a function of perceived 
end-to-end congestion?
Let’s first examine how a TCP sender limits the rate at which it sends traffic into 
its connection. In Section 3.5 we saw that each side of a TCP connection consists of 
a receive buffer, a send buffer, and several variables (LastByteRead, rwnd, and 
so on). The TCP congestion-control mechanism operating at the sender keeps track 
Host A
Network feedback via receiver
Direct network
feedback
Host B
Figure 3.49  ♦  
Two feedback pathways for network-indicated congestion 
information
298         Chapter 3    •    Transport Layer
of an additional variable, the congestion window. The congestion window, denoted 
cwnd, imposes a constraint on the rate at which a TCP sender can send traffic into 
the network. Specifically, the amount of unacknowledged data at a sender may not 
exceed the minimum of cwnd and rwnd, that is:
LastByteSent – LastByteAcked … min{cwnd, rwnd}
In order to focus on congestion control (as opposed to flow control), let us henceforth 
assume that the TCP receive buffer is so large that the receive-window constraint can 
be ignored; thus, the amount of unacknowledged data at the sender is solely limited 
by cwnd. We will also assume that the sender always has data to send, i.e., that all 
segments in the congestion window are sent.
The constraint above limits the amount of unacknowledged data at the sender 
and therefore indirectly limits the sender’s send rate. To see this, consider a connec-
tion for which loss and packet transmission delays are negligible. Then, roughly, at 
the beginning of every RTT, the constraint permits the sender to send cwnd bytes of 
data into the connection; at the end of the RTT the sender receives acknowledgments 
for the data. Thus the sender’s send rate is roughly cwnd/RTT bytes/sec. By adjusting 
the value of cwnd, the sender can therefore adjust the rate at which it sends data into 
its connection.
Let’s next consider how a TCP sender perceives that there is congestion on the 
path between itself and the destination. Let us define a “loss event” at a TCP sender 
as the occurrence of either a timeout or the receipt of three duplicate ACKs from the 
receiver. (Recall our discussion in Section 3.5.4 of the timeout event in Figure 3.33 
and the subsequent modification to include fast retransmit on receipt of three dupli-
cate ACKs.) When there is excessive congestion, then one (or more) router buffers 
along the path overflows, causing a datagram (containing a TCP segment) to be 
dropped. The dropped datagram, in turn, results in a loss event at the sender—either 
a timeout or the receipt of three duplicate ACKs—which is taken by the sender to be 
an indication of congestion on the sender-to-receiver path.
Having considered how congestion is detected, let’s next consider the more opti-
mistic case when the network is congestion-free, that is, when a loss event doesn’t 
occur. In this case, acknowledgments for previously unacknowledged segments 
will be received at the TCP sender. As we’ll see, TCP will take the arrival of these 
acknowledgments as an indication that all is well—that segments being transmitted 
into the network are being successfully delivered to the destination—and will use 
acknowledgments to increase its congestion window size (and hence its transmis-
sion rate). Note that if acknowledgments arrive at a relatively slow rate (e.g., if the 
end-end path has high delay or contains a low-bandwidth link), then the congestion 
window will be increased at a relatively slow rate. On the other hand, if acknowl-
edgments arrive at a high rate, then the congestion window will be increased more 
quickly. Because TCP uses acknowledgments to trigger (or clock) its increase in 
congestion window size, TCP is said to be self-clocking.
3.7    •    TCP Congestion Control         299
Given the mechanism of adjusting the value of cwnd to control the sending rate, 
the critical question remains: How should a TCP sender determine the rate at which it 
should send? If TCP senders collectively send too fast, they can congest the network, 
leading to the type of congestion collapse that we saw in Figure 3.48. Indeed, the ver-
sion of TCP that we’ll study shortly was developed in response to observed Internet 
congestion collapse [Jacobson 1988] under earlier versions of TCP. However, if TCP 
senders are too cautious and send too slowly, they could under utilize the bandwidth 
in the network; that is, the TCP senders could send at a higher rate without congest-
ing the network. How then do the TCP senders determine their sending rates such 
that they don’t congest the network but at the same time make use of all the avail-
able bandwidth? Are TCP senders explicitly coordinated, or is there a distributed 
approach in which the TCP senders can set their sending rates based only on local 
information? TCP answers these questions using the following guiding principles:
•	 A lost segment implies congestion, and hence, the TCP sender’s rate should be 
decreased when a segment is lost. Recall from our discussion in Section 3.5.4, 
that a timeout event or the receipt of four acknowledgments for a given segment 
(one original ACK and then three duplicate ACKs) is interpreted as an implicit 
“loss event” indication of the segment following the quadruply ACKed segment, 
triggering a retransmission of the lost segment. From a congestion-control stand-
point, the question is how the TCP sender should decrease its congestion window 
size, and hence its sending rate, in response to this inferred loss event.
•	 An acknowledged segment indicates that the network is delivering the sender’s 
segments to the receiver, and hence, the sender’s rate can be increased when an 
ACK arrives for a previously unacknowledged segment. The arrival of acknowl-
edgments is taken as an implicit indication that all is well—segments are being 
successfully delivered from sender to receiver, and the network is thus not con-
gested. The congestion window size can thus be increased.
•	 Bandwidth probing. Given ACKs indicating a congestion-free source-to-destina-
tion path and loss events indicating a congested path, TCP’s strategy for adjusting 
its transmission rate is to increase its rate in response to arriving ACKs until a loss 
event occurs, at which point, the transmission rate is decreased. The TCP sender 
thus increases its transmission rate to probe for the rate that at which congestion 
onset begins, backs off from that rate, and then to begins probing again to see 
if the congestion onset rate has changed. The TCP sender’s behavior is perhaps 
analogous to the child who requests (and gets) more and more goodies until finally 
he/she is finally told “No!”, backs off a bit, but then begins making requests again 
shortly afterwards. Note that there is no explicit signaling of congestion state by 
the network—ACKs and loss events serve as implicit signals—and that each TCP 
sender acts on local information asynchronously from other TCP senders.
Given this overview of TCP congestion control, we’re now in a position to consider the 
details of the celebrated TCP congestion-control algorithm, which was first described 
300         Chapter 3    •    Transport Layer
in [Jacobson 1988] and is standardized in [RFC 5681]. The algorithm has three major 
components: (1) slow start, (2) congestion avoidance, and (3) fast recovery. Slow start 
and congestion avoidance are mandatory components of TCP, differing in how they 
increase the size of cwnd in response to received ACKs. We’ll see shortly that slow 
start increases the size of cwnd more rapidly (despite its name!) than congestion avoid-
ance. Fast recovery is recommended, but not required, for TCP senders.
Slow Start
When a TCP connection begins, the value of cwnd is typically initialized to a small 
value of 1 MSS [RFC 3390], resulting in an initial sending rate of roughly MSS/
RTT. For example, if MSS = 500 bytes and RTT = 200 msec, the resulting initial 
sending rate is only about 20 kbps. Since the available bandwidth to the TCP sender 
may be much larger than MSS/RTT, the TCP sender would like to find the amount of 
available bandwidth quickly. Thus, in the slow-start state, the value of cwnd begins 
at 1 MSS and increases by 1 MSS every time a transmitted segment is first acknowl-
edged. In the example of Figure 3.50, TCP sends the first segment into the network 
Host A
Host B
one segment
two segments
four segments
RTT
Time
Time
Figure 3.50  ♦  TCP slow start
3.7    •    TCP Congestion Control         301
and waits for an acknowledgment. When this acknowledgment arrives, the TCP 
sender increases the congestion window by one MSS and sends out two maximum-
sized segments. These segments are then acknowledged, with the sender increasing 
the congestion window by 1 MSS for each of the acknowledged segments, giving a 
congestion window of 4 MSS, and so on. This process results in a doubling of the 
sending rate every RTT. Thus, the TCP send rate starts slow but grows exponentially 
during the slow start phase.
But when should this exponential growth end? Slow start provides several 
answers to this question. First, if there is a loss event (i.e., congestion) indicated 
by a timeout, the TCP sender sets the value of cwnd to 1 and begins the slow start 
process anew. It also sets the value of a second state variable, ssthresh (shorthand 
for “slow start threshold”) to cwnd/2—half of the value of the congestion window 
value when congestion was detected. The second way in which slow start may end 
is directly tied to the value of ssthresh. Since ssthresh is half the value of 
cwnd when congestion was last detected, it might be a bit reckless to keep doubling 
cwnd when it reaches or surpasses the value of ssthresh. Thus, when the value of 
cwnd equals ssthresh, slow start ends and TCP transitions into congestion avoid-
ance mode. As we’ll see, TCP increases cwnd more cautiously when in congestion-
avoidance mode. The final way in which slow start can end is if three duplicate 
ACKs are detected, in which case TCP performs a fast retransmit (see Section 3.5.4) 
and enters the fast recovery state, as discussed below. TCP’s behavior in slow start 
is summarized in the FSM description of TCP congestion control in Figure 3.51. The 
slow-start algorithm traces it roots to [Jacobson 1988]; an approach similar to slow 
start was also proposed independently in [Jain 1986].
Congestion Avoidance
On entry to the congestion-avoidance state, the value of cwnd is approximately half 
its value when congestion was last encountered—congestion could be just around the 
corner! Thus, rather than doubling the value of cwnd every RTT, TCP adopts a more 
conservative approach and increases the value of cwnd by just a single MSS every 
RTT [RFC 5681]. This can be accomplished in several ways. A common approach 
is for the TCP sender to increase cwnd by MSS bytes (MSS/cwnd) whenever a new 
acknowledgment arrives. For example, if MSS is 1,460 bytes and cwnd is 14,600 
bytes, then 10 segments are being sent within an RTT. Each arriving ACK (assuming 
one ACK per segment) increases the congestion window size by 1/10 MSS, and thus, 
the value of the congestion window will have increased by one MSS after ACKs 
when all 10 segments have been received.
But when should congestion avoidance’s linear increase (of 1 MSS per RTT) 
end? TCP’s congestion-avoidance algorithm behaves the same when a timeout occurs. 
As in the case of slow start: The value of cwnd is set to 1 MSS, and the value of 
ssthresh is updated to half the value of cwnd when the loss event occurred. Recall, 
however, that a loss event also can be triggered by a triple duplicate ACK event. 
 
302         Chapter 3    •    Transport Layer
In this case, the network is continuing to deliver segments from sender to receiver (as 
indicated by the receipt of duplicate ACKs). So TCP’s behavior to this type of loss 
event should be less drastic than with a timeout-indicated loss: TCP halves the value 
of cwnd (adding in 3 MSS for good measure to account for the triple duplicate ACKs 
received) and records the value of ssthresh to be half the value of cwnd when the 
triple duplicate ACKs were received. The fast-recovery state is then entered.
Fast Recovery
In fast recovery, the value of cwnd is increased by 1 MSS for every duplicate 
ACK received for the missing segment that caused TCP to enter the fast-recovery 
state. Eventually, when an ACK arrives for the missing segment, TCP enters the 
 
Slow
start
duplicate ACK
dupACKcount++
duplicate ACK
dupACKcount++
timeout
ssthresh=cwnd/2
cwnd=1 MSS
dupACKcount=0
cwnd=1 MSS
ssthresh=64 KB
dupACKcount=0
timeout
ssthresh=cwnd/2
cwnd=1
dupACKcount=0
timeout
ssthresh=cwnd/2
cwnd=1 MSS
dupACKcount=0
cwnd ssthresh
Congestion
avoidance
Fast
recovery
new ACK
cwnd=cwnd+MSS •(MSS/cwnd)
dupACKcount=0
transmit new segment(s), as allowed
new ACK
cwnd=cwnd+MSS
dupACKcount=0
transmit new segment(s), as allowed
retransmit missing segment
retransmit missing segment
dupACKcount==3
ssthresh=cwnd/2
cwnd=ssthresh+3•MSS
retransmit missing segment
duplicate ACK
cwnd=cwnd+MSS
transmit new segment(s), as allowed
dupACKcount==3
ssthresh=cwnd/2
cwnd=ssthresh+3•MSS
retransmit missing segment
retransmit missing segment
new ACK
cwnd=ssthresh
dupACKcount=0


Figure 3.51  ♦  FSM description of TCP congestion control
VideoNote
Examining the behavior 
of TCP
3.7    •    TCP Congestion Control         303
congestion-avoidance state after deflating cwnd. If a timeout event occurs, fast 
recovery transitions to the slow-start state after performing the same actions as in 
slow start and congestion avoidance: The value of cwnd is set to 1 MSS, and the 
value of ssthresh is set to half the value of cwnd when the loss event occurred.
TCP SPLITTING: OPTIMIZING THE PERFORMANCE OF CLOUD SERVICES
For cloud services such as search, e-mail, and social networks, it is desirable to provide a 
high-level of responsiveness, ideally giving users the illusion that the services are running 
within their own end systems (including their smartphones). This can be a major challenge, 
as users are often located far away from the data centers responsible for serving the 
dynamic content associated with the cloud services. Indeed, if the end system is far from 
a data center, then the RTT will be large, potentially leading to poor response time perfor-
mance due to TCP slow start.
As a case study, consider the delay in receiving a response for a search query. 
Typically, the server requires three TCP windows during slow start to deliver the response 
[Pathak 2010]. Thus the time from when an end system initiates a TCP connection until the 
time when it receives the last packet of the response is roughly 4 # RTT (one RTT to set up 
the TCP connection plus three RTTs for the three windows of data) plus the processing time 
in the data center. These RTT delays can lead to a noticeable delay in returning search 
results for a significant fraction of queries. Moreover, there can be significant packet loss 
in access networks, leading to TCP retransmissions and even larger delays.
One way to mitigate this problem and improve user-perceived performance is to  
(1) deploy front-end servers closer to the users, and (2) utilize TCP splitting by break-
ing the TCP connection at the front-end server. With TCP splitting, the client establishes 
a TCP connection to the nearby front-end, and the front-end maintains a persistent TCP 
connection to the data center with a very large TCP congestion window [Tariq 2008, 
Pathak 2010, Chen 2011]. With this approach, the response time roughly becomes 
4 # RTT
FE + RTT
BE + processing time, where RTT
FE is the round-trip time between client and 
front-end server, and RTT
BE is the round-trip time between the front-end server and the data 
center (back-end server). If the front-end server is close to client, then this response time 
approximately becomes RTT plus processing time, since RTT
FE is negligibly small and RTT
BE 
is approximately RTT. In summary, TCP splitting can reduce the networking delay roughly 
from 4 # RTT to RTT, significantly improving user-perceived performance, particularly for 
users who are far from the nearest data center. TCP splitting also helps reduce TCP  
retransmission delays caused by losses in access networks. Google and Akamai have 
made extensive use of their CDN servers in access networks (recall our discussion in 
Section 2.6) to perform TCP splitting for the cloud services they support [Chen 2011].
PRINCIPLES IN PRACTICE
304         Chapter 3    •    Transport Layer
Fast recovery is a recommended, but not required, component of TCP [RFC 
5681]. It is interesting that an early version of TCP, known as TCP Tahoe, uncon-
ditionally cut its congestion window to 1 MSS and entered the slow-start phase after 
either a timeout-indicated or triple-duplicate-ACK-indicated loss event. The newer 
version of TCP, TCP Reno, incorporated fast recovery.
Figure 3.52 illustrates the evolution of TCP’s congestion window for both Reno 
and Tahoe. In this figure, the threshold is initially equal to 8 MSS. For the first 
eight transmission rounds, Tahoe and Reno take identical actions. The congestion 
window climbs exponentially fast during slow start and hits the threshold at the fourth 
round of transmission. The congestion window then climbs linearly until a triple 
duplicate- ACK event occurs, just after transmission round 8. Note that the congestion 
window is 12 # MSS when this loss event occurs. The value of ssthresh is then set 
to 0.5 # cwnd = 6 # MSS. Under TCP Reno, the congestion window is set to cwnd = 
 
9 # MSS and then grows linearly. Under TCP Tahoe, the congestion window is set to 
1 MSS and grows exponentially until it reaches the value of ssthresh, at which 
point it grows linearly.
Figure 3.51 presents the complete FSM description of TCP’s congestion-control 
algorithms—slow start, congestion avoidance, and fast recovery. The figure also 
indicates where transmission of new segments or retransmitted segments can occur. 
Although it is important to distinguish between TCP error control/retransmission and 
TCP congestion control, it’s also important to appreciate how these two aspects of 
TCP are inextricably linked.
TCP Congestion Control: Retrospective
Having delved into the details of slow start, congestion avoidance, and fast recovery, 
 
it’s worthwhile to now step back and view the forest from the trees. Ignoring the 
0
1
0
2
3
4
5
6
7
8
Transmission round
TCP Tahoe
ssthresh
ssthresh
Congestion window
(in segments)
9
10 11 12 13 14 15
2
4
6
8
10
12
14
16
TCP Reno
Figure 3.52  ♦  Evolution of TCP’s congestion window (Tahoe and Reno)
3.7    •    TCP Congestion Control         305
initial slow-start period when a connection begins and assuming that losses are indi-
cated by triple duplicate ACKs rather than timeouts, TCP’s congestion control con-
sists of linear (additive) increase in cwnd of 1 MSS per RTT and then a halving 
(multiplicative decrease) of cwnd on a triple duplicate-ACK event. For this reason, 
TCP congestion control is often referred to as an additive-increase, multiplicative-
decrease (AIMD) form of congestion control. AIMD congestion control gives rise 
to the “saw tooth” behavior shown in Figure 3.53, which also nicely illustrates our 
earlier intuition of TCP “probing” for bandwidth—TCP linearly increases its con-
gestion window size (and hence its transmission rate) until a triple duplicate-ACK 
event occurs. It then decreases its congestion window size by a factor of two but 
then again begins increasing it linearly, probing to see if there is additional available 
bandwidth.
As noted previously, many TCP implementations use the Reno algorithm 
 
[Padhye 2001]. Many variations of the Reno algorithm have been proposed [RFC 
3782; RFC 2018]. The TCP Vegas algorithm [Brakmo 1995; Ahn 1995] attempts to 
avoid congestion while maintaining good throughput. The basic idea of Vegas is to 
(1) detect congestion in the routers between source and destination before packet loss 
occurs, and (2) lower the rate linearly when this imminent packet loss is detected. 
Imminent packet loss is predicted by observing the RTT. The longer the RTT of the 
packets, the greater the congestion in the routers. As of late 2015, the Ubuntu Linux 
implementation of TCP provided slowstart, congestion avoidance, fast recovery, fast 
retransmit, and SACK, by default; alternative congestion control algorithms, such as 
TCP Vegas and BIC [Xu 2004], are also provided. For a survey of the many flavors 
of TCP, see [Afanasyev 2010].
TCP’s AIMD algorithm was developed based on a tremendous amount of 
engineering insight and experimentation with congestion control in operational 
 
24 K
16 K
8 K
Time
Congestion window
Figure 3.53  ♦  Additive-increase, multiplicative-decrease congestion control
306         Chapter 3    •    Transport Layer
networks. Ten years after TCP’s development, theoretical analyses showed that 
TCP’s congestion-control algorithm serves as a distributed asynchronous-optimization 
algorithm that results in several important aspects of user and network performance 
being simultaneously optimized [Kelly 1998]. A rich theory of congestion control 
has since been developed [Srikant 2004].
Macroscopic Description of TCP Throughput
Given the saw-toothed behavior of TCP, it’s natural to consider what the average 
throughput (that is, the average rate) of a long-lived TCP connection might be. In this 
analysis we’ll ignore the slow-start phases that occur after timeout events. (These 
phases are typically very short, since the sender grows out of the phase exponentially 
fast.) During a particular round-trip interval, the rate at which TCP sends data is a 
function of the congestion window and the current RTT. When the window size is w 
bytes and the current round-trip time is RTT seconds, then TCP’s transmission rate is 
roughly w/RTT. TCP then probes for additional bandwidth by increasing w by 1 MSS 
each RTT until a loss event occurs. Denote by W the value of w when a loss event 
occurs. Assuming that RTT and W are approximately constant over the duration of 
the connection, the TCP transmission rate ranges from W/(2 · RTT) to W/RTT.
These assumptions lead to a highly simplified macroscopic model for the steady-
state behavior of TCP. The network drops a packet from the connection when the rate 
increases to W/RTT; the rate is then cut in half and then increases by MSS/RTT every 
RTT until it again reaches W/RTT. This process repeats itself over and over again. 
Because TCP’s throughput (that is, rate) increases linearly between the two extreme 
values, we have
average throughput of a connection = 0.75 # W
RTT
Using this highly idealized model for the steady-state dynamics of TCP, we 
can also derive an interesting expression that relates a connection’s loss rate to its 
available bandwidth [Mahdavi 1997]. This derivation is outlined in the homework 
problems. A more sophisticated model that has been found empirically to agree with 
measured data is [Padhye 2000].
TCP Over High-Bandwidth Paths
It is important to realize that TCP congestion control has evolved over the years and 
indeed continues to evolve. For a summary of current TCP variants and discussion 
of TCP evolution, see [Floyd 2001, RFC 5681, Afanasyev 2010]. What was good for 
the Internet when the bulk of the TCP connections carried SMTP, FTP, and Telnet 
traffic is not necessarily good for today’s HTTP-dominated Internet or for a future 
Internet with services that are still undreamed of.
3.7    •    TCP Congestion Control         307
The need for continued evolution of TCP can be illustrated by considering the 
high-speed TCP connections that are needed for grid- and cloud-computing appli-
cations. For example, consider a TCP connection with 1,500-byte segments and a 
 
100 ms RTT, and suppose we want to send data through this connection at 10 Gbps. 
Following [RFC 3649], we note that using the TCP throughput formula above, in 
order to achieve a 10 Gbps throughput, the average congestion window size would 
need to be 83,333 segments. That’s a lot of segments, leading us to be rather con-
cerned that one of these 83,333 in-flight segments might be lost. What would happen 
in the case of a loss? Or, put another way, what fraction of the transmitted segments 
could be lost that would allow the TCP congestion-control algorithm specified in 
Figure 3.51 still to achieve the desired 10 Gbps rate? In the homework questions for 
this chapter, you are led through the derivation of a formula relating the throughput 
of a TCP connection as a function of the loss rate (L), the round-trip time (RTT), and 
the maximum segment size (MSS):
average throughput of a connection = 1.22 # MSS
RTT 2L
Using this formula, we can see that in order to achieve a throughput of 10 Gbps, 
today’s TCP congestion-control algorithm can only tolerate a segment loss probabil-
ity of 2 · 10–10 (or equivalently, one loss event for every 5,000,000,000 segments)—a 
very low rate. This observation has led a number of researchers to investigate new 
versions of TCP that are specifically designed for such high-speed environments; see 
[Jin 2004; Kelly 2003; Ha 2008; RFC 7323] for discussions of these efforts.
3.7.1 Fairness
Consider K TCP connections, each with a different end-to-end path, but all pass-
ing through a bottleneck link with transmission rate R bps. (By bottleneck link, we 
mean that for each connection, all the other links along the connection’s path are not 
congested and have abundant transmission capacity as compared with the transmis-
sion capacity of the bottleneck link.) Suppose each connection is transferring a large 
file and there is no UDP traffic passing through the bottleneck link. A congestion-
control mechanism is said to be fair if the average transmission rate of each connec-
tion is approximately R/K; that is, each connection gets an equal share of the link 
 
bandwidth.
Is TCP’s AIMD algorithm fair, particularly given that different TCP connec-
tions may start at different times and thus may have different window sizes at a given 
point in time? [Chiu 1989] provides an elegant and intuitive explanation of why TCP 
congestion control converges to provide an equal share of a bottleneck link’s band-
width among competing TCP connections.
Let’s consider the simple case of two TCP connections sharing a single link 
with transmission rate R, as shown in Figure 3.54. Assume that the two connections 
308         Chapter 3    •    Transport Layer
have the same MSS and RTT (so that if they have the same congestion window size, 
then they have the same throughput), that they have a large amount of data to send, 
and that no other TCP connections or UDP datagrams traverse this shared link. Also, 
ignore the slow-start phase of TCP and assume the TCP connections are operating in 
CA mode (AIMD) at all times.
Figure 3.55 plots the throughput realized by the two TCP connections. If TCP is 
to share the link bandwidth equally between the two connections, then the realized 
throughput should fall along the 45-degree arrow (equal bandwidth share) emanating 
from the origin. Ideally, the sum of the two throughputs should equal R. (Certainly, 
each connection receiving an equal, but zero, share of the link capacity is not a desir-
able situation!) So the goal should be to have the achieved throughputs fall some-
where near the intersection of the equal bandwidth share line and the full bandwidth 
utilization line in Figure 3.55.
Suppose that the TCP window sizes are such that at a given point in time, con-
nections 1 and 2 realize throughputs indicated by point A in Figure 3.55. Because the 
amount of link bandwidth jointly consumed by the two connections is less than R, no 
loss will occur, and both connections will increase their window by 1 MSS per RTT 
as a result of TCP’s congestion-avoidance algorithm. Thus, the joint throughput of 
the two connections proceeds along a 45-degree line (equal increase for both connec-
tions) starting from point A. Eventually, the link bandwidth jointly consumed by the 
two connections will be greater than R, and eventually packet loss will occur. Sup-
pose that connections 1 and 2 experience packet loss when they realize throughputs 
indicated by point B. Connections 1 and 2 then decrease their windows by a factor of 
two. The resulting throughputs realized are thus at point C, halfway along a vector 
starting at B and ending at the origin. Because the joint bandwidth use is less than R 
at point C, the two connections again increase their throughputs along a 45-degree 
line starting from C. Eventually, loss will again occur, for example, at point D, and 
the two connections again decrease their window sizes by a factor of two, and so on. 
You should convince yourself that the bandwidth realized by the two connections 
eventually fluctuates along the equal bandwidth share line. You should also convince 
TCP connection 2
TCP connection 1
Bottleneck
router capacity R
Figure 3.54  ♦  Two TCP connections sharing a single bottleneck link
3.7    •    TCP Congestion Control         309
yourself that the two connections will converge to this behavior regardless of where 
they are in the two-dimensional space! Although a number of idealized assumptions 
lie behind this scenario, it still provides an intuitive feel for why TCP results in an 
equal sharing of bandwidth among connections.
In our idealized scenario, we assumed that only TCP connections traverse the 
bottleneck link, that the connections have the same RTT value, and that only a single 
TCP connection is associated with a host-destination pair. In practice, these condi-
tions are typically not met, and client-server applications can thus obtain very une-
qual portions of link bandwidth. In particular, it has been shown that when multiple 
connections share a common bottleneck, those sessions with a smaller RTT are able 
to grab the available bandwidth at that link more quickly as it becomes free (that is, 
open their congestion windows faster) and thus will enjoy higher throughput than 
those connections with larger RTTs [Lakshman 1997].
Fairness and UDP
We have just seen how TCP congestion control regulates an application’s trans-
mission rate via the congestion window mechanism. Many multimedia applications, 
such as Internet phone and video conferencing, often do not run over TCP for this 
very reason—they do not want their transmission rate throttled, even if the network 
is very congested. Instead, these applications prefer to run over UDP, which does not 
R
R
Equal
bandwidth
share
Connection 1 throughput
Connection 2 throughput
D
B
C
A
Full bandwidth
utilization line
Figure 3.55  ♦  Throughput realized by TCP connections 1 and 2
310         Chapter 3    •    Transport Layer
have built-in congestion control. When running over UDP, applications can pump 
their audio and video into the network at a constant rate and occasionally lose pack-
ets, rather than reduce their rates to “fair” levels at times of congestion and not lose 
any packets. From the perspective of TCP, the multimedia applications running over 
UDP are not being fair—they do not cooperate with the other connections nor adjust 
their transmission rates appropriately. Because TCP congestion control will decrease 
its transmission rate in the face of increasing congestion (loss), while UDP sources 
need not, it is possible for UDP sources to crowd out TCP traffic. An area of research 
today is thus the development of congestion-control mechanisms for the Internet that 
prevent UDP traffic from bringing the Internet’s throughput to a grinding halt [Floyd 
1999; Floyd 2000; Kohler 2006; RFC 4340].
Fairness and Parallel TCP Connections
But even if we could force UDP traffic to behave fairly, the fairness problem would 
still not be completely solved. This is because there is nothing to stop a TCP-based 
application from using multiple parallel connections. For example, Web browsers 
often use multiple parallel TCP connections to transfer the multiple objects within 
a Web page. (The exact number of multiple connections is configurable in most 
browsers.) When an application uses multiple parallel connections, it gets a larger 
fraction of the bandwidth in a congested link. As an example, consider a link of rate 
R supporting nine ongoing client-server applications, with each of the applications 
using one TCP connection. If a new application comes along and also uses one TCP 
connection, then each application gets approximately the same transmission rate of 
R/10. But if this new application instead uses 11 parallel TCP connections, then the 
new application gets an unfair allocation of more than R/2. Because Web traffic is so 
pervasive in the Internet, multiple parallel connections are not uncommon.
3.7.2 
Explicit Congestion Notification (ECN):  
Network-assisted Congestion Control
Since the initial standardization of slow start and congestion avoidance in the late 
1980’s [RFC 1122], TCP has implemented the form of end-end congestion control 
that we studied in Section 3.7.1: a TCP sender receives no explicit congestion indica-
tions from the network layer, and instead infers congestion through observed packet 
loss. More recently, extensions to both IP and TCP [RFC 3168] have been proposed, 
implemented, and deployed that allow the network to explicitly signal congestion 
to a TCP sender and receiver. This form of network-assisted congestion control is 
known as Explicit Congestion Notification. As shown in Figure 5.56, the TCP and 
IP protocols are involved.
At the network layer, two bits (with four possible values, overall) in the Type 
of Service field of the IP datagram header (which we’ll discuss in Section 4.3) are 
used for ECN. One setting of the ECN bits is used by a router to indicate that it (the 
3.7    •    TCP Congestion Control         311
router) is experiencing congestion. This congestion indication is then carried in the 
marked IP datagram to the destination host, which then informs the sending host, 
as shown in Figure 3.56. RFC 3168 does not provide a definition of when a router 
is congested; that decision is a configuration choice made possible by the router 
vendor, and decided by the network operator. However, RFC 3168 does recommend 
that an ECN congestion indication be set only in the face of persistent congestion. A 
second setting of the ECN bits is used by the sending host to inform routers that the 
sender and receiver are ECN-capable, and thus capable of taking action in response 
to ECN-indicated network congestion.
As shown in Figure 3.56, when the TCP in the receiving host receives an ECN 
congestion indication via a received datagram, the TCP in the receiving host informs 
the TCP in the sending host of the congestion indication by setting the ECE (Explicit 
Congestion Notification Echo) bit (see Figure 3.29) in a receiver-to-sender TCP 
ACK segment. The TCP sender, in turn, reacts to an ACK with an ECE congestion 
indication by halving the congestion window, as it would react to a lost segment 
using fast retransmit, and sets the CWR (Congestion Window Reduced) bit in the 
header of the next transmitted TCP sender-to-receiver segment.
Other transport-layer protocols besides TCP may also make use of network-
layer-signaled ECN. The Datagram Congestion Control Protocol (DCCP) [RFC 
4340] provides a low-overhead, congestion-controlled UDP-like unreliable service 
that utilizes ECN. DCTCP (Data Center TCP) [Alizadeh 2010], a version of TCP 
designed specifically for data center networks, also makes use of ECN.
ECN Echo = 1
Host A
Host B
ECN = 11
ECN Echo bit set in 
receiver-to-sender
TCP ACK segment
ECN bits set in IP
datagram header
at congested router
Figure 3.56  ♦  
Explicit Congestion Notification: network-assisted  
congestion control
312         Chapter 3    •    Transport Layer
3.8	 Summary
We began this chapter by studying the services that a transport-layer protocol can 
provide to network applications. At one extreme, the transport-layer protocol can be 
very simple and offer a no-frills service to applications, providing only a multiplexing/
demultiplexing function for communicating processes. The Internet’s UDP protocol is 
an example of such a no-frills transport-layer protocol. At the other extreme, a transport- 
layer protocol can provide a variety of guarantees to applications, such as reliable deliv-
ery of data, delay guarantees, and bandwidth guarantees. Nevertheless, the services 
that a transport protocol can provide are often constrained by the service model of the 
underlying network-layer protocol. If the network-layer protocol cannot provide delay 
or bandwidth guarantees to transport-layer segments, then the transport-layer protocol 
cannot provide delay or bandwidth guarantees for the messages sent between processes.
We learned in Section 3.4 that a transport-layer protocol can provide reliable 
data transfer even if the underlying network layer is unreliable. We saw that provid-
ing reliable data transfer has many subtle points, but that the task can be accom-
plished by carefully combining acknowledgments, timers, retransmissions, and 
sequence numbers.
Although we covered reliable data transfer in this chapter, we should keep in 
mind that reliable data transfer can be provided by link-, network-, transport-, or 
application-layer protocols. Any of the upper four layers of the protocol stack can 
implement acknowledgments, timers, retransmissions, and sequence numbers and 
provide reliable data transfer to the layer above. In fact, over the years, engineers 
and computer scientists have independently designed and implemented link-, net-
work-, transport-, and application-layer protocols that provide reliable data transfer 
(although many of these protocols have quietly disappeared).
In Section 3.5, we took a close look at TCP, the Internet’s connection-oriented and 
reliable transport-layer protocol. We learned that TCP is complex, involving connec-
tion management, flow control, and round-trip time estimation, as well as reliable data 
transfer. In fact, TCP is actually more complex than our description—we intentionally 
did not discuss a variety of TCP patches, fixes, and improvements that are widely 
implemented in various versions of TCP. All of this complexity, however, is hidden 
from the network application. If a client on one host wants to send data reliably to a 
server on another host, it simply opens a TCP socket to the server and pumps data into 
that socket. The client-server application is blissfully unaware of TCP’s complexity.
In Section 3.6, we examined congestion control from a broad perspective, and 
in Section 3.7, we showed how TCP implements congestion control. We learned that 
congestion control is imperative for the well-being of the network. Without conges-
tion control, a network can easily become gridlocked, with little or no data being 
transported end-to-end. In Section 3.7 we learned that TCP implements an end-to-
end congestion-control mechanism that additively increases its transmission rate 
when the TCP connection’s path is judged to be congestion-free, and multiplicatively 
3.8    •    Summary         313
decreases its transmission rate when loss occurs. This mechanism also strives to give 
each TCP connection passing through a congested link an equal share of the link 
bandwidth. We also examined in some depth the impact of TCP connection estab-
lishment and slow start on latency. We observed that in many important scenarios, 
connection establishment and slow start significantly contribute to end-to-end delay. 
We emphasize once more that while TCP congestion control has evolved over the 
years, it remains an area of intensive research and will likely continue to evolve in 
the upcoming years.
Our discussion of specific Internet transport protocols in this chapter has focused 
on UDP and TCP—the two “work horses” of the Internet transport layer. However, 
two decades of experience with these two protocols has identified circumstances in 
which neither is ideally suited. Researchers have thus been busy developing addi-
tional transport-layer protocols, several of which are now IETF proposed standards.
The Datagram Congestion Control Protocol (DCCP) [RFC 4340] provides a low-
overhead, message-oriented, UDP-like unreliable service, but with an application- 
selected form of congestion control that is compatible with TCP. If reliable or 
 
semi-reliable data transfer is needed by an application, then this would be performed 
within the application itself, perhaps using the mechanisms we have studied in 
 
Section 3.4. DCCP is envisioned for use in applications such as streaming media 
(see Chapter 9) that can exploit the tradeoff between timeliness and reliability of data 
delivery, but that want to be responsive to network congestion.
Google’s QUIC (Quick UDP Internet Connections) protocol [Iyengar 2016], 
implemented in Google’s Chromium browser, provides reliability via retransmission 
as well as error correction, fast-connection setup, and a rate-based congestion control 
algorithm that aims to be TCP friendly—all implemented as an application-level pro-
tocol on top of UDP. In early 2015, Google reported that roughly half of all requests 
from Chrome to Google servers are served over QUIC.
DCTCP (Data Center TCP) [Alizadeh 2010] is a version of TCP designed spe-
cifically for data center networks, and uses ECN to better support the mix of short- 
and long-lived flows that characterize data center workloads.
The Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is 
a reliable, message-oriented protocol that allows several different application-level 
“streams” to be multiplexed through a single SCTP connection (an approach known 
as “multi-streaming”). From a reliability standpoint, the different streams within the 
connection are handled separately, so that packet loss in one stream does not affect 
the delivery of data in other streams. QUIC provides similar multi-stream semantics. 
SCTP also allows data to be transferred over two outgoing paths when a host is con-
nected to two or more networks, optional delivery of out-of-order data, and a number 
of other features. SCTP’s flow- and congestion-control algorithms are essentially the 
same as in TCP.
The TCP-Friendly Rate Control (TFRC) protocol [RFC 5348] is a congestion-
control protocol rather than a full-fledged transport-layer protocol. It specifies a 
314         Chapter 3    •    Transport Layer
congestion-control mechanism that could be used in another transport protocol such 
as DCCP (indeed one of the two application-selectable protocols available in DCCP 
is TFRC). The goal of TFRC is to smooth out the “saw tooth” behavior (see Fig­
ure 3.53) in TCP congestion control, while maintaining a long-term sending rate that 
is “reasonably” close to that of TCP. With a smoother sending rate than TCP, TFRC 
is well-suited for multimedia applications such as IP telephony or streaming media 
where such a smooth rate is important. TFRC is an “equation-based” protocol that 
uses the measured packet loss rate as input to an equation [Padhye 2000] that esti-
mates what TCP’s throughput would be if a TCP session experiences that loss rate. 
This rate is then taken as TFRC’s target sending rate.
Only the future will tell whether DCCP, SCTP, QUIC, or TFRC will see wide-
spread deployment. While these protocols clearly provide enhanced capabilities 
over TCP and UDP, TCP and UDP have proven themselves “good enough” over the 
years. Whether “better” wins out over “good enough” will depend on a complex mix 
of technical, social, and business considerations.
In Chapter 1, we said that a computer network can be partitioned into the “net-
work edge” and the “network core.” The network edge covers everything that hap-
pens in the end systems. Having now covered the application layer and the transport 
layer, our discussion of the network edge is complete. It is time to explore the net-
work core! This journey begins in the next two chapters, where we’ll study the net-
work layer, and continues into Chapter 6, where we’ll study the link layer.
Homework Problems and Questions
Chapter 3 Review Questions
SECTIONS 3.1–3.3 
	R1.	 Suppose the network layer provides the following service. The network 
layer in the source host accepts a segment of maximum size 1,200 bytes and 
a destination host address from the transport layer. The network layer then 
guarantees to deliver the segment to the transport layer at the destination 
host. Suppose many network application processes can be running at the 
destination host.
a.	 Design the simplest possible transport-layer protocol that will get applica-
tion data to the desired process at the destination host. Assume the operat-
ing system in the destination host has assigned a 4-byte port number to 
each running application process.
b.	 Modify this protocol so that it provides a “return address” to the destina-
tion process.
c.	 In your protocols, does the transport layer “have to do anything” in the 
core of the computer network?
Homework Problems and Questions         315
	R2.	 Consider a planet where everyone belongs to a family of six, every family 
lives in its own house, each house has a unique address, and each person 
in a given house has a unique name. Suppose this planet has a mail service 
that delivers letters from source house to destination house. The mail service 
requires that (1) the letter be in an envelope, and that (2) the address of the 
destination house (and nothing more) be clearly written on the envelope. Sup-
pose each family has a delegate family member who collects and distributes 
letters for the other family members. The letters do not necessarily provide 
any indication of the recipients of the letters.
a.	 Using the solution to Problem R1 above as inspiration, describe a protocol 
that the delegates can use to deliver letters from a sending family member 
to a receiving family member.
b.	 In your protocol, does the mail service ever have to open the envelope and 
examine the letter in order to provide its service?
	R3.	 How is a UDP socket fully identified? What about a TCP socket? What is the 
difference between the full identification of both sockets?
	R4.	 Describe why an application developer might choose to run an application 
over UDP rather than TCP.
	R5.	 Why is it that voice and video traffic is often sent over TCP rather than UDP 
in today’s Internet? (Hint: The answer we are looking for has nothing to do 
with TCP’s congestion-control mechanism.)
	R6.	 Is it possible for an application to enjoy reliable data transfer even when the 
application runs over UDP? If so, how?
	R7.	 Suppose a process in Host C has a UDP socket with port number 6789. 
Suppose both Host A and Host B each send a UDP segment to Host C with 
destination port number 6789. Will both of these segments be directed to the 
same socket at Host C? If so, how will the process at Host C know that these 
two segments originated from two different hosts?
	R8.	 Suppose that a Web server runs in Host C on port 80. Suppose this Web 
server uses persistent connections, and is currently receiving requests from 
two different Hosts, A and B. Are all of the requests being sent through the 
same socket at Host C? If they are being passed through different sockets, do 
both of the sockets have port 80? Discuss and explain.
SECTION 3.4
	R9.	 In our rdt protocols, why did we need to introduce sequence numbers?
	
R10.	 In our rdt protocols, why did we need to introduce timers?
316         Chapter 3    •    Transport Layer
	
R11.	 Suppose that the roundtrip delay between sender and receiver is constant and 
known to the sender. Would a timer still be necessary in protocol rdt 3.0, 
assuming that packets can be lost? Explain.
	
R12.	 Visit the Go-Back-N Java applet at the companion Web site.
a.	 Have the source send five packets, and then pause the animation before 
any of the five packets reach the destination. Then kill the first packet and 
resume the animation. Describe what happens.
b.	 Repeat the experiment, but now let the first packet reach the destination 
and kill the first acknowledgment. Describe again what happens.
c.	 Finally, try sending six packets. What happens?
	
R13.	 Repeat R12, but now with the Selective Repeat Java applet. How are Selec-
tive Repeat and Go-Back-N different?
SECTION 3.5
	
R14.	 True or false?
a.	 Host A is sending Host B a large file over a TCP connection. Assume Host 
B has no data to send Host A. Host B will not send acknowledgments to 
Host A because Host B cannot piggyback the acknowledgments on data.
b.	 The size of the TCP rwnd never changes throughout the duration of the 
connection.
c.	 Suppose Host A is sending Host B a large file over a TCP connection. The 
number of unacknowledged bytes that A sends cannot exceed the size of 
the receive buffer.
d.	 Suppose Host A is sending a large file to Host B over a TCP connection. 
If the sequence number for a segment of this connection is m, then the 
sequence number for the subsequent segment will necessarily be m + 1.
e.	 The TCP segment has a field in its header for rwnd.
f.	 Suppose that the last SampleRTT in a TCP connection is equal to 1 sec. 
The current value of TimeoutInterval for the connection will neces-
sarily be Ú 1 sec.
g.	 Suppose Host A sends one segment with sequence number 38 and 4 
bytes of data over a TCP connection to Host B. In this same segment the 
acknowledgment number is necessarily 42.
	
R15.	 Suppose Host A sends two TCP segments back to back to Host B over a 
TCP connection. The first segment has sequence number 90; the second has 
sequence number 110.
a.	 How much data is in the first segment?
b.	 Suppose that the first segment is lost but the second segment arrives at 
B. In the acknowledgment that Host B sends to Host A, what will be the 
acknowledgment number?
Problems         317
	
R16.	 Consider the Telnet example discussed in Section 3.5. A few seconds after 
the user types the letter ‘C,’ the user types the letter ‘R.’ After typing the let-
ter ‘R,’ how many segments are sent, and what is put in the sequence number 
and acknowledgment fields of the segments?
SECTION 3.7
	
R17.	 Consider two hosts, Host A and Host B, transmitting a large file to Server C over 
a bottleneck link with a rate of R kbps. To transfer the file, the hosts use TCP 
with the same parameters (including MSS and RTT) and start their transmissions 
at the same time. Host A uses a single TCP connection for the entire file, while 
Host B uses 9 simultaneous TCP connections, each for a portion (i.e., a chunk) of 
the file. What is the overall transmission rate achieved by each host at the begin-
ning of the file transfer? (Hint: the overall transmission rate of a host is the sum 
of the transmission rates of its TCP connections.) Is this situation fair?
	
R18.	 True or false? Consider congestion control in TCP. When the timer expires at 
the sender, the value of ssthresh is set to one half of its previous value.
	
R19.	 According to the discussion of TCP splitting in the sidebar in Section 3.7, 
the response time with TCP splitting is approximately 4 3 RTTFE 1 RTTBE 1 
processing time, as opposed to 4 3 RTT 1 processing time when a direct 
connection is used. Assume that RTT BE is 0.5 3 RTT. For what values of 
RTTFE does TCP splitting have a shorter delay than a direct connection?
Problems
	 P1.	 Suppose Client A requests a web page from Server S through HTTP and its 
socket is associated with port 33000.
a.	 What are the source and destination ports for the segments sent from A to S?
b.	 What are the source and destination ports for the segments sent from S to A?
c.	 Can Client A contact to Server S using UDP as the transport protocol?
d.	 Can Client A request multiple resources in a single TCP connection?
	 P2.	 Consider Figure 3.5. What are the source and destination port values in the seg-
ments flowing from the server back to the clients’ processes? What are the IP 
addresses in the network-layer datagrams carrying the transport-layer segments?
	 P3.	 UDP and TCP use 1s complement for their checksums. Suppose you have 
the following three 8-bit bytes: 01010011, 01100110, 01110100. What is the 
1s complement of the sum of these 8-bit bytes? (Note that although UDP and 
TCP use 16-bit words in computing the checksum, for this problem you are 
being asked to consider 8-bit sums.) Show all work. Why is it that UDP takes 
the 1s complement of the sum; that is, why not just use the sum? With the 1s 
complement scheme, how does the receiver detect errors? Is it possible that a 
1-bit error will go undetected? How about a 2-bit error?
318         Chapter 3    •    Transport Layer
	 P4.	 Assume that a host receives a UDP segment with 01011101 11110010 (we 
separated the values of each byte with a space for clarity) as the checksum. 
The host adds the 16-bit words over all necessary fields excluding the check-
sum and obtains the value 00110010 00001101. Is the segment considered 
correctly received or not? What does the receiver do?
	 P5.	 Suppose that the UDP receiver computes the Internet checksum for the 
received UDP segment and finds that it matches the value carried in the 
checksum field. Can the receiver be absolutely certain that no bit errors have 
occurred? Explain.
	 P6.	 Consider our motivation for correcting protocol rdt2.1. Show that the 
receiver, shown in Figure 3.57, when operating with the sender shown in 
Figure 3.11, can lead the sender and receiver to enter into a deadlock state, 
where each is waiting for an event that will never occur.
	 P7.	 In protocol rdt3.0, the ACK packets flowing from the receiver to the 
sender do not have sequence numbers (although they do have an ACK field 
that contains the sequence number of the packet they are acknowledging). 
Why is it that our ACK packets do not require sequence numbers?
Wait for
0 from
below
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq0(rcvpkt)))
compute chksum
make_pkt(sndpkt,NAK,chksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq1(rcvpkt)))
compute chksum
make_pkt(sndpkt,NAK,chksum)
udt_send(sndpkt)
rdt_rvc(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq1(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
compute chksum
make_pkt(sendpkt,ACK,chksum)
udt_send(sndpkt)
rdt_rvc(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq0(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
compute chksum
make_pkt(sendpkt,ACK,chksum)
udt_send(sndpkt)
Wait for
1 from
below
Figure 3.57  ♦  An incorrect receiver for protocol rdt 2.1
Problems         319
	 P8.	 Draw the FSM for the receiver side of protocol rdt3.0.
	 P9.	 Give a trace of the operation of protocol rdt3.0 when data packets and 
acknowledgment packets are garbled. Your trace should be similar to that 
used in Figure 3.16.
	
P10.	 Consider a channel that can lose packets but has a maximum delay that is 
known. Modify protocol rdt2.1 to include sender timeout and retransmit. 
Informally argue why your protocol can communicate correctly over this 
channel.
	
P11.	 Consider the rdt2.2 receiver in Figure 3.14, and the creation of a new 
packet in the self-transition (i.e., the transition from the state back to 
itself) in the Wait-for-0-from-below and the Wait-for-1-from-below states: 
sndpkt=make_pkt(ACK,1,checksum) and sndpkt=make_
pkt(ACK,0,checksum). Would the protocol work correctly if this action 
were removed from the self-transition in the Wait-for-1-from-below state? 
Justify your answer. What if this event were removed from the self-transition 
in the Wait-for-0-from-below state? [Hint: In this latter case, consider what 
would happen if the first sender-to-receiver packet were corrupted.]
	
P12.	 The sender side of rdt3.0 simply ignores (that is, takes no action on)  
all received packets that are either in error or have the wrong value in the 
acknum field of an acknowledgment packet. Suppose that in such circum-
stances, rdt3.0 were simply to retransmit the current data packet. Would 
the protocol still work? (Hint: Consider what would happen if there were 
only bit errors; there are no packet losses but premature timeouts can occur. 
Consider how many times the nth packet is sent, in the limit as n approaches 
infinity.)
	
P13.	 Assume Host A is streaming a video from Server B using UDP. Also 
assume that the network suddenly becomes very congested while Host A is 
seeing the video. Is there any way to handle this situation with UDP? What 
about with TCP? Is there any other option?
	
P14.	 Consider a stop-and-wait data-transfer protocol that provides error checking 
and retransmissions but uses only negative acknowledgments. Assume that 
negative acknowledgments are never corrupted. Would such a protocol work 
over a channel with bit errors? What about over a lossy channel with bit 
errors?
320         Chapter 3    •    Transport Layer
	
P15.	 Consider the cross-country example shown in Figure 3.17. How big would 
the window size have to be for the channel utilization to be greater than 98 
percent? Suppose that the size of a packet is 1,500 bytes, including both 
header fields and data.
	
P16.	 Suppose an application uses rdt 3.0 as its transport layer protocol. As the 
stop-and-wait protocol has very low channel utilization (shown in the cross-
country example), the designers of this application let the receiver keep send-
ing back a number (more than two) of alternating ACK 0 and ACK 1 even if 
the corresponding data have not arrived at the receiver. Would this applica-
tion design increase the channel utilization? Why? Are there any potential 
problems with this approach? Explain.
	
P17.	 Consider two network entities, A and B, which are connected by a perfect 
bi-directional channel (i.e., any message sent will be received correctly; the 
channel will not corrupt, lose, or re-order packets). A and B are to deliver 
data messages to each other in an alternating manner: First, A must deliver 
a message to B, then B must deliver a message to A, then A must deliver a 
message to B and so on. If an entity is in a state where it should not attempt 
to deliver a message to the other side, and there is an event like rdt_
send(data) call from above that attempts to pass data down for transmis-
sion to the other side, this call from above can simply be ignored with a call 
to rdt_unable_to_send(data), which informs the higher layer that it 
is currently not able to send data. [Note: This simplifying assumption is made 
so you don’t have to worry about buffering data.]
	
	 Draw a FSM specification for this protocol (one FSM for A, and one FSM 
for B!). Note that you do not have to worry about a reliability mechanism 
here; the main point of this question is to create a FSM specification that 
reflects the synchronized behavior of the two entities. You should use the 
following events and actions that have the same meaning as protocol rdt1.0 in 
Figure 3.9: rdt_send(data), packet = make_pkt(data), udt_
send(packet), rdt_rcv(packet), extract (packet,data), 
deliver_data(data). Make sure your protocol reflects the strict alter-
nation of sending between A and B. Also, make sure to indicate the initial 
states for A and B in your FSM descriptions.
	
P18.	 In the generic SR protocol that we studied in Section 3.4.4, the sender 
transmits a message as soon as it is available (if it is in the window) without 
waiting for an acknowledgment. Suppose now that we want an SR protocol 
that sends messages two at a time. That is, the sender will send a pair of mes-
sages and will send the next pair of messages only when it knows that both 
messages in the first pair have been received correctly.
	
	 Suppose that the channel may lose messages but will not corrupt or reorder 
messages. Design an error-control protocol for the unidirectional reliable 
Problems         321
transfer of messages. Give an FSM description of the sender and receiver. 
Describe the format of the packets sent between sender and receiver, and vice 
versa. If you use any procedure calls other than those in Section 3.4  
(for example, udt_send(), start_timer(), rdt_rcv(), and so on), 
clearly state their actions. Give an example (a timeline trace of sender and 
receiver) showing how your protocol recovers from a lost packet.
	
P19.	 Suppose Host A and Host B use a GBN protocol with window size N 5 3 
and a long-enough range of sequence numbers. Assume Host A sends six 
application messages to Host B and that all messages are correctly received, 
except for the first acknowledgment and the fifth data segment. Draw a 
timing diagram (similar to Figure 3.22), showing the data segments and the 
acknowledgments sent along with the corresponding sequence and acknowl-
edge numbers, respectively.
	
P20.	 Consider a scenario in which Host A and Host B want to send messages to 
Host C. Hosts A and C are connected by a channel that can lose and corrupt 
(but not reorder) messages. Hosts B and C are connected by another channel 
(independent of the channel connecting A and C) with the same properties. 
The transport layer at Host C should alternate in delivering messages from  
A and B to the layer above (that is, it should first deliver the data from a packet 
from A, then the data from a packet from B, and so on). Design a stop-and-
wait-like error-control protocol for reliably transferring packets from A and 
B to C, with alternating delivery at C as described above. Give FSM descrip-
tions of A and C. (Hint: The FSM for B should be essentially the same as  
for A.) Also, give a description of the packet format(s) used.
	
P21.	 Suppose we have two network entities, A and B. B has a supply of data mes-
sages that will be sent to A according to the following conventions. When A 
gets a request from the layer above to get the next data (D) message from B, 
A must send a request (R) message to B on the A-to-B channel. Only when B 
receives an R message can it send a data (D) message back to A on the B-to-
A channel. A should deliver exactly one copy of each D message to the layer 
above. R messages can be lost (but not corrupted) in the A-to-B channel; D 
messages, once sent, are always delivered correctly. The delay along both 
channels is unknown and variable.
	
	 Design (give an FSM description of) a protocol that incorporates the appro-
priate mechanisms to compensate for the loss-prone A-to-B channel and 
implements message passing to the layer above at entity A, as discussed 
above. Use only those mechanisms that are absolutely necessary.
322         Chapter 3    •    Transport Layer
	
P22.	 Consider the GBN protocol with a sender window size of 4 and a sequence 
number range of 1,024. Suppose that at time t, the next in-order packet 
that the receiver is expecting has a sequence number of k. Assume that the 
medium does not reorder messages. Answer the following questions:
a.	 What are the possible sets of sequence numbers inside the sender’s  
window at time t? Justify your answer.
b.	 What are all possible values of the ACK field in all possible messages 
currently propagating back to the sender at time t? Justify your answer.
	
P23.	 Give one example where buffering out-of-order segments would significantly 
improve the throughput of a GBN protocol.
	
P24.	 Consider a scenario where Host A, Host B, and Host C are connected as 
a ring (i.e., Host A to Host B, Host B to Host C, and Host C to Host A). 
Assume that Host A and Host C run protocol rdt3.0, while Host B simply 
relays all messages received from Host A to Host C. Does this arrangement 
enable reliable delivery of messages from Host A to Host C? Can Host B tell 
if a certain message has been correctly received by Host A?
	
P25.	 Consider the Telnet case study in Section 3.5.2. Assume a Telnet session is 
already active between Host A and Server S. The user at Host A then types 
the word “Hello.”
a.	 How many TCP segments will be created at the transport layer of Host A?
b.	 Is there any guarantee that each segment will be sent into the TCP connec-
tion as soon as it is created?
c.	 Does TCP provide any mechanism that can be useful for an interactive 
Telnet session?
d.	 Would UDP offer a viable alternative to TCP for Telnet sessions over a 
reliable channel?
	
P26.	 Consider transferring an enormous file of L bytes from Host A to Host B. 
Assume an MSS of 536 bytes.
a.	 What is the maximum value of L such that TCP sequence numbers are not 
exhausted? Recall that the TCP sequence number field has 4 bytes.
b.	 For the L you obtain in (a), find how long it takes to transmit the file. 
Assume that a total of 66 bytes of transport, network, and data-link header 
are added to each segment before the resulting packet is sent out over a 
155 Mbps link. Ignore flow control and congestion control so A can pump 
out the segments back to back and continuously.
	
P27.	 Host A and B are communicating over a TCP connection, and Host B has 
already received from A all bytes up through byte 126. Suppose Host A  
then sends two segments to Host B back-to-back. The first and second  
Problems         323
segments contain 80 and 40 bytes of data, respectively. In the first segment, 
the sequence number is 127, the source port number is 302, and the des-
tination port number is 80. Host B sends an acknowledgment whenever it 
receives a segment from Host A.
a.	 In the second segment sent from Host A to B, what are the sequence num-
ber, source port number, and destination port number?
b.	 If the first segment arrives before the second segment, in the acknowledg-
ment of the first arriving segment, what is the acknowledgment number, 
the source port number, and the destination port number?
c.	 If the second segment arrives before the first segment, in the acknowledg-
ment of the first arriving segment, what is the acknowledgment number?
d.	 Suppose the two segments sent by A arrive in order at B. The first 
acknowledgment is lost and the second acknowledgment arrives after the 
first timeout interval. Draw a timing diagram, showing these segments 
and all other segments and acknowledgments sent. (Assume there is no 
additional packet loss.) For each segment in your figure, provide the 
sequence number and the number of bytes of data; for each acknowledg-
ment that you add, provide the acknowledgment number.
	
P28.	 Host A and B are directly connected with a 100 Mbps link. There is one TCP 
connection between the two hosts, and Host A is sending to Host B an enor-
mous file over this connection. Host A can send its application data into its 
TCP socket at a rate as high as 120 Mbps but Host B can read out of its TCP 
receive buffer at a maximum rate of 50 Mbps. Describe the effect of TCP 
flow control.
	
P29.	 SYN cookies were discussed in Section 3.5.6.
a.	 Why is it necessary for the server to use a special initial sequence number 
in the SYNACK?
b.	 Suppose an attacker knows that a target host uses SYN cookies. Can the 
attacker create half-open or fully open connections by simply sending an 
ACK packet to the target? Why or why not?
c.	 Suppose an attacker collects a large amount of initial sequence numbers sent 
by the server. Can the attacker cause the server to create many fully open 
connections by sending ACKs with those initial sequence numbers? Why?
	
P30.	 Consider the network shown in Scenario 2 in Section 3.6.1. Suppose both 
sending hosts A and B have some fixed timeout values.
a.	 Argue that increasing the size of the finite buffer of the router might pos-
sibly decrease the throughput (lout).
b.	 Now suppose both hosts dynamically adjust their timeout values (like 
what TCP does) based on the buffering delay at the router. Would increas-
ing the buffer size help to increase the throughput? Why?
324         Chapter 3    •    Transport Layer
	
P31.	 Suppose that the five measured SampleRTT values (see Section 3.5.3) 
are 106 ms, 120 ms, 140 ms, 90 ms, and 115 ms. Compute the Estimat-
edRTT after each of these SampleRTT values is obtained, using a value of 
α = 0.125 and assuming that the value of EstimatedRTT was 100 ms 
just before the first of these five samples were obtained. Compute also the 
DevRTT after each sample is obtained, assuming a value of β = 0.25 and 
assuming the value of DevRTT was 5 ms just before the first of these five 
samples was obtained. Last, compute the TCP TimeoutInterval after 
each of these samples is obtained.
	
P32.	 Consider the TCP procedure for estimating RTT. Suppose that α = 0.1. Let 
SampleRTT1 be the most recent sample RTT, let SampleRTT2 be the next 
most recent sample RTT, and so on.
a.	 For a given TCP connection, suppose four acknowledgments have  
been returned with corresponding sample RTTs: SampleRTT4,  
SampleRTT3, SampleRTT2, and SampleRTT1. Express  
EstimatedRTT in terms of the four sample RTTs.
b.	 Generalize your formula for n sample RTTs.
c.	 For the formula in part (b) let n approach infinity. Comment on why this 
averaging procedure is called an exponential moving average.
	
P33.	 In Section 3.5.3, we discussed TCP’s estimation of RTT. Why do you think 
TCP avoids measuring the SampleRTT for retransmitted segments?
	
P34.	 What is the relationship between the variable SendBase in Section 3.5.4 
and the variable LastByteRcvd in Section 3.5.5?
	
P35.	 What is the relationship between the variable LastByteRcvd in Section 
3.5.5 and the variable y in Section 3.5.4?
	
P36.	 In Section 3.5.4, we saw that TCP waits until it has received three dupli-
cate ACKs before performing a fast retransmit. Why do you think the TCP 
designers chose not to perform a fast retransmit after the first duplicate ACK 
for a segment is received?
	
P37.	 Compare GBN, SR, and TCP (no delayed ACK). Assume that the timeout 
values for all three protocols are sufficiently long such that 5 consecutive 
data segments and their corresponding ACKs can be received (if not lost in 
the channel) by the receiving host (Host B) and the sending host (Host A) 
respectively. Suppose Host A sends 5 data segments to Host B, and the 2nd 
segment (sent from A) is lost. In the end, all 5 data segments have been cor-
rectly received by Host B.
a.	 How many segments has Host A sent in total and how many ACKs has 
Host B sent in total? What are their sequence numbers? Answer this  
question for all three protocols.
Problems         325
b.	 If the timeout values for all three protocol are much longer than 5 RTT, 
then which protocol successfully delivers all five data segments in short-
est time interval?
	
P38.	 In our description of TCP in Figure 3.53, the value of the threshold,  
ssthresh, is set as ssthresh=cwnd/2 in several places and 
ssthresh value is referred to as being set to half the window size when a 
loss event occurred. Must the rate at which the sender is sending when the 
loss event occurred be approximately equal to cwnd segments per RTT? 
Explain your answer. If your answer is no, can you suggest a different  
manner in which ssthresh should be set?
	
P39.	 Consider Figure 3.46(b). If l′
in increases beyond R/2, can lout increase 
beyond R/3? Explain. Now consider Figure 3.46(c). If l′
in increases beyond 
R/2, can lout increase beyond R/4 under the assumption that a packet will be 
forwarded twice on average from the router to the receiver? Explain.
	
P40.	 Consider Figure 3.58. Assuming TCP Reno is the protocol experiencing the 
behavior shown above, answer the following questions. In all cases, you 
should provide a short discussion justifying your answer.
a.	 Identify the intervals of time when TCP slow start is operating.
b.	 Identify the intervals of time when TCP congestion avoidance is operating.
c.	 After the 16th transmission round, is segment loss detected by a triple 
duplicate ACK or by a timeout?
d.	 After the 22nd transmission round, is segment loss detected by a triple 
duplicate ACK or by a timeout?
VideoNote
Examining the behavior 
of TCP
0
0
2
4
6
8
10 12
Transmission round
14 16 18 20 22 24 26
5
10
15
20
25
Congestion window size (segments)
30
35
40
45
Figure 3.58  ♦  TCP window size as a function of time
326         Chapter 3    •    Transport Layer
e.	 What is the initial value of ssthresh at the first transmission round?
f.	 What is the value of ssthresh at the 18th transmission round?
g.	 What is the value of ssthresh at the 24th transmission round?
h.	 During what transmission round is the 70th segment sent?
i.	 Assuming a packet loss is detected after the 26th round by the receipt of 
a triple duplicate ACK, what will be the values of the congestion window 
size and of ssthresh?
j.	 Suppose TCP Tahoe is used (instead of TCP Reno), and assume that triple 
duplicate ACKs are received at the 16th round. What are the ssthresh 
and the congestion window size at the 19th round?
k.	 Again suppose TCP Tahoe is used, and there is a timeout event at  
22nd round. How many packets have been sent out from 17th round till 
22nd round, inclusive?
	
P41.	 Refer to Figure 3.55, which illustrates the convergence of TCP’s AIMD 
algorithm. Suppose that instead of a multiplicative decrease, TCP decreased 
the window size by a constant amount. Would the resulting AIAD algorithm 
converge to an equal share algorithm? Justify your answer using a diagram 
similar to Figure 3.55.
	
P42.	 In Section 3.5.4, we discussed the doubling of the timeout interval after a 
timeout event. This mechanism is a form of congestion control. Why does 
TCP need a window-based congestion-control mechanism (as studied in  
Section 3.7) in addition to this doubling-timeout-interval mechanism?
	
P43.	 Host A is sending an enormous file to Host B over a TCP connection. Over 
this connection there is never any packet loss and the timers never expire. 
Denote the transmission rate of the link connecting Host A to the Internet by 
R bps. Suppose that the process in Host A is capable of sending data into its 
TCP socket at a rate S bps, where S = 10 # R. Further suppose that the TCP 
receive buffer is large enough to hold the entire file, and the send buffer can 
hold only one percent of the file. What would prevent the process in Host 
A from continuously passing data to its TCP socket at rate S bps? TCP flow 
control? TCP congestion control? Or something else? Elaborate.
	
P44.	 Consider sending a large file from a host to another over a TCP connection 
that has no loss.
a.	 Suppose TCP uses AIMD for its congestion control without slow start. 
Assuming cwnd increases by 1 MSS every time a batch of ACKs is 
received and assuming approximately constant round-trip times, how long 
does it take for cwnd increase from 6 MSS to 12 MSS (assuming no loss 
events)?
b.	 What is the average throughout (in terms of MSS and RTT) for this con-
nection up through time = 6 RTT?
Problems         327
	
P45.	 Recall the macroscopic description of TCP throughput. In the period of time 
from when the connection’s rate varies from W/(2 · RTT) to W/RTT, only one 
packet is lost (at the very end of the period).
a.	 Show that the loss rate (fraction of packets lost) is equal to
L = loss rate =
1
3
8 W2 + 3
4 W
b.	 Use the result above to show that if a connection has loss rate L, then its 
average rate is approximately given by
≈1.22 # MSS
RTT 2L
	
P46.	 Consider that only a single TCP (Reno) connection uses one 10Mbps link 
which does not buffer any data. Suppose that this link is the only congested 
link between the sending and receiving hosts. Assume that the TCP sender 
has a huge file to send to the receiver, and the receiver’s receive buffer 
is much larger than the congestion window. We also make the following 
assumptions: each TCP segment size is 1,500 bytes; the two-way propagation 
delay of this connection is 150 msec; and this TCP connection is always in 
congestion avoidance phase, that is, ignore slow start.
a.	 What is the maximum window size (in segments) that this TCP connec-
tion can achieve?
b.	 What is the average window size (in segments) and average throughput 
(in bps) of this TCP connection?
c.	 How long would it take for this TCP connection to reach its maximum 
window again after recovering from a packet loss?
	
P47.	 Consider the scenario described in the previous problem. Suppose that the 
10Mbps link can buffer a finite number of segments. Argue that in order for 
the link to always be busy sending data, we would like to choose a buffer size 
that is at least the product of the link speed C and the two-way propagation 
delay between the sender and the receiver.
	
P48.	 Repeat Problem 46, but replacing the 10 Mbps link with a 10 Gbps link. Note 
that in your answer to part c, you will realize that it takes a very long time for 
the congestion window size to reach its maximum window size after recover-
ing from a packet loss. Sketch a solution to solve this problem.
	
P49.	 Let T (measured by RTT) denote the time interval that a TCP connection 
takes to increase its congestion window size from W/2 to W, where W is the 
maximum congestion window size. Argue that T is a function of TCP’s  
average throughput.
328         Chapter 3    •    Transport Layer
	
P50.	 Consider a simplified TCP’s AIMD algorithm where the congestion window 
size is measured in number of segments, not in bytes. In additive increase, the 
congestion window size increases by one segment in each RTT. In multipli-
cative decrease, the congestion window size decreases by half (if the result 
is not an integer, round down to the nearest integer). Suppose that two TCP 
connections, C1 and C2, share a single congested link of speed 30 segments 
per second. Assume that both C1 and C2 are in the congestion avoidance 
phase. Connection C1’s RTT is 50 msec and connection C2’s RTT is 100 
msec. Assume that when the data rate in the link exceeds the link’s speed, all 
TCP connections experience data segment loss.
a.	 If both C1 and C2 at time t0 have a congestion window of 10 segments, 
what are their congestion window sizes after 1000 msec?
b.	 In the long run, will these two connections get the same share of the band-
width of the congested link? Explain.
	
P51.	 Consider the network described in the previous problem. Now suppose that 
the two TCP connections, C1 and C2, have the same RTT of 100 msec.  
Suppose that at time t0, C1’s congestion window size is 15 segments but C2’s 
congestion window size is 10 segments.
a.	 What are their congestion window sizes after 2200 msec?
b.	 In the long run, will these two connections get about the same share of the 
bandwidth of the congested link?
c.	 We say that two connections are synchronized, if both connections reach 
their maximum window sizes at the same time and reach their minimum 
window sizes at the same time. In the long run, will these two connec-
tions get synchronized eventually? If so, what are their maximum window 
sizes?
d.	 Will this synchronization help to improve the utilization of the shared 
link? Why? Sketch some idea to break this synchronization.
	
P52.	 Consider a modification to TCP’s congestion control algorithm. Instead of 
additive increase, we can use multiplicative increase. A TCP sender increases 
its window size by a small positive constant a (0 6 a 6 1) whenever it 
receives a valid ACK. Find the functional relationship between loss rate L 
and maximum congestion window W. Argue that for this modified TCP, 
regardless of TCP’s average throughput, a TCP connection always spends the 
same amount of time to increase its congestion window size from W/2 to W.
	
P53.	 In our discussion of TCP futures in Section 3.7, we noted that to achieve a 
throughput of 10 Gbps, TCP could only tolerate a segment loss probability of 
2 # 10-10 (or equivalently, one loss event for every 5,000,000,000 segments). 
Show the derivation for the values of 2 # 10-10 (1 out of 5,000,000) for the 
RTT and MSS values given in Section 3.7. If TCP needed to support a 100 
Gbps connection, what would the tolerable loss be?
Programming Assignments         329
	
P54.	 In our discussion of TCP congestion control in Section 3.7, we implicitly 
assumed that the TCP sender always had data to send. Consider now the case 
that the TCP sender sends a large amount of data and then goes idle (since it 
has no more data to send) at t1. TCP remains idle for a relatively long period 
of time and then wants to send more data at t2. What are the advantages and 
disadvantages of having TCP use the cwnd and ssthresh values from t1 
when starting to send data at t2? What alternative would you recommend? 
Why?
	
P55.	 In this problem we investigate whether either UDP or TCP provides a degree 
of end-point authentication.
a.	 Consider a server that receives a request within a UDP packet and 
responds to that request within a UDP packet (for example, as done by a 
DNS server). If a client with IP address X spoofs its address with address 
Y, where will the server send its response?
b.	 Suppose a server receives a SYN with IP source address Y, and after 
responding with a SYNACK, receives an ACK with IP source address Y 
with the correct acknowledgment number. Assuming the server chooses a 
random initial sequence number and there is no “man-in-the-middle,” can 
the server be certain that the client is indeed at Y (and not at some other 
address X that is spoofing Y)?
	
P56.	 In this problem, we consider the delay introduced by the TCP slow-start 
phase. Consider a client and a Web server directly connected by one link of 
rate R. Suppose the client wants to retrieve an object whose size is exactly 
equal to 15 S, where S is the maximum segment size (MSS). Denote the 
round-trip time between client and server as RTT (assumed to be constant). 
Ignoring protocol headers, determine the time to retrieve the object (includ-
ing TCP connection establishment) when
a.	 4 S/R 7 S/R + RTT 7 2S/R
b.	 S/R + RTT 7 4 S/R
c.	 S/R 7 RTT.
Programming Assignments
Implementing a Reliable Transport Protocol
In this laboratory programming assignment, you will be writing the sending and 
receiving transport-level code for implementing a simple reliable data transfer pro-
tocol. There are two versions of this lab, the alternating-bit-protocol version and the 
GBN version. This lab should be fun—your implementation will differ very little 
from what would be required in a real-world situation.
330         Chapter 3    •    Transport Layer
Since you probably don’t have standalone machines (with an OS that you can 
modify), your code will have to execute in a simulated hardware/software environ-
ment. However, the programming interface provided to your routines—the code that 
would call your entities from above and from below—is very close to what is done 
in an actual UNIX environment. (Indeed, the software interfaces described in this 
programming assignment are much more realistic than the infinite loop senders and 
receivers that many texts describe.) Stopping and starting timers are also simulated, 
and timer interrupts will cause your timer handling routine to be activated.
The full lab assignment, as well as code you will need to compile with your own 
code, are available at this book’s Web site: www.pearsonglobaleditions.com/kurose.
Wireshark Lab: Exploring TCP
In this lab, you’ll use your Web browser to access a file from a Web server. As in 
earlier Wireshark labs, you’ll use Wireshark to capture the packets arriving at your 
computer. Unlike earlier labs, you’ll also be able to download a Wireshark-readable 
packet trace from the Web server from which you downloaded the file. In this server 
trace, you’ll find the packets that were generated by your own access of the Web 
server. You’ll analyze the client- and server-side traces to explore aspects of TCP. 
In particular, you’ll evaluate the performance of the TCP connection between your 
computer and the Web server. You’ll trace TCP’s window behavior, and infer packet 
loss, retransmission, flow control and congestion control behavior, and estimated 
roundtrip time.
As is the case with all Wireshark labs, the full description of this lab is available 
at this book’s Web site, www.pearsonglobaleditions.com/kurose.
Wireshark Lab: Exploring UDP
In this short lab, you’ll do a packet capture and analysis of your favorite application 
that uses UDP (for example, DNS or a multimedia application such as Skype). As 
we learned in Section 3.3, UDP is a simple, no-frills transport protocol. In this lab, 
you’ll investigate the header fields in the UDP segment as well as the checksum 
calculation.
As is the case with all Wireshark labs, the full description of this lab is available 
at this book’s Web site, www.pearsonglobaleditions.com/kurose.
331
Please describe one or two of the most exciting projects you have worked on during your 
career. What were the biggest challenges?
School teaches us lots of ways to find answers. In every interesting problem I’ve worked 
on, the challenge has been finding the right question. When Mike Karels and I started look-
ing at TCP congestion, we spent months staring at protocol and packet traces asking “Why 
is it failing?”. One day in Mike’s office, one of us said “The reason I can’t figure out why 
it fails is because I don’t understand how it ever worked to begin with.” That turned out to 
be the right question and it forced us to figure out the “ack clocking” that makes TCP work. 
After that, the rest was easy.
More generally, where do you see the future of networking and the Internet?
For most people, the Web is the Internet. Networking geeks smile politely since we know 
the Web is an application running over the Internet but what if they’re right? The Internet 
is about enabling conversations between pairs of hosts. The Web is about distributed infor-
mation production and consumption. “Information propagation” is a very general view of 
communication of which “pairwise conversation” is a tiny subset. We need to move into the 
larger tent. Networking today deals with broadcast media (radios, PONs, etc.) by pretending 
it’s a point-to-point wire. That’s massively inefficient. Terabits-per-second of data are being 
exchanged all over the World via thumb drives or smart phones but we don’t know how to 
treat that as “networking”. ISPs are busily setting up caches and CDNs to scalably distribute 
video and audio. Caching is a necessary part of the solution but there’s no part of today’s 
networking—from Information, Queuing or Traffic Theory down to the Internet protocol 
Van Jacobson works at Google and was previously a Research 
Fellow at PARC. Prior to that, he was co-founder and Chief Scientist 
of Packet Design. Before that, he was Chief Scientist at Cisco. 
Before joining Cisco, he was head of the Network Research 
Group at Lawrence Berkeley National Laboratory and taught at UC 
Berkeley and Stanford. Van received the ACM SIGCOMM Award 
in 2001 for outstanding lifetime contribution to the field of commu-
nication networks and the IEEE Kobayashi Award in 2002 for “con-
tributing to the understanding of network congestion and developing 
congestion control mechanisms that enabled the successful scaling 
of the Internet”. He was elected to the U.S. National Academy of 
Engineering in 2004.
Van Jacobson
AN INTERVIEW WITH...
332
specs—that tells us how to engineer and deploy it. I think and hope that over the next few 
years, networking will evolve to embrace the much larger vision of communication that 
underlies the Web.
What people inspired you professionally?
When I was in grad school, Richard Feynman visited and gave a colloquium. He talked 
about a piece of Quantum theory that I’d been struggling with all semester and his explana-
tion was so simple and lucid that what had been incomprehensible gibberish to me became 
obvious and inevitable. That ability to see and convey the simplicity that underlies our  
complex world seems to me a rare and wonderful gift.
What are your recommendations for students who want careers in computer science and 
networking?
It’s a wonderful field—computers and networking have probably had more impact on society 
than any invention since the book. Networking is fundamentally about connecting stuff, and 
studying it helps you make intellectual connections: Ant foraging & Bee dances demonstrate 
protocol design better than RFCs, traffic jams or people leaving a packed stadium are the 
essence of congestion, and students finding flights back to school in a post-Thanksgiving  
blizzard are the core of dynamic routing. If you’re interested in lots of stuff and want to 
have an impact, it’s hard to imagine a better field.
333
We learned in the previous chapter that the transport layer provides various forms 
of process-to-process communication by relying on the network layer’s host-to-host 
communication service. We also learned that the transport layer does so without any 
knowledge about how the network layer actually implements this service. So perhaps 
you’re now wondering, what’s under the hood of the host-to-host communication 
service, what makes it tick?
In this chapter and the next, we’ll learn exactly how the network layer can pro-
vide its host-to-host communication service. We’ll see that unlike the transport and 
application layers, there is a piece of the network layer in each and every host and 
router in the network. Because of this, network-layer protocols are among the most 
challenging (and therefore among the most interesting!) in the protocol stack.
Since the network layer is arguably the most complex layer in the protocol 
stack, we’ll have a lot of ground to cover here. Indeed, there is so much to cover 
that we cover the network layer in two chapters. We’ll see that the network layer 
can be decomposed into two interacting parts, the data plane and the control plane. 
In Chapter 4, we’ll first cover the data plane functions of the network layer—the 
per-router functions in the network layer that determine how a datagram (that is, a 
network-layer packet) arriving on one of a router’s input links is forwarded to one 
of that router’s output links. We’ll cover both traditional IP forwarding (where for-
warding is based on a datagram’s destination address) and generalized forwarding 
(where forwarding and other functions may be performed using values in several 
different fields in the datagram’s header). We’ll study the IPv4 and IPv6 protocols 
and addressing in detail. In Chapter 5, we’ll cover the control plane functions of 
the network layer—the network-wide logic that controls how a datagram is routed 
4
Chapter
The Network 
Layer: Data 
Plane
334         Chapter 4    •    The Network Layer: Data Plane
among routers along an end-to-end path from the source host to the destination host. 
We’ll cover routing algorithms, as well as routing protocols, such as OSPF and BGP, 
that are in widespread use in today’s Internet. Traditionally, these control-plane rout-
ing protocols and data-plane forwarding functions have been implemented together, 
monolithically, within a router. Software-defined networking (SDN) explicitly sepa-
rates the data plane and control plane by implementing these control plane functions 
as a separate service, typically in a remote “controller.” We’ll also cover SDN con-
trollers in Chapter 5.
This distinction between data-plane and control-plane functions in the network 
layer is an important concept to keep in mind as you learn about the network layer —
it will help structure your thinking about the network layer and reflects a modern 
view of the network layer’s role in computer networking.
4.1	 Overview of Network Layer
Figure 4.1 shows a simple network with two hosts, H1 and H2, and several routers on 
the path between H1 and H2. Let’s suppose that H1 is sending information to H2, and 
consider the role of the network layer in these hosts and in the intervening routers. The 
network layer in H1 takes segments from the transport layer in H1, encapsulates each 
segment into a datagram, and then sends the datagrams to its nearby router, R1. At the 
receiving host, H2, the network layer receives the datagrams from its nearby router 
R2, extracts the transport-layer segments, and delivers the segments up to the transport 
layer at H2. The primary data-plane role of each router is to forward datagrams from 
its input links to its output links; the primary role of the network control plane is to 
coordinate these local, per-router forwarding actions so that datagrams are ultimately 
transferred end-to-end, along paths of routers between source and destination hosts. 
Note that the routers in Figure 4.1 are shown with a truncated protocol stack, that is, 
with no upper layers above the network layer, because routers do not run application- 
 
and transport-layer protocols such as those we examined in Chapters 2 and 3.
4.1.1 Forwarding and Routing: The Data and  
Control Planes
The primary role of the network layer is deceptively simple—to move packets from 
a sending host to a receiving host. To do so, two important network-layer functions 
can be identified:
•	 Forwarding. When a packet arrives at a router’s input link, the router must move 
the packet to the appropriate output link. For example, a packet arriving from 
Host H1 to Router R1 in Figure 4.1 must be forwarded to the next router on 
a path to H2. As we will see, forwarding is but one function (albeit the most 
 
4.1    •    Overview of Network Layer         335
Data link
Physical
Network
Data link
Physical
Network
End system H1
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Home Network
End system H2
Data link
Physical
Application
Transport
Network
Router R2
Data link
Physical
Network
Data link
Physical
Network
Data link
Physical
Network
Router R1
Data link
Physical
Application
Transport
Network
Figure 4.1  ♦  The network layer
336         Chapter 4    •    The Network Layer: Data Plane
common and important one!) implemented in the data plane. In the more general 
case, which we’ll cover in Section 4.4, a packet might also be blocked from exit-
ing a router (e.g., if the packet originated at a known malicious sending host, or if 
the packet were destined to a forbidden destination host), or might be duplicated 
and sent over multiple outgoing links.
•	 Routing. The network layer must determine the route or path taken by packets as 
they flow from a sender to a receiver. The algorithms that calculate these paths 
are referred to as routing algorithms. A routing algorithm would determine, for 
example, the path along which packets flow from H1 to H2 in Figure 4.1. Routing 
is implemented in the control plane of the network layer.
The terms forwarding and routing are often used interchangeably by authors dis-
cussing the network layer. We’ll use these terms much more precisely in this book. 
 
Forwarding refers to the router-local action of transferring a packet from an input 
link interface to the appropriate output link interface. Forwarding takes place at very 
short timescales (typically a few nanoseconds), and thus is typically implemented in 
hardware. Routing refers to the network-wide process that determines the end-to-end 
paths that packets take from source to destination. Routing takes place on much longer 
timescales (typically seconds), and as we will see is often implemented in software. 
Using our driving analogy, consider the trip from Pennsylvania to Florida undertaken 
by our traveler back in Section 1.3.1. During this trip, our driver passes through many 
interchanges en route to Florida. We can think of forwarding as the process of getting 
through a single interchange: A car enters the interchange from one road and deter-
mines which road it should take to leave the interchange. We can think of routing as 
the process of planning the trip from Pennsylvania to Florida: Before embarking on 
the trip, the driver has consulted a map and chosen one of many paths possible, with 
each path consisting of a series of road segments connected at interchanges.
A key element in every network router is its forwarding table. A router forwards 
a packet by examining the value of one or more fields in the arriving packet’s header, 
and then using these header values to index into its forwarding table. The value stored 
in the forwarding table entry for those values indicates the outgoing link interface at 
that router to which that packet is to be forwarded. For example, in Figure 4.2, a packet 
with header field value of 0110 arrives to a router. The router indexes into its forward-
ing table and determines that the output link interface for this packet is interface 2. 
The router then internally forwards the packet to interface 2. In Section 4.2, we’ll look 
inside a router and examine the forwarding function in much greater detail. Forward-
ing is the key function performed by the data-plane functionality of the network layer.
Control Plane: The Traditional Approach 
But now you are undoubtedly wondering how a router’s forwarding tables are con-
figured in the first place. This is a crucial issue, one that exposes the important inter-
play between forwarding (in data plane) and routing (in control plane). As shown 
 
4.1    •    Overview of Network Layer         337
in Figure 4.2, the routing algorithm determines the contents of the routers’ forward-
ing tables. In this example, a routing algorithm runs in each and every router and 
both forwarding and routing functions are contained within a router. As we’ll see in 
Sections 5.3 and 5.4, the routing algorithm function in one router communicates with 
the routing algorithm function in other routers to compute the values for its forward-
ing table. How is this communication performed? By exchanging routing messages 
containing routing information according to a routing protocol! We’ll cover routing 
algorithms and protocols in Sections 5.2 through 5.4.
The distinct and different purposes of the forwarding and routing functions can 
be further illustrated by considering the hypothetical (and unrealistic, but technically 
feasible) case of a network in which all forwarding tables are configured directly by 
human network operators physically present at the routers. In this case, no routing 
protocols would be required! Of course, the human operators would need to interact 
with each other to ensure that the forwarding tables were configured in such a way 
that packets reached their intended destinations. It’s also likely that human configu-
ration would be more error-prone and much slower to respond to changes in the net-
work topology than a routing protocol. We’re thus fortunate that all networks have 
both a forwarding and a routing function!
0110
Local forwarding
table
header
0100
0110
0111
1001
3
2
2
1
output
Control plane
Data plane
Routing
Algorithm
Values in arriving
packet’s header
1
2
3
Figure 4.2  ♦  Routing algorithms determine values in forward tables
338         Chapter 4    •    The Network Layer: Data Plane
Control Plane: The SDN Approach 
The approach to implementing routing functionality shown in Figure 4.2—with each 
router having a routing component that communicates with the routing component of 
other routers—has been the traditional approach adopted by routing vendors in their 
products, at least until recently. Our observation that humans could manually configure 
forwarding tables does suggest, however, that there may be other ways for control-
plane functionality to determine the contents of the data-plane forwarding tables.
Figure 4.3 shows an alternate approach in which a physically separate (from the 
routers), remote controller computes and distributes the forwarding tables to be used 
by each and every router.  Note that the data plane components of Figures 4.2 and 4.3 
are identical. In Figure 4.3, however, control-plane routing functionality is separated 
0110
Local forwarding
table
header
0100
0110
0111
1001
3
2
2
1
output
Remote Controller
Values in arriving
packet’s header
1
2
3
Control plane
Data plane
Figure 4.3  ♦  
A remote controller determines and distributes values in 
­
forwarding tables
4.1    •    Overview of Network Layer         339
from the physical router—the routing device performs forwarding only, while the 
remote controller computes and distributes forwarding tables. The remote controller 
might be implemented in a remote data center with high reliability and redundancy, 
and might be managed by the ISP or some third party. How might the routers and 
the remote controller communicate? By exchanging messages containing forwarding 
tables and other pieces of routing information. The control-plane approach shown 
in Figure 4.3 is at the heart of software-defined networking (SDN), where the net-
work is “software-defined” because the controller that computes forwarding tables 
and interacts with routers is implemented in software. Increasingly, these software 
implementations are also open, i.e., similar to Linux OS code, the code is publically 
available, allowing ISPs (and networking researchers and students!) to innovate and 
propose changes to the software that controls network-layer functionality. We will 
cover the SDN control plane in Section 5.5.
4.1.2 Network Service Model
Before delving into the network layer’s data plane, let’s wrap up our introduction 
by taking the broader view and consider the different types of service that might be 
offered by the network layer. When the transport layer at a sending host transmits a 
packet into the network (that is, passes it down to the network layer at the sending 
host), can the transport layer rely on the network layer to deliver the packet to the 
destination? When multiple packets are sent, will they be delivered to the transport 
layer in the receiving host in the order in which they were sent? Will the amount 
of time between the sending of two sequential packet transmissions be the same 
as the amount of time between their reception? Will the network provide any feed-
back about congestion in the network? The answers to these questions and others 
are determined by the service model provided by the network layer. The network 
service model defines the characteristics of end-to-end delivery of packets between 
sending and receiving hosts.
Let’s now consider some possible services that the network layer could provide. 
These services could include:
•	 Guaranteed delivery. This service guarantees that a packet sent by a source host 
will eventually arrive at the destination host.
•	 Guaranteed delivery with bounded delay. This service not only guarantees 
delivery of the packet, but delivery within a specified host-to-host delay bound 
 
(for example, within 100 msec).
•	 In-order packet delivery. This service guarantees that packets arrive at the desti-
nation in the order that they were sent.
•	 Guaranteed minimal bandwidth. This network-layer service emulates the behav-
ior of a transmission link of a specified bit rate (for example, 1 Mbps) between 
sending and receiving hosts. As long as the sending host transmits bits (as part 
340         Chapter 4    •    The Network Layer: Data Plane
of packets) at a rate below the specified bit rate, then all packets are eventually 
delivered to the destination host.
•	 Security. The network layer could encrypt all datagrams at the source and decrypt them 
at the destination, thereby providing confidentiality to all transport-layer segments.
This is only a partial list of services that a network layer could provide—there are 
countless variations possible.
The Internet’s network layer provides a single service, known as best-effort 
service. With best-effort service, packets are neither guaranteed to be received in the 
order in which they were sent, nor is their eventual delivery even guaranteed. There 
is no guarantee on the end-to-end delay nor is there a minimal bandwidth guaran-
tee. It might appear that best-effort service is a euphemism for no service at all—a 
network that delivered no packets to the destination would satisfy the definition of 
 
best-effort delivery service! Other network architectures have defined and imple-
mented service models that go beyond the Internet’s best-effort service. For example, 
the ATM network architecture [MFA Forum 2016, Black 1995] provides for guaran-
teed in-order delay, bounded delay, and guaranteed minimal bandwidth. There have 
also been proposed service model extensions to the Internet architecture; for exam-
ple, the Intserv architecture [RFC 1633] aims to provide end-end delay guarantees 
and congestion-free communication. Interestingly, in spite of these well-developed 
alternatives, the Internet’s basic best-effort service model combined with adequate 
bandwidth provisioning have arguably proven to be more than “good enough” to 
enable an amazing range of applications, including streaming video services such 
as Netflix and voice-and-video-over-IP, real-time conferencing applications such as 
Skype and Facetime.
An Overview of Chapter 4
Having now provided an overview of the network layer, we’ll cover the data-plane 
component of the network layer in the following sections in this chapter. In Section 
4.2, we’ll dive down into the internal hardware operations of a router, including input 
and output packet processing, the router’s internal switching mechanism, and packet 
queueing and scheduling. In Section 4.3, we’ll take a look at traditional IP forward-
ing, in which packets are forwarded to output ports based on their destination IP 
addresses. We’ll encounter IP addressing, the celebrated IPv4 and IPv6 protocols and 
more. In Section 4.4, we’ll cover more generalized forwarding, where packets may 
be forwarded to output ports based on a large number of header values (i.e., not only 
based on destination IP address). Packets may be blocked or duplicated at the router, 
or may have certain header field values rewritten—all under software control. This 
more generalized form of packet forwarding is a key component of a modern network 
data plane, including the data plane in software-defined networks (SDN).
We mention here in passing that the terms forwarding and switching are often 
used interchangeably by computer-networking researchers and practitioners; we’ll 
4.2    •    What’s Inside a Router?         341
use both terms interchangeably in this textbook as well. While we’re on the topic 
of terminology, it’s also worth mentioning two other terms that are often used 
interchangeably, but that we will use more carefully. We’ll reserve the term packet 
switch to mean a general packet-switching device that transfers a packet from input 
link interface to output link interface, according to values in a packet’s header fields. 
Some packet switches, called link-layer switches (examined in Chapter 6), base 
their forwarding decision on values in the fields of the link-layer frame; switches 
are thus referred to as link-layer (layer 2) devices. Other packet switches, called 
routers, base their forwarding decision on header field values in the network-layer 
datagram. Routers are thus network-layer (layer 3) devices. (To fully appreciate this 
important distinction, you might want to review Section 1.5.2, where we discuss 
network-layer datagrams and link-layer frames and their relationship.) Since our 
focus in this chapter is on the network layer, we’ll mostly use the term router in 
place of packet switch.
4.2	 What’s Inside a Router?
Now that we’ve overviewed the data and control planes within the network layer, the 
important distinction between forwarding and routing, and the services and functions of 
the network layer, let’s turn our attention to its forwarding function—the actual transfer 
of packets from a router’s incoming links to the appropriate outgoing links at that router.
A high-level view of a generic router architecture is shown in Figure 4.4. Four 
router components can be identified:
Input port
Output port
Input port
Output port
Routing
processor
Routing, management
control plane (software)
Forwarding
data plane (hardware)
Switch
fabric
Figure 4.4  ♦  Router architecture
342         Chapter 4    •    The Network Layer: Data Plane
•	 Input ports. An input port performs several key functions. It performs the physi-
cal layer function of terminating an incoming physical link at a router; this is 
shown in the leftmost box of an input port and the rightmost box of an output 
port in Figure 4.4. An input port also performs link-layer functions needed to 
interoperate with the link layer at the other side of the incoming link; this is 
represented by the middle boxes in the input and output ports. Perhaps most cru-
cially, a lookup function is also performed at the input port; this will occur in the 
rightmost box of the input port. It is here that the forwarding table is consulted 
to determine the router output port to which an arriving packet will be forwarded 
via the switching fabric. Control packets (for example, packets carrying routing 
protocol information) are forwarded from an input port to the routing processor. 
Note that the term “port” here—referring to the physical input and output router 
interfaces—is distinctly different from the software ports associated with network 
applications and sockets discussed in Chapters 2 and 3. In practice, the number of 
ports supported by a router can range from a relatively small number in enterprise 
routers, to hundreds of 10 Gbps ports in a router at an ISP’s edge, where the num-
ber of incoming lines tends to be the greatest. The Juniper MX2020, edge router, 
for example, supports up to 960 10 Gbps Ethernet ports, with an overall router 
system capacity of 80 Tbps [Juniper MX 2020 2016].
•	 Switching fabric. The switching fabric connects the router’s input ports to its 
output ports. This switching fabric is completely contained within the router—a 
network inside of a network router!
•	 Output ports. An output port stores packets received from the switching fabric 
and transmits these packets on the outgoing link by performing the necessary 
link-layer and physical-layer functions. When a link is bidirectional (that is, car-
ries traffic in both directions), an output port will typically be paired with the 
input port for that link on the same line card.
•	 Routing processor. The routing processor performs control-plane functions. In tra-
ditional routers, it executes the routing protocols (which we’ll study in Sections 
5.3 and 5.4), maintains routing tables and attached link state information, and com-
putes the forwarding table for the router. In SDN routers, the routing processor is 
responsible for communicating with the remote controller in order to (among other 
activities) receive forwarding table entries computed by the remote controller, and 
install these entries in the router’s input ports. The routing processor also performs 
the network management functions that we’ll study in Section 5.7.
A router’s input ports, output ports, and switching fabric are almost always 
implemented in hardware, as shown in Figure 4.4. To appreciate why a hardware 
implementation is needed, consider that with a 10 Gbps input link and a 64-byte IP 
datagram, the input port has only 51.2 ns to process the datagram before another 
datagram may arrive. If N ports are combined on a line card (as is often done in 
practice), the datagram-processing pipeline must operate N times faster—far too 
4.2    •    What’s Inside a Router?         343
fast for software implementation. Forwarding hardware can be implemented either 
using a router vendor’s own hardware designs, or constructed using purchased 
 
merchant-silicon chips (e.g., as sold by companies such as Intel and Broadcom).
While the data plane operates at the nanosecond time scale, a router’s control 
functions—executing the routing protocols, responding to attached links that go up 
or down, communicating with the remote controller (in the SDN case) and perform-
ing management functions—operate at the millisecond or second timescale. These 
control plane functions are thus usually implemented in software and execute on the 
routing processor (typically a traditional CPU).
Before delving into the details of router internals, let’s return to our analogy 
from the beginning of this chapter, where packet forwarding was compared to cars 
entering and leaving an interchange. Let’s suppose that the interchange is a rounda-
bout, and that as a car enters the roundabout, a bit of processing is required. Let’s 
consider what information is required for this processing:
•	 Destination-based forwarding. Suppose the car stops at an entry station and indi-
cates its final destination (not at the local roundabout, but the ultimate destination 
of its journey). An attendant at the entry station looks up the final destination, 
determines the roundabout exit that leads to that final destination, and tells the 
driver which roundabout exit to take.
•	 Generalized forwarding. The attendant could also determine the car’s exit ramp on 
the basis of many other factors besides the destination. For example, the selected 
exit ramp might depend on the car’s origin, for example the state that issued the 
car’s license plate. Cars from a certain set of states might be directed to use one exit 
ramp (that leads to the destination via a slow road), while cars from other states 
might be directed to use a different exit ramp (that leads to the destination via super-
highway). The same decision might be made based on the model, make and year 
of the car. Or a car not deemed roadworthy might be blocked and not be allowed to 
pass through the roundabout. In the case of generalized forwarding, any number of 
factors may contribute to the attendant’s choice of the exit ramp for a given car.
Once the car enters the roundabout (which may be filled with other cars entering 
from other input roads and heading to other roundabout exits), it eventually leaves at 
the prescribed roundabout exit ramp, where it may encounter other cars leaving the 
roundabout at that exit.
We can easily recognize the principal router components in Figure 4.4 in this 
analogy—the entry road and entry station correspond to the input port (with a lookup 
function to determine to local outgoing port); the roundabout corresponds to the 
switch fabric; and the roundabout exit road corresponds to the output port. With this 
analogy, it’s instructive to consider where bottlenecks might occur. What happens if 
cars arrive blazingly fast (for example, the roundabout is in Germany or Italy!) but 
the station attendant is slow? How fast must the attendant work to ensure there’s no 
backup on an entry road? Even with a blazingly fast attendant, what happens if cars 
344         Chapter 4    •    The Network Layer: Data Plane
traverse the roundabout slowly—can backups still occur? And what happens if most 
of the cars entering at all of the roundabout’s entrance ramps all want to leave the 
roundabout at the same exit ramp—can backups occur at the exit ramp or elsewhere? 
How should the roundabout operate if we want to assign priorities to different cars, 
or block certain cars from entering the roundabout in the first place? These are all 
analogous to critical questions faced by router and switch designers.
In the following subsections, we’ll look at router functions in more detail. [Iyer 
2008, Chao 2001; Chuang 2005; Turner 1988; McKeown 1997a; Partridge 1998; Sopra-
nos 2011] provide a discussion of specific router architectures. For concreteness and 
simplicity, we’ll initially assume in this section that forwarding decisions are based only 
on the packet’s destination address, rather than on a generalized set of packet header 
fields. We will cover the case of more generalized packet forwarding in Section 4.4.
4.2.1 Input Port Processing and Destination-Based Forwarding
A more detailed view of input processing is shown in Figure 4.5. As just discussed, 
the input port’s line-termination function and link-layer processing implement the 
physical and link layers for that individual input link. The lookup performed in the 
input port is central to the router’s operation—it is here that the router uses the for-
warding table to look up the output port to which an arriving packet will be forwarded 
via the switching fabric. The forwarding table is either computed and updated by the 
routing processor (using a routing protocol to interact with the routing processors in 
other network routers) or is received from a remote SDN controller. The forwarding 
table is copied from the routing processor to the line cards over a separate bus (e.g., 
a PCI bus) indicated by the dashed line from the routing processor to the input line 
cards in Figure 4.4. With such a shadow copy at each line card, forwarding decisions 
can be made locally, at each input port, without invoking the centralized routing pro-
cessor on a per-packet basis and thus avoiding a centralized processing bottleneck.
Let’s now consider the “simplest” case that the output port to which an incoming 
packet is to be switched is based on the packet’s destination address. In the case of 
32-bit IP addresses, a brute-force implementation of the forwarding table would have 
one entry for every possible destination address. Since there are more than 4 billion 
possible addresses, this option is totally out of the question.
Line
termination
Data link
processing
(protocol,
decapsulation)
Lookup, fowarding,
queuing
Switch
fabric
Figure 4.5  ♦  Input port processing
4.2    •    What’s Inside a Router?         345
As an example of how this issue of scale can be handled, let’s suppose that our 
router has four links, numbered 0 through 3, and that packets are to be forwarded to 
the link interfaces as follows:
	
Destination Address Range	
Link Interface
	
11001000 00010111 00010000 00000000	
	
through	
0
	
11001000 00010111 00010111 11111111	
	
11001000 00010111 00011000 00000000	
	
through	
1
	
11001000 00010111 00011000 11111111	
	
11001000 00010111 00011001 00000000	
	
through	
2
	
11001000 00010111 00011111 11111111	
	
Otherwise	
3
Clearly, for this example, it is not necessary to have 4 billion entries in the router’s 
forwarding table. We could, for example, have the following forwarding table with 
just four entries:
	
Prefix	
Link Interface
	
11001000 00010111 00010	
0
	
11001000 00010111 00011000	
1
	
11001000 00010111 00011	
2
	
Otherwise	
3
With this style of forwarding table, the router matches a prefix of the packet’s des-
tination address with the entries in the table; if there’s a match, the router forwards 
the packet to a link associated with the match. For example, suppose the packet’s 
destination address is 11001000 00010111 00010110 10100001; because 
the 21-bit prefix of this address matches the first entry in the table, the router forwards 
the packet to link interface 0. If a prefix doesn’t match any of the first three entries, 
then the router forwards the packet to the default interface 3. Although this sounds 
simple enough, there’s a very important subtlety here. You may have noticed that it is 
possible for a destination address to match more than one entry. For example, the first 
24 bits of the address 11001000 00010111 00011000 10101010 match the 
second entry in the table, and the first 21 bits of the address match the third entry in the 
table. When there are multiple matches, the router uses the longest prefix matching 
rule; that is, it finds the longest matching entry in the table and forwards the packet to 
the link interface associated with the longest prefix match. We’ll see exactly why this 
346         Chapter 4    •    The Network Layer: Data Plane
longest prefix-matching rule is used when we study Internet addressing in more detail 
in Section 4.3.
Given the existence of a forwarding table, lookup is conceptually simple—­
hardware logic just searches through the forwarding table looking for the longest 
prefix match. But at Gigabit transmission rates, this lookup must be performed in 
nanoseconds (recall our earlier example of a 10 Gbps link and a 64-byte IP data-
gram). Thus, not only must lookup be performed in hardware, but techniques beyond 
a simple linear search through a large table are needed; surveys of fast lookup algo-
rithms can be found in [Gupta 2001, Ruiz-Sanchez 2001]. Special attention must 
also be paid to memory access times, resulting in designs with embedded on-chip 
DRAM and faster SRAM (used as a DRAM cache) memories. In practice, Ternary 
Content Addressable Memories (TCAMs) are also often used for lookup [Yu 2004]. 
With a TCAM, a 32-bit IP address is presented to the memory, which returns the 
content of the forwarding table entry for that address in essentially constant time. 
The Cisco Catalyst 6500 and 7600 Series routers and switches can hold upwards of 
a million TCAM forwarding table entries [Cisco TCAM 2014].
Once a packet’s output port has been determined via the lookup, the packet 
can be sent into the switching fabric. In some designs, a packet may be temporarily 
blocked from entering the switching fabric if packets from other input ports are cur-
rently using the fabric. A blocked packet will be queued at the input port and then 
scheduled to cross the fabric at a later point in time. We’ll take a closer look at the 
blocking, queuing, and scheduling of packets (at both input ports and output ports) 
shortly. Although “lookup” is arguably the most important action in input port pro-
cessing, many other actions must be taken: (1) physical- and link-layer processing 
must occur, as discussed previously; (2) the packet’s version number, checksum and 
time-to-live field—all of which we’ll study in Section 4.3—must be checked and the 
latter two fields rewritten; and (3) counters used for network management (such as 
the number of IP datagrams received) must be updated.
Let’s close our discussion of input port processing by noting that the input port 
steps of looking up a destination IP address (“match”) and then sending the packet 
into the switching fabric to the specified output port (“action”) is a specific case of a 
more general “match plus action” abstraction that is performed in many networked 
devices, not just routers. In link-layer switches (covered in Chapter 6), link-layer 
destination addresses are looked up and several actions may be taken in addition to 
sending the frame into the switching fabric towards the output port. In firewalls (cov-
ered in Chapter 8)—devices that filter out selected incoming packets—an incom-
ing packet whose header matches a given criteria (e.g., a combination of source/
destination IP addresses and transport-layer port numbers) may be dropped (action). 
In a network address translator (NAT, covered in Section 4.3), an incoming packet 
whose transport-layer port number matches a given value will have its port number 
rewritten before forwarding (action). Indeed, the “match plus action” abstraction is 
both powerful and prevalent in network devices today, and is central to the notion of 
generalized forwarding that we’ll study in Section 4.4.
4.2    •    What’s Inside a Router?         347
4.2.2 Switching
The switching fabric is at the very heart of a router, as it is through this fabric that 
the packets are actually switched (that is, forwarded) from an input port to an output 
port. Switching can be accomplished in a number of ways, as shown in Figure 4.6:
•	 Switching via memory. The simplest, earliest routers were traditional computers, 
with switching between input and output ports being done under direct control of 
the CPU (routing processor). Input and output ports functioned as traditional I/O 
devices in a traditional operating system. An input port with an arriving packet 
first signaled the routing processor via an interrupt. The packet was then copied 
from the input port into processor memory. The routing processor then extracted 
the destination address from the header, looked up the appropriate output port 
in the forwarding table, and copied the packet to the output port’s buffers. In 
this scenario, if the memory bandwidth is such that a maximum of B packets per 
second can be written into, or read from, memory, then the overall forwarding 
throughput (the total rate at which packets are transferred from input ports to out-
put ports) must be less than B/2. Note also that two packets cannot be forwarded 
Memory
A
B
C
X
Y
Z
Memory
Key:
Input port
Output port
A
X
Y
Z
B
C
Crossbar
A
B
C
X
Y
Z
Bus
Figure 4.6  ♦  Three switching techniques
348         Chapter 4    •    The Network Layer: Data Plane
at the same time, even if they have different destination ports, since only one 
memory read/write can be done at a time over the shared system bus.
	
Some modern routers switch via memory. A major difference from early routers, 
however, is that the lookup of the destination address and the storing of the packet 
into the appropriate memory location are performed by processing on the input line 
cards. In some ways, routers that switch via memory look very much like shared-
memory multiprocessors, with the processing on a line card switching (writing) 
packets into the memory of the appropriate output port. Cisco’s Catalyst 8500 
series switches [Cisco 8500 2016] internally switches packets via a shared memory.
•	 Switching via a bus. In this approach, an input port transfers a packet directly to the 
output port over a shared bus, without intervention by the routing processor. This is 
typically done by having the input port pre-pend a switch-internal label (header) to 
the packet indicating the local output port to which this packet is being transferred 
and transmitting the packet onto the bus. All output ports receive the packet, but 
only the port that matches the label will keep the packet. The label is then removed 
at the output port, as this label is only used within the switch to cross the bus. If mul-
tiple packets arrive to the router at the same time, each at a different input port, all 
but one must wait since only one packet can cross the bus at a time. Because every 
packet must cross the single bus, the switching speed of the router is limited to the 
bus speed; in our roundabout analogy, this is as if the roundabout could only contain 
one car at a time. Nonetheless, switching via a bus is often sufficient for routers that 
operate in small local area and enterprise networks. The Cisco 6500 router [Cisco 
6500 2016] internally switches packets over a 32-Gbps-backplane bus.
•	 Switching via an interconnection network. One way to overcome the bandwidth 
limitation of a single, shared bus is to use a more sophisticated interconnection net-
work, such as those that have been used in the past to interconnect processors in a 
multiprocessor computer architecture. A crossbar switch is an interconnection net-
work consisting of 2N buses that connect N input ports to N output ports, as shown 
in Figure 4.6. Each vertical bus intersects each horizontal bus at a crosspoint, 
which can be opened or closed at any time by the switch fabric controller (whose 
logic is part of the switching fabric itself). When a packet arrives from port A and 
needs to be forwarded to port Y, the switch controller closes the crosspoint at the 
intersection of busses A and Y, and port A then sends the packet onto its bus, which 
is picked up (only) by bus Y. Note that a packet from port B can be forwarded to 
port X at the same time, since the A-to-Y and B-to-X packets use different input 
and output busses. Thus, unlike the previous two switching approaches, cross-
bar switches are capable of forwarding multiple packets in parallel. A crossbar 
switch is non-blocking—a packet being forwarded to an output port will not be 
blocked from reaching that output port as long as no other packet is currently being 
forwarded to that output port. However, if two packets from two different input 
ports are destined to that same output port, then one will have to wait at the input, 
since only one packet can be sent over any given bus at a time. Cisco 12000 series 
4.2    •    What’s Inside a Router?         349
switches [Cisco 12000 2016] use a crossbar switching network; the Cisco 7600 
series can be configured to use either a bus or crossbar switch [Cisco 7600 2016].
	
More sophisticated interconnection networks use multiple stages of switching 
elements to allow packets from different input ports to proceed towards the same 
output port at the same time through the multi-stage switching fabric. See [Tobagi 
1990] for a survey of switch architectures. The Cisco CRS employs a three-stage 
non-blocking switching strategy. A router’s switching capacity can also be scaled 
by running multiple switching fabrics in parallel. In this approach, input ports 
and output ports are connected to N switching fabrics that operate in parallel. An 
input port breaks a packet into K smaller chunks, and sends (“sprays”) the chunks 
through K of these N switching fabrics to the selected output port, which reas-
sembles the K chunks back into the original packet.
4.2.3 Output Port Processing
Output port processing, shown in Figure 4.7, takes packets that have been stored 
in the output port’s memory and transmits them over the output link. This includes 
selecting and de-queueing packets for transmission, and performing the needed link-
layer and physical-layer transmission functions.
4.2.4 Where Does Queuing Occur?
If we consider input and output port functionality and the configurations shown 
 
in Figure 4.6, it’s clear that packet queues may form at both the input ports and the 
output ports, just as we identified cases where cars may wait at the inputs and out-
puts of the traffic intersection in our roundabout analogy. The location and extent of 
queueing (either at the input port queues or the output port queues) will depend on 
the traffic load, the relative speed of the switching fabric, and the line speed. Let’s 
now consider these queues in a bit more detail, since as these queues grow large, the 
router’s memory can eventually be exhausted and packet loss will occur when no 
memory is available to store arriving packets. Recall that in our earlier ­
discussions, 
we said that packets were “lost within the network” or “dropped at a router.” It is here, 
at these queues within a router, where such packets are actually dropped and lost.
Line
termination
Data link
processing
(protocol,
encapsulation)
Queuing (buffer
management)
Switch
fabric
Figure 4.7  ♦  Output port processing
350         Chapter 4    •    The Network Layer: Data Plane
Suppose that the input and output line speeds (transmission rates) all have an 
identical transmission rate of Rline packets per second, and that there are N input ports 
and N output ports. To further simplify the discussion, let’s assume that all packets 
have the same fixed length, and that packets arrive to input ports in a synchronous 
manner. That is, the time to send a packet on any link is equal to the time to receive a 
packet on any link, and during such an interval of time, either zero or one packets can 
arrive on an input link. Define the switching fabric transfer rate Rswitch as the rate at 
which packets can be moved from input port to output port. If Rswitch is N times faster 
than Rline, then only negligible queuing will occur at the input ports. This is because 
even in the worst case, where all N input lines are receiving packets, and all packets 
are to be forwarded to the same output port, each batch of N packets (one packet per 
input port) can be cleared through the switch fabric before the next batch arrives.
Input Queueing
But what happens if the switch fabric is not fast enough (relative to the input line 
speeds) to transfer all arriving packets through the fabric without delay? In this case, 
packet queuing can also occur at the input ports, as packets must join input port 
queues to wait their turn to be transferred through the switching fabric to the output 
port. To illustrate an important consequence of this queuing, consider a crossbar 
switching fabric and suppose that (1) all link speeds are identical, (2) that one packet 
can be transferred from any one input port to a given output port in the same amount 
of time it takes for a packet to be received on an input link, and (3) packets are moved 
from a given input queue to their desired output queue in an FCFS manner. Multiple 
packets can be transferred in parallel, as long as their output ports are different. How-
ever, if two packets at the front of two input queues are destined for the same output 
queue, then one of the packets will be blocked and must wait at the input queue—the 
switching fabric can transfer only one packet to a given output port at a time.
Figure 4.8 shows an example in which two packets (darkly shaded) at the front 
of their input queues are destined for the same upper-right output port. Suppose that 
the switch fabric chooses to transfer the packet from the front of the upper-left queue. 
In this case, the darkly shaded packet in the lower-left queue must wait. But not only 
must this darkly shaded packet wait, so too must the lightly shaded packet that is 
queued behind that packet in the lower-left queue, even though there is no conten-
tion for the middle-right output port (the destination for the lightly shaded packet). 
This phenomenon is known as head-of-the-line (HOL) blocking in an input-queued 
switch—a queued packet in an input queue must wait for transfer through the fabric 
(even though its output port is free) because it is blocked by another packet at the 
head of the line. [Karol 1987] shows that due to HOL blocking, the input queue will 
grow to unbounded length (informally, this is equivalent to saying that significant 
packet loss will occur) under certain assumptions as soon as the packet arrival rate 
on the input links reaches only 58 percent of their capacity. A number of solutions to 
HOL blocking are discussed in [McKeown 1997].
4.2    •    What’s Inside a Router?         351
Output Queueing
Let’s next consider whether queueing can occur at a switch’s output ports. Suppose 
that Rswitch is again N times faster than Rline and that packets arriving at each of the N 
input ports are destined to the same output port. In this case, in the time it takes to send a 
 
single packet onto the outgoing link, N new packets will arrive at this output port 
(one from each of the N input ports). Since the output port can transmit only a single 
packet in a unit of time (the packet transmission time), the N arriving packets will 
have to queue (wait) for transmission over the outgoing link. Then N more packets 
can possibly arrive in the time it takes to transmit just one of the N packets that had 
just previously been queued. And so on. Thus, packet queues can form at the output 
ports even when the switching fabric is N times faster than the port line speeds. 
Eventually, the number of queued packets can grow large enough to exhaust avail-
able memory at the output port.
Switch
fabric
Output port contention at time t —
one dark packet can be transferred
Light blue packet experiences HOL blocking
Switch
fabric
Key:
destined for upper output 
port
destined for middle output 
port
destined for lower output 
port
Figure 4.8  ♦  HOL blocking at and input-queued switch
352         Chapter 4    •    The Network Layer: Data Plane
When there is not enough memory to buffer an incoming packet, a decision must 
be made to either drop the arriving packet (a policy known as drop-tail) or remove 
one or more already-queued packets to make room for the newly arrived packet. In 
some cases, it may be advantageous to drop (or mark the header of) a packet before 
the buffer is full in order to provide a congestion signal to the sender. A number of 
proactive packet-dropping and -marking policies (which collectively have become 
known as active queue management (AQM) algorithms) have been proposed and 
analyzed [Labrador 1999, Hollot 2002]. One of the most widely studied and imple-
mented AQM algorithms is the Random Early Detection (RED) algorithm [Chris-
tiansen 2001; Floyd 2016].
Output port queuing is illustrated in Figure 4.9. At time t, a packet has arrived 
at each of the incoming input ports, each destined for the uppermost outgoing port. 
Assuming identical line speeds and a switch operating at three times the line speed, one 
time unit later (that is, in the time needed to receive or send a packet), all three original 
packets have been transferred to the outgoing port and are queued awaiting transmis-
sion. In the next time unit, one of these three packets will have been transmitted over the 
outgoing link. In our example, two new packets have arrived at the incoming side of the 
switch; one of these packets is destined for this uppermost output port. A consequence 
Switch
fabric
Output port contention at time t
One packet time later
Switch
fabric
Figure 4.9  ♦  Output port queueing
4.2    •    What’s Inside a Router?         353
of such queuing is that a packet scheduler at the output port must choose one packet, 
among those queued, for transmission—a topic we’ll cover in the following section.
Given that router buffers are needed to absorb the fluctuations in traffic load, a 
natural question to ask is how much buffering is required. For many years, the rule of 
thumb [RFC 3439] for buffer sizing was that the amount of buffering (B) should be 
equal to an average round-trip time (RTT, say 250 msec) times the link capacity (C). 
This result is based on an analysis of the queueing dynamics of a relatively small num-
ber of TCP flows [Villamizar 1994]. Thus, a 10 Gbps link with an RTT of 250 msec 
would need an amount of buffering equal to B 5 RTT · C 5 2.5 Gbits of buffers. More 
recent theoretical and experimental efforts [Appenzeller 2004], however, suggest that 
when there are a large number of TCP flows (N) passing through a link, the amount of 
buffering needed is B = RTI # C> 1N. With a large number of flows typically pass-
ing through large backbone router links (see, e.g., [Fraleigh 2003]), the value of N can 
be large, with the decrease in needed buffer size becoming quite significant. [Appen-
zeller 2004; Wischik 2005; Beheshti 2008] provide very readable discussions of the 
buffer-sizing problem from a theoretical, implementation, and operational standpoint.
4.2.5 Packet Scheduling
Let’s now return to the question of determining the order in which queued packets are 
transmitted over an outgoing link. Since you yourself have undoubtedly had to wait in 
long lines on many occasions and observed how waiting customers are served, you’re 
no doubt familiar with many of the queueing disciplines commonly used in routers. 
There is first-come-first-served (FCFS, also known as first-in-first-out, FIFO). The 
British are famous for patient and orderly FCFS queueing at bus stops and in the mar-
ketplace (“Oh, are you queueing?”). Other countries operate on a priority basis, with 
one class of waiting customers given priority service over other waiting customers. 
There is also round-robin queueing, where customers are again divided into classes 
(as in priority queueing) but each class of customer is given service in turn.
First-in-First-Out (FIFO)
Figure 4.10 shows the queuing model abstraction for the FIFO link-scheduling dis-
cipline. Packets arriving at the link output queue wait for transmission if the link is 
currently busy transmitting another packet. If there is not sufficient buffering space 
to hold the arriving packet, the queue’s packet-discarding policy then determines 
whether the packet will be dropped (lost) or whether other packets will be removed 
from the queue to make space for the arriving packet, as discussed above. In our 
 
discussion below, we’ll ignore packet discard. When a packet is completely transmit-
ted over the outgoing link (that is, receives service) it is removed from the queue.
The FIFO (also known as first-come-first-served, or FCFS) scheduling discipline 
selects packets for link transmission in the same order in which they arrived at the 
output link queue. We’re all familiar with FIFO queuing from service centers, where 
354         Chapter 4    •    The Network Layer: Data Plane
arriving customers join the back of the single waiting line, remain in order, and are 
then served when they reach the front of the line. Figure 4.11 shows the FIFO queue in 
operation. Packet arrivals are indicated by numbered arrows above the upper timeline, 
with the number indicating the order in which the packet arrived. Individual packet 
departures are shown below the lower timeline. The time that a packet spends in service 
(being transmitted) is indicated by the shaded rectangle between the two timelines. In 
our examples here, let’s assume that each packet takes three units of time to be transmit-
ted. Under the FIFO discipline, packets leave in the same order in which they arrived. 
Note that after the departure of packet 4, the link remains idle (since packets 1 through 
4 have been transmitted and removed from the queue) until the arrival of packet 5.
Priority Queuing
Under priority queuing, packets arriving at the output link are classified into prior-
ity classes upon arrival at the queue, as shown in Figure 4.12. In practice, a net-
work operator may configure a queue so that packets carrying network management 
information (e.g., as indicated by the source or destination TCP/UDP port number) 
receive priority over user traffic; additionally, real-time voice-over-IP packets might 
receive priority over non-real traffic such as SMTP or IMAP e-mail packets. Each 
Arrivals
Departures
Queue
(waiting area)
Link
(server)
Figure 4.10  ♦  FIFO queueing abstraction
Time
Arrivals
Departures
Packet
in service
Time
1
1
2
3
4
5
2
3
1
t = 0
t = 2
t = 4
t = 6
t = 8
t = 10
t = 12
t = 14
2
3
4
5
4
5
Figure 4.11  ♦  The FIFO queue in operation
4.2    •    What’s Inside a Router?         355
priority class typically has its own queue. When choosing a packet to transmit, the 
priority queuing discipline will transmit a packet from the highest priority class that 
has a nonempty queue (that is, has packets waiting for transmission). The choice 
among packets in the same priority class is typically done in a FIFO manner.
Figure 4.13 illustrates the operation of a priority queue with two priority classes. 
Packets 1, 3, and 4 belong to the high-priority class, and packets 2 and 5 belong to 
the low-priority class. Packet 1 arrives and, finding the link idle, begins transmission. 
During the transmission of packet 1, packets 2 and 3 arrive and are queued in the low- 
and high-priority queues, respectively. After the transmission of packet 1, packet 3 (a 
high-priority packet) is selected for transmission over packet 2 (which, even though 
it arrived earlier, is a low-priority packet). At the end of the transmission of packet 
3, packet 2 then begins transmission. Packet 4 (a high-priority packet) arrives during 
the transmission of packet 2 (a low-priority packet). Under a non-preemptive pri-
ority queuing discipline, the transmission of a packet is not interrupted once it has 
Arrivals
Departures
Low-priority queue
(waiting area)
Classify
High-priority queue
(waiting area)
Link
(server)
Figure 4.12  ♦  The priority queueing model
Arrivals
Departures
Packet
in service
1
1
2
3
4
5
2
3
1
4
5
Time
Time
t = 0
t = 2
t = 4
t = 6
t = 8
t = 10
t = 12
t = 14
2
3
4
5
Figure 4.13  ♦  The priority queue in operation
356         Chapter 4    •    The Network Layer: Data Plane
begun. In this case, packet 4 queues for transmission and begins being transmitted 
after the transmission of packet 2 is completed.
Round Robin and Weighted Fair Queuing (WFQ)
Under the round robin queuing discipline, packets are sorted into classes as with 
priority queuing. However, rather than there being a strict service priority among 
classes, a round robin scheduler alternates service among the classes. In the simplest 
form of round robin scheduling, a class 1 packet is transmitted, followed by a class 
2 packet, followed by a class 1 packet, followed by a class 2 packet, and so on. A 
so-called work-conserving queuing discipline will never allow the link to remain 
idle whenever there are packets (of any class) queued for transmission. A work-
conserving round robin discipline that looks for a packet of a given class but finds 
none will immediately check the next class in the round robin sequence.
Figure 4.14 illustrates the operation of a two-class round robin queue. In this 
example, packets 1, 2, and 4 belong to class 1, and packets 3 and 5 belong to the 
second class. Packet 1 begins transmission immediately upon arrival at the output 
queue. Packets 2 and 3 arrive during the transmission of packet 1 and thus queue for 
transmission. After the transmission of packet 1, the link scheduler looks for a class 2 
packet and thus transmits packet 3. After the transmission of packet 3, the scheduler 
looks for a class 1 packet and thus transmits packet 2. After the transmission of packet 
2, packet 4 is the only queued packet; it is thus transmitted immediately after packet 2.
A generalized form of round robin queuing that has been widely implemented 
in routers is the so-called weighted fair queuing (WFQ) discipline [Demers 1990; 
Parekh 1993; Cisco QoS 2016]. WFQ is illustrated in Figure 4.15. Here, arriving 
packets are classified and queued in the appropriate per-class waiting area. As in 
round robin scheduling, a WFQ scheduler will serve classes in a circular manner—
first serving class 1, then serving class 2, then serving class 3, and then (assuming 
there are three classes) repeating the service pattern. WFQ is also a work-conserving 
Arrivals
Packet
in service
1
1
2
3
4
5
2
3
1
2
3
4
5
4
5
Departures
Time
Time
t = 0
t = 2
t = 4
t = 6
t = 8
t = 10
t = 12
t = 14
Figure 4.14  ♦  The two-class robin queue in operation
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         357
queuing discipline and thus will immediately move on to the next class in the service 
sequence when it finds an empty class queue.
WFQ differs from round robin in that each class may receive a differential amount 
of service in any interval of time. Specifically, each class, i, is assigned a weight, wi. 
Under WFQ, during any interval of time during which there are class i packets to send, 
class i will then be guaranteed to receive a fraction of service equal to wi >(gwj), where 
the sum in the denominator is taken over all classes that also have packets queued for 
transmission. In the worst case, even if all classes have queued packets, class i will still 
be guaranteed to receive a fraction wi >(gwj) of the bandwidth, where in this worst 
case the sum in the denominator is over all classes. Thus, for a link with transmission 
rate R, class i will always achieve a throughput of at least R · wi >(gwj). Our descrip-
tion of WFQ has been idealized, as we have not considered the fact that packets are 
discrete and a packet’s transmission will not be interrupted to begin transmission of 
another packet; [Demers 1990; Parekh 1993] discuss this packetization issue.
4.3	 The Internet Protocol (IP): IPv4, Addressing, 
IPv6, and More
Our study of the network layer thus far in Chapter 4—the notion of the data and con-
trol plane component of the network layer, our distinction between forwarding and 
routing, the identification of various network service models, and our look inside a 
router—have often been without reference to any specific computer network archi-
tecture or protocol. In this section we’ll focus on key aspects of the network layer on 
today’s Internet and the celebrated Internet Protocol (IP).
There are two versions of IP in use today. We’ll first examine the widely 
deployed IP protocol version 4, which is usually referred to simply as IPv4 [RFC 
Classify
Arrivals
Departures
w1
w2
w3
Link
Figure 4.15  ♦  Weighted fair queueing
358         Chapter 4    •    The Network Layer: Data Plane
791] in Section 4.3.1. We’ll examine IP version 6 [RFC 2460; RFC 4291], which has 
been proposed to replace IPv4, in Section 4.3.5. In between, we’ll primarily cover 
Internet addressing—a topic that might seem rather dry and detail-oriented but we’ll 
see is crucial to understanding how the Internet’s network layer works. To master IP 
addressing is to master the Internet’s network layer itself!
4.3.1 IPv4 Datagram Format
Recall that the Internet’s network-layer packet is referred to as a datagram. We begin 
our study of IP with an overview of the syntax and semantics of the IPv4 datagram. 
You might be thinking that nothing could be drier than the syntax and semantics of a 
packet’s bits. Nevertheless, the datagram plays a central role in the Internet—every 
networking student and professional needs to see it, absorb it, and master it. (And 
just to see that protocol headers can indeed be fun to study, check out [Pomeranz 
2010]). The IPv4 datagram format is shown in Figure 4.16. The key fields in the IPv4 
datagram are the following:
•	 Version number. These 4 bits specify the IP protocol version of the datagram. 
By looking at the version number, the router can determine how to interpret the 
remainder of the IP datagram. Different versions of IP use different datagram 
formats. The datagram format for IPv4 is shown in Figure 4.16. The datagram 
format for the new version of IP (IPv6) is discussed in Section 4.3.5.
Version
Type of service
Header
length
Upper-layer
protocol
16-bit Identiﬁer
Time-to-live
13-bit Fragmentation offset
Flags
Datagram length (bytes)
Header checksum
32 bits
32-bit Source IP address
32-bit Destination IP address
Options (if any)
Data
Figure 4.16  ♦  IPv4 datagram format
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         359
•	 Header length. Because an IPv4 datagram can contain a variable number of 
options (which are included in the IPv4 datagram header), these 4 bits are needed 
to determine where in the IP datagram the payload (e.g., the transport-layer seg-
ment being encapsulated in this datagram) actually begins. Most IP datagrams do 
not contain options, so the typical IP datagram has a 20-byte header.
•	 Type of service. The type of service (TOS) bits were included in the IPv4 header 
to allow different types of IP datagrams to be distinguished from each other. For 
example, it might be useful to distinguish real-time datagrams (such as those used 
by an IP telephony application) from non-real-time traffic (for example, FTP). The 
specific level of service to be provided is a policy issue determined and config-
ured by the network administrator for that router. We also learned in Section 3.7.2 
that two of the TOS bits are used for Explicit Congestion ­
Notification.
•	 Datagram length. This is the total length of the IP datagram (header plus data), 
measured in bytes. Since this field is 16 bits long, the theoretical maximum size of 
the IP datagram is 65,535 bytes. However, datagrams are rarely larger than 1,500 
bytes, which allows an IP datagram to fit in the payload field of a maximally sized 
Ethernet frame.
•	 Identifier, flags, fragmentation offset. These three fields have to do with so-called 
IP fragmentation, a topic we will consider shortly. Interestingly, the new version 
of IP, IPv6, does not allow for fragmentation.
•	 Time-to-live. The time-to-live (TTL) field is included to ensure that datagrams 
do not circulate forever (due to, for example, a long-lived routing loop) in the 
network. This field is decremented by one each time the datagram is processed by 
a router. If the TTL field reaches 0, a router must drop that datagram.
•	 Protocol. This field is typically used only when an IP datagram reaches its final 
destination. The value of this field indicates the specific transport-layer protocol 
to which the data portion of this IP datagram should be passed. For example, a 
value of 6 indicates that the data portion is passed to TCP, while a value of 17 indi-
cates that the data is passed to UDP. For a list of all possible values, see [IANA 
Protocol Numbers 2016]. Note that the protocol number in the IP datagram has 
a role that is analogous to the role of the port number field in the transport-layer 
segment. The protocol number is the glue that binds the network and transport 
layers together, whereas the port number is the glue that binds the transport and 
application layers together. We’ll see in Chapter 6 that the link-layer frame also 
has a special field that binds the link layer to the network layer.
•	 Header checksum. The header checksum aids a router in detecting bit errors in 
a received IP datagram. The header checksum is computed by treating each 2 
bytes in the header as a number and summing these numbers using 1s complement 
arithmetic. As discussed in Section 3.3, the 1s complement of this sum, known 
as the Internet checksum, is stored in the checksum field. A router computes the 
header checksum for each received IP datagram and detects an error condition if 
360         Chapter 4    •    The Network Layer: Data Plane
the checksum carried in the datagram header does not equal the computed check-
sum. Routers typically discard datagrams for which an error has been detected. 
Note that the checksum must be recomputed and stored again at each router, since 
the TTL field, and possibly the options field as well, will change. An interesting 
discussion of fast algorithms for computing the Internet checksum is [RFC 1071]. 
A question often asked at this point is, why does TCP/IP perform error checking at 
both the transport and network layers? There are several reasons for this repetition. 
First, note that only the IP header is checksummed at the IP layer, while the TCP/
UDP checksum is computed over the entire TCP/UDP segment. Second, TCP/
UDP and IP do not necessarily both have to belong to the same protocol stack. 
TCP can, in principle, run over a different network-layer protocol (for example, 
ATM) [Black 1995]) and IP can carry data that will not be passed to TCP/UDP.
•	 Source and destination IP addresses. When a source creates a datagram, it inserts 
its IP address into the source IP address field and inserts the address of the ulti-
mate destination into the destination IP address field. Often the source host deter-
mines the destination address via a DNS lookup, as discussed in Chapter 2. We’ll 
discuss IP addressing in detail in Section 4.3.3.
•	 Options. The options fields allow an IP header to be extended. Header options 
were meant to be used rarely—hence the decision to save overhead by not includ-
ing the information in options fields in every datagram header. However, the 
mere existence of options does complicate matters—since datagram headers can 
be of variable length, one cannot determine a priori where the data field will start. 
Also, since some datagrams may require options processing and others may not, 
the amount of time needed to process an IP datagram at a router can vary greatly. 
These considerations become particularly important for IP processing in high-
performance routers and hosts. For these reasons and others, IP options were not 
included in the IPv6 header, as discussed in Section 4.3.5.
•	 Data (payload). Finally, we come to the last and most important field—the raison 
d’etre for the datagram in the first place! In most circumstances, the data field of 
the IP datagram contains the transport-layer segment (TCP or UDP) to be deliv-
ered to the destination. However, the data field can carry other types of data, such 
as ICMP messages (discussed in Section 5.6).
Note that an IP datagram has a total of 20 bytes of header (assuming no options). 
If the datagram carries a TCP segment, then each (non-fragmented) datagram carries 
a total of 40 bytes of header (20 bytes of IP header plus 20 bytes of TCP header) 
along with the application-layer message.
4.3.2 IPv4 Datagram Fragmentation
We’ll see in Chapter 6 that not all link-layer protocols can carry network-layer 
packets of the same size. Some protocols can carry big datagrams, whereas other 
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         361
­
protocols can carry only little datagrams. For example, Ethernet frames can carry up to 
1,500 bytes of data, whereas frames for some wide-area links can carry no more than 
576 bytes. The maximum amount of data that a link-layer frame can carry is called 
the maximum transmission unit (MTU). Because each IP datagram is encapsulated 
within the link-layer frame for transport from one router to the next router, the MTU 
of the link-layer protocol places a hard limit on the length of an IP datagram. Having 
a hard limit on the size of an IP datagram is not much of a problem. What is a prob-
lem is that each of the links along the route between sender and destination can use 
different link-layer protocols, and each of these protocols can have different MTUs.
To understand the forwarding issue better, imagine that you are a router that inter-
connects several links, each running different link-layer protocols with different MTUs. 
Suppose you receive an IP datagram from one link. You check your forwarding table to 
determine the outgoing link, and this outgoing link has an MTU that is smaller than the 
length of the IP datagram. Time to panic—how are you going to squeeze this oversized 
IP datagram into the payload field of the link-layer frame? The solution is to fragment 
the payload in the IP datagram into two or more smaller IP datagrams, encapsulate each 
of these smaller IP datagrams in a separate link-layer frame; and send these frames over 
the outgoing link. Each of these smaller datagrams is referred to as a fragment.
Fragments need to be reassembled before they reach the transport layer at the 
destination. Indeed, both TCP and UDP are expecting to receive complete, unfrag-
mented segments from the network layer. The designers of IPv4 felt that reassem-
bling datagrams in the routers would introduce significant complication into the 
protocol and put a damper on router performance. (If you were a router, would you 
want to be reassembling fragments on top of everything else you had to do?) Sticking 
to the principle of keeping the network core simple, the designers of IPv4 decided to 
put the job of datagram reassembly in the end systems rather than in network routers.
When a destination host receives a series of datagrams from the same source, it 
needs to determine whether any of these datagrams are fragments of some original, 
larger datagram. If some datagrams are fragments, it must further determine when 
it has received the last fragment and how the fragments it has received should be 
pieced back together to form the original datagram. To allow the destination host 
to perform these reassembly tasks, the designers of IP (version 4) put identification, 
flag, and fragmentation offset fields in the IP datagram header. When a datagram is 
created, the sending host stamps the datagram with an identification number as well 
as source and destination addresses. Typically, the sending host increments the iden-
tification number for each datagram it sends. When a router needs to fragment a data-
gram, each resulting datagram (that is, fragment) is stamped with the source address, 
destination address, and identification number of the original datagram. When the 
destination receives a series of datagrams from the same sending host, it can examine 
the identification numbers of the datagrams to determine which of the datagrams are 
actually fragments of the same larger datagram. Because IP is an unreliable service, 
one or more of the fragments may never arrive at the destination. For this reason, in 
order for the destination host to be absolutely sure it has received the last fragment of 
362         Chapter 4    •    The Network Layer: Data Plane
the original datagram, the last fragment has a flag bit set to 0, whereas all the other 
fragments have this flag bit set to 1. Also, in order for the destination host to deter-
mine whether a fragment is missing (and also to be able to reassemble the fragments 
in their proper order), the offset field is used to specify where the fragment fits within 
the original IP datagram.
Figure 4.17 illustrates an example. A datagram of 4,000 bytes (20 bytes of IP 
header plus 3,980 bytes of IP payload) arrives at a router and must be forwarded 
to a link with an MTU of 1,500 bytes. This implies that the 3,980 data bytes in the 
original datagram must be allocated to three separate fragments (each of which is 
also an IP datagram). 
The online material for this book, and the problems at the end of this chapter will 
allow you to explore fragmentation in more detail. Also, on this book’s Web site, we 
provide a Java applet that generates fragments. You provide the incoming datagram 
size, the MTU, and the incoming datagram identification. The applet automatically 
generates the fragments for you. See http://www.pearsonglobaleditions.com/kurose.
4.3.3 IPv4 Addressing
We now turn our attention to IPv4 addressing. Although you may be thinking that 
addressing must be a straightforward topic, hopefully by the end of this section you’ll 
Fragmentation:
In: one large datagram (4,000 bytes)
Out: 3 smaller datagrams
Reassembly:
In: 3 smaller datagrams
Out: one large datagram (4,000 bytes)
Link MTU: 1,500 bytes
Figure 4.17  ♦  IP fragmentation and reassembly
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         363
be convinced that Internet addressing is not only a juicy, subtle, and interesting topic 
but also one that is of central importance to the Internet. An excellent treatment of 
IPv4 addressing can be found in the first chapter in [Stewart 1999].
Before discussing IP addressing, however, we’ll need to say a few words about 
how hosts and routers are connected into the Internet. A host typically has only a 
single link into the network; when IP in the host wants to send a datagram, it does 
so over this link. The boundary between the host and the physical link is called 
an interface. Now consider a router and its interfaces. Because a router’s job is to 
receive a datagram on one link and forward the datagram on some other link, a router 
necessarily has two or more links to which it is connected. The boundary between the 
router and any one of its links is also called an interface. A router thus has multiple 
interfaces, one for each of its links. Because every host and router is capable of send-
ing and receiving IP datagrams, IP requires each host and router interface to have 
its own IP address. Thus, an IP address is technically associated with an interface, 
rather than with the host or router containing that interface.
Each IP address is 32 bits long (equivalently, 4 bytes), and there are thus a total 
of 232 (or approximately 4 billion) possible IP addresses. These addresses are typically 
written in so-called dotted-decimal notation, in which each byte of the address is 
written in its decimal form and is separated by a period (dot) from other bytes in the 
address. For example, consider the IP address 193.32.216.9. The 193 is the decimal 
equivalent of the first 8 bits of the address; the 32 is the decimal equivalent of the sec-
ond 8 bits of the address, and so on. Thus, the address 193.32.216.9 in binary notation is
11000001 00100000 11011000 00001001
Each interface on every host and router in the global Internet must have an IP address 
that is globally unique (except for interfaces behind NATs, as discussed in Section 
4.3.4). These addresses cannot be chosen in a willy-nilly manner, however. A portion 
of an interface’s IP address will be determined by the subnet to which it is connected.
Figure 4.18 provides an example of IP addressing and interfaces. In this figure, 
one router (with three interfaces) is used to interconnect seven hosts. Take a close 
look at the IP addresses assigned to the host and router interfaces, as there are sev-
eral things to notice. The three hosts in the upper-left portion of Figure 4.18, and 
the router interface to which they are connected, all have an IP address of the form 
223.1.1.xxx. That is, they all have the same leftmost 24 bits in their IP address. These 
four interfaces are also interconnected to each other by a network that contains no 
routers. This network could be interconnected by an Ethernet LAN, in which case 
the interfaces would be interconnected by an Ethernet switch (as we’ll discuss in 
Chapter 6), or by a wireless access point (as we’ll discuss in Chapter 7). We’ll repre-
sent this routerless network connecting these hosts as a cloud for now, and dive into 
the internals of such networks in Chapters 6 and 7.
In IP terms, this network interconnecting three host interfaces and one router 
interface forms a subnet [RFC 950]. (A subnet is also called an IP network or simply 
364         Chapter 4    •    The Network Layer: Data Plane
a network in the Internet literature.) IP addressing assigns an address to this subnet: 
223.1.1.0/24, where the /24 (“slash-24”) notation, sometimes known as a subnet 
mask, indicates that the leftmost 24 bits of the 32-bit quantity define the subnet 
address. The 223.1.1.0/24 subnet thus consists of the three host interfaces (223.1.1.1, 
223.1.1.2, and 223.1.1.3) and one router interface (223.1.1.4). Any additional hosts 
attached to the 223.1.1.0/24 subnet would be required to have an address of the form 
223.1.1.xxx. There are two additional subnets shown in Figure 4.18: the 223.1.2.0/24 
network and the 223.1.3.0/24 subnet. Figure 4.19 illustrates the three IP subnets pre-
sent in Figure 4.18.
The IP definition of a subnet is not restricted to Ethernet segments that connect 
multiple hosts to a router interface. To get some insight here, consider Figure 4.20, 
which shows three routers that are interconnected with each other by point-to-point 
links. Each router has three interfaces, one for each point-to-point link and one for 
the broadcast link that directly connects the router to a pair of hosts. What subnets 
are present here? Three subnets, 223.1.1.0/24, 223.1.2.0/24, and 223.1.3.0/24, are 
similar to the subnets we encountered in Figure 4.18. But note that there are three 
additional subnets in this example as well: one subnet, 223.1.9.0/24, for the inter-
faces that connect routers R1 and R2; another subnet, 223.1.8.0/24, for the interfaces 
that connect routers R2 and R3; and a third subnet, 223.1.7.0/24, for the interfaces 
that connect routers R3 and R1. For a general interconnected system of routers and 
hosts, we can use the following recipe to define the subnets in the system:
223.1.1.1
223.1.2.1
223.1.2.2
223.1.1.2
223.1.1.4
223.1.2.9
223.1.3.27
223.1.1.3
223.1.3.1
223.1.3.2
Figure 4.18  ♦  Interface addresses and subnets
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         365
To determine the subnets, detach each interface from its host or router, creating 
islands of isolated networks, with interfaces terminating the end points of the 
isolated networks. Each of these isolated networks is called a subnet.
If we apply this procedure to the interconnected system in Figure 4.20, we get six 
islands or subnets.
From the discussion above, it’s clear that an organization (such as a company or 
academic institution) with multiple Ethernet segments and point-to-point links will 
have multiple subnets, with all of the devices on a given subnet having the same subnet 
address. In principle, the different subnets could have quite different subnet addresses. 
In practice, however, their subnet addresses often have much in common. To understand 
why, let’s next turn our attention to how addressing is handled in the global Internet.
The Internet’s address assignment strategy is known as Classless Interdomain 
Routing (CIDR—pronounced cider) [RFC 4632]. CIDR generalizes the notion of 
subnet addressing. As with subnet addressing, the 32-bit IP address is divided into 
two parts and again has the dotted-decimal form a.b.c.d/x, where x indicates the 
number of bits in the first part of the address.
The x most significant bits of an address of the form a.b.c.d/x constitute the 
network portion of the IP address, and are often referred to as the prefix (or network 
prefix) of the address. An organization is typically assigned a block of contiguous 
addresses, that is, a range of addresses with a common prefix (see the Principles in 
Practice feature). In this case, the IP addresses of devices within the organization 
will share the common prefix. When we cover the Internet’s BGP routing protocol in 
223.1.1.0/24
223.1.2.0/24
223.1.3.0/24
Figure 4.19  ♦  Subnet addresses
366         Chapter 4    •    The Network Layer: Data Plane
Section 5.4, we’ll see that only these x leading prefix bits are considered by routers 
outside the organization’s network. That is, when a router outside the organization 
forwards a datagram whose destination address is inside the organization, only the 
leading x bits of the address need be considered. This considerably reduces the size 
of the forwarding table in these routers, since a single entry of the form a.b.c.d/x will 
be sufficient to forward packets to any destination within the organization.
The remaining 32-x bits of an address can be thought of as distinguishing among the 
devices within the organization, all of which have the same network prefix. These are 
the bits that will be considered when forwarding packets at routers within the organiza-
tion. These lower-order bits may (or may not) have an additional subnetting structure, 
such as that discussed above. For example, suppose the first 21 bits of the CIDRized 
address a.b.c.d/21 specify the organization’s network prefix and are common to the IP 
addresses of all devices in that organization. The remaining 11 bits then identify the 
specific hosts in the organization. The organization’s internal structure might be such 
that these 11 rightmost bits are used for subnetting within the organization, as discussed 
above. For example, a.b.c.d/24 might refer to a specific subnet within the organization.
Before CIDR was adopted, the network portions of an IP address were constrained 
to be 8, 16, or 24 bits in length, an addressing scheme known as classful addressing, 
223.1.8.1
223.1.8.0
223.1.9.1
223.1.7.1
223.1.2.6
223.1.2.1
223.1.2.2
223.1.3.1
223.1.3.2
223.1.1.3
223.1.7.0
223.1.9.2
223.1.3.27
223.1.1.1
223.1.1.4
R1
R2
R3
Figure 4.20  ♦  Three routers interconnecting six subnets
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         367
since subnets with 8-, 16-, and 24-bit subnet addresses were known as class A, B, and 
C networks, respectively. The requirement that the subnet portion of an IP address be 
exactly 1, 2, or 3 bytes long turned out to be problematic for supporting the rapidly 
growing number of organizations with small and medium-sized subnets. A class C (/24) 
subnet could accommodate only up to 28 2 2 5 254 hosts (two of the 28 5 256 addresses 
are reserved for special use)—too small for many organizations. However, a class B 
(/16) subnet, which supports up to 65,634 hosts, was too large. Under classful address-
ing, an organization with, say, 2,000 hosts was typically allocated a class B (/16) subnet 
address. This led to a rapid depletion of the class B address space and poor utilization of 
the assigned address space. For example, the organization that used a class B address for 
its 2,000 hosts was allocated enough of the address space for up to 65,534 interfaces—
leaving more than 63,000 addresses that could not be used by other organizations.
This example of an ISP that connects eight organizations to the Internet nicely illustrates 
how carefully allocated CIDRized addresses facilitate routing. Suppose, as shown in Figure 
4.21, that the ISP (which we’ll call Fly-By-Night-ISP) advertises to the outside world that it 
should be sent any datagrams whose first 20 address bits match 200.23.16.0/20. The 
rest of the world need not know that within the address block 200.23.16.0/20 there are 
in fact eight other organizations, each with its own subnets. This ability to use a single 
prefix to advertise multiple networks is often referred to as address aggregation (also 
route aggregation or route summarization).
Address aggregation works extremely well when addresses are allocated in blocks 
to ISPs and then from ISPs to client organizations. But what happens when addresses 
are not allocated in such a hierarchical manner? What would happen, for example, if 
Fly-By-Night-ISP acquires ISPs-R-Us and then has Organization 1 connect to the Internet 
through its subsidiary ISPs-R-Us? As shown in Figure 4.21, the subsidiary ISPs-R-Us owns 
the address block 199.31.0.0/16, but Organization 1’s IP addresses are unfortunately 
outside of this address block. What should be done here? Certainly, Organization 1 could 
renumber all of its routers and hosts to have addresses within the ISPs-R-Us address block. 
But this is a costly solution, and Organization 1 might well be reassigned to another 
subsidiary in the future. The solution typically adopted is for Organization 1 to keep its 
IP addresses in 200.23.18.0/23. In this case, as shown in Figure 4.22, Fly-By-Night-ISP 
continues to advertise the address block 200.23.16.0/20 and ISPs-R-Us continues to 
advertise 199.31.0.0/16. However, ISPs-R-Us now also advertises the block of addresses 
for Organization 1, 200.23.18.0/23. When other routers in the larger Internet see the 
address blocks 200.23.16.0/20 (from Fly-By-Night-ISP) and 200.23.18.0/23 (from ISPs-
R-Us) and want to route to an address in the block 200.23.18.0/23, they will use longest 
prefix matching (see Section 4.2.1), and route toward ISPs-R-Us, as it advertises the long-
est (i.e., most-specific) address prefix that matches the destination address.
PRINCIPLES IN PRACTICE
368         Chapter 4    •    The Network Layer: Data Plane
Organization 0
200.23.16.0/23
Organization 1
Fly-By-Night-ISP
“Send me anything
  with addresses
  beginning
  200.23.16.0/20”
ISPs-R-Us
200.23.18.0/23
Organization 2
200.23.20.0/23
Organization 7
200.23.30.0/23
Internet
“Send me anything
  with addresses
  beginning
  199.31.0.0/16”
Figure 4.21  ♦  Hierarchical addressing and route aggregation
Organization 0
200.23.16.0/23
Organization 2
Fly-By-Night-ISP
“Send me anything
  with addresses
  beginning
  200.23.16.0/20”
ISPs-R-Us
200.23.20.0/23
Organization 7
200.23.30.0/23
Organization 1
200.23.18.0/23
Internet
“Send me anything
  with addresses
  beginning
  199.31.0.0/16 or
  200.23.18.0/23”
Figure 4.22  ♦  ISPs-R-Us has a more specific route to Organization 1
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         369
We would be remiss if we did not mention yet another type of IP address, the IP 
broadcast address 255.255.255.255. When a host sends a datagram with destination 
address 255.255.255.255, the message is delivered to all hosts on the same subnet. 
Routers optionally forward the message into neighboring subnets as well (although 
they usually don’t).
Having now studied IP addressing in detail, we need to know how hosts and 
subnets get their addresses in the first place. Let’s begin by looking at how an organi-
zation gets a block of addresses for its devices, and then look at how a device (such 
as a host) is assigned an address from within the organization’s block of addresses.
Obtaining a Block of Addresses
In order to obtain a block of IP addresses for use within an organization’s subnet, 
a network administrator might first contact its ISP, which would provide addresses 
from a larger block of addresses that had already been allocated to the ISP. For 
example, the ISP may itself have been allocated the address block 200.23.16.0/20. 
The ISP, in turn, could divide its address block into eight equal-sized contiguous 
address blocks and give one of these address blocks out to each of up to eight organi-
zations that are supported by this ISP, as shown below. (We have underlined the 
subnet part of these addresses for your convenience.)
ISP’s block:         200.23.16.0/20         11001000 00010111 00010000 00000000
Organization 0     200.23.16.0/23         11001000 00010111 00010000 00000000
Organization 1     200.23.18.0/23         11001000 00010111 00010010 00000000
Organization 2     200.23.20.0/23         11001000 00010111 00010100 00000000
    …   …                                                    …
Organization 7     200.23.30.0/23         11001000 00010111 00011110 00000000
While obtaining a set of addresses from an ISP is one way to get a block of 
addresses, it is not the only way. Clearly, there must also be a way for the ISP itself 
to get a block of addresses. Is there a global authority that has ultimate responsibil-
ity for managing the IP address space and allocating address blocks to ISPs and 
other organizations? Indeed there is! IP addresses are managed under the authority 
of the Internet Corporation for Assigned Names and Numbers (ICANN) [ICANN 
2016], based on guidelines set forth in [RFC 7020]. The role of the nonprofit ICANN 
organization [NTIA 1998] is not only to allocate IP addresses, but also to manage 
the DNS root servers. It also has the very contentious job of assigning domain names 
and resolving domain name disputes. The ICANN allocates addresses to regional 
Internet registries (for example, ARIN, RIPE, APNIC, and LACNIC, which together 
form the Address Supporting Organization of ICANN [ASO-ICANN 2016]), and 
handle the allocation/management of addresses within their regions.
370         Chapter 4    •    The Network Layer: Data Plane
Obtaining a Host Address: The Dynamic Host Configuration Protocol
Once an organization has obtained a block of addresses, it can assign individual 
IP addresses to the host and router interfaces in its organization. A system admin-
istrator will typically manually configure the IP addresses into the router (often 
remotely, with a network management tool). Host addresses can also be config-
ured manually, but typically this is done using the Dynamic Host Configuration 
Protocol (DHCP) [RFC 2131]. DHCP allows a host to obtain (be allocated) an 
IP address automatically. A network administrator can configure DHCP so that a 
given host receives the same IP address each time it connects to the network, or a 
host may be assigned a temporary IP address that will be different each time the 
host connects to the network. In addition to host IP address assignment, DHCP also 
allows a host to learn additional information, such as its subnet mask, the address 
of its first-hop router (often called the default gateway), and the address of its local 
DNS server.
Because of DHCP’s ability to automate the network-related aspects of connect-
ing a host into a network, it is often referred to as a plug-and-play or zeroconf 
(zero-configuration) protocol. This capability makes it very attractive to the network 
administrator who would otherwise have to perform these tasks manually! DHCP 
is also enjoying widespread use in residential Internet access networks, enterprise 
networks, and in wireless LANs, where hosts join and leave the network frequently. 
Consider, for example, the student who carries a laptop from a dormitory room to 
a library to a classroom. It is likely that in each location, the student will be con-
necting into a new subnet and hence will need a new IP address at each location. 
DHCP is ideally suited to this situation, as there are many users coming and going, 
and addresses are needed for only a limited amount of time. The value of DHCP’s 
plug-and-play capability is clear, since it’s unimaginable that a system administrator 
would be able to reconfigure laptops at each location, and few students (except those 
taking a computer networking class!) would have the expertise to configure their 
laptops manually.
DHCP is a client-server protocol. A client is typically a newly arriving host 
wanting to obtain network configuration information, including an IP address for 
itself. In the simplest case, each subnet (in the addressing sense of Figure 4.20) will 
have a DHCP server. If no server is present on the subnet, a DHCP relay agent (typi-
cally a router) that knows the address of a DHCP server for that network is needed. 
Figure 4.23 shows a DHCP server attached to subnet 223.1.2/24, with the router 
serving as the relay agent for arriving clients attached to subnets 223.1.1/24 and 
223.1.3/24. In our discussion below, we’ll assume that a DHCP server is available 
on the subnet.
For a newly arriving host, the DHCP protocol is a four-step process, as shown in 
Figure 4.24 for the network setting shown in Figure 4.23. In this figure, yiaddr (as 
in “your Internet address”) indicates the address being allocated to the newly arriving 
client. The four steps are:
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         371
•	 DHCP server discovery. The first task of a newly arriving host is to find a DHCP 
server with which to interact. This is done using a DHCP discover message, 
which a client sends within a UDP packet to port 67. The UDP packet is encap-
sulated in an IP datagram. But to whom should this datagram be sent? The host 
doesn’t even know the IP address of the network to which it is attaching, much 
less the address of a DHCP server for this network. Given this, the DHCP client 
creates an IP datagram containing its DHCP discover message along with the 
broadcast destination IP address of 255.255.255.255 and a “this host” source IP 
address of 0.0.0.0. The DHCP client passes the IP datagram to the link layer, 
which then broadcasts this frame to all nodes attached to the subnet (we will cover 
the details of link-layer broadcasting in Section 6.4).
•	 DHCP server offer(s). A DHCP server receiving a DHCP discover message 
responds to the client with a DHCP offer message that is broadcast to all nodes 
on the subnet, again using the IP broadcast address of 255.255.255.255. (You 
might want to think about why this server reply must also be broadcast). Since 
several DHCP servers can be present on the subnet, the client may find itself in 
the enviable position of being able to choose from among several offers. Each 
223.1.1.1
223.1.1.2
223.1.1.4
223.1.2.9
223.1.3.27
223.1.1.3
223.1.3.1
223.1.3.2
223.1.2.1
223.1.2.5
223.1.2.2
Arriving
DHCP
client
DHCP
server
Figure 4.23  ♦  DHCP client and server
372         Chapter 4    •    The Network Layer: Data Plane
server offer message contains the transaction ID of the received discover mes-
sage, the proposed IP address for the client, the network mask, and an IP address 
lease time—the amount of time for which the IP address will be valid. It is com-
mon for the server to set the lease time to several hours or days [Droms 2002].
•	 DHCP request. The newly arriving client will choose from among one or more 
server offers and respond to its selected offer with a DHCP request message, 
echoing back the configuration parameters.
•	 DHCP ACK. The server responds to the DHCP request message with a DHCP 
ACK message, confirming the requested parameters.
DHCP server:
223.1.2.5
Arriving client
DHCP discover
Time
Time
src: 0.0.0.0, 68
dest: 255.255.255.255,67
DHCPDISCOVER
yiaddr: 0.0.0.0
transaction ID: 654
src: 223.1.2.5, 67
dest: 255.255.255.255,68
DHCPOFFER
yiaddrr: 223.1.2.4
transaction ID: 654
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP offer
src: 223.1.2.5, 67
dest: 255.255.255.255,68
DHCPACK
yiaddrr: 223.1.2.4
transaction ID: 655
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP ACK
src: 0.0.0.0, 68
dest: 255.255.255.255, 67
DHCPREQUEST
yiaddrr: 223.1.2.4
transaction ID: 655
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP request
Figure 4.24  ♦  DHCP client-server interaction
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         373
Once the client receives the DHCP ACK, the interaction is complete and the 
client can use the DHCP-allocated IP address for the lease duration. Since a client 
may want to use its address beyond the lease’s expiration, DHCP also provides a 
mechanism that allows a client to renew its lease on an IP address.
From a mobility aspect, DHCP does have one very significant shortcoming. 
Since a new IP address is obtained from DHCP each time a node connects to a new 
subnet, a TCP connection to a remote application cannot be maintained as a mobile 
node moves between subnets. In Chapter 6, we will examine mobile IP—an exten-
sion to the IP infrastructure that allows a mobile node to use a single permanent 
address as it moves between subnets. Additional details about DHCP can be found in 
[Droms 2002] and [dhc 2016]. An open source reference implementation of DHCP 
is available from the Internet Systems Consortium [ISC 2016].
4.3.4 Network Address Translation (NAT)
Given our discussion about Internet addresses and the IPv4 datagram format, we’re 
now well aware that every IP-capable device needs an IP address. With the prolif-
eration of small office, home office (SOHO) subnets, this would seem to imply that 
whenever a SOHO wants to install a LAN to connect multiple machines, a range of 
addresses would need to be allocated by the ISP to cover all of the SOHO’s IP devices 
(including phones, tablets, gaming devices, IP TVs, printers and more). If the subnet 
grew bigger, a larger block of addresses would have to be allocated. But what if the 
ISP had already allocated the contiguous portions of the SOHO network’s current 
address range? And what typical homeowner wants (or should need) to know how 
to manage IP addresses in the first place? Fortunately, there is a simpler approach 
to address allocation that has found increasingly widespread use in such scenarios: 
network address translation (NAT) [RFC 2663; RFC 3022; Huston 2004, Zhang 
2007; Cisco NAT 2016].
Figure 4.25 shows the operation of a NAT-enabled router. The NAT-enabled 
router, residing in the home, has an interface that is part of the home network on 
the right of Figure 4.25. Addressing within the home network is exactly as we have 
seen above—all four interfaces in the home network have the same subnet address 
of 10.0.0/24. The address space 10.0.0.0/8 is one of three portions of the IP address 
space that is reserved in [RFC 1918] for a private network or a realm with private 
addresses, such as the home network in Figure 4.25. A realm with private addresses 
refers to a network whose addresses only have meaning to devices within that net-
work. To see why this is important, consider the fact that there are hundreds of thou-
sands of home networks, many using the same address space, 10.0.0.0/24. Devices 
within a given home network can send packets to each other using 10.0.0.0/24 
addressing. However, packets forwarded beyond the home network into the larger 
global Internet clearly cannot use these addresses (as either a source or a destina-
tion address) because there are hundreds of thousands of networks using this block 
of addresses. That is, the 10.0.0.0/24 addresses can only have meaning within the 
374         Chapter 4    •    The Network Layer: Data Plane
given home network. But if private addresses only have meaning within a given 
network, how is addressing handled when packets are sent to or received from the 
global Internet, where addresses are necessarily unique? The answer lies in under-
standing NAT.
The NAT-enabled router does not look like a router to the outside world. Instead 
the NAT router behaves to the outside world as a single device with a single IP 
address. In Figure 4.25, all traffic leaving the home router for the larger Internet has 
a source IP address of 138.76.29.7, and all traffic entering the home router must have 
a destination address of 138.76.29.7. In essence, the NAT-enabled router is hiding 
the details of the home network from the outside world. (As an aside, you might 
wonder where the home network computers get their addresses and where the router 
gets its single IP address. Often, the answer is the same—DHCP! The router gets its 
address from the ISP’s DHCP server, and the router runs a DHCP server to provide 
addresses to computers within the NAT-DHCP-router-controlled home network’s 
address space.)
If all datagrams arriving at the NAT router from the WAN have the same desti-
nation IP address (specifically, that of the WAN-side interface of the NAT router), 
then how does the router know the internal host to which it should forward a given 
datagram? The trick is to use a NAT translation table at the NAT router, and to 
include port numbers as well as IP addresses in the table entries.
3
2
10.0.0.1
138.76.29.7
10.0.0.4
10.0.0.2
10.0.0.3
NAT translation table
WAN side
138.76.29.7, 5001
LAN side
10.0.0.1, 3345
. . .
. . .
S = 138.76.29.7, 5001
D = 128.119.40.186, 80 
1
4
S = 128.119.40.186, 80
D = 138.76.29.7, 5001
S = 128.119.40.186, 80
D = 10.0.0.1, 3345 
S = 10.0.0.1, 3345
D = 128.119.40.186, 80
Figure 4.25  ♦  Network address translation
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         375
Consider the example in Figure 4.25. Suppose a user sitting in a home net-
work behind host 10.0.0.1 requests a Web page on some Web server (port 80) 
with IP address 128.119.40.186. The host 10.0.0.1 assigns the (arbitrary) source 
port number 3345 and sends the datagram into the LAN. The NAT router receives 
the datagram, generates a new source port number 5001 for the datagram, replaces 
the source IP address with its WAN-side IP address 138.76.29.7, and replaces the 
original source port number 3345 with the new source port number 5001. When 
generating a new source port number, the NAT router can select any source port 
number that is not currently in the NAT translation table. (Note that because a port 
number field is 16 bits long, the NAT protocol can support over 60,000 simul-
taneous connections with a single WAN-side IP address for the router!) NAT 
in the router also adds an entry to its NAT translation table. The Web server, 
blissfully unaware that the arriving datagram containing the HTTP request has 
been manipulated by the NAT router, responds with a datagram whose destination 
address is the IP address of the NAT router, and whose destination port number is 
5001. When this datagram arrives at the NAT router, the router indexes the NAT 
translation table using the destination IP address and destination port number to 
obtain the appropriate IP address (10.0.0.1) and destination port number (3345) 
for the browser in the home network. The router then rewrites the datagram’s 
destination address and destination port number, and forwards the datagram into 
the home network.
NAT has enjoyed widespread deployment in recent years. But NAT is not with-
out detractors. First, one might argue that, port numbers are meant to be used for 
addressing processes, not for addressing hosts. This violation can indeed cause prob-
lems for servers running on the home network, since, as we have seen in Chapter 2, 
server processes wait for incoming requests at well-known port numbers and peers in 
a P2P protocol need to accept incoming connections when acting as servers. Techni-
cal solutions to these problems include NAT traversal tools [RFC 5389] and Uni-
versal Plug and Play (UPnP), a protocol that allows a host to discover and configure 
a nearby NAT [UPnP Forum 2016].
More “philosophical” arguments have also been raised against NAT by archi-
tectural purists. Here, the concern is that routers are meant to be layer 3 (i.e., net-
work-layer) devices, and should process packets only up to the network layer. NAT 
violates this principle that hosts should be talking directly with each other, without 
interfering nodes modifying IP addresses, much less port numbers. But like it or not, 
NAT has not become an important component of the Internet, as have other so-called 
middleboxes [Sekar 2011] that operate at the network layer but have functions that 
are quite different from routers. Middleboxes do not perform traditional datagram 
forwarding, but instead perform functions such as NAT, load balancing of traffic 
flows, traffic firewalling (see accompanying sidebar), and more. The generalized 
forwarding paradigm that we’ll study shortly in Section 4.4 allows a number of these 
middlebox functions, as well as traditional router forwarding, to be accomplished in 
a common, integrated manner.
376         Chapter 4    •    The Network Layer: Data Plane
4.3.5 IPv6
In the early 1990s, the Internet Engineering Task Force began an effort to develop a 
successor to the IPv4 protocol. A prime motivation for this effort was the realization 
that the 32-bit IPv4 address space was beginning to be used up, with new subnets 
INSPECTING DATAGRAMS: FIREWALLS AND INTRUSION DETECTION  
SYSTEMS
Suppose you are assigned the task of administering a home, departmental, university, or 
corporate network. Attackers, knowing the IP address range of your network, can easily 
send IP datagrams to addresses in your range. These datagrams can do all kinds of devi-
ous things, including mapping your network with ping sweeps and port scans, crashing 
vulnerable hosts with malformed packets, scanning for open TCP/UDP ports on servers 
in your network, and infecting hosts by including malware in the packets. As the network 
administrator, what are you going to do about all those bad guys out there, each capable 
of sending malicious packets into your network? Two popular defense mechanisms to mali-
cious packet attacks are firewalls and intrusion detection systems (IDSs).
As a network administrator, you may first try installing a firewall between your network 
and the Internet. (Most access routers today have firewall capability.) Firewalls inspect the 
datagram and segment header fields, denying suspicious datagrams entry into the internal 
network. For example, a firewall may be configured to block all ICMP echo request pack-
ets (see Section 5.6), thereby preventing an attacker from doing a traditional port scan 
across your IP address range. Firewalls can also block packets based on source and des-
tination IP addresses and port numbers. Additionally, firewalls can be configured to track 
TCP connections, granting entry only to datagrams that belong to approved connections.
Additional protection can be provided with an IDS. An IDS, typically situated at the 
network boundary, performs “deep packet inspection,” examining not only header fields 
but also the payloads in the datagram (including application-layer data). An IDS has a 
database of packet signatures that are known to be part of attacks. This database is auto-
matically updated as new attacks are discovered. As packets pass through the IDS, the 
IDS attempts to match header fields and payloads to the signatures in its signature data-
base. If such a match is found, an alert is created. An intrusion prevention system (IPS) is 
similar to an IDS, except that it actually blocks packets in addition to creating alerts. In 
Chapter 8, we’ll explore firewalls and IDSs in more detail.
Can firewalls and IDSs fully shield your network from all attacks? The answer is clearly 
no, as attackers continually find new attacks for which signatures are not yet available. 
But firewalls and traditional signature-based IDSs are useful in protecting your network 
from known attacks.
Focus on Security
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         377
and IP nodes being attached to the Internet (and being allocated unique IP addresses) 
at a breathtaking rate. To respond to this need for a large IP address space, a new 
IP protocol, IPv6, was developed. The designers of IPv6 also took this opportunity 
to tweak and augment other aspects of IPv4, based on the accumulated operational 
experience with IPv4.
The point in time when IPv4 addresses would be completely allocated (and 
hence no new networks could attach to the Internet) was the subject of considerable 
debate. The estimates of the two leaders of the IETF’s Address Lifetime Expec-
tations working group were that addresses would become exhausted in 2008 and 
2018, respectively [Solensky 1996]. In February 2011, IANA allocated out the last 
remaining pool of unassigned IPv4 addresses to a regional registry. While these reg-
istries still have available IPv4 addresses within their pool, once these addresses are 
exhausted, there are no more available address blocks that can be allocated from a 
central pool [Huston 2011a]. A recent survey of IPv4 address-space exhaustion, and 
the steps taken to prolong the life of the address space is [Richter 2015].
Although the mid-1990s estimates of IPv4 address depletion suggested that 
a considerable amount of time might be left until the IPv4 address space was 
exhausted, it was realized that considerable time would be needed to deploy a new 
technology on such an extensive scale, and so the process to develop IP version 6 
(IPv6) [RFC 2460] was begun [RFC 1752]. (An often-asked question is what hap-
pened to IPv5? It was initially envisioned that the ST-2 protocol would become 
IPv5, but ST-2 was later dropped.) An excellent source of information about IPv6 
is [Huitema 1998].
IPv6 Datagram Format
The format of the IPv6 datagram is shown in Figure 4.26. The most important 
changes introduced in IPv6 are evident in the datagram format:
•	 Expanded addressing capabilities. IPv6 increases the size of the IP address from 
32 to 128 bits. This ensures that the world won’t run out of IP addresses. Now, 
every grain of sand on the planet can be IP-addressable. In addition to unicast and 
multicast addresses, IPv6 has introduced a new type of address, called an anycast 
address, that allows a datagram to be delivered to any one of a group of hosts. 
(This feature could be used, for example, to send an HTTP GET to the nearest of 
a number of mirror sites that contain a given document.)
•	 A streamlined 40-byte header. As discussed below, a number of IPv4 fields have 
been dropped or made optional. The resulting 40-byte fixed-length header allows 
for faster processing of the IP datagram by a router. A new encoding of options 
allows for more flexible options processing.
•	 Flow labeling. IPv6 has an elusive definition of a flow. RFC 2460 states that this 
allows “labeling of packets belonging to particular flows for which the sender 
378         Chapter 4    •    The Network Layer: Data Plane
requests special handling, such as a non-default quality of service or real-time 
service.” For example, audio and video transmission might likely be treated as 
a flow. On the other hand, the more traditional applications, such as file transfer 
and e-mail, might not be treated as flows. It is possible that the traffic carried by a 
high-priority user (for example, someone paying for better service for their traffic) 
 
might also be treated as a flow. What is clear, however, is that the designers of 
IPv6 foresaw the eventual need to be able to differentiate among the flows, even 
if the exact meaning of a flow had yet to be determined.
As noted above, a comparison of Figure 4.26 with Figure 4.16 reveals the sim-
pler, more streamlined structure of the IPv6 datagram. The following fields are 
defined in IPv6:
•	 Version. This 4-bit field identifies the IP version number. Not surprisingly, IPv6 
carries a value of 6 in this field. Note that putting a 4 in this field does not create 
a valid IPv4 datagram. (If it did, life would be a lot simpler—see the discussion 
below regarding the transition from IPv4 to IPv6.)
•	 Traffic class. The 8-bit traffic class field, like the TOS field in IPv4, can be used 
to give priority to certain datagrams within a flow, or it can be used to give pri-
ority to datagrams from certain applications (for example, voice-over-IP) over 
datagrams from other applications (for example, SMTP e-mail).
•	 Flow label. As discussed above, this 20-bit field is used to identify a flow of datagrams.
•	 Payload length. This 16-bit value is treated as an unsigned integer giving the 
number of bytes in the IPv6 datagram following the fixed-length, 40-byte data-
gram header.
Version
Trafﬁc class
Payload length
Next hdr
Hop limit
Flow label
32 bits
Source address
(128 bits)
Destination address
(128 bits)
Data
Figure 4.26  ♦  IPv6 datagram format
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         379
•	 Next header. This field identifies the protocol to which the contents (data field) of 
this datagram will be delivered (for example, to TCP or UDP). The field uses the 
same values as the protocol field in the IPv4 header.
•	 Hop limit. The contents of this field are decremented by one by each router 
that forwards the datagram. If the hop limit count reaches zero, the datagram is 
­
discarded.
•	 Source and destination addresses. The various formats of the IPv6 128-bit address 
are described in RFC 4291.
•	 Data. This is the payload portion of the IPv6 datagram. When the datagram 
reaches its destination, the payload will be removed from the IP datagram and 
passed on to the protocol specified in the next header field.
The discussion above identified the purpose of the fields that are included in the 
IPv6 datagram. Comparing the IPv6 datagram format in Figure 4.26 with the IPv4 
datagram format that we saw in Figure 4.16, we notice that several fields appearing 
in the IPv4 datagram are no longer present in the IPv6 datagram:
•	 Fragmentation/reassembly. IPv6 does not allow for fragmentation and reassem-
bly at intermediate routers; these operations can be performed only by the source 
and destination. If an IPv6 datagram received by a router is too large to be for-
warded over the outgoing link, the router simply drops the datagram and sends a 
“Packet Too Big” ICMP error message (see Section 5.6) back to the sender. The 
sender can then resend the data, using a smaller IP datagram size. Fragmentation 
and reassembly is a time-consuming operation; removing this functionality from 
the routers and placing it squarely in the end systems considerably speeds up IP 
forwarding within the network.
•	 Header checksum. Because the transport-layer (for example, TCP and UDP) and 
link-layer (for example, Ethernet) protocols in the Internet layers perform check-
summing, the designers of IP probably felt that this functionality was sufficiently 
redundant in the network layer that it could be removed. Once again, fast pro-
cessing of IP packets was a central concern. Recall from our discussion of IPv4 
in Section 4.3.1 that since the IPv4 header contains a TTL field (similar to the 
hop limit field in IPv6), the IPv4 header checksum needed to be recomputed at 
every router. As with fragmentation and reassembly, this too was a costly opera-
tion in IPv4.
•	 Options. An options field is no longer a part of the standard IP header. How-
ever, it has not gone away. Instead, the options field is one of the possible next 
headers pointed to from within the IPv6 header. That is, just as TCP or UDP 
protocol headers can be the next header within an IP packet, so too can an 
options field. The removal of the options field results in a fixed-length, 40-byte 
IP header.
380         Chapter 4    •    The Network Layer: Data Plane
Transitioning from IPv4 to IPv6
Now that we have seen the technical details of IPv6, let us consider a very practi-
cal matter: How will the public Internet, which is based on IPv4, be transitioned to 
IPv6? The problem is that while new IPv6-capable systems can be made backward-
compatible, that is, can send, route, and receive IPv4 datagrams, already deployed 
IPv4-capable systems are not capable of handling IPv6 datagrams. Several options 
are possible [Huston 2011b, RFC 4213].
One option would be to declare a flag day—a given time and date when all 
Internet machines would be turned off and upgraded from IPv4 to IPv6. The last 
major technology transition (from using NCP to using TCP for reliable transport 
service) occurred almost 35 years ago. Even back then [RFC 801], when the Internet 
was tiny and still being administered by a small number of “wizards,” it was real-
ized that such a flag day was not possible. A flag day involving billions of devices 
is even more unthinkable today.
The approach to IPv4-to-IPv6 transition that has been most widely adopted in 
practice involves tunneling [RFC 4213]. The basic idea behind tunneling—a key 
concept with applications in many other scenarios beyond IPv4-to-IPv6 transition, 
including wide use in the all-IP cellular networks that we’ll cover in Chapter 7—is 
the following. Suppose two IPv6 nodes (in this example, B and E in Figure 4.27) 
want to interoperate using IPv6 datagrams but are connected to each other by inter-
vening IPv4 routers. We refer to the intervening set of IPv4 routers between two 
IPv6 routers as a tunnel, as illustrated in Figure 4.27. With tunneling, the IPv6 node 
on the sending side of the tunnel (in this example, B) takes the entire IPv6 datagram 
and puts it in the data (payload) field of an IPv4 datagram. This IPv4 datagram is 
then addressed to the IPv6 node on the receiving side of the tunnel (in this exam-
ple, E) and sent to the first node in the tunnel (in this example, C). The intervening 
IPv4 routers in the tunnel route this IPv4 datagram among themselves, just as they 
would any other datagram, blissfully unaware that the IPv4 datagram itself con-
tains a complete IPv6 datagram. The IPv6 node on the receiving side of the tunnel 
eventually receives the IPv4 datagram (it is the destination of the IPv4 datagram!), 
determines that the IPv4 datagram contains an IPv6 datagram (by observing that 
the protocol number field in the IPv4 datagram is 41 [RFC 4213], indicating that 
the IPv4 payload is a IPv6 datagram), extracts the IPv6 datagram, and then routes 
the IPv6 datagram exactly as it would if it had received the IPv6 datagram from a 
directly connected IPv6 neighbor.
We end this section by noting that while the adoption of IPv6 was initially slow 
to take off [Lawton 2001; Huston 2008b], momentum has been building. NIST 
[NIST IPv6 2015] reports that more than a third of US government second-level 
domains are IPv6-enabled. On the client side, Google reports that only about 8 per-
cent of the clients accessing Google services do so via IPv6 [Google IPv6 2015]. But 
other recent measurements [Czyz 2014] indicate that IPv6 adoption is accelerating. 
The proliferation of devices such as IP-enabled phones and other portable devices 
4.3    •    The Internet Protocol (IP): IPv4, Addressing, IPv6, and More         381
provides an additional push for more widespread deployment of IPv6. Europe’s 
Third Generation Partnership Program [3GPP 2016] has specified IPv6 as the stand-
ard addressing scheme for mobile multimedia.
One important lesson that we can learn from the IPv6 experience is that it is enor-
mously difficult to change network-layer protocols. Since the early 1990s, numerous 
new network-layer protocols have been trumpeted as the next major revolution for 
the Internet, but most of these protocols have had limited penetration to date. These 
protocols include IPv6, multicast protocols, and resource reservation protocols; a 
discussion of these latter two protocols can be found in the online supplement to 
this text. Indeed, introducing new protocols into the network layer is like replac-
ing the foundation of a house—it is difficult to do without tearing the whole house 
down or at least temporarily relocating the house’s residents. On the other hand, the 
Internet has witnessed rapid deployment of new protocols at the application layer. 
The classic examples, of course, are the Web, instant messaging, streaming media, 
distributed games, and various forms of social media. Introducing new application-
layer protocols is like adding a new layer of paint to a house—it is relatively easy to 
do, and if you choose an attractive color, others in the neighborhood will copy you. 
A
B
C
D
E
F
IPv6
A to B: IPv6
Physical view
B to C: IPv4
(encapsulating IPv6)
D to E: IPv4
(encapsulating IPv6)
E to F: IPv6
IPv6
IPv4
IPv4
IPv6
IPv6
Flow: X
Source: A
Dest: F
data
Source: B
Dest: E
Source: B
Dest: E
A
B
E
F
IPv6
Logical view
IPv6
Tunnel
IPv6
IPv6
Flow: X
Source: A
Dest: F
data
Flow: X
Source: A
Dest: F
data
Flow: X
Source: A
Dest: F
data
Figure 4.27  ♦  Tunneling
382         Chapter 4    •    The Network Layer: Data Plane
In summary, in the future we can certainly expect to see changes in the Internet’s 
network layer, but these changes will likely occur on a time scale that is much slower 
than the changes that will occur at the application layer.
4.4	 Generalized Forwarding and SDN
In Section 4.2.1, we noted that an Internet router’s forwarding decision has tradition-
ally been based solely on a packet’s destination address. In the previous section, 
however, we’ve also seen that there has been a proliferation of middleboxes that 
perform many layer-3 functions. NAT boxes rewrite header IP addresses and port 
numbers; firewalls block traffic based on  header-field values or redirect packets for 
additional processing, such as deep packet inspection (DPI). Load-balancers forward 
packets requesting a given service (e.g., an HTTP request) to one of a set of a set of 
servers that provide that service. [RFC 3234] lists a number of common middlebox 
functions.
This proliferation of middleboxes, layer-2 switches, and layer-3 routers [Qazi 
2013]—each with its own specialized hardware, software and management inter-
faces—has undoubtedly resulted in costly headaches for many network operators. 
However, recent advances in software-defined networking have promised, and are 
now delivering, a unified approach towards providing many of these network-layer 
functions, and certain link-layer functions as well, in a modern, elegant, and inte-
grated manner.
Recall that Section 4.2.1 characterized destination-based forwarding as the two 
steps of looking up a destination IP address (“match”), then sending the packet into 
the switching fabric to the specified output port (“action”). Let’s now consider a 
significantly more general “match-plus-action” paradigm, where the “match” can 
be made over multiple header fields associated with different protocols at differ-
ent layers in the protocol stack. The “action” can include forwarding the packet to 
one or more output ports (as in destination-based forwarding), load balancing pack-
ets across multiple outgoing interfaces that lead to a service (as in load balancing), 
rewriting header values (as in NAT), purposefully blocking/dropping a packet (as in 
a firewall), sending a packet to a special server for further processing and action (as 
in DPI), and more.
In generalized forwarding, a match-plus-action table generalizes the notion of 
the destination-based forwarding table that we encountered in Section 4.2.1. Because 
forwarding decisions may be made using network-layer and/or link-layer source 
and destination addresses, the forwarding devices shown in Figure 4.28 are more 
accurately described as “packet switches” rather than layer 3 “routers” or layer 2 
“switches.” Thus, in the remainder of this section, and in Section 5.5, we’ll refer 
4.4    •    Generalized Forwarding and SDN         383
to these devices as packet switches, adopting the terminology that is gaining wide-
spread adoption in SDN literature.
Figure 4.28 shows a match-plus-action table in each packet switch, with the 
table being computed, installed, and updated by a remote controller. We note that 
while it is possible for the control components at the individual packet switch to 
interact with each other (e.g., in a manner similar to that in Figure 4.2), in practice 
generalized match-plus-action capabilities are implemented via a remote controller 
that computes, installs, and updates these tables. You might take a minute to compare 
Figures 4.2, 4.3 and 4.28—what similarities and differences do you notice between 
destination-based forwarding shown in Figure 4.2 and 4.3, and generalized forward-
ing shown in Figure 4.28?
1101
0100
Remote Controller
Values in arriving
packet’s header
1
2
3
Local ﬂow table
...
...
...
...
...
...
...
...
...
...
...
...
Headers Counters Actions
Control plane
Data plane
Figure 4.28  ♦  
Generalized forwarding: Each packet switch contains a match-plus-action  
table that is computed and distributed by a remote controller
384         Chapter 4    •    The Network Layer: Data Plane
Our following discussion of generalized forwarding will be based on OpenFlow 
[McKeown 2008, OpenFlow 2009, Casado 2014, Tourrilhes 2014]—a highly vis-
ible and successful standard that has pioneered the notion of the match-plus-action 
forwarding abstraction and controllers, as well as the SDN revolution more gener-
ally [Feamster 2013]. We’ll primarily consider OpenFlow 1.0, which introduced key 
SDN abstractions and functionality in a particularly clear and concise manner. Later 
versions of OpenFlow introduced additional capabilities as a result of experience 
gained through implementation and use; current and earlier versions of the Open-
Flow standard can be found at [ONF 2016].
Each entry in the match-plus-action forwarding table, known as a flow table in 
OpenFlow, includes:
•	 A set of header field values to which an incoming packet will be matched. As in 
the case of destination-based forwarding, hardware-based matching is most rap-
idly performed in TCAM memory, with more than a million destination address 
entries being possible [Bosshart 2013]. A packet that matches no flow table entry 
can be dropped or sent to the remote controller for more processing. In practice, 
a flow table may be implemented by multiple flow tables for performance or cost 
reasons [Bosshart 2013], but we’ll focus here on the abstraction of a single flow 
table.
•	 A set of counters that are updated as packets are matched to flow table entries. 
These counters might include the number of packets that have been matched by 
that table entry, and the time since the table entry was last updated.
•	 A set of actions to be taken when a packet matches a flow table entry. These 
actions might be to forward the packet to a given output port, to drop the packet, 
makes copies of the packet and sent them to multiple output ports, and/or to 
rewrite selected header fields.
We’ll explore matching and actions in more detail in Sections 4.4.1 and 4.4.2, 
respectively. We’ll then study how the network-wide collection of per-packet switch 
matching rules can be used to implement a wide range of functions including routing, 
layer-2 switching, firewalling, load-balancing, virtual networks, and more in Sec-
tion 4.4.3. In closing, we note that the flow table is essentially an API, the abstrac-
tion through which an individual packet switch’s behavior can be programmed; 
we’ll see in Section 4.4.3 that network-wide behaviors can similarly be programmed 
by appropriately programming/configuring these tables in a collection of network 
packet switches [Casado 2014].
4.4.1 Match
Figure 4.29 shows the eleven packet-header fields and the incoming port ID 
that can be matched in an OpenFlow 1.0 match-plus-action rule. Recall from 
4.4    •    Generalized Forwarding and SDN         385
Section 1.5.2 that a link-layer (layer 2) frame arriving to a packet switch will 
contain a network-layer (layer 3) datagram as its payload, which in turn will typi-
cally contain a transport-layer (layer 4) segment. The first observation we make 
is that OpenFlow’s match abstraction allows for a match to be made on selected 
fields from three layers of protocol headers (thus rather brazenly defying the lay-
ering principle we studied in Section 1.5). Since we’ve not yet covered the link 
layer, suffice it to say that the source and destination MAC addresses shown in 
Figure 4.29 are the link-layer addresses associated with the frame’s sending and 
receiving interfaces; by forwarding on the basis of Ethernet addresses rather than 
IP addresses, we can see that an OpenFlow-enabled device can equally perform 
as a router (layer-3 device) forwarding datagrams as well as a switch (layer-2 
device) forwarding frames. The Ethernet type field corresponds to the upper layer 
protocol (e.g., IP) to which the frame’s payload will be de-multiplexed, and the 
VLAN fields are concerned with so-called virtual local area networks that we’ll 
study in Chapter 6. The set of twelve values that can be matched in the OpenFlow 
1.0 specification has grown to 41 values in more recent OpenFlow specifications 
[Bosshart 2014].
The ingress port refers to the input port at the packet switch on which a packet 
is received. The packet’s IP source address, IP destination address, IP protocol field, 
and IP type of service fields were discussed earlier in Section 4.3.1. The transport-layer 
source and destination port number fields can also be matched.
Flow table entries may also have wildcards. For example, an IP address of 
128.119.*.* in a flow table will match the corresponding address field of any data-
gram that has 128.119 as the first 16 bits of its address. Each flow table entry also has 
an associated priority. If a packet matches multiple flow table entries, the selected 
match and corresponding action will be that of the highest priority entry with which 
the packet matches.
Lastly, we observe that not all fields in an IP header can be matched. For exam-
ple OpenFlow does not allow matching on the basis of TTL field or datagram length 
field. Why are some fields allowed for matching, while others are not? Undoubtedly, 
the answer has to do with the tradeoff between functionality and complexity. The 
“art” in choosing an abstraction is to provide for enough functionality to accomplish 
a task (in this case to implement, configure, and manage a wide range of network-
layer functions that had previously been implemented through an assortment of 
Ingress
Port
Src
MAC
Dst
MAC
Eth
Type
VLAN
ID
VLAN
Pri
IP Src
IP Dst
IP
Proto
IP
TOS
TCP/UDP
Src Port
TCP/UDP
Dst Port
Transport layer
Network layer
Link layer
Figure 4.29  ♦  Packet matching fields, OpenFlow 1.0 flow table
386         Chapter 4    •    The Network Layer: Data Plane
network-layer devices), without over-burdening the abstraction with so much detail 
and generality that it becomes bloated and unusable. Butler Lampson has famously 
noted [Lampson 1983]:
Do one thing at a time, and do it well. An interface should capture the minimum 
essentials of an abstraction. Don’t generalize; generalizations are generally 
wrong.
Given OpenFlow’s success, one can surmise that its designers indeed chose their 
abstraction well. Additional details of OpenFlow matching can be found in [Open-
Flow 2009, ONF 2016].
4.4.2 Action
As shown in Figure 4.28, each flow table entry has a list of zero or more actions 
that determine the processing that is to be applied to a packet that matches a flow 
table entry. If there are multiple actions, they are performed in the order specified 
in the list.
Among the most important possible actions are:
•	 Forwarding. An incoming packet may be forwarded to a particular physical out-
put port, broadcast over all ports (except the port on which it arrived) or multi-
cast over a selected set of ports. The packet may be encapsulated and sent to the 
remote controller for this device. That controller then may (or may not) take some 
action on that packet, including installing new flow table entries, and may return 
the packet to the device for forwarding under the updated set of flow table rules.
•	 Dropping. A flow table entry with no action indicates that a matched packet 
should be dropped.
•	 Modify-field. The values in ten packet header fields (all layer 2, 3, and 4 fields 
shown in Figure 4.29 except the IP Protocol field) may be re-written before the 
packet is forwarded to the chosen output port.
4.4.3 OpenFlow Examples of Match-plus-action in Action
Having now considered both the match and action components of generalized 
forwarding, let’s put these ideas together in the context of the sample network 
shown in Figure 4.30. The network has 6 hosts (h1, h2, h3, h4, h5 and h6) and 
three packet switches (s1, s2 and s3), each with four local interfaces (numbered 
1 through 4). We’ll consider a number of network-wide behaviors that we’d like 
to implement, and the flow table entries in s1, s2 and s3 needed to implement this 
behavior.
4.4    •    Generalized Forwarding and SDN         387
A First Example: Simple Forwarding
As a very simple example, suppose that the desired forwarding behavior is that 
 
packets from h5 or h6 destined to h3 or h4 are to be forwarded from s3 to s1, and then 
from s1 to s2 (thus completely avoiding the use of the link between s3 and s2). The 
flow table entry in s1 would be:
s1 Flow Table (Example 1)
Match
Action
Ingress Port = 1 ; IP Src = 10.3.*.* ; IP Dst = 10.2.*.*
Forward(4)
…
…
Of course, we’ll also need a flow table entry in s3 so that datagrams sent from 
h5 or h6 are forwarded to s1 over outgoing interface 3:
s3 Flow Table (Example 1)
Match
Action
IP Src = 10.3.*.* ; IP Dst = 10.2.*.*
Forward(3)
…
…
1
4
s3
s3
s1
s2
2
3
1
2
3
4
Host h6
10.3.0.6
OpenFlow controller
Host h5
10.3.0.5
Host h1
10.1.0.1
Host h2
10.1.0.2
Host h3
10.2.0.3
Host h4
10.2.0.4
1
4
2
3
Figure 4.30  ♦  
OpenFlow match-plus-action network with three packet 
switches, 6 hosts, and an OpenFlow controller
388         Chapter 4    •    The Network Layer: Data Plane
Lastly, we’ll also need a flow table entry in s2 to complete this first example, so 
that datagrams arriving from s1 are forwarded to their destination, either host h3 or h4:
s2 Flow Table (Example 1)
Match
Action
Ingress port = 2 ; IP Dst = 10.2.0.3
Forward(3)
Ingress port = 2 ; IP Dst = 10.2.0.4
Forward(4)
…
…
A Second Example: Load Balancing
As a second example, let’s consider a load-balancing scenario, where datagrams from 
h3 destined to 10.1.*.* are to be forwarded over the direct link between s2 and s1, while 
datagrams from h4 destined to 10.1.*.* are to be forwarded over the link between s2 
and s3 (and then from s3 to s1). Note that this behavior couldn’t be achieved with IP’s 
destination-based forwarding. In this case, the flow table in s2 would be:
s2 Flow Table (Example 2)
Match
Action
Ingress port = 3; IP Dst = 10.1.*.*
Forward(2)
Ingress port = 4; IP Dst = 10.1.*.*
Forward(1)
…
…
Flow table entries are also needed at s1 to forward the datagrams received from 
s2 to either h1 or h2; and flow table entries are needed at s3 to forward datagrams 
received on interface 4 from s2 over interface 3 towards s1. See if you can figure out 
these flow table entries at s1 and s3.
A Third Example: Firewalling
As a third example, let’s consider a firewall scenario in which s2 wants only to 
receive (on any of its interfaces) traffic sent from hosts attached to s3.
s2 Flow Table (Example 3)
Match
Action
IP Src = 10.3.*.* IP Dst = 10.2.0.3
Forward(3)
IP Src = 10.3.*.* IP Dst = 10.2.0.4
Forward(4)
…
…
Homework Problems and Questions         389
If there were no other entries in s2’s flow table, then only traffic from 10.3.*.* would 
be forwarded to the hosts attached to s2.
Although we’ve only considered a few basic scenarios here, the versatility and 
advantages of generalized forwarding are hopefully apparent. In homework prob-
lems, we’ll explore how flow tables can be used to create many different logical 
behaviors, including virtual networks—two or more logically separate networks 
(each with their own independent and distinct forwarding behavior)—that use the 
same physical set of packet switches and links. In Section 5.5, we’ll return to flow 
tables when we study the SDN controllers that compute and distribute the flow tables, 
and the protocol used for communicating between a packet switch and its controller.
4.5	 Summary
In this chapter we’ve covered the data plane functions of the network layer—the per-
router functions that determine how packets arriving on one of a router’s input links 
are forwarded to one of that router’s output links. We began by taking a detailed look 
at the internal operations of a router, studying input and output port functionality and 
destination-based forwarding, a router’s internal switching mechanism, packet queue 
management and more. We covered both traditional IP forwarding (where forward-
ing is based on a datagram’s destination address) and generalized forwarding (where 
forwarding and other functions may be performed using values in several different 
fields in the datagram’s header) and seen the versatility of the latter approach.  We 
also studied the IPv4 and IPv6 protocols in detail, and Internet addressing, which we 
found to be much deeper, subtler, and more interesting than we might have expected.
With our newfound understanding of the network-layer’s data plane, we’re now 
ready to dive into the network layer’s control plane in Chapter 5!
Homework Problems and Questions
Chapter 4 Review Questions
SECTION 4.1
	R1.	 Let’s review some of the terminology used in this textbook. Recall that the 
name of a transport-layer packet is segment and that the name of a link-layer 
packet is frame. What is the name of a network-layer packet? Recall that both 
routers and link-layer switches are called packet switches. What is the funda-
mental difference between a router and link-layer switch?
	R2.	 We noted that network layer functionality can be broadly divided into  
data plane functionality and control plane functionality. What are the main 
functions of the data plane? Of the control plane?
390         Chapter 4    •    The Network Layer: Data Plane
	R3.	 We made a distinction between the forwarding function and the routing func-
tion performed in the network layer. What are the key differences between 
routing and forwarding?
	R4.	 What is the role of the forwarding table within a router?
	R5.	 We said that a network layer’s service model “defines the characteristics of 
end-to-end transport of packets between sending and receiving hosts.” What is 
the service model of the Internet’s network layer? What guarantees are made by 
the Internet’s service model regarding the host-to-host delivery of datagrams?
SECTION 4.2
	R6.	 In Section 4.2, we saw that a router typically consists of input ports, output 
ports, a switching fabric and a routing processor. Which of these are imple-
mented in hardware and which are implemented in software? Why? Return-
ing to the notion of the network layer’s data plane and control plane, which 
are implemented in hardware and which are implemented in software? Why?
	R7.	 What does each input port of a high speed router store to facilitate fast for-
warding decisions?
	R8.	 What is meant by destination-based forwarding? How does this differ from 
generalized forwarding (assuming you’ve read Section 4.4, which of the two 
approaches are adopted by Software-Defined Networking)?
	R9.	 Suppose that an arriving packet matches two or more entries in a router’s 
forwarding table. With traditional destination-based forwarding, what rule 
does a router apply to determine which of these rules should be applied to 
determine the output port to which the arriving packet should be switched?
	
R10.	 Switching in a router forwards data from an input port to an output port. 
What is the advantage of switching via an interconnection network over 
switching via memory and switching via bus?
	
R11.	 What is the role of a packet scheduler at the output port of a router?
	
R12.	 What is a drop-tail policy? What are AQM algorithms? Which is the most 
widely studied and implemented AQM algorithm? How does it work?
	
R13.	 What is HOL blocking? Does it occur in input ports or output ports?
	
R14.	 In Section 4.2, we studied FIFO, Priority, Round Robin (RR), and Weighted 
Fair Queueing (WFQ) packet scheduling disciplines? Which of these queueing 
disciplines ensure that all packets depart in the order in which they arrived?
	
R15.	 Give an example showing why a network operator might want one class of 
packets to be given priority over another class of packets.
	
R16.	 What is an essential different between RR and WFQ packet scheduling? Is 
there a case (Hint: Consider the WFQ weights) where RR and WFQ will 
behave exactly the same?
Homework Problems and Questions         391
SECTION 4.3
	
R17.	 Suppose Host A sends Host B a TCP segment encapsulated in an IP data-
gram. When Host B receives the datagram, how does the network layer in 
Host B know it should pass the segment (that is, the payload of the datagram) 
to TCP rather than to UDP or to some other upper-layer protocol?
	
R18.	 What field in the IP header can be used to ensure that a packet is forwarded 
through no more than N routers?
	
R19.	 Recall that we saw the Internet checksum being used in both transport-layer 
segment (in UDP and TCP headers, Figures 3.7 and 3.29 respectively) and in 
network-layer datagrams (IP header, Figure 4.16). Now consider a transport 
layer segment encapsulated in an IP datagram. Are the checksums in the seg-
ment header and datagram header computed over any common bytes in the IP 
datagram? Explain your answer.
	
R20.	 When a large datagram is fragmented into multiple smaller datagrams, where 
are these smaller datagrams reassembled into a single larger datagram?
	
R21.	 A router has eight interfaces. How many IP addresses will it have?
	
R22.	 What is the 32-bit binary equivalent of the IP address 202.3.14.25?
	
R23.	 Visit a host that uses DHCP to obtain its IP address, network mask, default 
router, and IP address of its local DNS server. List these values.
	
R24.	 Suppose there are four routers between a source host and a destination host. 
Ignoring fragmentation, an IP datagram sent from the source host to the 
destination host will travel over how many interfaces? How many forward-
ing tables will be indexed to move the datagram from the source to the 
destination?
	
R25.	 Suppose an application generates chunks of 40 bytes of data every 20 msec, 
and each chunk gets encapsulated in a TCP segment and then an IP datagram. 
What percentage of each datagram will be overhead, and what percentage 
will be application data?
	
R26.	 Suppose you purchase a wireless router and connect it to your cable modem. 
Also suppose that your ISP dynamically assigns your connected device (that 
is, your wireless router) one IP address. Also suppose that you have five PCs 
at home that use 802.11 to wirelessly connect to your wireless router. How 
are IP addresses assigned to the five PCs? Does the wireless router use NAT? 
Why or why not?
	
R27.	 What is meant by the term “route aggregation”? Why is it useful for a router 
to perform route aggregation?
	
R28.	 What is meant by a “plug-and-play” or “zeroconf” protocol?
	
R29.	 What is a private network address? Should a datagram with a private network 
address ever be present in the larger public Internet? Explain.
392         Chapter 4    •    The Network Layer: Data Plane
	
R30.	 Compare and contrast the IPv4 and the IPv6 header fields. Do they have any 
fields in common?
	
R31.	 It has been said that when IPv6 tunnels through IPv4 routers, IPv6 treats the 
IPv4 tunnels as link-layer protocols. Do you agree with this statement? Why 
or why not?
SECTION 4.4
	
R32.	 How does generalized forwarding differ from destination-based 
­
forwarding?
	
R33.	 What is the difference between a forwarding table that we encountered in 
destination-based forwarding in Section 4.1 and OpenFlow’s flow table that 
we encountered in Section 4.4?
	
R34.	 What is meant by the “match plus action” operation of a router or switch? In 
the case of destination-based forwarding packet switch, what is matched and 
what is the action taken? In the case of an SDN, name three fields that can be 
matched, and three actions that can be taken. 
	
R35.	 Name three header fields in an IP datagram that can be “matched” in Open-
Flow 1.0 generalized forwarding. What are three IP datagram header fields 
that cannot be “matched” in OpenFlow?
Problems
	 P1.	 Consider the network below.
a.	 Show the forwarding table in router A, such that all traffic destined to host 
H3 is forwarded through interface 3.
b.	 Can you write down a forwarding table in router A, such that all traffic 
from H1 destined to host H3 is forwarded through interface 3, while all 
traffic from H2 destined to host H3 is forwarded through interface 4? 
(Hint: This is a trick question.)
B
A
1
3
2
4
2
D
1
2
3
H3
H1
H2
1
1
2
C
Problems         393
	 P2.	 Suppose two packets arrive to two different input ports of a router at exactly 
the same time. Also suppose there are no other packets anywhere in the 
router.
a.	 Suppose the two packets are to be forwarded to two different output ports. 
Is it possible to forward the two packets through the switch fabric at the 
same time when the fabric uses a shared bus?
b.	 Suppose the two packets are to be forwarded to two different output ports. 
Is it possible to forward the two packets through the switch fabric at the 
same time when the fabric uses switching via memory?
c.	 Suppose the two packets are to be forwarded to the same output port. Is it 
possible to forward the two packets through the switch fabric at the same 
time when the fabric uses a crossbar?
	 P3.	 In Section 4.2, we noted that the maximum queuing delay is (n–1)D if the 
switching fabric is n times faster than the input line rates. Suppose that all 
packets are of the same length, n packets arrive at the same time to the n 
input ports, and all n packets want to be forwarded to different output ports. 
What is the maximum delay for a packet for the (a) memory, (b) bus, and  
(c) crossbar switching fabrics?
	P4.	 Consider the switch shown below. Suppose that all datagrams have the same 
fixed length, that the switch operates in a slotted, synchronous manner, 
and that in one time slot a datagram can be transferred from an input port 
to an output port. The switch fabric is a crossbar so that at most one data-
gram can be transferred to a given output port in a time slot, but different 
output ports can receive datagrams from different input ports in a single 
time slot. What is the minimal number of time slots needed to transfer 
the packets shown from input ports to their output ports, assuming any 
input queue scheduling order you want (i.e., it need not have HOL block-
ing)? What is the largest number of slots needed, assuming the worst-case 
scheduling order you can devise, assuming that a non-empty input queue is 
never idle?
X Y
Switch
fabric
Output port X
Output port Y
Output port Z
X
Y
Z
394         Chapter 4    •    The Network Layer: Data Plane
	 P5.	 Consider a datagram network using 32-bit host addresses. Suppose a router 
has four links, numbered 0 through 3, and packets are to be forwarded to the 
link interfaces as follows:
	
Destination Address Range	
Link Interface
	
11100000 00000000 00000000 00000000
	
through	
0
	
11100000 00000000 11111111 11111111
	
11100000 00000001 00000000 00000000
	
through	
1
	
11100000 00000001 11111111 11111111
	
11100000 00000010 00000000 00000000
	
through	
2
	
11100001 11111111 11111111 11111111
	
otherwise	
3
a.	 Provide a forwarding table that has five entries, uses longest prefix match-
ing, and forwards packets to the correct link interfaces.
b.	 Describe how your forwarding table determines the appropriate link inter-
face for datagrams with destination addresses:
	
11111000 10010001 01010001 01010101
	
11100000 00000000 11000011 00111100
	
11100001 10000000 00010001 01110111
	 P6.	 Consider a datagram network using 8-bit host addresses. Suppose a router 
uses longest prefix matching and has the following forwarding table:
Prefix Match
Interface
  00
0
  01
1
100
2
otherwise
3
	
	 For each of the four interfaces, give the associated range of destination host 
addresses and the number of addresses in the range.
Problems         395
	 P7.	 Consider a datagram network using 8-bit host addresses. Suppose a router 
uses longest prefix matching and has the following forwarding table:
Prefix Match
Interface
  11
0
101
1
100
2
otherwise
3
	
	 For each of the four interfaces, give the associated range of destination host 
addresses and the number of addresses in the range.
	 P8.	 Consider a router that interconnects three subnets: Subnet 1, Subnet 2, and  
Subnet 3. Suppose all of the interfaces in each of these three subnets are 
required to have the prefix 223.1.17/24. Also suppose that Subnet 1 is required 
to support up to 62 interfaces, Subnet 2 is to support up to 106 interfaces, and 
Subnet 3 is to support up to 15 interfaces. Provide three network addresses 
(of the form a.b.c.d/x) that satisfy these constraints.
	 P9.	 Suppose there are 35 hosts in a subnet. What should the IP address structure 
look like?
	
P10.	 What is the problem of NAT in P2P applications? How can it be avoided? Is 
there a special name for this solution?
	
P11.	 Consider a subnet with prefix 192.168.56.128/26. Give an example of one 
IP address (of form xxx.xxx.xxx.xxx) that can be assigned to this network. 
Suppose an ISP owns the block of addresses of the form 192.168.56.32/26. 
Suppose it wants to create four subnets from this block, with each block 
having the same number of IP addresses. What are the prefixes (of form 
a.b.c.d/x) for the four subnets?
	
P12.	 Consider the topology shown in Figure 4.20. Denote the three subnets with 
hosts (starting clockwise at 12:00) as Networks A, B, and C. Denote the  
subnets without hosts as Networks D, E, and F.
a.	 Assign network addresses to each of these six subnets, with the following 
constraints: All addresses must be allocated from 214.97.254/23; Subnet A 
should have enough addresses to support 250 interfaces; Subnet B should 
have enough addresses to support 120 interfaces; and Subnet C should 
396         Chapter 4    •    The Network Layer: Data Plane
have enough addresses to support 120 interfaces. Of course, subnets D, E 
and F should each be able to support two interfaces. For each subnet, the 
assignment should take the form a.b.c.d/x or a.b.c.d/x – e.f.g.h/y.
b.	 Using your answer to part (a), provide the forwarding tables (using long-
est prefix matching) for each of the three routers.
	
P13.	 IPsec has been designed to be backward compatible with IPv4 and IPv6. In 
particular, in order to reap the benefits of IPsec, we don’t need to replace the 
protocol stacks in all the routers and hosts in the Internet. For example, using 
the transport mode (one of two IPsec “modes”), if two hosts want to securely 
communicate, IPsec needs to be available only in those two hosts. Discuss 
the services provided by an IPsec session.
	
P14.	 Consider sending a 1,600-byte datagram into a link that has an MTU of 
500 bytes. Suppose the original datagram is stamped with the identification 
number 291. How many fragments are generated? What are the values in the 
various fields in the IP datagram(s) generated related to fragmentation?
	
P15.	 Suppose datagrams are limited to 1,500 bytes (including header) between 
source Host A and destination Host B. Assuming a 20-byte IP header, how 
many datagrams would be required to send an MP3 consisting of 5 million 
bytes? Explain how you computed your answer.
	
P16.	 Consider the network setup in Figure 4.25. Suppose that the ISP instead 
assigns the router the address 24.34.112.235 and that the network address  
of the home network is 192.168.1/24.
a.	 Assign addresses to all interfaces in the home network.
b.	 Suppose each host has two ongoing TCP connections, all to port 80 at 
host 128.119.40.86. Provide the six corresponding entries in the NAT 
translation table.
	
P17.	 Suppose you are interested in detecting the number of hosts behind a NAT. 
You observe that the IP layer stamps an identification number sequentially on 
each IP packet. The identification number of the first IP packet generated by 
a host is a random number, and the identification numbers of the subsequent 
IP packets are sequentially assigned. Assume all IP packets generated by 
hosts behind the NAT are sent to the outside world.
a.	 Based on this observation, and assuming you can sniff all packets sent by 
the NAT to the outside, can you outline a simple technique that detects the 
number of unique hosts behind a NAT? Justify your answer.
b.	 If the identification numbers are not sequentially assigned but randomly 
assigned, would your technique work? Justify your answer.
	
P18.	 In this problem we’ll explore the impact of NATs on P2P applications. 
Suppose a peer with username Arnold discovers through querying that a 
peer with username Bernard has a file it wants to download. Also suppose 
Problems         397
that Bernard and Arnold are both behind a NAT. Try to devise a technique 
that will allow Arnold to establish a TCP connection with Bernard without 
application-specific NAT configuration. If you have difficulty devising such 
a technique, discuss why.
	
P19.	 Consider the SDN OpenFlow network shown in Figure 4.30. Suppose that 
the desired forwarding behavior for datagrams arriving at s2 is as follows:
•	 any datagrams arriving on input port 1 from hosts h5 or h6 that are des-
tined to hosts h1 or h2 should be forwarded over output port 2;
•	 any datagrams arriving on input port 2 from hosts h1 or h2 that are des-
tined to hosts h5 or h6 should be forwarded over output port 1;
•	 any arriving datagrams on input ports 1 or 2 and destined to hosts h3 or h4 
should be delivered to the host specified;
•	 hosts h3 and h4 should be able to send datagrams to each other.
Specify the flow table entries in s2 that implement this forwarding behavior.
	
P20.	 Consider again the SDN OpenFlow network shown in Figure 4.30. Suppose 
that the desired forwarding behavior for datagrams arriving from hosts h3 or 
h4 at s2 is as follows:
•	 any datagrams arriving from host h3 and destined for h1, h2, h5 or h6 
should be forwarded in a clockwise direction in the network;
•	 any datagrams arriving from host h4 and destined for h1, h2, h5 or h6 
should be forwarded in a counter-clockwise direction in the network.
Specify the flow table entries in s2 that implement this forwarding behavior.
	
P21.	 Consider again the scenario from P19 above. Give the flow tables entries at 
packet switches s1 and s3, such that any arriving datagrams with a source 
address of h3 or h4 are routed to the destination hosts specified in the desti-
nation address field in the IP datagram. (Hint: Your forwarding table rules 
should include the cases that an arriving datagram is destined for a directly 
attached host or should be forwarded to a neighboring router for eventual 
host delivery there.)
	
P22.	 Consider again the SDN OpenFlow network shown in Figure 4.30. Suppose 
we want switch s2 to function as a firewall. Specify the flow table in s2 that 
implements the following firewall behaviors (specify a different flow table 
for each of the four firewalling behaviors below) for delivery of datagrams 
destined to h3 and h4. You do not need to specify the forwarding behavior in 
s2 that forwards traffic to other routers.
•	 Only traffic arriving from hosts h1 and h6 should be delivered to hosts h3 
or h4 (i.e., that arriving traffic from hosts h2 and h5 is blocked).
•	 Only TCP traffic is allowed to be delivered to hosts h3 or h4 (i.e., that 
UDP traffic is blocked).
398         Chapter 4    •    The Network Layer: Data Plane
•	 Only traffic destined to h3 is to be delivered (i.e., all traffic to h4 is 
blocked).
•	 Only UDP traffic from h1 and destined to h3 is to be delivered. All other 
traffic is blocked.
Wireshark Lab
In the Web site for this textbook, www.pearsonglobaleditions.com/kurose, you’ll 
find a Wireshark lab assignment that examines the operation of the IP protocol, and 
the IP datagram format in particular.
399
What brought you to specialize in networking?
I was working as a programmer at UCLA in the late 1960s. My job was supported by 
the US Defense Advanced Research Projects Agency (called ARPA then, called DARPA 
now). I was working in the laboratory of Professor Leonard Kleinrock on the Network 
Measurement Center of the newly created ARPAnet. The first node of the ARPAnet was 
installed at UCLA on September 1, 1969. I was responsible for programming a computer 
that was used to capture performance information about the ARPAnet and to report this 
information back for comparison with mathematical models and predictions of the perfor-
mance of the network.
Several of the other graduate students and I were made responsible for working on 
the so-called host-level protocols of the ARPAnet—the procedures and formats that would 
allow many different kinds of computers on the network to interact with each other. It 
was a fascinating exploration into a new world (for me) of distributed computing and 
communication.
Did you imagine that IP would become as pervasive as it is today when you first designed 
the protocol?
When Bob Kahn and I first worked on this in 1973, I think we were mostly very focused on 
the central question: How can we make heterogeneous packet networks interoperate with 
one another, assuming we cannot actually change the networks themselves? We hoped that 
we could find a way to permit an arbitrary collection of packet-switched networks to be 
interconnected in a transparent fashion, so that host computers could communicate end-to-end 
without having to do any translations in between. I think we knew that we were dealing 
Vinton G. Cerf is Vice President and Chief Internet Evangelist for 
Google. He served for over 16 years at MCI in various positions, 
ending up his tenure there as Senior Vice President for Technology 
Strategy. He is widely known as the co-designer of the TCP/IP  
protocols and the architecture of the Internet. During his time from 1976 
 
to 1982 at the US Department of Defense Advanced Research 
Projects Agency (DARPA), he played a key role leading the develop-
ment of Internet and Internet-related data packet and security  
techniques. He received the US Presidential Medal of Freedom in 
2005 and the US National Medal of Technology in 1997. He 
holds a BS in Mathematics from Stanford University and an MS and 
PhD in computer science from UCLA.
Vinton G. Cerf
AN INTERVIEW WITH…
400
with powerful and expandable technology, but I doubt we had a clear image of what the 
world would be like with hundreds of millions of computers all interlinked on the Internet.
What do you now envision for the future of networking and the Internet? What major 
challenges/obstacles do you think lie ahead in their development?
I believe the Internet itself and networks in general will continue to proliferate. Already 
there is convincing evidence that there will be billions of Internet-enabled devices on the 
Internet, including appliances like cell phones, refrigerators, personal digital assistants, home 
servers, televisions, as well as the usual array of laptops, servers, and so on. Big challenges 
include support for mobility, battery life, capacity of the access links to the network, and abil-
ity to scale the optical core of the network up in an unlimited fashion. Designing an interplan-
etary extension of the Internet is a project in which I am deeply engaged at the Jet Propulsion 
Laboratory. We will need to cut over from IPv4 [32-bit addresses] to IPv6 [128 bits].  
The list is long!
Who has inspired you professionally?
My colleague Bob Kahn; my thesis advisor, Gerald Estrin; my best friend, Steve Crocker 
(we met in high school and he introduced me to computers in 1960!); and the thousands of 
engineers who continue to evolve the Internet today.
Do you have any advice for students entering the networking/Internet field?
Think outside the limitations of existing systems—imagine what might be possible; but then 
do the hard work of figuring out how to get there from the current state of affairs. Dare to 
dream: A half dozen colleagues and I at the Jet Propulsion Laboratory have been working 
on the design of an interplanetary extension of the terrestrial Internet. It may take decades 
to implement this, mission by mission, but to paraphrase: “A man’s reach should exceed his 
grasp, or what are the heavens for?”
401
In this chapter, we’ll complete our journey through the network layer by covering the 
control-plane component of the network layer—the network-wide logic that con-
trols not only how a datagram is forwarded among routers along an end-to-end path 
from the source host to the destination host, but also how network-layer components 
and services are configured and managed. In Section 5.2, we’ll cover traditional 
routing algorithms for computing least cost paths in a graph; these algorithms are the 
basis for two widely deployed Internet routing protocols: OSPF and BGP, that we’ll 
cover in Sections 5.3 and 5.4, respectively. As we’ll see, OSPF is a routing protocol 
that operates within a single ISP’s network. BGP is a routing protocol that serves to 
interconnect all of the networks in the Internet; BGP is thus often referred to as the 
“glue” that holds the Internet together. Traditionally, control-plane routing protocols 
have been implemented together with data-plane forwarding functions, monolithi-
cally, within a router. As we learned in the introduction to Chapter 4, software-
defined networking (SDN) makes a clear separation between the data and control 
planes, implementing control-plane functions in a separate “controller” service that 
is distinct, and remote, from the forwarding components of the routers it controls. 
We’ll cover SDN controllers in Section 5.5.
In Sections 5.6 and 5.7 we’ll cover some of the nuts and bolts of managing an 
IP network: ICMP (the Internet Control Message Protocol) and SNMP (the Simple 
Network Management Protocol).
5
Chapter 
The Network 
Layer: Control 
Plane
402         Chapter 5    •    The Network Layer: Control Plane
5.1	 Introduction
Let’s quickly set the context for our study of the network control plane by recall-
ing Figures 4.2 and 4.3. There, we saw that the forwarding table (in the case of 
­
destination-based forwarding) and the flow table (in the case of generalized forward-
ing) were the principal elements that linked the network layer’s data and control 
planes. We learned that these tables specify the local data-plane forwarding behavior 
of a router. We saw that in the case of generalized forwarding, the actions taken (Sec-
tion 4.4.2) could include not only forwarding a packet to a router’s output port, but 
also dropping a packet, replicating a packet, and/or rewriting layer 2, 3 or 4 packet-
header fields.
In this chapter, we’ll study how those forwarding and flow tables are computed, 
maintained and installed. In our introduction to the network layer in Section 4.1, we 
learned that there are two possible approaches for doing so.
•	 Per-router control. Figure 5.1 illustrates the case where a routing algorithm runs 
in each and every router; both a forwarding and a routing function are contained 
Control plane
Data plane
Routing
Algorithm
Forwarding
Table
Figure 5.1  ♦  
Per-router control: Individual routing algorithm components 
interact in the control plane
5.1    •    Introduction         403
within each router. Each router has a routing component that communicates with 
the routing components in other routers to compute the values for its forwarding 
table. This per-router control approach has been used in the Internet for decades. 
The OSPF and BGP protocols that we’ll study in Sections 5.3 and 5.4 are based 
on this per-router approach to control.
•	 Logically centralized control. Figure 5.2 illustrates the case in which a logically 
centralized controller computes and distributes the forwarding tables to be used 
by each and every router. As we saw in Section 4.4, the  generalized match-plus-
action abstraction allows the router to perform traditional IP forwarding as well 
as a rich set of other functions (load sharing, firewalling, and NAT) that had been 
previously implemented in separate middleboxes.
Logically centralized routing controller
Control plane
Data plane
Control
Agent (CA)
CA
CA
CA
CA
Figure 5.2  ♦  
Logically centralized control: A distinct, typically remote,  
controller interacts with local control agents (CAs)
404         Chapter 5    •    The Network Layer: Control Plane
The controller interacts with a control agent (CA) in each of the routers via a 
well-defined protocol to configure and manage that router’s flow table. Typically, 
the CA has minimum functionality; its job is to communicate with the controller, 
and to do as the controller commands. Unlike the routing algorithms in Figure 
5.1, the CAs do not directly interact with each other nor do they actively take part 
in computing the forwarding table. This is a key distinction between per-router 
control and logically centralized control.
By “logically centralized” control [Levin 2012] we mean that the routing 
control service is accessed as if it were a single central service point, even though 
the service is likely to be implemented via multiple servers for fault-tolerance, 
and performance scalability reasons. As we will see in Section 5.5, SDN adopts 
this notion of a logically centralized controller—an approach that is finding 
increased use in production deployments. Google uses SDN to control the rout-
ers in its internal B4 global wide-area network that interconnects its data centers 
 
[Jain 2013]. SWAN [Hong 2013], from Microsoft Research, uses a logically cen-
tralized controller to manage routing and forwarding between a wide area network 
and a data center network. China Telecom and China Unicom are using SDN both 
within data centers and between data centers [Li 2015]. AT&T has noted [AT&T 
2013] that it “supports many SDN capabilities and independently defined, propri-
etary mechanisms that fall under the SDN architectural framework.”
5.2	 Routing Algorithms
In this section we’ll study routing algorithms, whose goal is to determine good 
paths (equivalently, routes), from senders to receivers, through the network of 
routers. Typically, a “good” path is one that has the least cost. We’ll see that in 
practice, however, real-world concerns such as policy issues (for example, a rule 
such as “router x, belonging to organization Y, should not forward any packets 
originating from the network owned by organization Z ”) also come into play. We 
note that whether the network control plane adopts a per-router control approach 
or a logically centralized approach, there must always be a well-defined sequence 
of routers that a packet will cross in traveling from sending to receiving host. Thus, 
the routing algorithms that compute these paths are of fundamental importance, 
and another candidate for our top-10 list of fundamentally important networking 
concepts.
A graph is used to formulate routing problems. Recall that a graph G = (N, E) 
is a set N of nodes and a collection E of edges, where each edge is a pair of nodes 
from N. In the context of network-layer routing, the nodes in the graph represent 
5.2    •    Routing Algorithms         405
routers—the points at which packet-forwarding decisions are made—and the edges 
connecting these nodes represent the physical links between these routers. Such 
a graph abstraction of a computer network is shown in Figure 5.3. To view some 
graphs representing real network maps, see [Dodge 2016, Cheswick 2000]; for 
a discussion of how well different graph-based models model the Internet, see 
 
[Zegura 1997, Faloutsos 1999, Li 2004].
As shown in Figure 5.3, an edge also has a value representing its cost. Typically, 
an edge’s cost may reflect the physical length of the corresponding link (for example, 
a transoceanic link might have a higher cost than a short-haul terrestrial link), the link 
speed, or the monetary cost associated with a link. For our purposes, we’ll simply 
take the edge costs as a given and won’t worry about how they are determined. For 
any edge (x, y) in E, we denote c(x, y) as the cost of the edge between nodes x and y. 
If the pair (x, y) does not belong to E, we set c(x, y) = ∞. Also, we’ll only consider 
undirected graphs (i.e., graphs whose edges do not have a direction) in our discussion 
here, so that edge (x, y) is the same as edge (y, x) and that c(x, y) = c(y, x); however, 
the algorithms we’ll study can be easily extended to the case of directed links with a 
different cost in each direction. Also, a node y is said to be a neighbor of node x if 
(x, y) belongs to E.
Given that costs are assigned to the various edges in the graph abstraction, 
a natural goal of a routing algorithm is to identify the least costly paths between 
sources and destinations. To make this problem more precise, recall that a path 
in a graph G = (N, E) is a sequence of nodes (x1, x2, g, xp) such that each 
of the pairs (x1, x2), (x2, x3), g, (xp-1, xp) are edges in E. The cost of a path 
(x1, x2, g, xp) is simply the sum of all the edge costs along the path, that is, 
x
y
v
3
5
2
5
2
3
1
1
2
1
u
z
w
Figure 5.3  ♦  Abstract graph model of a computer network
406         Chapter 5    •    The Network Layer: Control Plane
c(x1, x2) + c(x2, x3) + g+ c(xp-1, xp). Given any two nodes x and y, there are typi-
cally many paths between the two nodes, with each path having a cost. One or more 
of these paths is a least-cost path. The least-cost problem is therefore clear: Find a 
path between the source and destination that has least cost. In Figure 5.3, for exam-
ple, the least-cost path between source node u and destination node w is (u, x, y, w) 
with a path cost of 3. Note that if all edges in the graph have the same cost, the least-
cost path is also the shortest path (that is, the path with the smallest number of links 
between the source and the destination).
As a simple exercise, try finding the least-cost path from node u to z in 
Figure 5.3 and reflect for a moment on how you calculated that path. If you are 
like most people, you found the path from u to z by examining Figure 5.3, tracing 
a few routes from u to z, and somehow convincing yourself that the path you had 
chosen had the least cost among all possible paths. (Did you check all of the 17 pos-
sible paths between u and z? Probably not!) Such a calculation is an example of a 
centralized routing algorithm—the routing algorithm was run in one location, your 
brain, with complete information about the network. Broadly, one way in which 
we can classify routing algorithms is according to whether they are centralized or 
decentralized.
•	 A centralized routing algorithm computes the least-cost path between a source 
and destination using complete, global knowledge about the network. That is, the 
algorithm takes the connectivity between all nodes and all link costs as inputs. 
This then requires that the algorithm somehow obtain this information before 
actually performing the calculation. The calculation itself can be run at one site 
(e.g., a logically centralized controller as in Figure 5.2) or could be replicated in 
the routing component of each and every router (e.g., as in Figure 5.1). The key 
distinguishing feature here, however, is that the algorithm has complete informa-
tion about connectivity and link costs. Algorithms with global state information 
are often referred to as link-state (LS) algorithms, since the algorithm must 
be aware of the cost of each link in the network. We’ll study LS algorithms in 
 
Section 5.2.1.
•	 In a decentralized routing algorithm, the calculation of the least-cost path is 
carried out in an iterative, distributed manner by the routers. No node has com-
plete information about the costs of all network links. Instead, each node begins 
with only the knowledge of the costs of its own directly attached links. Then, 
through an iterative process of calculation and exchange of information with its 
neighboring nodes, a node gradually calculates the least-cost path to a destination 
or set of destinations. The decentralized routing algorithm we’ll study below in 
 
Section 5.2.2 is called a distance-vector (DV) algorithm, because each node main-
tains a vector of estimates of the costs (distances) to all other nodes in the net-
work. Such decentralized algorithms, with interactive message exchange between 
5.2    •    Routing Algorithms         407
neighboring routers is perhaps more naturally suited to control planes where the 
routers interact directly with each other, as in Figure 5.1.
A second broad way to classify routing algorithms is according to whether they 
are static or dynamic. In static routing algorithms, routes change very slowly over 
time, often as a result of human intervention (for example, a human manually editing 
a link costs). Dynamic routing algorithms change the routing paths as the network 
traffic loads or topology change. A dynamic algorithm can be run either periodically 
or in direct response to topology or link cost changes. While dynamic algorithms 
are more responsive to network changes, they are also more susceptible to problems 
such as routing loops and route oscillation.
A third way to classify routing algorithms is according to whether they are load-
sensitive or load-insensitive. In a load-sensitive algorithm, link costs vary dynami-
cally to reflect the current level of congestion in the underlying link. If a high cost 
is associated with a link that is currently congested, a routing algorithm will tend 
to choose routes around such a congested link. While early ARPAnet routing algo-
rithms were load-sensitive [McQuillan 1980], a number of difficulties were encoun-
tered [Huitema 1998]. Today’s Internet routing algorithms (such as RIP, OSPF, and 
BGP) are load-insensitive, as a link’s cost does not explicitly reflect its current (or 
recent past) level of congestion.
5.2.1 The Link-State (LS) Routing Algorithm
Recall that in a link-state algorithm, the network topology and all link costs are 
known, that is, available as input to the LS algorithm. In practice this is accom-
plished by having each node broadcast link-state packets to all other nodes in 
the network, with each link-state packet containing the identities and costs of 
its attached links. In practice (for example, with the Internet’s OSPF routing 
protocol, discussed in Section 5.3) this is often accomplished by a link-state 
broadcast algorithm ­
[Perlman 1999]. The result of the nodes’ broadcast is that 
all nodes have an identical and complete view of the network. Each node can 
then run the LS algorithm and compute the same set of least-cost paths as every 
other node.
The link-state routing algorithm we present below is known as Dijkstra’s 
algorithm, named after its inventor. A closely related algorithm is Prim’s algo-
rithm; see [Cormen 2001] for a general discussion of graph algorithms. Dijkstra’s 
algorithm computes the least-cost path from one node (the source, which we will 
refer to as u) to all other nodes in the network. Dijkstra’s algorithm is iterative and 
has the property that after the kth iteration of the algorithm, the least-cost paths 
are known to k destination nodes, and among the least-cost paths to all destination 
408         Chapter 5    •    The Network Layer: Control Plane
nodes, these k paths will have the k smallest costs. Let us define the following 
notation:
•	 D(v): cost of the least-cost path from the source node to destination v as of this 
iteration of the algorithm.
•	 p(v): previous node (neighbor of v) along the current least-cost path from the 
source to v.
•	 N′: subset of nodes; v is in N′ if the least-cost path from the source to v is defini-
tively known.
The centralized routing algorithm consists of an initialization step followed by 
a loop. The number of times the loop is executed is equal to the number of nodes in 
the network. Upon termination, the algorithm will have calculated the shortest paths 
from the source node u to every other node in the network.
Link-State (LS) Algorithm for Source Node u
1  Initialization: 
2   N’ = {u}
3   for all nodes v
4     if v is a neighbor of u
5       then D(v) = c(u,v)
6     else D(v) = ∞
7
8  Loop
9   find w not in N’ such that D(w) is a minimum
10  add w to N’
11  update D(v) for each neighbor v of w and not in N’:
12        D(v) = min(D(v), D(w)+ c(w,v) )
13   /* new cost to v is either old cost to v or known
14    least path cost to w plus cost from w to v */
15 until N’= N
As an example, let’s consider the network in Figure 5.3 and compute the least-
cost paths from u to all possible destinations. A tabular summary of the algorithm’s 
computation is shown in Table 5.1, where each line in the table gives the values of 
the algorithm’s variables at the end of the iteration. Let’s consider the few first steps 
in detail.
•	 In the initialization step, the currently known least-cost paths from u to its directly 
attached neighbors, v, x, and w, are initialized to 2, 1, and 5, respectively. Note in 
5.2    •    Routing Algorithms         409
particular that the cost to w is set to 5 (even though we will soon see that a lesser-cost 
 
path does indeed exist) since this is the cost of the direct (one hop) link from u to 
w. The costs to y and z are set to infinity because they are not directly connected 
to u.
•	 In the first iteration, we look among those nodes not yet added to the set N′ and 
find that node with the least cost as of the end of the previous iteration. That node 
is x, with a cost of 1, and thus x is added to the set N′. Line 12 of the LS algorithm 
is then performed to update D(v) for all nodes v, yielding the results shown in the 
second line (Step 1) in Table 5.1. The cost of the path to v is unchanged. The cost 
of the path to w (which was 5 at the end of the initialization) through node x is 
found to have a cost of 4. Hence this lower-cost path is selected and w’s predeces-
sor along the shortest path from u is set to x. Similarly, the cost to y (through x) is 
computed to be 2, and the table is updated accordingly.
•	 In the second iteration, nodes v and y are found to have the least-cost paths (2), 
and we break the tie arbitrarily and add y to the set N′ so that N′ now contains u, 
x, and y. The cost to the remaining nodes not yet in N′, that is, nodes v, w, and z, 
are updated via line 12 of the LS algorithm, yielding the results shown in the third 
row in Table 5.1.
•	 And so on . . . 
When the LS algorithm terminates, we have, for each node, its predecessor 
along the least-cost path from the source node. For each predecessor, we also have its 
predecessor, and so in this manner we can construct the entire path from the source to 
all destinations. The forwarding table in a node, say node u, can then be constructed 
from this information by storing, for each destination, the next-hop node on the least-
cost path from u to the destination. Figure 5.4 shows the resulting least-cost paths 
and forwarding table in u for the network in Figure 5.3.
Table 5.1  ♦  Running the link-state algorithm on the network in Figure 5.3
step
N’
D (v), p (v)
D (w), p (w)
D (x), p (x)
D (y), p (y)
D (z), p (z)
0
u
2, u
5, u
1,u
∞
∞
1
ux
2, u
4, x
2, x
∞
2
uxy
2, u
3, y
4, y
3
uxyv
3, y
4, y
4
uxyvw
4, y
5
uxyvwz
410         Chapter 5    •    The Network Layer: Control Plane
What is the computational complexity of this algorithm? That is, given n nodes 
(not counting the source), how much computation must be done in the worst case to 
find the least-cost paths from the source to all destinations? In the first iteration, we 
need to search through all n nodes to determine the node, w, not in N′ that has the 
minimum cost. In the second iteration, we need to check n - 1 nodes to determine 
the minimum cost; in the third iteration n - 2 nodes, and so on. Overall, the total 
number of nodes we need to search through over all the iterations is n(n + 1)/2, and 
thus we say that the preceding implementation of the LS algorithm has worst-case 
complexity of order n squared: O(n2). (A more sophisticated implementation of this 
algorithm, using a data structure known as a heap, can find the minimum in line 9 in 
logarithmic rather than linear time, thus reducing the complexity.)
Before completing our discussion of the LS algorithm, let us consider a pathol-
ogy that can arise. Figure 5.5 shows a simple network topology where link costs are 
equal to the load carried on the link, for example, reflecting the delay that would 
be experienced. In this example, link costs are not symmetric; that is, c(u,v) equals 
c(v,u) only if the load carried on both directions on the link (u,v) is the same. In this 
example, node z originates a unit of traffic destined for w, node x also originates a 
unit of traffic destined for w, and node y injects an amount of traffic equal to e, also 
destined for w. The initial routing is shown in Figure 5.5(a) with the link costs cor-
responding to the amount of traffic carried.
When the LS algorithm is next run, node y determines (based on the link costs 
shown in Figure 5.5(a)) that the clockwise path to w has a cost of 1, while the coun-
terclockwise path to w (which it had been using) has a cost of 1 + e. Hence y’s least-
cost path to w is now clockwise. Similarly, x determines that its new least-cost path to 
w is also clockwise, resulting in costs shown in Figure 5.5(b). When the LS algorithm 
is run next, nodes x, y, and z all detect a zero-cost path to w in the counterclockwise 
direction, and all route their traffic to the counterclockwise routes. The next time the 
LS algorithm is run, x, y, and z all then route their traffic to the clockwise routes.
What can be done to prevent such oscillations (which can occur in any algo-
rithm, not just an LS algorithm, that uses a congestion or delay-based link metric)? 
One solution would be to mandate that link costs not depend on the amount of traffic 
Destination 
Link
v
w
x
y
z
(u, v)
(u, x)
(u, x)
(u, x)
(u, x)
x
y
v
u
z
w
Figure 5.4  ♦  Least cost path and forwarding table for node u
5.2    •    Routing Algorithms         411
carried—an unacceptable solution since one goal of routing is to avoid highly con-
gested (for example, high-delay) links. Another solution is to ensure that not all rout-
ers run the LS algorithm at the same time. This seems a more reasonable solution, 
since we would hope that even if routers ran the LS algorithm with the same perio-
dicity, the execution instance of the algorithm would not be the same at each node. 
Interestingly, researchers have found that routers in the Internet can self-synchronize 
among themselves [Floyd Synchronization 1994]. That is, even though they initially 
execute the algorithm with the same period but at different instants of time, the algo-
rithm execution instance can eventually become, and remain, synchronized at the 
routers. One way to avoid such self-synchronization is for each router to randomize 
the time it sends out a link advertisement.
Having studied the LS algorithm, let’s consider the other major routing algo-
rithm that is used in practice today—the distance-vector routing algorithm.
w
y
z
x
1
0
0
0
e
1 + e
1
a.  Initial routing
1
e
w
y
z
x
2 + e
1 + e
1
0
0
0
b.  x, y detect better path
     to w, clockwise
w
y
z
x
0
0
0
1
1 + e
2+ e
c.  x, y, z detect better path
     to w, counterclockwise
w
y
z
x
2 + e
1 + e
1
0
0
0
d.  x, y, z, detect better path
     to w, clockwise
1
1
e
1
1
e
1
1
e
Figure 5.5  ♦  Oscillations with congestion-sensitive routing
412         Chapter 5    •    The Network Layer: Control Plane
5.2.2 The Distance-Vector (DV) Routing Algorithm
Whereas the LS algorithm is an algorithm using global information, the distance-
vector (DV) algorithm is iterative, asynchronous, and distributed. It is distributed in 
that each node receives some information from one or more of its directly attached 
neighbors, performs a calculation, and then distributes the results of its calculation 
back to its neighbors. It is iterative in that this process continues on until no more 
information is exchanged between neighbors. (Interestingly, the algorithm is also 
self-terminating—there is no signal that the computation should stop; it just stops.) 
The algorithm is asynchronous in that it does not require all of the nodes to operate in 
lockstep with each other. We’ll see that an asynchronous, iterative, self-terminating, 
distributed algorithm is much more interesting and fun than a centralized algorithm!
Before we present the DV algorithm, it will prove beneficial to discuss an impor-
tant relationship that exists among the costs of the least-cost paths. Let dx(y) be the 
cost of the least-cost path from node x to node y. Then the least costs are related by 
the celebrated Bellman-Ford equation, namely,
	
dx(y) = minv5c(x, v) + dv( y)6,
(5.1)
where the minv in the equation is taken over all of x’s neighbors. The Bellman-
Ford equation is rather intuitive. Indeed, after traveling from x to v, if we then take 
the least-cost path from v to y, the path cost will be c(x, v) + dv(y). Since we must 
begin by traveling to some neighbor v, the least cost from x to y is the minimum of 
c(x, v) + dv(y) taken over all neighbors v.
But for those who might be skeptical about the validity of the equation, let’s 
check it for source node u and destination node z in Figure 5.3. The source node u 
has three neighbors: nodes v, x, and w. By walking along various paths in the graph, 
it is easy to see that dv(z) = 5, dx(z) = 3, and dw(z) = 3. Plugging these values into 
Equation 5.1, along with the costs c(u, v) = 2, c(u, x) = 1, and c(u, w) = 5, gives 
du(z) = min52 + 5, 5 + 3, 1 + 36 = 4, which is obviously true and which is 
exactly what the Dijskstra algorithm gave us for the same network. This quick veri-
fication should help relieve any skepticism you may have.
The Bellman-Ford equation is not just an intellectual curiosity. It actually has signif-
icant practical importance: the solution to the Bellman-Ford equation provides the entries 
in node x’s forwarding table. To see this, let v* be any neighboring node that achieves 
the minimum in Equation 5.1. Then, if node x wants to send a packet to node y along a 
least-cost path, it should first forward the packet to node v*. Thus, node x’s forwarding 
table would specify node v* as the next-hop router for the ultimate destination y. Another 
important practical contribution of the Bellman-Ford equation is that it suggests the form 
of the neighbor-to-neighbor communication that will take place in the DV algorithm.
The basic idea is as follows. Each node x begins with Dx(y), an estimate of the cost 
of the least-cost path from itself to node y, for all nodes, y, in N. Let Dx = [Dx(y): y in N] 
be node x’s distance vector, which is the vector of cost estimates from x to all other nodes, 
y, in N. With the DV algorithm, each node x maintains the following routing information:
5.2    •    Routing Algorithms         413
•	 For each neighbor v, the cost c(x,v) from x to directly attached neighbor, v
•	 Node x’s distance vector, that is, Dx = [Dx(y): y in N], containing x’s estimate of 
its cost to all destinations, y, in N
•	 The distance vectors of each of its neighbors, that is, Dv = [Dv(y): y in N] for 
each neighbor v of x
In the distributed, asynchronous algorithm, from time to time, each node sends a 
copy of its distance vector to each of its neighbors. When a node x receives a new 
distance vector from any of its neighbors w, it saves w’s distance vector, and then 
uses the Bellman-Ford equation to update its own distance vector as follows:
Dx(y) = minv5c(x, v) + Dv(y)6  for each node y in N
If node x’s distance vector has changed as a result of this update step, node x will then 
send its updated distance vector to each of its neighbors, which can in turn update 
their own distance vectors. Miraculously enough, as long as all the nodes continue 
to exchange their distance vectors in an asynchronous fashion, each cost estimate 
Dx(y) converges to dx(y), the actual cost of the least-cost path from node x to node y 
[Bertsekas 1991]!
Distance-Vector (DV) Algorithm
At each node, x:
1  Initialization:
2    for all destinations y in N:
3       Dx(y)= c(x,y)/* if y is not a neighbor then c(x,y)= ∞ */
4    for each neighbor w
5       Dw(y) = ? for all destinations y in N
6    for each neighbor w
7       send distance vector  Dx = [Dx(y): y in N] to w
8
9  loop 
10    wait  (until I see a link cost change to some neighbor w or
11            until I receive a distance vector from some neighbor w)
12
13    for each y in N:
14        Dx(y) = minv{c(x,v) + Dv(y)}
15
16 if Dx(y) changed for any destination y
17       send distance vector Dx  = [Dx(y): y in N] to all neighbors
18
19 forever 
414         Chapter 5    •    The Network Layer: Control Plane
In the DV algorithm, a node x updates its distance-vector estimate when it either 
sees a cost change in one of its directly attached links or receives a distance-vector 
update from some neighbor. But to update its own forwarding table for a given des-
tination y, what node x really needs to know is not the shortest-path distance to y but 
instead the neighboring node v*(y) that is the next-hop router along the shortest path 
to y. As you might expect, the next-hop router v*(y) is the neighbor v that achieves 
the minimum in Line 14 of the DV algorithm. (If there are multiple neighbors v that 
achieve the minimum, then v*(y) can be any of the minimizing neighbors.) Thus, 
in Lines 13–14, for each destination y, node x also determines v*(y) and updates its 
forwarding table for destination y.
Recall that the LS algorithm is a centralized algorithm in the sense that it 
requires each node to first obtain a complete map of the network before running the 
Dijkstra algorithm. The DV algorithm is decentralized and does not use such global 
information. Indeed, the only information a node will have is the costs of the links 
to its directly attached neighbors and information it receives from these neighbors. 
Each node waits for an update from any neighbor (Lines 10–11), calculates its new 
distance vector when receiving an update (Line 14), and distributes its new distance 
vector to its neighbors (Lines 16–17). DV-like algorithms are used in many routing 
protocols in practice, including the Internet’s RIP and BGP, ISO IDRP, Novell IPX, 
and the original ARPAnet.
Figure 5.6 illustrates the operation of the DV algorithm for the simple three-
node network shown at the top of the figure. The operation of the algorithm is illus-
trated in a synchronous manner, where all nodes simultaneously receive distance 
vectors from their neighbors, compute their new distance vectors, and inform their 
neighbors if their distance vectors have changed. After studying this example, you 
should convince yourself that the algorithm operates correctly in an asynchronous 
manner as well, with node computations and update generation/reception occurring 
at any time.
The leftmost column of the figure displays three initial routing tables for each 
of the three nodes. For example, the table in the upper-left corner is node x’s ini-
tial routing table. Within a specific routing table, each row is a distance vector— 
specifically, each node’s routing table includes its own distance vector and that 
of each of its neighbors. Thus, the first row in node x’s initial routing table is 
Dx = [Dx(x), Dx(y), Dx(z)] = [0, 2, 7]. The second and third rows in this table are 
the most recently received distance vectors from nodes y and z, respectively. Because 
at initialization node x has not received anything from node y or z, the entries in 
 
the second and third rows are initialized to infinity.
After initialization, each node sends its distance vector to each of its two neigh-
bors. This is illustrated in Figure 5.6 by the arrows from the first column of tables 
to the second column of tables. For example, node x sends its distance vector Dx = 
[0, 2, 7] to both nodes y and z. After receiving the updates, each node recomputes its 
own distance vector. For example, node x computes
5.2    •    Routing Algorithms         415
 
Dx(x) = 0
 
Dx(y) = min5c(x,y) + Dy(y), c(x,z) + Dz(y)6 = min52 + 0, 7 + 16 = 2
 
Dx(z) = min5c(x,y) + Dy(z), c(x,z) + Dz(z)6 = min52 + 1, 7 + 06 = 3
The second column therefore displays, for each node, the node’s new distance vector 
along with distance vectors just received from its neighbors. Note, for example, that 
Node y table
Node x table
0 2 7
x y z
` ` `
` ` `
Time
7
2
1
y
x
z
Node z table
from
cost to
x
y
z
0 2 3
x y z
2 0 1
7 1 0
from
cost to
x
y
z
0 2 3
x y z
2 0 1
3 1 0
from
cost to
x
y
z
2 0 1
x y z
` ` `
` ` `
from
cost to
x
y
z
0 2 7
x y z
2 0 1
7 1 0
from
cost to
x
y
z
0 2 3
x y z
2 0 1
3 1 0
from
cost to
x
y
z
7 1 0
x y z
` ` `
` ` `
from
cost to
x
y
z
0 2 7
x y z
2 0 1
3 1 0
from
cost to
x
y
z
0 2 3
x y z
2 0 1
3 1 0
from
cost to
x
y
z
Figure 5.6  ♦  Distance-vector (DV) algorithm in operation
416         Chapter 5    •    The Network Layer: Control Plane
node x’s estimate for the least cost to node z, Dx(z), has changed from 7 to 3. Also 
note that for node x, neighboring node y achieves the minimum in line 14 of the DV 
algorithm; thus at this stage of the algorithm, we have at node x that v*(y) = y and 
v*(z) = y.
After the nodes recompute their distance vectors, they again send their updated 
distance vectors to their neighbors (if there has been a change). This is illustrated in 
Figure 5.6 by the arrows from the second column of tables to the third column of 
tables. Note that only nodes x and z send updates: node y’s distance vector didn’t 
change so node y doesn’t send an update. After receiving the updates, the nodes then 
recompute their distance vectors and update their routing tables, which are shown in 
the third column.
The process of receiving updated distance vectors from neighbors, recomputing 
routing table entries, and informing neighbors of changed costs of the least-cost path 
to a destination continues until no update messages are sent. At this point, since no 
update messages are sent, no further routing table calculations will occur and the 
algorithm will enter a quiescent state; that is, all nodes will be performing the wait in 
Lines 10–11 of the DV algorithm. The algorithm remains in the quiescent state until 
a link cost changes, as discussed next.
Distance-Vector Algorithm: Link-Cost Changes and Link Failure
When a node running the DV algorithm detects a change in the link cost from 
itself to a neighbor (Lines 10–11), it updates its distance vector (Lines 13–14) and, 
if there’s a change in the cost of the least-cost path, informs its neighbors (Lines 
16–17) of its new distance vector. Figure 5.7(a) illustrates a scenario where the link 
cost from y to x changes from 4 to 1. We focus here only on y’ and z’s distance table 
entries to destination x. The DV algorithm causes the following sequence of events 
to occur:
•	 At time t0, y detects the link-cost change (the cost has changed from 4 to 1), 
updates its distance vector, and informs its neighbors of this change since its dis-
tance vector has changed.
•	 At time t1, z receives the update from y and updates its table. It computes a new 
least cost to x (it has decreased from a cost of 5 to a cost of 2) and sends its new 
distance vector to its neighbors.
•	 At time t2, y receives z’s update and updates its distance table. y’s least costs do 
not change and hence y does not send any message to z. The algorithm comes to 
a quiescent state.
Thus, only two iterations are required for the DV algorithm to reach a quiescent 
 
state. The good news about the decreased cost between x and y has propagated 
quickly through the network.
5.2    •    Routing Algorithms         417
Let’s now consider what can happen when a link cost increases. Suppose that 
the link cost between x and y increases from 4 to 60, as shown in Figure 5.7(b).
	1.	 Before the link cost changes, Dy(x) = 4, Dy(z) = 1, Dz(y) = 1, and Dz(x) = 5. 
At time t0, y detects the link-cost change (the cost has changed from 4 to 60). y 
computes its new minimum-cost path to x to have a cost of
Dy(x) = min5c(y,x) + Dx(x), c(y,z) + Dz(x)6 = min560 + 0, 1 + 56 = 6
	
	 Of course, with our global view of the network, we can see that this new cost via 
z is wrong. But the only information node y has is that its direct cost to x is 60 
and that z has last told y that z could get to x with a cost of 5. So in order to get 
to x, y would now route through z, fully expecting that z will be able to get to x 
with a cost of 5. As of t1 we have a routing loop—in order to get to x, y routes 
through z, and z routes through y. A routing loop is like a black hole—a packet 
destined for x arriving at y or z as of t1 will bounce back and forth between these 
two nodes forever (or until the forwarding tables are changed).
	2.	 Since node y has computed a new minimum cost to x, it informs z of its new 
distance vector at time t1.
	3.	 Sometime after t1, z receives y’s new distance vector, which indicates that y’s 
minimum cost to x is 6. z knows it can get to y with a cost of 1 and hence com-
putes a new least cost to x of Dz(x) = min550 + 0,1 + 66 = 7. Since z’s 
least cost to x has increased, it then informs y of its new distance vector at t2.
	4.	 In a similar manner, after receiving z’s new distance vector, y determines 
Dy(x) = 8 and sends z its distance vector. z then determines Dz(x) = 9 and 
sends y its distance vector, and so on.
How long will the process continue? You should convince yourself that the loop will 
persist for 44 iterations (message exchanges between y and z)—until z eventually 
computes the cost of its path via y to be greater than 50. At this point, z will (finally!) 
determine that its least-cost path to x is via its direct connection to x. y will then 
50
4
1
60
1
y
x
a.
b.
z
50
4
1
y
x
z
Figure 5.7  ♦  Changes in link cost
418         Chapter 5    •    The Network Layer: Control Plane
route to x via z. The result of the bad news about the increase in link cost has indeed 
 
traveled slowly! What would have happened if the link cost c(y, x) had changed from 
4 to 10,000 and the cost c(z, x) had been 9,999? Because of such scenarios, the prob-
lem we have seen is sometimes referred to as the count-to-infinity ­
problem.
Distance-Vector Algorithm: Adding Poisoned Reverse
The specific looping scenario just described can be avoided using a technique known 
as poisoned reverse. The idea is simple—if z routes through y to get to destination x, 
then z will advertise to y that its distance to x is infinity, that is, z will advertise to y 
that Dz(x) = ∞ (even though z knows Dz(x) = 5 in truth). z will continue telling this 
little white lie to y as long as it routes to x via y. Since y believes that z has no path 
to x, y will never attempt to route to x via z, as long as z continues to route to x via y 
(and lies about doing so).
Let’s now see how poisoned reverse solves the particular looping problem we 
encountered before in Figure 5.5(b). As a result of the poisoned reverse, y’s distance 
table indicates Dz(x) = ∞. When the cost of the (x, y) link changes from 4 to 60 at 
time t0, y updates its table and continues to route directly to x, albeit at a higher cost 
of 60, and informs z of its new cost to x, that is, Dy(x) = 60. After receiving the 
update at t1, z immediately shifts its route to x to be via the direct (z, x) link at a cost 
of 50. Since this is a new least-cost path to x, and since the path no longer passes 
through y, z now informs y that Dz(x) = 50 at t2. After receiving the update from 
z, y updates its distance table with Dy(x) = 51. Also, since z is now on y’s least-
cost path to x, y poisons the reverse path from z to x by informing z at time t3 that 
Dy(x) = ∞ (even though y knows that Dy(x) = 51 in truth).
Does poisoned reverse solve the general count-to-infinity problem? It does not. 
You should convince yourself that loops involving three or more nodes (rather than 
simply two immediately neighboring nodes) will not be detected by the poisoned 
reverse technique.
A Comparison of LS and DV Routing Algorithms
The DV and LS algorithms take complementary approaches toward computing rout-
ing. In the DV algorithm, each node talks to only its directly connected neighbors, 
but it provides its neighbors with least-cost estimates from itself to all the nodes 
(that it knows about) in the network. The LS algorithm requires global information. 
Consequently, when implemented in each and every router, e.g., as in Figure 4.2 and 
5.1, each node would need to communicate with all other nodes (via broadcast), but 
it tells them only the costs of its directly connected links. Let’s conclude our study 
of LS and DV algorithms with a quick comparison of some of their attributes. Recall 
that N is the set of nodes (routers) and E is the set of edges (links).
•	 Message complexity. We have seen that LS requires each node to know the cost 
of each link in the network. This requires O(|N| |E|) messages to be sent. Also, 
5.3    •    Intra-AS Routing in the Internet: OSPF         419
whenever a link cost changes, the new link cost must be sent to all nodes. The DV 
algorithm requires message exchanges between directly connected neighbors at 
each iteration. We have seen that the time needed for the algorithm to converge 
can depend on many factors. When link costs change, the DV algorithm will 
propagate the results of the changed link cost only if the new link cost results in a 
changed least-cost path for one of the nodes attached to that link.
•	 Speed of convergence. We have seen that our implementation of LS is an O(|N|2) 
algorithm requiring O(|N| |E|)) messages. The DV algorithm can converge slowly 
and can have routing loops while the algorithm is converging. DV also suffers 
from the count-to-infinity problem.
•	 Robustness. What can happen if a router fails, misbehaves, or is sabotaged? 
Under LS, a router could broadcast an incorrect cost for one of its attached links 
(but no others). A node could also corrupt or drop any packets it received as part 
of an LS broadcast. But an LS node is computing only its own forwarding tables; 
other nodes are performing similar calculations for themselves. This means route 
calculations are somewhat separated under LS, providing a degree of robustness. 
Under DV, a node can advertise incorrect least-cost paths to any or all destina-
tions. (Indeed, in 1997, a malfunctioning router in a small ISP provided national 
backbone routers with erroneous routing information. This caused other routers 
to flood the malfunctioning router with traffic and caused large portions of the 
Internet to become disconnected for up to several hours [Neumann 1997].) More 
generally, we note that, at each iteration, a node’s calculation in DV is passed on 
to its neighbor and then indirectly to its neighbor’s neighbor on the next iteration. 
In this sense, an incorrect node calculation can be diffused through the entire 
network under DV.
In the end, neither algorithm is an obvious winner over the other; indeed, both algo-
rithms are used in the Internet.
5.3	 Intra-AS Routing in the Internet: OSPF
In our study of routing algorithms so far, we’ve viewed the network simply as a 
collection of interconnected routers. One router was indistinguishable from another 
in the sense that all routers executed the same routing algorithm to compute routing 
paths through the entire network. In practice, this model and its view of a homog-
enous set of routers all executing the same routing algorithm is simplistic for two 
important reasons:
•	 Scale. As the number of routers becomes large, the overhead involved in communi-
cating, computing, and storing routing information becomes prohibitive. Today’s 
Internet consists of hundreds of millions of routers. Storing routing information 
 
420         Chapter 5    •    The Network Layer: Control Plane
for possible destinations at each of these routers would clearly require enormous 
amounts of memory. The overhead required to broadcast connectivity and link 
cost updates among all of the routers would be huge! A distance-vector algorithm 
that iterated among such a large number of routers would surely never converge. 
Clearly, something must be done to reduce the complexity of route computation 
in a network as large as the Internet.
•	 Administrative autonomy. As described in Section 1.3, the Internet is a network 
of ISPs, with each ISP consisting of its own network of routers. An ISP generally 
desires to operate its network as it pleases (for example, to run whatever rout-
ing algorithm it chooses within its network) or to hide aspects of its network’s 
internal organization from the outside. Ideally, an organization should be able to 
operate and administer its network as it wishes, while still being able to connect 
its network to other outside networks.
Both of these problems can be solved by organizing routers into autonomous 
­
systems (ASs), with each AS consisting of a group of routers that are under the same 
administrative control. Often the routers in an ISP, and the links that interconnect 
them, constitute a single AS. Some ISPs, however, partition their network into multi-
ple ASs. In particular, some tier-1 ISPs use one gigantic AS for their entire network, 
whereas others break up their ISP into tens of interconnected ASs. An autonomous 
system is identified by its globally unique autonomous system number (ASN) [RFC 
1930]. AS numbers, like IP addresses, are assigned by ICANN regional registries 
[ICANN 2016].
Routers within the same AS all run the same routing algorithm and have infor-
mation about each other. The routing algorithm ­
running within an autonomous sys-
tem is called an intra-autonomous system routing ­
protocol.
Open Shortest Path First (OSPF) 
OSPF routing and its closely related cousin, IS-IS, are widely used for intra-AS 
routing in the Internet. The Open in OSPF indicates that the routing protocol speci-
fication is publicly available (for example, as opposed to Cisco’s EIGRP protocol, 
which was only recently became open [Savage 2015], after roughly 20 years as a 
Cisco-proprietary protocol). The most recent version of OSPF, version 2, is defined 
in [RFC 2328], a public document.
OSPF is a link-state protocol that uses flooding of link-state information 
and a Dijkstra’s least-cost path algorithm. With OSPF, each router constructs 
a complete topological map (that is, a graph) of the entire autonomous system. 
Each router then locally runs Dijkstra’s shortest-path algorithm to determine a 
shortest-path tree to all subnets, with itself as the root node. Individual link costs 
are configured by the network administrator (see sidebar, Principles and Practice: 
Setting OSPF Weights). The administrator might choose to set all link costs to 1, 
5.3    •    Intra-AS Routing in the Internet: OSPF         421
thus achieving minimum-hop routing, or might choose to set the link weights to 
be inversely proportional to link capacity in order to discourage traffic from using 
low-bandwidth links. OSPF does not mandate a policy for how link weights are 
set (that is the job of the ­
network administrator), but instead provides the mecha-
nisms (protocol) for determining least-cost path routing for the given set of link 
weights.
With OSPF, a router broadcasts routing information to all other routers in the 
autonomous system, not just to its neighboring routers. A router broadcasts link-state 
information whenever there is a change in a link’s state (for example, a change in 
cost or a change in up/down status). It also broadcasts a link’s state periodically (at 
least once every 30 minutes), even if the link’s state has not changed. RFC 2328 
notes that “this periodic updating of link state advertisements adds robustness to the 
link state algorithm.” OSPF advertisements are contained in OSPF messages that are 
SETTING OSPF LINK WEIGHTS
Our discussion of link-state routing has implicitly assumed that link weights are set, a 
routing algorithm such as OSPF is run, and traffic flows according to the routing tables 
computed by the LS algorithm. In terms of cause and effect, the link weights are given (i.e., 
they come first) and result (via Dijkstra’s algorithm) in routing paths that minimize overall 
cost. In this viewpoint, link weights reflect the cost of using a link (e.g., if link weights are 
inversely proportional to capacity, then the use of high-capacity links would have smaller 
weight and thus be more attractive from a routing standpoint) and Dijsktra’s algorithm 
serves to minimize overall cost.
In practice, the cause and effect relationship between link weights and routing paths 
may be reversed, with network operators configuring link weights in order to obtain rout-
ing paths that achieve certain traffic engineering goals [Fortz 2000, Fortz 2002]. For 
example, suppose a network operator has an estimate of traffic flow entering the network 
at each ingress point and destined for each egress point. The operator may then want 
to put in place a specific routing of ingress-to-egress flows that minimizes the maximum 
utilization over all of the network’s links. But with a routing algorithm such as OSPF, the 
operator’s main “knobs” for tuning the routing of flows through the network are the link 
weights. Thus, in order to achieve the goal of minimizing the maximum link utilization, the 
operator must find the set of link weights that achieves this goal. This is a reversal of the 
cause and effect relationship—the desired routing of flows is known, and the OSPF link 
weights must be found such that the OSPF routing algorithm results in this desired routing 
of flows.
Principles in Practice
422         Chapter 5    •    The Network Layer: Control Plane
carried directly by IP, with an upper-layer protocol of 89 for OSPF. Thus, the OSPF 
protocol must itself implement functionality such as reliable message transfer and 
link-state broadcast. The OSPF protocol also checks that links are operational (via a 
HELLO message that is sent to an attached neighbor) and allows an OSPF router to 
obtain a neighboring router’s database of network-wide link state.
Some of the advances embodied in OSPF include the following:
•	 Security. Exchanges between OSPF routers (for example, link-state updates) can 
be authenticated. With authentication, only trusted routers can participate in the 
OSPF protocol within an AS, thus preventing malicious intruders (or networking 
students taking their newfound knowledge out for a joyride) from injecting incor-
rect information into router tables. By default, OSPF packets between routers are 
not authenticated and could be forged. Two types of authentication can be con-
figured—simple and MD5 (see Chapter 8 for a discussion on MD5 and authenti-
cation in general). With simple authentication, the same password is configured 
on each router. When a router sends an OSPF packet, it includes the password in 
plaintext. Clearly, simple authentication is not very secure. MD5 authentication is 
based on shared secret keys that are configured in all the routers. For each OSPF 
packet that it sends, the router computes the MD5 hash of the content of the OSPF 
packet appended with the secret key. (See the discussion of message authentica-
tion codes in Chapter 8.) Then the router includes the resulting hash value in the 
OSPF packet. The receiving router, using the preconfigured secret key, will com-
pute an MD5 hash of the packet and compare it with the hash value that the packet 
carries, thus verifying the packet’s authenticity. Sequence numbers are also used 
with MD5 authentication to protect against replay attacks.
•	 Multiple same-cost paths. When multiple paths to a destination have the same 
cost, OSPF allows multiple paths to be used (that is, a single path need not be 
chosen for carrying all traffic when multiple equal-cost paths exist).
•	 Integrated support for unicast and multicast routing. Multicast OSPF (MOSPF) 
[RFC 1584] provides simple extensions to OSPF to provide for multicast routing. 
MOSPF uses the existing OSPF link database and adds a new type of link-state 
advertisement to the existing OSPF link-state broadcast mechanism.
•	 Support for hierarchy within a single AS. An OSPF autonomous system can 
be configured hierarchically into areas. Each area runs its own OSPF link-state 
routing algorithm, with each router in an area broadcasting its link state to all 
other routers in that area. Within each area, one or more area border routers are 
responsible for routing packets outside the area. Lastly, exactly one OSPF area 
in the AS is configured to be the backbone area. The primary role of the back-
bone area is to route traffic between the other areas in the AS. The backbone 
always contains all area border routers in the AS and may contain non-border 
routers as well. Inter-area routing within the AS requires that the packet be first 
5.4    •    Routing Among the ISPs: BGP         423
routed to an area border router (intra-area routing), then routed through the back-
bone to the area border router that is in the destination area, and then routed to 
the final destination.
OSPF is a relatively complex protocol, and our coverage here has been necessar-
ily brief; [Huitema 1998; Moy 1998; RFC 2328] provide additional details.
5.4	 Routing Among the ISPs: BGP
We just learned that OSPF is an example of an intra-AS routing protocol. When 
routing a packet between a source and destination within the same AS, the route the 
packet follows is entirely determined by the intra-AS routing protocol. However, to 
route a packet across multiple ASs, say from a smartphone in Timbuktu to a server 
in a datacenter in Silicon Valley, we need an inter-autonomous system routing 
protocol. Since an inter-AS routing protocol involves coordination among multiple 
ASs, communicating ASs must run the same inter-AS routing protocol. In fact, in the 
Internet, all ASs run the same inter-AS routing protocol, called the Border Gateway 
Protocol, more commonly known as BGP [RFC 4271; Stewart 1999].
BGP is arguably the most important of all the Internet protocols (the only other 
contender would be the IP protocol that we studied in Section 4.3), as it is the pro-
tocol that glues the thousands of ISPs in the Internet together. As we will soon see, 
BGP is a decentralized and asynchronous protocol in the vein of distance-vector 
routing described in Section 5.2.2. Although BGP is a complex and challenging pro-
tocol, to understand the Internet on a deep level, we need to become familiar with 
its underpinnings and operation. The time we devote to learning BGP will be well 
worth the effort.
5.4.1 The Role of BGP
To understand the responsibilities of BGP, consider an AS and an arbitrary router 
in that AS. Recall that every router has a forwarding table, which plays the central 
role in the process of forwarding arriving packets to outbound router links. As we 
have learned, for destinations that are within the same AS, the entries in the router’s 
forwarding table are determined by the AS’s intra-AS routing protocol. But what 
about destinations that are outside of the AS? This is precisely where BGP comes to 
the rescue.
In BGP, packets are not routed to a specific destination address, but instead to 
CIDRized prefixes, with each prefix representing a subnet or a collection of subnets. 
In the world of BGP, a destination may take the form 138.16.68/22, which for this 
424         Chapter 5    •    The Network Layer: Control Plane
example includes 1,024 IP addresses. Thus, a router’s forwarding table will have 
entries of the form (x, I), where x is a prefix (such as 138.16.68/22) and I is an inter-
face number for one of the router’s interfaces.
As an inter-AS routing protocol, BGP provides each router a means to:
	1.	 Obtain prefix reachability information from neighboring ASs. In particular, 
BGP allows each subnet to advertise its existence to the rest of the Internet. A 
subnet screams, “I exist and I am here,” and BGP makes sure that all the rout-
ers in the Internet know about this subnet. If it weren’t for BGP, each subnet 
would be an isolated island—alone, unknown and unreachable by the rest of the 
Internet.
	2.	 Determine the “best” routes to the prefixes. A router may learn about two or 
more different routes to a specific prefix. To determine the best route, the router 
will locally run a BGP route-selection procedure (using the prefix reachability 
information it obtained via neighboring routers). The best route will be deter-
mined based on policy as well as the reachability information.
Let us now delve into how BGP carries out these two tasks.
5.4.2 Advertising BGP Route Information
Consider the network shown in Figure 5.8. As we can see, this simple network has 
three autonomous systems: AS1, AS2, and AS3. As shown, AS3 includes a subnet 
with prefix x. For each AS, each router is either a gateway router or an internal 
router. A gateway router is a router on the edge of an AS that directly connects to 
one or more routers in other ASs. An internal router connects only to hosts and 
routers within its own AS. In AS1, for example, router 1c is a gateway router; routers 
1a, 1b, and 1d are internal routers.
Let’s consider the task of advertising reachability information for prefix x to 
all of the routers shown in Figure 5.8. At a high level, this is straightforward. First, 
AS3 sends a BGP message to AS2, saying that x exists and is in AS3; let’s denote 
this message as “AS3 x”. Then AS2 sends a BGP message to AS1, saying that x 
exists and that you can get to x by first passing through AS2 and then going to AS3; 
let’s denote that message as “AS2 AS3 x”. In this manner, each of the autonomous 
systems will not only learn about the existence of x, but also learn about a path of 
autonomous systems that leads to x. 
Although the discussion in the above paragraph about advertising BGP reacha-
bility information should get the general idea across, it is not precise in the sense that 
autonomous systems do not actually send messages to each other, but instead routers 
do. To understand this, let’s now re-examine the example in Figure 5.8. In BGP, 
5.4    •    Routing Among the ISPs: BGP         425
pairs of routers exchange routing information over semi-permanent TCP connections 
using port 179. Each such TCP connection, along with all the BGP messages sent 
over the connection, is called a BGP connection. Furthermore, a BGP connection 
that spans two ASs is called an external BGP (eBGP) connection, and a BGP ses-
sion between routers in the same AS is called an internal BGP (iBGP) connection. 
Examples of BGP connections for the network in Figure 5.8 are shown in Figure 5.9. 
There is typically one eBGP connection for each link that directly connects gateway 
routers in different ASs; thus, in Figure 5.9, there is an eBGP connection between 
gateway routers 1c and 2a and an eBGP connection between gateway routers 2c 
 
and 3a.
There are also iBGP connections between routers within each of the ASs. In 
particular, Figure 5.9 displays a common configuration of one BGP connection for 
each pair of routers internal to an AS, creating a mesh of TCP connections within 
each AS. In Figure 5.9, the eBGP connections are shown with the long dashes; the 
iBGP connections are shown with the short dashes. Note that iBGP connections do 
not always correspond to physical links.
In order to propagate the reachability information, both iBGP and eBGP ses-
sions are used. Consider again advertising the reachability information for prefix x 
to all routers in AS1 and AS2. In this process, gateway router 3a first sends an eBGP 
message “AS3 x” to gateway router 2c. Gateway router 2c then sends the iBGP 
message “AS3 x” to all of the other routers in AS2, including to gateway router 2a. 
Gateway router 2a then sends the eBGP message “AS2 AS3 x” to gateway router 1c. 
2b
2d
2a
2c
AS2
1b
1d
1a
1c
AS1
3b
3d
3a
3c
AS3
X
Figure 5.8  ♦  
Network with three autonomous systems. AS3 includes a  
subnet with prefix x
426         Chapter 5    •    The Network Layer: Control Plane
Finally, gateway router 1c uses iBGP to send the message “AS2 AS3 x” to all the 
routers in AS1. After this process is complete, each router in AS1 and AS2 is aware 
of the existence of x and is also aware of an AS path that leads to x.
Of course, in a real network, from a given router there may be many different 
paths to a given destination, each through a different sequence of ASs. For example, 
consider the network in Figure 5.10, which is the original network in Figure 5.8, with 
an additional physical link from router 1d to router 3d. In this case, there are two 
paths from AS1 to x: the path “AS2 AS3 x” via router 1c; and the new path “AS3 x” 
via the router 1d.
5.4.3 Determining the Best Routes
As we have just learned, there may be many paths from a given router to a destina-
tion subnet. In fact, in the Internet, routers often receive reachability information 
about dozens of different possible paths. How does a router choose among these 
paths (and then configure its forwarding table accordingly)?
Before addressing this critical question, we need to introduce a little more 
BGP terminology. When a router advertises a prefix across a BGP connection, it 
includes with the prefix several BGP attributes. In BGP jargon, a prefix along with 
its attributes is called a route. Two of the more important attributes are AS-PATH 
and NEXT-HOP. The AS-PATH attribute contains the list of ASs through which the 
eBGP
Key:
iBGP
2b
2d
2a
2c
AS2
1b
1d
1a
1c
AS1
3b
3d
3a
3c
AS3
X
Figure 5.9  ♦  eBGP and iBGP connections
5.4    •    Routing Among the ISPs: BGP         427
advertisement has passed, as we’ve seen in our examples above. To generate the AS-
PATH value, when a prefix is passed to an AS, the AS adds its ASN to the existing 
list in the AS-PATH. For example, in Figure 5.10, there are two routes from AS1 
to subnet x: one which uses the AS-PATH “AS2 AS3”; and another that uses the 
AS-PATH “A3”. BGP routers also use the AS-PATH attribute to detect and prevent 
looping advertisements; specifically, if a router sees that its own AS is contained in 
the path list, it will reject the advertisement.
Providing the critical link between the inter-AS and intra-AS routing protocols, 
the NEXT-HOP attribute has a subtle but important use. The NEXT-HOP is the IP 
address of the router interface that begins the AS-PATH. To gain insight into this 
attribute, let’s again refer to Figure 5.10. As indicated in Figure 5.10, the NEXT-
HOP attribute for the route “AS2 AS3 x” from AS1 to x that passes through AS2 
is the IP address of the left interface on router 2a. The NEXT-HOP attribute for the 
route “AS3 x” from AS1 to x that bypasses AS2 is the IP address of the leftmost 
interface of router 3d. In summary, in this toy example, each router in AS1 becomes 
aware of two BGP routes to prefix x:
IP address of leftmost interface for router 2a; AS2 AS3; x
IP address of leftmost interface of router 3d; AS3; x
Here, each BGP route is written as a list with three components: NEXT-HOP; AS-
PATH; destination prefix. In practice, a BGP route includes additional attributes, 
which we will ignore for the time being. Note that the NEXT-HOP attribute is an IP 
NEXT-HOP
NEXT-HOP
2b
2d
2a
2c
AS2
1b
1d
1a
1c
AS1
3b
3d
3a
3c
AS3
X
Figure 5.10  ♦  
Network augmented with peering link between AS1  
and AS3
428         Chapter 5    •    The Network Layer: Control Plane
address of a router that does not belong to AS1; however, the subnet that contains 
this IP address directly attaches to AS1.
Hot Potato Routing
We are now finally in position to talk about BGP routing algorithms in a precise 
manner. We will begin with one of the simplest routing algorithms, namely, hot 
potato routing.
Consider router 1b in the network in Figure 5.10. As just described, this router 
will learn about two possible BGP routes to prefix x. In hot potato routing, the route 
chosen (from among all  possible routes) is that route with the least cost to the NEXT-
HOP router beginning that route. In this example, router 1b will consult its intra-AS 
routing information to find the least-cost intra-AS path to NEXT-HOP router 2a and 
the least-cost intra-AS path to NEXT-HOP router 3d, and then select the route with 
the smallest of these least-cost paths. For example, suppose that cost is defined as the 
number of links traversed. Then the least cost from router 1b to router 2a is 2, the least 
cost from router 1b to router 2d is 3, and router 2a would therefore be selected. Router 
1b would then consult its forwarding table (configured by its intra-AS algorithm) and 
find the interface I that is on the least-cost path to router 2a. It then adds (x, I) to its 
forwarding table.
The steps for adding an outside-AS prefix in a router’s forwarding table for hot 
potato routing are summarized in Figure 5.11. It is important to note that when add-
ing an outside-AS prefix into a forwarding table, both the inter-AS routing protocol 
(BGP) and the intra-AS routing protocol (e.g., OSPF) are used.
The idea behind hot-potato routing is for router 1b to get packets out of its 
AS as quickly as possible (more specifically, with the least cost possible) without 
worrying about the cost of the remaining portions of the path outside of its AS to 
the destination. In the name “hot potato routing,” a packet is analogous to a hot 
potato that is burning in your hands. Because it is burning hot, you want to pass it 
off to another person (another AS) as quickly as possible. Hot potato routing is thus 
Learn from inter-AS
protocol that subnet
x is reachable via
multiple gateways.
Use routing info from
intra-AS protocol to
determine costs of
least-cost paths to
each of the gateways.
Hot potato routing:
Choose the gateway
that has the
smallest least cost.
Determine from
forwarding table the
interface I that leads
to least-cost gateway.
Enter (x,I) in
forwarding table.
Figure 5.11  ♦  
Steps in adding outside-AS destination in a router’s 
­
forwarding table
5.4    •    Routing Among the ISPs: BGP         429
a selfish ­
algorithm—it tries to reduce the cost in its own AS while ignoring the other 
components of the end-to-end costs outside its AS. Note that with hot potato routing, 
two routers in the same AS may choose two different AS paths to the same prefix. 
For example, we just saw that router 1b would send packets through AS2 to reach 
 
x. However, router 1d would bypass AS2 and send packets directly to AS3 to reach x.
Route-Selection Algorithm
In practice, BGP uses an algorithm that is more complicated than hot potato routing, 
but nevertheless incorporates hot potato routing. For any given destination prefix, the 
input into BGP’s route-selection algorithm is the set of all routes to that prefix that have 
been learned and accepted by the router. If there is only one such route, then BGP obvi-
ously selects that route. If there are two or more routes to the same prefix, then BGP 
sequentially invokes the following elimination rules until one route remains:
	1.	 A route is assigned a local preference value as one of its attributes (in addition 
to the AS-PATH and NEXT-HOP attributes). The local preference of a route 
could have been set by the router or could have been learned from another router 
in the same AS. The value of the local preference attribute is a policy decision 
that is left entirely up to the AS’s network administrator. (We will shortly dis-
cuss BGP policy issues in some detail.) The routes with the highest local prefer-
ence values are selected.
	2.	 From the remaining routes (all with the same highest local preference value), 
the route with the shortest AS-PATH is selected. If this rule were the only rule 
for route selection, then BGP would be using a DV algorithm for path determi-
nation, where the distance metric uses the number of AS hops rather than the 
number of router hops.
	3.	 From the remaining routes (all with the same highest local preference value and 
the same AS-PATH length), hot potato routing is used, that is, the route with the 
closest NEXT-HOP router is selected.
	4.	 If more than one route still remains, the router uses BGP identifiers to select the 
route; see [Stewart 1999].
As an example, let’s again consider router 1b in Figure 5.10. Recall that there 
are exactly two BGP routes to prefix x, one that passes through AS2 and one that 
bypasses AS2. Also recall that if hot potato routing on its own were used, then BGP 
would route packets through AS2 to prefix x. But in the above route-selection algo-
rithm, rule 2 is applied before rule 3, causing BGP to select the route that bypasses 
AS2, since that route has a shorter AS PATH. So we see that with the above route-
selection algorithm, BGP is no longer a selfish algorithm—it first looks for routes 
with short AS paths (thereby likely reducing end-to-end delay).
As noted above, BGP is the de facto standard for inter-AS routing for the 
Internet. To see the contents of various BGP routing tables (large!) extracted from 
430         Chapter 5    •    The Network Layer: Control Plane
routers in tier-1 ISPs, see http://www.routeviews.org. BGP routing tables often 
contain over half a million routes (that is, prefixes and corresponding attributes). 
Statistics about the size and characteristics of BGP routing tables are presented in 
[Potaroo 2016].
5.4.4 IP-Anycast
In addition to being the Internet’s inter-AS routing protocol, BGP is often used to 
implement the IP-anycast service [RFC 1546, RFC 7094], which is commonly used 
in DNS. To motivate IP-anycast, consider that in many applications, we are inter-
ested in (1) replicating the same content on different servers in many different dis-
persed geographical locations, and (2) having each user access the content from the 
server that is closest. For example, a CDN may replicate videos and other objects on 
servers in different countries. Similarly, the DNS system can replicate DNS records 
on DNS servers throughout the world. When a user wants to access this replicated 
content, it is desirable to point the user to the “nearest” server with the replicated 
content. BGP’s route-selection algorithm provides an easy and natural mechanism 
for doing so.
To make our discussion concrete, let’s describe how a CDN might use IP-­
anycast. As shown in Figure 5.12, during the IP-anycast configuration stage, the 
CDN company assigns the same IP address to each of its servers, and uses stand-
ard BGP to advertise this IP address from each of the servers. When a BGP router 
receives multiple route advertisements for this IP address, it treats these advertise-
ments as providing different paths to the same physical location (when, in fact, 
the advertisements are for different paths to different physical locations). When 
configuring its routing table, each router will locally use the BGP route-selec-
tion algorithm to pick the “best” (for example, closest, as determined by AS-hop 
counts) route to that IP address. For example, if one BGP route (corresponding to 
one location) is only one AS hop away from the router, and all other BGP routes 
(corresponding to other locations) are two or more AS hops away, then the BGP 
router would choose to route packets to the location that is one hop away. After 
this initial BGP address-advertisement phase, the CDN can do its main job of dis-
tributing content. When a client requests the video, the CDN returns to the client 
the common IP address used by the geographically dispersed servers, no matter 
where the client is located. When the client sends a request to that IP address, 
Internet routers then forward the request packet to the “closest” server, as defined 
by the BGP route-selection algorithm.
Although the above CDN example nicely illustrates how IP-anycast can be 
used, in practice CDNs generally choose not to use IP-anycast because BGP routing 
changes can result in different packets of the same TCP connection arriving at dif-
ferent instances of the Web server. But IP-anycast is extensively used by the DNS 
system to direct DNS queries to the closest root DNS server. Recall from Section 
2.4, there are currently 13 IP addresses for root DNS servers. But corresponding 
5.4    •    Routing Among the ISPs: BGP         431
to each of these addresses, there are multiple DNS root servers, with some of these 
addresses having over 100 DNS root servers scattered over all corners of the world. 
When a DNS query is sent to one of these 13 IP addresses, IP anycast is used to route 
the query to the nearest of the DNS root servers that is responsible for that address. 
5.4.5 Routing Policy
When a router selects a route to a destination, the AS routing policy can trump all 
other considerations, such as shortest AS path or hot potato routing. Indeed, in the 
route-selection algorithm, routes are first selected according to the local-preference 
attribute, whose value is fixed by the policy of the local AS.
Let’s illustrate some of the basic concepts of BGP routing policy with a simple 
example. Figure 5.13 shows six interconnected autonomous systems: A, B, C, W, X, 
and Y. It is important to note that A, B, C, W, X, and Y are ASs, not routers. Let’s 
AS1
AS3
3b
3c
3a
1a
1c
1b
1d
AS2
AS4
2a
2c
4a
4c
4b
Advertise
212.21.21.21
CDN Server B
CDN Server A
Advertise
212.21.21.21
Receive BGP 
advertisements for
212.21.21.21 from
AS1 and from AS4.
Forward toward
Server B since it is
closer.
2b
Figure 5.12  ♦  Using IP-anycast to bring users to the closest CDN server
432         Chapter 5    •    The Network Layer: Control Plane
assume that autonomous systems W, X, and Y are access ISPs and that A, B, and C 
are backbone provider networks. We’ll also assume that A, B, and C, directly send 
traffic to each other, and provide full BGP information to their customer networks. 
All traffic entering an ISP access network must be destined for that network, and 
 
all traffic leaving an ISP access network must have originated in that network. 
 
W and Y are clearly access ISPs. X is a multi-homed access ISP, since it is con-
nected to the rest of the network via two different providers (a scenario that is becom-
ing increasingly common in practice). However, like W and Y, X itself must be the 
source/destination of all traffic leaving/entering X. But how will this stub network 
behavior be implemented and enforced? How will X be prevented from forwarding 
traffic between B and C? This can easily be accomplished by controlling the manner 
in which BGP routes are advertised. In particular X will function as an access ISP 
network if it advertises (to its neighbors B and C) that it has no paths to any other 
destinations except itself. That is, even though X may know of a path, say XCY, that 
reaches network Y, it will not advertise this path to B. Since B is unaware that X has 
a path to Y, B would never forward traffic destined to Y (or C) via X. This simple 
example illustrates how a selective route advertisement policy can be used to imple-
ment customer/provider routing relationships.
Let’s next focus on a provider network, say AS B. Suppose that B has learned 
(from A) that A has a path AW to W. B can thus install the route AW into its routing 
information base. Clearly, B also wants to advertise the path BAW to its customer, 
X, so that X knows that it can route to W via B. But should B advertise the path 
BAW to C? If it does so, then C could route traffic to W via BAW. If A, B, and C are 
all backbone providers, than B might rightly feel that it should not have to shoulder 
the burden (and cost!) of carrying transit traffic between A and C. B might rightly 
feel that it is A’s and C’s job (and cost!) to make sure that C can route to/from A’s 
customers via a direct connection between A and C. There are currently no official 
standards that govern how backbone ISPs route among themselves. However, a rule 
of thumb followed by commercial ISPs is that any traffic flowing across an ISP’s 
backbone network must have either a source or a destination (or both) in a network 
that is a customer of that ISP; otherwise the traffic would be getting a free ride on the 
ISP’s network. Individual peering agreements (that would govern questions such as 
A
W
X
Y
B
Key:
Provider
network
Customer
network
C
Figure 5.13  ♦  A simple BGP policy scenario
5.4    •    Routing Among the ISPs: BGP         433
those raised above) are typically negotiated between pairs of ISPs and are often con-
fidential; [Huston 1999a] provides an interesting discussion of peering agreements. 
For a detailed description of how routing policy reflects commercial relationships 
among ISPs, see [Gao 2001; Dmitiropoulos 2007]. For a discussion of BGP routing 
polices from an ISP standpoint, see [Caesar 2005b].
WHY ARE THERE DIFFERENT INTER-AS AND INTRA-AS ROUTING 
PROTOCOLS?
Having now studied the details of specific inter-AS and intra-AS routing protocols deployed 
in today’s Internet, let’s conclude by considering perhaps the most fundamental question 
we could ask about these protocols in the first place (hopefully, you have been wondering 
this all along, and have not lost the forest for the trees!): Why are different inter-AS and 
intra-AS routing protocols used?
The answer to this question gets at the heart of the differences between the goals of 
routing within an AS and among ASs:
• 
Policy. Among ASs, policy issues dominate. It may well be important that traffic origi-
nating in a given AS not be able to pass through another specific AS. Similarly, a 
given AS may well want to control what transit traffic it carries between other ASs. We 
have seen that BGP carries path attributes and provides for controlled distribution of 
routing information so that such policy-based routing decisions can be made. Within 
an AS, everything is nominally under the same administrative control, and thus policy 
issues play a much less important role in choosing routes within the AS.
• 
Scale. The ability of a routing algorithm and its data structures to scale to handle 
routing to/among large numbers of networks is a critical issue in inter-AS routing. 
Within an AS, scalability is less of a concern. For one thing, if a single ISP becomes 
too large, it is always possible to divide it into two ASs and perform inter-AS routing 
between the two new ASs. (Recall that OSPF allows such a hierarchy to be built by 
splitting an AS into areas.)
• 
Performance. Because inter-AS routing is so policy oriented, the quality (for example, 
performance) of the routes used is often of secondary concern (that is, a longer or 
more costly route that satisfies certain policy criteria may well be taken over a route 
that is shorter but does not meet that criteria). Indeed, we saw that among ASs, there 
is not even the notion of cost (other than AS hop count) associated with routes. Within 
a single AS, however, such policy concerns are of less importance, allowing routing to 
focus more on the level of performance realized on a route.
PRINCIPLES IN PRACTICE
434         Chapter 5    •    The Network Layer: Control Plane
This completes our brief introduction to BGP. Understanding BGP is important 
because it plays a central role in the Internet. We encourage you to see the references 
[Griffin 2012; Stewart 1999; Labovitz 1997; Halabi 2000; Huitema 1998; Gao 2001; 
Feamster 2004; Caesar 2005b; Li 2007] to learn more about BGP.
5.4.6 
Putting the Pieces Together: Obtaining  
Internet Presence
Although this subsection is not about BGP per se, it brings together many of the 
protocols and concepts we’ve seen thus far, including IP addressing, DNS, and BGP.
Suppose you have just created a small company that has a number of servers, 
including a public Web server that describes your company’s products and services, 
a mail server from which your employees obtain their e-mail messages, and a DNS 
server. Naturally, you would like the entire world to be able to visit your Web site in 
order to learn about your exciting products and services. Moreover, you would like your 
employees to be able to send and receive e-mail to potential customers throughout the 
world.
To meet these goals, you first need to obtain Internet connectivity, which is 
done by contracting with, and connecting to, a local ISP. Your company will have 
a gateway router, which will be connected to a router in your local ISP. This con-
nection might be a DSL connection through the existing telephone infrastructure, a 
leased line to the ISP’s router, or one of the many other access solutions described in 
Chapter 1. Your local ISP will also provide you with an IP address range, e.g., a /24 
address range consisting of 256 addresses. Once you have your physical connectivity 
and your IP address range, you will assign one of the IP addresses (in your address 
range) to your Web server, one to your mail server, one to your DNS server, one to 
your gateway router, and other IP addresses to other servers and networking devices 
in your company’s network.
In addition to contracting with an ISP, you will also need to contract with an 
Internet registrar to obtain a domain name for your company, as described in Chapter 
2. For example, if your company’s name is, say, Xanadu Inc., you will naturally try 
to obtain the domain name xanadu.com. Your company must also obtain presence 
in the DNS system. Specifically, because outsiders will want to contact your DNS 
server to obtain the IP addresses of your servers, you will also need to provide your 
registrar with the IP address of your DNS server. Your registrar will then put an 
entry for your DNS server (domain name and corresponding IP address) in the .com 
top-level-domain servers, as described in Chapter 2. After this step is completed, any 
user who knows your domain name (e.g., xanadu.com) will be able to obtain the IP 
address of your DNS server via the DNS system.
So that people can discover the IP addresses of your Web server, in your DNS 
server you will need to include entries that map the host name of your Web server 
(e.g., www.xanadu.com) to its IP address. You will want to have similar entries for 
5.5    •    The SDN Control Plane         435
other publicly available servers in your company, including your mail server. In this 
manner, if Alice wants to browse your Web server, the DNS system will contact your 
DNS server, find the IP address of your Web server, and give it to Alice. Alice can 
then establish a TCP connection directly with your Web server.
However, there still remains one other necessary and crucial step to allow out-
siders from around the world to access your Web server. Consider what happens 
when Alice, who knows the IP address of your Web server, sends an IP datagram 
(e.g., a TCP SYN segment) to that IP address. This datagram will be routed through 
the Internet, visiting a series of routers in many different ASs, and eventually reach 
your Web server. When any one of the routers receives the datagram, it is going 
to look for an entry in its forwarding table to determine on which outgoing port it 
should forward the datagram. Therefore, each of the routers needs to know about the 
existence of your company’s /24 prefix (or some aggregate entry). How does a router 
become aware of your company’s prefix? As we have just seen, it becomes aware of 
it from BGP! Specifically, when your company contracts with a local ISP and gets 
assigned a prefix (i.e., an address range), your local ISP will use BGP to advertise 
your prefix to the ISPs to which it connects. Those ISPs will then, in turn, use BGP 
to propagate the advertisement. Eventually, all Internet routers will know about your 
prefix (or about some aggregate that includes your prefix) and thus be able to appro-
priately forward datagrams destined to your Web and mail servers.
5.5	 The SDN Control Plane
In this section, we’ll dive into the SDN control plane—the network-wide logic that 
controls packet forwarding among a network’s SDN-enabled devices, as well as the 
configuration and management of these devices and their services. Our study here 
builds on our earlier discussion of generalized SDN forwarding in Section 4.4, so you 
might want to first review that section, as well as Section 5.1 of this chapter, before 
continuing on. As in Section 4.4, we’ll again  adopt the terminology used in the SDN 
literature and refer to the network’s forwarding devices as “packet switches” (or just 
switches, with “packet” being understood), since forwarding decisions can be made 
on the basis of network-layer source/destination addresses, link-layer source/destina-
tion addresses, as well as many other values in transport-, network-, and link-layer 
packet-header fields.
Four key characteristics of an SDN architecture can be identified [Kreutz 2015]:
•	 Flow-based forwarding. Packet forwarding by SDN-controlled switches can be 
based on any number of header field values in the transport-layer, network-layer, 
or link-layer header. We saw in Section 4.4 that the OpenFlow1.0 abstraction 
allows forwarding based on eleven different header field values. This contrasts 
436         Chapter 5    •    The Network Layer: Control Plane
sharply with the traditional approach to router-based forwarding that we studied 
in Sections 5.2–5.4, where forwarding of IP datagrams was based solely on a 
datagram’s destination IP address. Recall from Figure 5.2 that packet forwarding 
rules are specified in a switch’s flow table; it is the job of the SDN control plane 
to compute, manage and install flow table entries in all of the network’s switches.
•	 Separation of data plane and control plane. This separation is shown clearly 
in Figures 5.2 and 5.14. The data plane consists of the network’s switches— 
relatively simple (but fast) devices that execute the “match plus action” rules in 
their flow tables. The control plane consists of servers and software that deter-
mine and manage the switches’ flow tables.
•	 Network control functions: external to data-plane switches. Given that the “S” in 
SDN is for “software,” it’s perhaps not surprising that the SDN control plane is 
implemented in software. Unlike traditional routers, however, this software exe-
cutes on servers that are both distinct and remote from the network’s switches. As 
shown in Figure 5.14, the control plane itself consists of two components—an SDN 
controller (or network operating system [Gude 2008]) and a set of network-control 
applications. The controller maintains accurate network state information (e.g., the 
state of remote links, switches, and hosts); provides this information to the network-
control applications running in the control plane; and provides the means through 
which these applications can monitor, program, and control the underlying network 
devices. Although the controller in Figure 5.14 is shown as a single central server, 
in practice the controller is only logically centralized; it is typically implemented on 
several servers that provide coordinated, scalable performance and high availability.
•	 A programmable network. The network is programmable through the network-
control applications running in the control plane. These applications represent 
the “brains” of the SDN control plane, using the APIs provided by the SDN 
controller to specify and control the data plane in the network devices. For exam-
ple, a routing network-control application might determine the end-end paths 
between sources and destinations (e.g., by executing Dijkstra’s algorithm using 
the node-state and link-state information maintained by the SDN controller). 
Another network application might perform access control, i.e., determine which 
packets are to be blocked at a switch, as in our third example in Section 4.4.3. 
Yet another application might forward packets in a manner that performs server 
load balancing (the second example we considered in Section 4.4.3).
From this discussion, we can see that SDN represents a significant “unbundling” 
of network functionality—data plane switches, SDN controllers, and network-control 
 
applications are separate entities that may each be provided by different vendors 
and organizations. This contrasts with the pre-SDN model in which a switch/router 
(together with its embedded control plane software and protocol implementations) 
was monolithic, vertically integrated, and sold by a single vendor. This unbundling 
5.5    •    The SDN Control Plane         437
of network functionality in SDN has been likened to the earlier evolution from main-
frame computers (where hardware, system software, and applications were provided 
by a single vendor) to personal computers (with their separate hardware, operating 
systems, and applications). The unbundling of computing hardware, system soft-
ware, and applications has arguably led to a rich, open ecosystem driven by innova-
tion in all three of these areas; one hope for SDN is that it too will lead to a such rich 
innovation.
Given our understanding of the SDN architecture of Figure 5.14, many questions 
naturally arise. How and where are the flow tables actually computed? How are these 
tables updated in response to events at SDN-controlled devices (e.g., an attached link 
going up/down)? And how are the flow table entries at multiple switches coordinated 
in such a way as to result in orchestrated and consistent network-wide functionality 
(e.g., end-to-end paths for forwarding packets from sources to destinations, or coor-
dinated distributed firewalls)? It is the role of the SDN control plane to provide these, 
and many other, capabilities.
Routing
Network-control Applications
Control
plane
Data
plane
SDN-Controlled Switches
Access
Control
Load
Balancer
Northbound
API
Southbound
API
SDN Controller
(network operating system)
Figure 5.14  ♦  
Components of the SDN architecture: SDN-controlled 
switches, the SDN controller, network-control applications
438         Chapter 5    •    The Network Layer: Control Plane
5.5.2 
The SDN Control Plane: SDN Controller and  
SDN Network-control Applications
Let’s begin our discussion of the SDN control plane in the abstract, by consider-
ing the generic capabilities that the control plane must provide. As we’ll see, this 
abstract, “first principles” approach will lead us to an overall architecture that reflects 
how SDN control planes have been implemented in practice.
As noted above, the SDN control plane divides broadly into two components—
the SDN controller and the SDN network-control applications. Let’s explore the 
controller first. Many SDN controllers have been developed since the earliest SDN 
controller [Gude 2008]; see [Kreutz 2015] for an extremely thorough and up-to-date 
survey. Figure 5.15 provides a more detailed view of a generic SDN controller. A 
controller’s functionality can be broadly organized into three layers. Let’s consider 
these layers in an uncharacteristically bottom-up fashion:
•	 A communication layer: communicating between the SDN controller and con-
trolled network devices. Clearly, if an SDN controller is going to control the 
operation of a remote SDN-enabled switch, host, or other device, a protocol is 
needed to transfer information between the controller and that device. In addition, 
a device must be able to communicate locally-observed events to the controller 
 
(e.g., a message indicating that an attached link has gone up or down, that a 
device has just joined the network, or a heartbeat indicating that a device is up and 
operational). These events provide the SDN controller with an up-to-date view 
of the network’s state. This protocol constitutes the lowest layer of the controller 
architecture, as shown in Figure 5.15. The communication between the controller 
and the controlled devices cross what has come to be known as the controller’s 
“southbound” interface. In Section 5.5.2, we’ll study OpenFlow—a specific pro-
tocol that provides this communication functionality. OpenFlow is implemented 
in most, if not all, SDN controllers.
•	 A network-wide state-management layer. The ultimate control decisions made by 
the SDN control plane—e.g., configuring flow tables in all switches to achieve 
the desired end-end forwarding, to implement load balancing, or to implement a 
particular firewalling capability—will require that the controller have up-to-date 
information about state of the networks’ hosts, links, switches, and other SDN-
controlled devices. A switch’s flow table contains counters whose values might 
also be profitably used by network-control applications; these values should thus 
be available to the applications. Since the ultimate aim of the control plane is to 
determine flow tables for the various controlled devices, a controller might also 
maintain a copy of these tables. These pieces of information all constitute exam-
ples of the network-wide “state” maintained by the SDN controller.
•	 The interface to the network-control application layer. The controller interacts 
with network-control applications through its “northbound” interface. This API 
5.5    •    The SDN Control Plane         439
allows network-control applications to read/write network state and flow tables 
within the state-management layer. Applications can register to be notified when 
state-change events occur, so that they can take actions in response to network 
event notifications sent from SDN-controlled devices. Different types of APIs 
may be provided; we’ll see that two popular SDN controllers communicate with 
their applications using a REST [Fielding 2000] request-response interface.
We have noted several times that an SDN controller can be considered to be 
­
“logically centralized,” i.e., that the controller may be viewed externally (e.g., from the 
point of view of SDN-controlled devices and external network-control applications) 
Routing
Access
Control
Load
Balancer
Interface, abstractions for  network control apps
Network
graph
RESTful
API
Intent
Communication to/from controlled devices
Network-wide distributed, robust state management
Link-state
info
Host info
Switch
info
Statistics
Flow
tables
OpenFlow
SNMP
SDN Controller
Northbound
API
Southbound
API
Figure 5.15  ♦  Components of an SDN controller
440         Chapter 5    •    The Network Layer: Control Plane
as a single, monolithic service. However, these services and the databases used to 
hold state information are implemented in practice by a distributed set of servers 
for fault tolerance, high availability, or for performance reasons. With controller 
functions being implemented by a set of servers, the semantics of the controller’s 
internal operations (e.g., maintaining logical time ordering of events, consistency, 
consensus, and more) must be considered [Panda 2013]. Such concerns are com-
mon across many different distributed systems; see [Lamport 1989, Lampson 1996] 
for elegant solutions to these challenges. Modern controllers such as OpenDaylight 
[OpenDaylight Lithium 2016] and ONOS [ONOS 2016] (see sidebar) have placed 
considerable emphasis on architecting a logically centralized but physically distrib-
uted controller platform that provides scalable services and high availability to the 
controlled devices and network-control applications alike.
The architecture depicted in Figure 5.15 closely resembles the architecture of the 
originally proposed NOX controller in 2008 [Gude 2008], as well as that of today’s 
OpenDaylight [OpenDaylight Lithium 2016] and ONOS [ONOS 2016] SDN control-
lers (see sidebar). We’ll cover an example of controller operation in Section 5.5.3. 
First, however, let’s examine the OpenFlow protocol, which lies in the controller’s 
communication layer.
5.5.2 OpenFlow Protocol
The OpenFlow protocol [OpenFlow 2009, ONF 2016] operates between an SDN 
controller and an SDN-controlled switch or other device implementing the Open-
Flow API that we studied earlier in Section 4.4. The OpenFlow protocol operates 
over TCP, with a default port number of 6653.
Among the important messages flowing from the controller to the controlled 
switch are the following:
•	 Configuration. This message allows the controller to query and set a switch’s 
configuration parameters.
•	 Modify-State. This message is used by a controller to add/delete or modify entries 
in the switch’s flow table, and to set switch port properties.
•	 Read-State. This message is used by a controller to collect statistics and counter 
values from the switch’s flow table and ports.
•	 Send-Packet. This message is used by the controller to send a specific packet out 
of a specified port at the controlled switch. The message itself contains the packet 
to be sent in its payload.
Among the messages flowing from the SDN-controlled switch to the controller 
are the following:
•	 Flow-Removed. This message informs the controller that a flow table entry has 
been removed, for example by a timeout or as the result of a received modify-state 
message.
5.5    •    The SDN Control Plane         441
•	 Port-status. This message is used by a switch to inform the controller of a change 
in port status.
•	 Packet-in. Recall from Section 4.4 that a packet arriving at a switch port and not 
matching any flow table entry is sent to the controller for additional processing. 
Matched packets may also be sent to the controller, as an action to be taken on a 
match. The packet-in message is used to send such packets to the controller.
Additional OpenFlow messages are defined in [OpenFlow 2009, ONF 2016].
Google’s Software-Defined GLOBAL Network
Recall from the case study in Section 2.6 that Google deploys a dedicated wide-area 
network (WAN) that interconnects its data centers and server clusters (in IXPs and ISPs). 
This network, called B4, has a Google-designed SDN control plane built on OpenFlow. 
Google’s network is able to drive WAN links at near 70% utilization over the long run  
(a two to three fold increase over typical link utilizations) and split application flows among 
multiple paths based on application priority and existing flow demands [Jain 2013].
The Google B4 network is particularly it well-suited for SDN: (i) Google controls all 
devices from the edge servers in IXPs and ISPs to routers in their network core; (ii) the 
most bandwidth-intensive applications are large-scale data copies between sites that can 
defer to higher-priority interactive applications during times of resource congestion;  
(iii) with only a few dozen data centers being connected, centralized control is feasible.
Google’s B4 network uses custom-built switches, each implementing a slightly extended ver-
sion of OpenFlow, with a local Open Flow Agent (OFA) that is similar in spirit to the control 
agent we encountered in Figure 5.2. Each OFA in turn connects to an Open Flow Controller 
(OFC) in the network control server (NCS), using a separate “out of band” network, distinct 
from the network that carries data-center traffic between data centers. The OFC thus provides 
the services used by the NCS to communicate with its controlled switches, similar in spirit to 
the lowest layer in the SDN architecture shown in Figure 5.15. In B4, the OFC also performs 
state management functions, keeping node and link status in a Network Information Base 
(NIB). Google’s implementation of the OFC is based on the ONIX SDN controller [Koponen 
2010]. Two routing protocols, BGP (for routing between the data centers) and IS-IS (a close 
relative of OSPF, for routing within a data center), are implemented. Paxos [Chandra 2007] is 
used to execute hot replicas of NCS components to protect against failure.
A traffic engineering network-control application, sitting logically above the set of 
network control servers, interacts with these servers to provide global, network-wide band-
width provisioning for groups of application flows. With B4, SDN made an important 
leap forward into the operational networks of a global network provider. See [Jain 2013] 
for a detailed description of B4.
PRINCIPLES IN PRACTICE
442         Chapter 5    •    The Network Layer: Control Plane
5.5.3	Data and Control Plane Interaction: An Example
In order to solidify our understanding of the interaction between SDN-controlled 
switches and the SDN controller, let’s consider the example shown in Figure 5.16, 
in which Dijkstra’s algorithm (which we studied in Section 5.2) is used to determine 
shortest path routes. The SDN scenario in Figure 5.16 has two important differ-
ences from the earlier per-router-control scenario of Sections 5.2.1 and 5.3, where 
­
Dijkstra’s algorithm was implemented in each and every router and link-state updates 
were flooded among all network routers:
•	 Dijkstra’s algorithm is executed as a separate application, outside of the packet 
switches.
•	 Packet switches send link updates to the SDN controller and not to each other.
In this example, let’s assume that the link between switch s1 and s2 goes 
down; that shortest path routing is implemented, and consequently and that incom-
ing and outgoing flow forwarding rules at s1, s3, and s4 are affected, but that s2’s 
Figure 5.16  ♦  SDN controller scenario: Link-state change
Network
graph
RESTful
API
Intent
Statistics
Flow
tables
OpenFlow
SNMP
Dijkstra’s link-state
Routing
4
3
2
1
5
s1
s2
s3
s4
6
Link-state
info
Host info
Switch
info
5.5    •    The SDN Control Plane         443
operation is unchanged. Let’s also assume that OpenFlow is used as the communi-
cation layer protocol, and that the control plane performs no other function other 
than link-state routing.
	1.	 Switch s1, experiencing a link failure between itself and s2, notifies the SDN 
controller of the link-state change using the OpenFlow port-status message.
	2.	 The SDN controller receives the OpenFlow message indicating the link-state 
change, and notifies the link-state manager, which updates a link-state ­
database.
	3.	 The network-control application that implements Dijkstra’s link-state routing 
has previously registered to be notified when link state changes. That applica-
tion receives the notification of the link-state change.
	4.	 The link-state routing application interacts with the link-state manager to get 
updated link state; it might also consult other components in the state-­
management 
layer. It then computes the new least-cost paths.
	5.	 The link-state routing application then interacts with the flow table manager, 
which determines the flow tables to be updated.
	6.	 The flow table manager then uses the OpenFlow protocol to update flow table 
entries at affected switches—s1 (which will now route packets destined to s2 via s4), 
s2 (which will now begin receiving packets from s1 via intermediate switch s4), and 
s4 (which must now forward packets from s1 destined to s2).
This example is simple but illustrates how the SDN control plane provides control-
plane services (in this case network-layer routing) that had been previously imple-
mented with per-router control exercised in each and every network router. One can 
now easily appreciate how an SDN-enabled ISP could easily switch from least-cost 
path routing to a more hand-tailored approach to routing. Indeed, since the controller 
can tailor the flow tables as it pleases, it can implement any form of forwarding that 
it pleases—simply by changing its application-control software. This ease of change 
should be contrasted to the case of a traditional per-router control plane, where soft-
ware in all routers (which might be provided to the ISP by multiple independent 
vendors) must be changed.
5.5.4	SDN: Past and Future
Although the intense interest in SDN is a relatively recent phenomenon, the techni-
cal roots of SDN, and the separation of the data and control planes in particular, go 
back considerably further. In 2004, [Feamster 2004, Lakshman 2004, RFC 3746] all 
argued for the separation of the network’s data and control planes. [van der Merwe 
1998] describes a control framework for ATM networks [Black 1995] with multiple 
controllers, each controlling a number of ATM switches. The Ethane project [Casado 
2007] pioneered the notion of a network of simple flow-based Ethernet switches with 
match-plus-action flow tables, a centralized controller that managed flow admission 
444         Chapter 5    •    The Network Layer: Control Plane
and routing, and the forwarding of unmatched packets from the switch to the control-
ler. A network of more than 300 Ethane switches was operational in 2007. Ethane 
quickly evolved into the OpenFlow project, and the rest (as the saying goes) is history!
Numerous research efforts are aimed at developing future SDN architectures 
and capabilities. As we have seen, the SDN revolution is leading to the disruptive 
replacement of dedicated monolithic switches and routers (with both data and con-
trol planes) by simple commodity switching hardware and a sophisticated software 
control plane. A generalization of SDN known as network functions virtualization 
(NFV) similarly aims at disruptive replacement of sophisticated middleboxes (such 
as middleboxes with dedicated hardware and proprietary software for media caching/
service) with simple commodity servers, switching, and storage [Gember-Jacobson 
2014]. A second area of important research seeks to extend SDN concepts from the 
intra-AS setting to the inter-AS setting [Gupta 2014].
SDN Controller Case Studies: The OpenDaylight  
and ONOS Controllers
In the earliest days of SDN, there was a single SDN protocol (OpenFlow [McKeown 2008; 
OpenFlow 2009]) and a single SDN controller (NOX [Gude 2008]). Since then, the num-
ber of SDN controllers in particular has grown significantly [Kreutz 2015]. Some SDN 
controllers are company-specific and proprietary, e.g., ONIX [Koponen 2010], Juniper 
Networks Contrail [Juniper Contrail 2016], and Google’s controller [Jain 2013] for its  
B4 wide-area network. But many more controllers are open-source and implemented in a 
variety of programming languages [Erickson 2013]. Most recently, the OpenDaylight  
controller [OpenDaylight Lithium 2016] and the ONOS controller [ONOS 2016] have 
found considerable industry support. They are both open-source and are being developed 
in partnership with the Linux Foundation.
The OpenDaylight Controller
Figure 5.17 presents a simplified view of the OpenDaylight Lithium SDN controller platform 
[OpenDaylight Lithium 2016]. ODL’s main set of controller components correspond closely 
to those we developed in Figure 5.15.
Network-Service Applications are the applications that determine how data-plane for-
warding and other services, such as firewalling and load balancing, are accomplished in 
the controlled switches. Unlike the canonical controller in Figure 5.15, the ODL controller 
has two interfaces through which applications may communicate with native controller 
services and each other: external applications communicate with controller modules using 
a REST request-response API running over HTTP. Internal applications communicate with 
each other via the Service Abstraction Layer (SAL). The choice as to whether a control-
ler application is implemented externally or internally is up to the application designer; 
Principles in Practice
5.5    •    The SDN Control Plane         445
Figure 5.17  ♦  The OpenDaylight controller
REST API
Trafﬁc
Engineering
Service Abstraction Layer (SAL)
OpenFlow 1.0
SNMP
OVSDB
Access
Control
Network service
apps
Basic Network Service Functions
Topology
manager
Switch
manager
Stats
manager
Forwarding
manager
Host
manager
Network
Service Apps
ODL
Controller
the particular configuration of applications shown in Figure 5.17 is only meant as an 
­
example.
ODL’s Basic Network-Service Functions are at the heart of the controller, and they 
correspond closely to the network-wide state management capabilities that we encoun-
tered in Figure 5.15. The SAL is the controller’s nerve center, allowing controller 
­
components and applications to invoke each other’s services and to subscribe to events 
they generate. It also provides a uniform abstract interface to the specific underlying 
communications protocols in the communication layer, including OpenFlow and SNMP 
(the Simple Network Management Protocol—a network management protocol that we 
will cover in Section 5.7). OVSDB is a protocol used to manage data center switching, 
an important application area for SDN technology. We’ll introduce data center network-
ing in Chapter 6.
446         Chapter 5    •    The Network Layer: Control Plane
The ONOS Controller
Figure 5.18 presents a simplified view of the ONOS controller ONOS 2016]. Similar 
to the canonical controller in Figure 5.15, three layers can be identified in the ONOS 
­
controller:
• 
Northbound abstractions and protocols.  A unique feature of ONOS is its intent 
framework, which allows an application to request a high-level service (e.g., to setup 
a connection between host A and Host B, or conversely to not allow Host A and host 
B to communicate) without having to know the details of how this service is performed. 
State information is provided to network-control applications across the northbound API 
either synchronously (via query) or asynchronously (via listener callbacks, e.g., when 
network state changes).
• 
Distributed core.  The state of the network’s links, hosts, and devices is maintained 
in ONOS’s distributed core. ONOS is deployed as a service on a set of intercon-
nected servers, with each server running an identical copy of the ONOS software; an 
increased number of servers offers an increased service capacity. The ONOS core 
Figure 5.18  ♦  ONOS controller architecture
Intent
REST   API
Hosts
Paths
Topology
Devices
Links
Flow rules
Statistics
Device
Link
Host
Flow
Packet
OpenFlow
Netconf
OVSDB
Network
control apps
Northbound
abstractions,
protocols
ONOS
distributed
core
Southbound
abstractions,
protocols
5.6    •    ICMP: The Internet Control Message Protocol         447
5.6	 ICMP: The Internet Control Message Protocol
The Internet Control Message Protocol (ICMP), specified in [RFC 792], is used by 
hosts and routers to communicate network-layer information to each other. The most 
typical use of ICMP is for error reporting. For example, when running an HTTP 
session, you may have encountered an error message such as “Destination network 
unreachable.” This message had its origins in ICMP. At some point, an IP router was 
unable to find a path to the host specified in your HTTP request. That router created 
and sent an ICMP message to your host indicating the error.
ICMP is often considered part of IP, but architecturally it lies just above IP, as 
ICMP messages are carried inside IP datagrams. That is, ICMP messages are carried 
as IP payload, just as TCP or UDP segments are carried as IP payload. Similarly, 
when a host receives an IP datagram with ICMP specified as the upper-layer protocol 
(an upper-layer protocol number of 1), it demultiplexes the datagram’s contents to 
ICMP, just as it would demultiplex a datagram’s content to TCP or UDP.
ICMP messages have a type and a code field, and contain the header and the first 
8 bytes of the IP datagram that caused the ICMP message to be generated in the first 
place (so that the sender can determine the datagram that caused the error). Selected 
ICMP message types are shown in Figure 5.19. Note that ICMP messages are used 
not only for signaling error conditions.
The well-known ping program sends an ICMP type 8 code 0 message to the 
specified host. The destination host, seeing the echo request, sends back a type 0 
code 0 ICMP echo reply. Most TCP/IP implementations support the ping server 
directly in the operating system; that is, the server is not a process. Chapter 11 of 
[Stevens 1990] provides the source code for the ping client program. Note that the 
client program needs to be able to instruct the operating system to generate an ICMP 
message of type 8 code 0.
Another interesting ICMP message is the source quench message. This message 
is seldom used in practice. Its original purpose was to perform congestion control—to 
 
allow a congested router to send an ICMP source quench message to a host to force 
provides the mechanisms for service replication and coordination among instances, 
providing the applications above and the network devices below with the abstraction 
of logically centralized core services.
• 
Southbound abstractions and protocols.  The southbound abstractions mask the hetero-
geneity of the underlying hosts, links, switches, and protocols, allowing the distributed 
core to be both device and protocol agnostic. Because of this abstraction, the south-
bound interface below the distributed core is logically higher than in our canonical 
controller in Figure 5.14 or the ODL controller in Figure 5.17.
448         Chapter 5    •    The Network Layer: Control Plane
that host to reduce its transmission rate. We have seen in Chapter 3 that TCP has its 
own congestion-control mechanism that operates at the transport layer, without the 
use of network-layer feedback such as the ICMP source quench message.
In Chapter 1 we introduced the Traceroute program, which allows us to trace a 
route from a host to any other host in the world.  Interestingly, Traceroute is imple-
mented with ICMP messages. To determine the names and addresses of the routers 
between source and destination, Traceroute in the source sends a series of ordinary IP 
datagrams to the destination. Each of these datagrams carries a UDP segment with an 
unlikely UDP port number. The first of these datagrams has a TTL of 1, the second of 2, 
the third of 3, and so on. The source also starts timers for each of the datagrams. When 
the nth datagram arrives at the nth router, the nth router observes that the TTL of the 
datagram has just expired. According to the rules of the IP protocol, the router discards 
the datagram and sends an ICMP warning message to the source (type 11 code 0). This 
warning message includes the name of the router and its IP address. When this ICMP 
message arrives back at the source, the source obtains the round-trip time from the 
timer and the name and IP address of the nth router from the ICMP message.
How does a Traceroute source know when to stop sending UDP segments? 
Recall that the source increments the TTL field for each datagram it sends. Thus, one 
of the datagrams will eventually make it all the way to the destination host. Because 
this datagram contains a UDP segment with an unlikely port number, the destination 
Figure 5.19  ♦  ICMP message types
ICMP Type
Code
Description
0
3
3
3
3
3
3
4
8
9
10
11
12
0
0
1
2
3
6
7
0
0
0
0
0
0
echo reply (to ping)
destination network unreachable
destination host unreachable
destination protocol unreachable
destination port unreachable
destination network unknown
destination host unknown
source quench (congestion control)
echo request
router advertisement
router discovery
TTL expired
IP header bad
5.7    •    Network Management and SNMP         449
host sends a port unreachable ICMP message (type 3 code 3) back to the source. 
When the source host receives this particular ICMP message, it knows it does not 
need to send additional probe packets. (The standard Traceroute program actually 
sends sets of three packets with the same TTL; thus the Traceroute output provides 
three results for each TTL.)
In this manner, the source host learns the number and the identities of routers 
that lie between it and the destination host and the round-trip time between the two 
hosts. Note that the Traceroute client program must be able to instruct the operating 
system to generate UDP datagrams with specific TTL values and must also be able to 
be notified by its operating system when ICMP messages arrive. Now that you under-
stand how Traceroute works, you may want to go back and play with it some more.
A new version of ICMP has been defined for IPv6 in RFC 4443. In addition to 
reorganizing the existing ICMP type and code definitions, ICMPv6 also added new 
types and codes required by the new IPv6 functionality. These include the “Packet 
Too Big” type and an “unrecognized IPv6 options” error code.
5.7	 Network Management and SNMP
Having now made our way to the end of our study of the network layer, with only 
the link-layer before us, we’re well aware that a network consists of many com-
plex, interacting pieces of hardware and software—from the links, switches, routers, 
hosts, and other devices that comprise the physical components of the network to 
the many protocols that control and coordinate these devices. When hundreds or 
thousands of such components are brought together by an organization to form a 
network, the job of the network administrator to keep the network “up and running” 
is surely a challenge. We saw in Section 5.5 that the logically centralized controller 
can help with this process in an SDN context. But the challenge of network manage-
ment has been around long before SDN, with a rich set of network management tools 
and approaches that help the network administrator monitor, manage, and control the 
network. We’ll study these tools and techniques in this section.
An often-asked question is “What is network management?” A well-conceived, 
single-sentence (albeit a rather long run-on sentence) definition of network manage-
ment from [Saydam 1996] is:
Network management includes the deployment, integration, and coordination of 
the hardware, software, and human elements to monitor, test, poll, configure, ana-
lyze, evaluate, and control the network and element resources to meet the real-time, 
operational performance, and Quality of Service requirements at a reasonable cost.
Given this broad definition, we’ll cover only the rudiments of network man-
agement in this section—the architecture, protocols, and information base used by 
450         Chapter 5    •    The Network Layer: Control Plane
a network administrator in performing their task. We’ll not cover the administrator’s 
decision-making processes, where topics such as fault identification [Labovitz 1997; 
Steinder 2002; Feamster 2005; Wu 2005; Teixeira 2006], anomaly detection [Lakhina 
2005; Barford 2009], network design/engineering to meet contracted Service Level 
Agreements (SLA’s) [Huston 1999a], and more come into consideration. Our focus 
is thus purposefully narrow; the interested reader should consult these references, the 
excellent network-management text by Subramanian [Subramanian 2000], and the 
more detailed treatment of network management available on the Web site for this text.
5.7.1	The Network Management Framework
Figure 5.20 shows the key components of network management:
•	 The managing server is an application, typically with a human in the loop, run-
ning in a centralized network management station in the network operations center 
(NOC). The managing server is the locus of activity for network management; it 
controls the collection, processing, analysis, and/or display of network management 
information. It is here that actions are initiated to control network behavior and here 
that the human network administrator interacts with the network’s devices.
•	 A managed device is a piece of network equipment (including its software) that 
resides on a managed network. A managed device might be a host, router, switch, 
middlebox, modem, thermometer, or other network-connected device. There may 
be several so-called managed objects within a managed device. These managed 
objects are the actual pieces of hardware within the managed device (for example, 
a network interface card is but one component of a host or router), and configura-
tion parameters for these hardware and software components (for example, an 
intra-AS routing protocol such as OSPF).
•	 Each managed object within a managed device associated information that is collected 
into a Management Information Base (MIB); we’ll see that the values of these 
pieces of information are available to (and in many cases able to be set by) the man-
aging server. A MIB object might be a counter, such as the number of IP datagrams 
discarded at a router due to errors in an IP datagram header, or the number of UDP 
segments received at a host; descriptive information such as the version of the soft-
ware running on a DNS server; status information such as whether a particular device 
is functioning correctly; or protocol-specific information such as a routing path to a 
destination. MIB objects are specified in a data description language known as SMI 
(Structure of Management Information) [RFC 2578; RFC 2579; RFC 2580]. A formal 
definition language is used to ensure that the syntax and semantics of the network 
management data are well defined and unambiguous. Related MIB objects are gath-
ered into MIB modules. As of mid-2015, there were nearly 400 MIB modules defined 
by RFCs, and a much larger number of vendor-specific (private) MIB modules.
•	 Also resident in each managed device is a network management agent, a pro-
cess running in the managed device that communicates with the managing server, 
5.7    •    Network Management and SNMP         451
taking local actions at the managed device under the command and control of the 
managing server. The network management agent is similar to the routing agent 
that we saw in Figure 5.2.
•	 The final component of a network management framework is the network 
­
management protocol. The protocol runs between the managing server and the 
managed devices, allowing the managing server to query the status of managed 
devices and indirectly take actions at these devices via its agents. Agents can use 
the network management protocol to inform the managing server of exceptional 
events (for example, component failures or violation of performance thresholds). 
It’s important to note that the network management protocol does not itself man-
age the network. Instead, it provides capabilities that a network administrator can 
use to manage (“monitor, test, poll, configure, analyze, evaluate, and control”) the 
network. This is a subtle, but important, distinction. In the following section, we’ll 
cover the Internet’s SNMP (Simple Network Management Protocol) ­
protocol.
Figure 5.20  ♦  
Elements of network management: Managing server, 
­
managed devices, MIB data, remote agents, SNMP
Managing server
Agent
Agent
Managed
device
Managed
device
Managed
device
Managed
device
Agent
MIB data
MIB data
Agent MIB data
MIB data
SNMP
protocol
Key:
Agent MIB data
Managed
device
452         Chapter 5    •    The Network Layer: Control Plane
5.7.2	The Simple Network Management Protocol (SNMP)
The Simple Network Management Protocol version 2 (SNMPv2) [RFC 3416] 
is an application-layer protocol used to convey network-management control and 
information messages between a managing server and an agent executing on behalf 
of that managing server. The most common usage of SNMP is in a request-response 
mode in which an SNMP managing server sends a request to an SNMP agent, who 
receives the request, performs some action, and sends a reply to the request. Typi-
cally, a request will be used to query (retrieve) or modify (set) MIB object values 
associated with a managed device. A second common usage of SNMP is for an agent 
to send an unsolicited message, known as a trap message, to a managing server. Trap 
messages are used to notify a managing server of an exceptional situation (e.g., a 
link interface going up or down) that has resulted in changes to MIB object values.
SNMPv2 defines seven types of messages, known generically as protocol data 
units—PDUs—as shown in Table 5.2 and described below. The format of the PDU 
is shown in Figure 5.21.
•	 The GetRequest, GetNextRequest, and GetBulkRequest PDUs are 
all sent from a managing server to an agent to request the value of one or more MIB 
objects at the agent’s managed device. The MIB objects whose values are being 
Table 5.2  ♦  SNMPv2 PDU types
SNMPv2 PDU Type
Sender-receiver
Description
GetRequest
manager-to-agent
get value of one or more MIB object instances
GetNextRequest
manager-to-agent
get value of next MIB object instance in list or table
GetBulkRequest
manager-to-agent
get values in large block of data, for example, values 
in a large table
InformRequest
manager-to-manager
inform remote managing entity of MIB values remote 
to its access
SetRequest
manager-to-agent
set value of one or more MIB object instances
Response
agent-to-manager or
generated in response to 
manager-to-manager
  GetRequest,  
  GetNextRequest,  
  GetBulkRequest,  
  SetRequest PDU, or  
  InformRequest
SNMPv2-Trap
agent-to-manager
inform manager of an exceptional event #
5.7    •    Network Management and SNMP         453
requested are specified in the variable binding portion of the PDU. ­
GetRequest, 
GetNextRequest, and GetBulkRequest differ in the granularity of their 
data requests. GetRequest can request an arbitrary set of MIB values; multiple 
GetNextRequests can be used to sequence through a list or table of MIB 
objects; GetBulkRequest allows a large block of data to be returned, avoiding 
the overhead incurred if multiple GetRequest or ­
GetNextRequest mes-
sages were to be sent. In all three cases, the agent responds with a Response 
PDU containing the object identifiers and their associated values.
•	 The SetRequest PDU is used by a managing server to set the value of one or 
more MIB objects in a managed device. An agent replies with a Response PDU 
with the “noError” error status to confirm that the value has indeed been set.
•	 The InformRequest PDU is used by a managing server to notify another 
managing server of MIB information that is remote to the receiving server.
•	 The Response PDU is typically sent from a managed device to the managing server 
in response to a request message from that server, returning the requested information.
•	 The final type of SNMPv2 PDU is the trap message. Trap messages are generated 
asynchronously; that is, they are not generated in response to a received request but 
rather in response to an event for which the managing server requires notification. 
RFC 3418 defines well-known trap types that include a cold or warm start by a 
device, a link going up or down, the loss of a neighbor, or an authentication failure 
event. A received trap request has no required response from a managing server.
Given the request-response nature of SNMP, it is worth noting here that although 
SNMP PDUs can be carried via many different transport protocols, the SNMP PDU 
is typically carried in the payload of a UDP datagram. Indeed, RFC 3417 states 
Figure 5.21  ♦  SNMP PDU format
PDU
type
(0–3)
Request
Id
Error
Status
(0–5)
Error
Index
Name
Value
Name
Name
Value
PDU
Type
(4)
Enterprise
Agent
Addr
Trap
Type
(0–7)
Speciﬁc
code
Time
stamp
Value
Get/set header
Trap header
Trap information
SNMP PDU
Variables to get/set
454         Chapter 5    •    The Network Layer: Control Plane
that UDP is “the ­
preferred transport mapping.” However, since UDP is an unreli-
able transport protocol, there is no guarantee that a request, or its response, will be 
received at the intended destination. The request ID field of the PDU (see Figure 
5.21) is used by the managing server to number its requests to an agent; the agent’s 
response takes its request ID from that of the received request. Thus, the request ID 
field can be used by the managing server to detect lost requests or replies. It is up to 
the managing server to decide whether to retransmit a request if no corresponding 
response is received after a given amount of time. In particular, the SNMP standard 
does not mandate any particular procedure for retransmission, or even if retransmis-
sion is to be done in the first place. It only requires that the managing server “needs 
to act responsibly in respect to the frequency and duration of retransmissions.” This, 
of course, leads one to wonder how a “responsible” protocol should act!
SNMP has evolved through three versions. The designers of SNMPv3 have said 
that “SNMPv3 can be thought of as SNMPv2 with additional security and administra-
tion capabilities” [RFC 3410]. Certainly, there are changes in SNMPv3 over SNMPv2, 
but nowhere are those changes more evident than in the area of administration and secu-
rity. The central role of security in SNMPv3 was particularly important, since the lack 
of adequate security resulted in SNMP being used primarily for monitoring rather than 
control (for example, SetRequest is rarely used in SNMPv1). Once again, we see that 
­
security—a topic we’ll cover in detail in Chapter 8 — is of critical concern, but once again 
a concern whose importance had been realized perhaps a bit late and only then “added on.”
5.7	 Summary
We have now completed our two-chapter journey into the network core—a jour-
ney that began with our study of the network layer’s data plane in Chapter 4 and 
finished here with our study of the network layer’s control plane. We learned that 
the control plane is the network-wide logic that controls not only how a datagram 
is forwarded among routers along an end-to-end path from the source host to the 
destination host, but also how network-layer components and services are config-
ured and managed.
We learned that there are two broad approaches towards building a control 
plane: traditional per-router control (where a routing algorithm runs in each and 
every router and the routing component in the router communicates with the routing 
components in other routers) and software-defined networking (SDN) control (where 
a logically centralized controller computes and distributes the forwarding tables to 
be used by each and every router). We studied two fundamental routing algorithms 
for computing least cost paths in a graph—link-state routing and distance-vector 
routing—in Section 5.2; these algorithms find application in both per-router control 
and in SDN control. These algorithms are the basis for two widely-deployed Internet 
Homework Problems and Questions         455
routing protocols, OSPF and BGP, that we covered in Sections 5.3 and 5.4. We 
covered the SDN approach to the network-layer control plane in Section 5.5, inves-
tigating SDN network-control applications, the SDN controller, and the OpenFlow 
protocol for communicating between the controller and SDN-controlled devices. In 
Sections 5.6 and 5.7, we covered some of the nuts and bolts of managing an IP net-
work: ICMP (the Internet Control Message Protocol) and SNMP (the Simple Net-
work Management Protocol).
Having completed our study of the network layer, our journey now takes us 
one step further down the protocol stack, namely, to the link layer. Like the network 
layer, the link layer is part of each and every network-connected device. But we will 
see in the next chapter that the link layer has the much more localized task of moving 
packets between nodes on the same link or LAN. Although this task may appear on 
the surface to be rather simple compared with that of the network layer’s tasks, we 
will see that the link layer involves a number of important and fascinating issues that 
can keep us busy for a long time.
Homework Problems and Questions
Chapter 5 Review Questions
SECTION 5.1
	R1.	 What is meant by a control plane that is based on per-router control? In such 
cases, when we say the network control and data planes are implemented 
“monolithically,” what do we mean?
	R2.	 What is meant by a control plane that is based on logically centralized 
control? In such cases, are the data plane and the control plane implemented 
within the same device or in separate devices? Explain.
SECTION 5.2
	R3.	 Compare and contrast the properties of a centralized and a distributed routing 
algorithm. Give an example of a routing protocol that takes a centralized and 
a decentralized approach.
	R4.	 Compare and contrast static and dynamic routing algorithms.
	R5.	 What is the “count to infinity” problem in distance vector routing?
	R6.	 How is a least cost path calculated in a decentralized routing algorithm?
SECTIONS 5.3–5.4
	R7.	 Why are different inter-AS and intra-AS protocols used in the Internet?
	R8.	 True or false: When an OSPF route sends its link state information, it is sent 
only to those nodes directly attached neighbors. Explain.
456         Chapter 5    •    The Network Layer: Control Plane
	R9.	 What is meant by an area in an OSPF autonomous system? Why was the 
concept of an area introduced?
	
R10.	 Define and contrast the following terms: subnet, prefix, and BGP route.
	
R11.	 How does BGP use the NEXT-HOP attribute? How does it use the AS-PATH 
attribute?
	
R12.	 Describe how a network administrator of an upper-tier ISP can implement 
policy when configuring BGP.
	
R13.	 True or false: When a BGP router receives an advertised path from its neigh-
bor, it must add its own identity to the received path and then send that new 
path on to all of its neighbors. Explain.
SECTION 5.5
	
R14.	 Describe the main role of the communication layer, the network-wide state-­
management layer, and the network-control application layer in an SDN 
controller.
	
R15.	 Suppose you wanted to implement a new routing protocol in the SDN control 
plane. At which layer would you implement that protocol? Explain.
	
R16.	 What types of messages flow across an SDN controller’s northbound and 
southbound APIs? Who is the recipient of these messages sent from the 
controller across the southbound interface, and who sends messages to the 
controller across the northbound interface?
	
R17.	 Describe the purpose of two types of OpenFlow messages (of your choosing) 
that are sent from a controlled device to the controller. Describe the purpose 
of two types of Openflow messages (of your choosing) that are send from the 
controller to a controlled device.
	
R18.	 What is the purpose of the service abstraction layer in the OpenDaylight SDN 
controller?
SECTIONS 5.6–5.7
	
R19.	 Names four different types of ICMP messages
	
R20.	 What two types of ICMP messages are received at the sending host executing 
the Traceroute program?
	
R21.	 Define the following terms in the context of SNMP: managing server, 
­
managed device, network management agent and MIB.
	
R22.	 What are the purposes of the SNMP GetRequest and SetRequest messages?
	
R23.	 What is the purpose of the SNMP trap message?
Problems         457
Problems
	 P1.	 Looking at Figure 5.3, enumerate the paths from y to u that do not contain 
any loops.
	 P2.	 Repeat Problem P1 for paths from x to z, z to u, and z to w.
	 P3.	 Consider the following network. With the indicated link costs, use Dijkstra’s 
shortest-path algorithm to compute the shortest path from x to all network nodes. 
Show how the algorithm works by computing a table similar to Table 5.1. 
x
v
t
y
z
u
w
6
12
8
7
8
3
6
4
3
2
4
3
	 P4.	 Consider the network shown in Problem P3. Using Dijkstra’s algorithm, and 
showing your work using a table similar to Table 5.1, do the following:
a.	 Compute the shortest path from t to all network nodes.
b.	 Compute the shortest path from u to all network nodes.
c.	 Compute the shortest path from v to all network nodes.
d.	 Compute the shortest path from w to all network nodes.
e.	 Compute the shortest path from y to all network nodes.
f.	 Compute the shortest path from z to all network nodes.
	 P5.	 Consider the network shown below, and assume that each node initially 
knows the costs to each of its neighbors. Consider the distance-vector algo-
rithm and show the distance table entries at node z.
u
z
v
y
2
3
6
2
3
1
x
VideoNote
Dijkstra’s algorithm: 
discussion and example
458         Chapter 5    •    The Network Layer: Control Plane
	 P6.	 Consider a general topology (that is, not the specific network shown above) and a 
synchronous version of the distance-vector algorithm. Suppose that at each itera-
tion, a node exchanges its distance vectors with its neighbors and receives their 
distance vectors. Assuming that the algorithm begins with each node knowing 
only the costs to its immediate neighbors, what is the maximum number of itera-
tions required before the distributed algorithm converges? Justify your answer.
	 P7.	 Consider the network fragment shown below. x has only two attached neigh-
bors, w and y. w has a minimum-cost path to destination u (not shown) of 5, 
and y has a minimum-cost path to u of 6. The complete paths from w and y 
to u (and between w and y) are not shown. All link costs in the network have 
strictly positive integer values.
x
y
w
2
2
5
a.	 Give x’s distance vector for destinations w, y, and u.
b.	 Give a link-cost change for either c(x,w) or c(x,y) such that x will inform 
its neighbors of a new minimum-cost path to u as a result of executing the 
distance-vector algorithm.
c.	 Give a link-cost change for either c(x,w) or c(x,y) such that x will not 
inform its neighbors of a new minimum-cost path to u as a result of 
executing the distance-vector algorithm.
	 P8.	 Consider the three-node topology shown in Figure 5.6. Rather than having 
the link costs shown in Figure 5.6, the link costs are c(x,y) = 3, c(y,z) = 6, 
c(z,x) = 4. Compute the distance tables after the initialization step and after 
each iteration of a synchronous version of the distance-vector algorithm (as 
we did in our earlier discussion of Figure 5.6).
	 P9.	 Can the poisoned reverse solve the general count-to-infinity problem? Justify 
your answer.
	
P10.	 Argue that for the distance-vector algorithm in Figure 5.6, each value in the 
distance vector D(x) is non-increasing and will eventually stabilize in a finite 
number of steps.
	
P11.	 Consider Figure 5.7. Suppose there is another router w, connected to router 
y and z. The costs of all links are given as follows: c(x,y) = 4, c(x,z) = 50, 
c(y,w) = 1, c(z,w) = 1, c(y,z) = 3. Suppose that poisoned reverse is used in 
the distance-vector routing algorithm.
Problems         459
a.	 When the distance vector routing is stabilized, router w, y, and z inform their 
distances to x to each other. What distance values do they tell each other?
b.	 Now suppose that the link cost between x and y increases to 60. Will there be 
a count-to-infinity problem even if poisoned reverse is used? Why or why not? 
If there is a count-to-infinity problem, then how many iterations are needed for 
the distance-vector routing to reach a stable state again? Justify your answer.
c.	 How do you modify c(y,z) such that there is no count-to-infinity problem 
at all if c(y,x) changes from 4 to 60?
	
P12.	 What is the message complexity of LS routing algorithm?
	
P13.	 Will a BGP router always choose the loop-free route with the shortest ASpath 
length? Justify your answer.
	
P14.	 Consider the network shown below. Suppose AS3 and AS2 are running 
OSPF for their intra-AS routing protocol. Suppose AS1 and AS4 are running 
RIP for their intra-AS routing protocol. Suppose eBGP and iBGP are used 
for the inter-AS routing protocol. Initially suppose there is no physical link 
between AS2 and AS4.
a.	 Router 3c learns about prefix x from which routing protocol: OSPF, RIP, 
eBGP, or iBGP?
b.	 Router 3a learns about x from which routing protocol?
c.	 Router 1c learns about x from which routing protocol?
d.	 Router 1d learns about x from which routing protocol?
AS4
AS3
AS1
AS2
x
4b
4c
4a
3c
3b
3a
1c
1b
1d
1a
I1
I2
2c
2a
2b
460         Chapter 5    •    The Network Layer: Control Plane
	
P15.	 Referring to the previous problem, once router 1d learns about x it will put an 
entry (x, I) in its forwarding table.
a.	 Will I be equal to I1 or I2 for this entry? Explain why in one sentence.
b.	 Now suppose that there is a physical link between AS2 and AS4, shown 
by the dotted line. Suppose router 1d learns that x is accessible via AS2 as 
well as via AS3. Will I be set to I1 or I2? Explain why in one sentence.
c.	 Now suppose there is another AS, called AS5, which lies on the path 
between AS2 and AS4 (not shown in diagram). Suppose router 1d learns 
that x is accessible via AS2 AS5 AS4 as well as via AS3 AS4. Will I be 
set to I1 or I2? Explain why in one sentence.
P16.	 Consider the following network. ISP B provides national backbone service 
to regional ISP A. ISP C provides national backbone service to regional 
ISP D. Each ISP consists of one AS. B and C peer with each other in two 
places using BGP. Consider traffic going from A to D. B would prefer 
to hand that traffic over to C on the West Coast (so that C would have 
to absorb the cost of carrying the traffic cross-country), while C would 
prefer to get the traffic via its East Coast peering point with B (so that B 
would have carried the traffic across the country). What BGP mechanism 
might C use, so that B would hand over A-to-D traffic at its East Coast 
peering point? To answer this question, you will need to dig into the BGP 
­
specification.
ISP B
ISP C
ISP D
ISP A
Socket Programming Assignment         461
	
P17.	 In Figure 5.13, consider the path information that reaches stub networks W, 
X, and Y. Based on the information available at W and X, what are their 
respective views of the network topology? Justify your answer. The topology 
view at Y is shown below.
W
Y
X
A
C
Stub network
Y’s view of
the topology
	
P18.	 Consider Figure 5.13. B would never forward traffic destined to Y via X based 
on BGP routing. But there are some very popular applications for which data 
packets go to X first and then flow to Y. Identify one such application, and 
describe how data packets follow a path not given by BGP routing.
	
P19.	 In Figure 5.13, suppose that there is another stub network V that is a cus-
tomer of ISP A. Suppose that B and C have a peering relationship, and A is 
a customer of both B and C. Suppose that A would like to have the traffic 
destined to W to come from B only, and the traffic destined to V from either 
B or C. How should A advertise its routes to B and C? What AS routes does 
C receive?
	
P20.	 Suppose ASs X and Z are not directly connected but instead are connected 
by AS Y. Further suppose that X has a peering agreement with Y, and that Y 
has a peering agreement with Z. Finally, suppose that Z wants to transit all 
of Y’s traffic but does not want to transit X’s traffic. Does BGP allow Z to 
­
implement this policy?
	
P21.	 Consider the two ways in which communication occurs between a managing 
entity and a managed device: request-response mode and trapping. What are 
the pros and cons of these two approaches, in terms of (1) overhead, (2) noti-
fication time when exceptional events occur, and (3) robustness with respect 
to lost messages between the managing entity and the device?
	
P22.	 In Section 5.7 we saw that it was preferable to transport SNMP messages in 
unreliable UDP datagrams. Why do you think the designers of SNMP chose 
UDP rather than TCP as the transport protocol of choice for SNMP?
Socket Programming Assignment
At the end of Chapter 2, there are four socket programming assignments. Below, 
you will find a fifth assignment which employs ICMP, a protocol discussed in this 
chapter.
462         Chapter 5    •    The Network Layer: Control Plane
Assignment 5: ICMP Ping
Ping is a popular networking application used to test from a remote location whether 
a particular host is up and reachable. It is also often used to measure latency between 
the client host and the target host. It works by sending ICMP “echo request” packets 
(i.e., ping packets) to the target host and listening for ICMP “echo response” replies 
(i.e., pong packets). Ping measures the RRT, records packet loss, and calculates a 
statistical summary of multiple ping-pong exchanges (the minimum, mean, max, and 
standard deviation of the round-trip times).
In this lab, you will write your own Ping application in Python. Your application 
will use ICMP. But in order to keep your program simple, you will not exactly follow 
the official specification in RFC 1739. Note that you will only need to write the client 
side of the program, as the functionality needed on the server side is built into almost 
all operating systems. You can find full details of this assignment, as well as important 
snippets of the Python code, at the Web site http://www.pearsonglobaleditions.com/
kurose.
Programming Assignment
In this programming assignment, you will be writing a “distributed” set of proce-
dures that implements a distributed asynchronous distance-vector routing for the 
network shown below.
You are to write the following routines that will “execute” asynchronously 
within the emulated environment provided for this assignment. For node 0, you will 
write the routines:
3
2
0
1
7
3
1
2
1
•	 rtinit0(). This routine will be called once at the beginning of the emulation. 
rtinit0() has no arguments. It should initialize your distance table in node 0 to 
reflect the direct costs of 1, 3, and 7 to nodes 1, 2, and 3, respectively. In the 
figure above, all links are bidirectional and the costs in both directions are identi-
cal. After initializing the distance table and any other data structures needed by 
your node 0 routines, it should then send its directly connected neighbors (in this 
case, 1, 2, and 3) the cost of its minimum-cost paths to all other network nodes. 
Wireshark Lab         463
This minimum-cost information is sent to neighboring nodes in a routing update 
packet by calling the routine tolayer2(), as described in the full assignment. The 
format of the routing update packet is also described in the full assignment.
•	 rtupdate0(struct rtpkt *rcvdpkt). This routine will be called when node 0 receives 
a routing packet that was sent to it by one of its directly connected neighbors. 
The parameter *rcvdpkt is a pointer to the packet that was received. rtupdate0() 
is the “heart” of the distance-vector algorithm. The values it receives in a routing 
update packet from some other node i contain i’s current shortest-path costs to 
all other network nodes. rtupdate0() uses these received values to update its own 
distance table (as specified by the distance-vector algorithm). If its own minimum 
cost to another node changes as a result of the update, node 0 informs its directly 
connected neighbors of this change in minimum cost by sending them a rout-
ing packet. Recall that in the distance-vector algorithm, only directly connected 
nodes will exchange routing packets. Thus, nodes 1 and 2 will communicate with 
each other, but nodes 1 and 3 will not communicate with each other.
Similar routines are defined for nodes 1, 2, and 3. Thus, you will write eight pro-
cedures in all: rtinit0(), rtinit1(), rtinit2(), rtinit3(), rtupdate0(), rtupdate1(), rtup-
date2(), and rtupdate3(). These routines will together implement a distributed, 
asynchronous computation of the distance tables for the topology and costs shown in 
the figure on the preceding page.
You can find the full details of the programming assignment, as well as C code 
that you will need to create the simulated hardware/software environment, at http://
www.pearsonglobaleditions.com/kurose. A Java version of the assignment is also 
available.
Wireshark Lab
In the Web site for this textbook, www.pearsonglobaleditions.com/kurose, you’ll 
find a Wireshark lab assignment that examines the use of the ICMP protocol in the 
ping and traceroute commands.
464
Please describe one or two of the most exciting projects you have worked on during your 
career. What were the biggest challenges?
When I was a researcher at AT&T, a group of us designed a new way to manage rout-
ing in Internet Service Provider backbone networks. Traditionally, network operators 
configure each router individually, and these routers run distributed protocols to compute 
paths through the network. We believed that network management would be simpler and 
more flexible if network operators could exercise direct control over how routers forward 
traffic based on a network-wide view of the topology and traffic. The Routing Control 
Platform (RCP) we designed and built could compute the routes for all of AT&T’s back-
bone on a single commodity computer, and could control legacy routers without modi-
fication. To me, this project was exciting because we had a provocative idea, a working 
system, and ultimately a real deployment in an operational network. Fast forward a few 
years, and software-defined networking (SDN) has become a mainstream technology, 
and standard protocols (like OpenFlow) have made it much easier to tell the underlying 
switches what to do.
Jennifer Rexford is a Professor in the Computer Science department 
at Princeton University. Her research has the broad goal of making 
computer networks easier to design and manage, with particular 
emphasis on routing protocols. From 1996–2004, she was a mem-
ber of the Network Management and Performance department at 
AT&T Labs–Research. While at AT&T, she designed techniques and 
tools for network measurement, traffic engineering, and router con-
figuration that were deployed in AT&T’s backbone network. Jennifer 
is co-author of the book “Web Protocols and Practice: Networking 
Protocols, Caching, and Traffic Measurement,” published by 
Addison-Wesley in May 2001. She served as the chair of ACM 
SIGCOMM from 2003 to 2007. She received her BSE degree in 
electrical engineering from Princeton University in 1991, and her 
PhD degree in electrical engineering and computer science from 
the University of Michigan in 1996. In 2004, Jennifer was the win-
ner of ACM’s Grace Murray Hopper Award for outstanding young 
computer professional and appeared on the MIT TR-100 list of top 
innovators under the age of 35.
Jennifer Rexford
An Interview With…
465
How do you think software-defined networking should evolve in the future?
In a major break from the past, control-plane software can be created by many different 
programmers, not just at companies selling network equipment. Yet, unlike the applications 
running on a server or a smart phone, controller apps must work together to handle the same 
traffic. Network operators do not want to perform load balancing on some traffic and rout-
ing on other traffic; instead, they want to perform load balancing and routing, together, on 
the same traffic. Future SDN controller platforms should offer good programming abstrac-
tions for composing independently written multiple controller applications together. More 
broadly, good programming abstractions can make it easier to create controller applications, 
without having to worry about low-level details like flow table entries, traffic counters, bit 
patterns in packet headers, and so on. Also, while an SDN controller is logically central-
ized, the network still consists of a distributed collection of devices. Future controllers 
should offer good abstractions for updating the flow tables across the network, so apps can 
reason about what happens to packets in flight while the devices are updated. Programming 
abstractions for control-plane software is an exciting area for interdisciplinary research 
between computer networking, distributed systems, and programming languages, with a real 
chance for practical impact in the years ahead.
Where do you see the future of networking and the Internet?
Networking is an exciting field because the applications and the underlying technologies 
change all the time. We are always reinventing ourselves! Who would have predicted even 
ten years ago the dominance of smart phones, allowing mobile users to access existing 
applications as well as new location-based services? The emergence of cloud computing is 
fundamentally changing the relationship between users and the applications they run, and 
networked sensors and actuators (the “Internet of Things”) are enabling a wealth of new 
applications (and security vulnerabilities!). The pace of innovation is truly inspiring.
The underlying network is a crucial component in all of these innovations. Yet, the 
network is notoriously “in the way”—limiting performance, compromising reliability, con-
straining applications, and complicating the deployment and management of services. We 
should strive to make the network of the future as invisible as the air we breathe, so it never 
stands in the way of new ideas and valuable services. To do this, we need to raise the level 
of abstraction above individual network devices and protocols (and their attendant acro-
nyms!), so we can reason about the network and the user’s high-level goals as a whole.
What people inspired you professionally?
I’ve long been inspired by Sally Floyd at the International Computer Science Institute. Her 
research is always purposeful, focusing on the important challenges facing the Internet. She 
digs deeply into hard questions until she understands the problem and the space of solutions 
466
completely, and she devotes serious energy into “making things happen,” such as push-
ing her ideas into protocol standards and network equipment. Also, she gives back to the 
community, through professional service in numerous standards and research organizations 
and by creating tools (such as the widely used ns-2 and ns-3 simulators) that enable other 
researchers to succeed. She retired in 2009 but her influence on the field will be felt for 
years to come.
What are your recommendations for students who want careers in computer science and 
networking?
Networking is an inherently interdisciplinary field. Applying techniques from other disci-
plines breakthroughs in networking come from such diverse areas as queuing theory, game 
theory, control theory, distributed systems, network optimization, programming languages, 
machine learning, algorithms, data structures, and so on. I think that becoming conversant 
in a related field, or collaborating closely with experts in those fields, is a wonderful way 
to put networking on a stronger foundation, so we can learn how to build networks that are 
worthy of society’s trust. Beyond the theoretical disciplines, networking is exciting because 
we create real artifacts that real people use. Mastering how to design and build systems—by 
gaining experience in operating systems, computer architecture, and so on—is another fan-
tastic way to amplify your knowledge of networking to help make the world a better place.
467
In the previous two chapters we learned that the network layer provides a commu-
nication service between any two network hosts. Between the two hosts, datagrams 
travel over a series of communication links, some wired and some wireless, starting 
at the source host, passing through a series of packet switches (switches and routers) 
and ending at the destination host. As we continue down the protocol stack, from 
the network layer to the link layer, we naturally wonder how packets are sent across 
the individual links that make up the end-to-end communication path. How are the 
network-layer datagrams encapsulated in the link-layer frames for transmission over 
a single link? Are different link-layer protocols used in the different links along the 
communication path? How are transmission conflicts in broadcast links resolved? 
Is there addressing at the link layer and, if so, how does the link-layer addressing 
operate with the network-layer addressing we learned about in Chapter 4? And what 
exactly is the difference between a switch and a router? We’ll answer these and other 
important questions in this chapter.
In discussing the link layer, we’ll see that there are two fundamentally ­
different 
types of link-layer channels. The first type are broadcast channels, which connect 
multiple hosts in wireless LANs, satellite networks, and hybrid fiber-coaxial cable 
(HFC) access networks. Since many hosts are connected to the same broadcast com-
munication channel, a so-called medium access protocol is needed to coordinate 
frame transmission. In some cases, a central controller may be used to coordinate 
6
CHAPTER
The Link Layer 
and LANs
468         Chapter 6    •    The Link Layer and LANs
transmissions; in other cases, the hosts themselves coordinate transmissions. The 
second type of link-layer channel is the point-to-point communication link, such as 
that often found between two routers connected by a long-distance link, or between 
a user’s office computer and the nearby Ethernet switch to which it is connected. 
Coordinating access to a point-to-point link is simpler; the reference material on this 
book’s Web site has a detailed discussion of the Point-to-Point Protocol (PPP), which 
is used in settings ranging from dial-up service over a telephone line to high-speed 
point-to-point frame transport over fiber-optic links.
We’ll explore several important link-layer concepts and technologies in this ­
chapter. 
We’ll dive deeper into error detection and correction, a topic we touched on briefly 
in Chapter 3. We’ll consider multiple access networks and switched LANs, including 
Ethernet—by far the most prevalent wired LAN technology. We’ll also look at virtual 
LANs, and data center networks. Although WiFi, and more generally wireless LANs, 
are link-layer topics, we’ll postpone our study of these important topics until Chapter 7.
6.1	 Introduction to the Link Layer
Let’s begin with some important terminology. We’ll find it convenient in this chapter 
to refer to any device that runs a link-layer (i.e., layer 2) protocol as a node. Nodes 
include hosts, routers, switches, and WiFi access points (discussed in Chapter 7). We 
will also refer to the communication channels that connect adjacent nodes along the 
communication path as links. In order for a datagram to be transferred from source host 
to destination host, it must be moved over each of the individual links in the end-to-
end path. As an example, in the company network shown at the bottom of Figure 6.1, 
consider sending a datagram from one of the wireless hosts to one of the servers. This 
datagram will actually pass through six links: a WiFi link between sending host and 
WiFi access point, an Ethernet link between the access point and a link-layer switch; 
a link between the link-layer switch and the router, a link between the two routers; an 
Ethernet link between the router and a link-layer switch; and finally an Ethernet link 
between the switch and the server. Over a given link, a transmitting node encapsulates 
the datagram in a link-layer frame and transmits the frame into the link.
In order to gain further insight into the link layer and how it relates to the 
­
network layer, let’s consider a transportation analogy. Consider a travel agent who 
is planning a trip for a tourist traveling from Princeton, New Jersey, to Lausanne, 
Switzerland. The travel agent decides that it is most convenient for the tourist to take 
a limousine from Princeton to JFK airport, then a plane from JFK airport to Geneva’s 
airport, and finally a train from Geneva’s airport to Lausanne’s train station. Once 
the travel agent makes the three reservations, it is the responsibility of the Princeton 
limousine company to get the tourist from Princeton to JFK; it is the responsibility of 
the airline company to get the tourist from JFK to Geneva; and it is the responsibility 
6.1    •    Introduction to the Link Layer         469
Figure 6.1  ♦  Six link-layer hops between wireless host and server
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Home Network
470         Chapter 6    •    The Link Layer and LANs
of the Swiss train service to get the tourist from Geneva to Lausanne. Each of the 
three segments of the trip is “direct” between two “adjacent” locations. Note that the 
three transportation segments are managed by different companies and use entirely 
different transportation modes (limousine, plane, and train). Although the transporta-
tion modes are different, they each provide the basic service of moving passengers 
from one location to an adjacent location. In this transportation analogy, the tourist is 
a datagram, each transportation segment is a link, the transportation mode is a link-
layer protocol, and the travel agent is a routing protocol.
6.1.1	The Services Provided by the Link Layer
Although the basic service of any link layer is to move a datagram from one node 
to an adjacent node over a single communication link, the details of the provided 
service can vary from one link-layer protocol to the next. Possible services that can 
be offered by a link-layer protocol include:
•	 Framing. Almost all link-layer protocols encapsulate each network-layer data-
gram within a link-layer frame before transmission over the link. A frame consists 
of a data field, in which the network-layer datagram is inserted, and a number of 
header fields. The structure of the frame is specified by the link-layer protocol. 
We’ll see several different frame formats when we examine specific link-layer 
protocols in the second half of this chapter.
•	 Link access. A medium access control (MAC) protocol specifies the rules by 
which a frame is transmitted onto the link. For point-to-point links that have a 
single sender at one end of the link and a single receiver at the other end of the 
link, the MAC protocol is simple (or nonexistent)—the sender can send a frame 
whenever the link is idle. The more interesting case is when multiple nodes share 
a single broadcast link—the so-called multiple access problem. Here, the MAC 
protocol serves to coordinate the frame transmissions of the many nodes.
•	 Reliable delivery. When a link-layer protocol provides reliable delivery service, 
it guarantees to move each network-layer datagram across the link without error. 
Recall that certain transport-layer protocols (such as TCP) also provide a reliable 
delivery service. Similar to a transport-layer reliable delivery service, a link-layer 
reliable delivery service can be achieved with acknowledgments and retransmis-
sions (see Section 3.4). A link-layer reliable delivery service is often used for 
links that are prone to high error rates, such as a wireless link, with the goal of 
correcting an error locally—on the link where the error occurs—rather than forc-
ing an end-to-end retransmission of the data by a transport- or application-layer 
protocol. However, link-layer reliable delivery can be considered an unnecessary 
overhead for low bit-error links, including fiber, coax, and many twisted-pair 
copper links. For this reason, many wired link-layer protocols do not provide a 
reliable delivery service.
6.1    •    Introduction to the Link Layer         471
•	 Error detection and correction. The link-layer hardware in a receiving node can incor-
rectly decide that a bit in a frame is zero when it was transmitted as a one, and vice 
versa. Such bit errors are introduced by signal attenuation and electromagnetic noise. 
Because there is no need to forward a datagram that has an error, many link-layer pro-
tocols provide a mechanism to detect such bit errors. This is done by having the trans-
mitting node include error-detection bits in the frame, and having the receiving node 
perform an error check. Recall from Chapters 3 and 4 that the Internet’s transport layer 
and network layer also provide a limited form of error detection—the Internet check-
sum. Error detection in the link layer is usually more sophisticated and is implemented 
in hardware. Error correction is similar to error detection, except that a receiver not 
only detects when bit errors have occurred in the frame but also determines exactly 
where in the frame the errors have occurred (and then corrects these errors).
6.1.2	Where Is the Link Layer Implemented?
Before diving into our detailed study of the link layer, let’s conclude this introduction 
by considering the question of where the link layer is implemented. We’ll focus here 
on an end system, since we learned in Chapter 4 that the link layer is implemented in 
a router’s line card. Is a host’s link layer implemented in hardware or software? Is it 
implemented on a separate card or chip, and how does it interface with the rest of a 
host’s hardware and operating system components?
Figure 6.2 shows a typical host architecture. For the most part, the link layer is 
implemented in a network adapter, also sometimes known as a network interface 
card (NIC). At the heart of the network adapter is the link-layer controller, usually a 
single, special-purpose chip that implements many of the link-layer services (fram-
ing, link access, error detection, and so on). Thus, much of a link-layer controller’s 
functionality is implemented in hardware. For example, Intel’s 710 adapter [Intel 
2016] implements the Ethernet protocols we’ll study in Section 6.5; the Atheros 
AR5006 [Atheros 2016] controller implements the 802.11 WiFi protocols we’ll 
study in Chapter 7. Until the late 1990s, most network adapters were physically 
separate cards (such as a PCMCIA card or a plug-in card fitting into a PC’s PCI 
card slot) but increasingly, network adapters are being integrated onto the host’s 
motherboard—a so-called LAN-on-motherboard configuration.
On the sending side, the controller takes a datagram that has been created and 
stored in host memory by the higher layers of the protocol stack, encapsulates the 
datagram in a link-layer frame (filling in the frame’s various fields), and then trans-
mits the frame into the communication link, following the link-access protocol. On 
the receiving side, a controller receives the entire frame, and extracts the network-
layer datagram. If the link layer performs error detection, then it is the sending con-
troller that sets the error-detection bits in the frame header and it is the receiving 
controller that performs error detection.
Figure 6.2 shows a network adapter attaching to a host’s bus (e.g., a PCI 
or PCI-X bus), where it looks much like any other I/O device to the other host 
472         Chapter 6    •    The Link Layer and LANs
components. Figure 6.2 also shows that while most of the link layer is imple-
mented in hardware, part of the link layer is implemented in software that runs 
on the host’s CPU. The software components of the link layer implement higher-
level link-layer functionality such as assembling link-layer addressing informa-
tion and activating the controller hardware. On the receiving side, link-layer 
software responds to controller interrupts (e.g., due to the receipt of one or more 
frames), handling error conditions and passing a datagram up to the network 
layer. Thus, the link layer is a combination of hardware and software—the place 
in the protocol stack where software meets hardware. [Intel 2016] provides a read-
able overview (as well as a detailed description) of the XL710 controller from a 
software-programming point of view.
6.2	 Error-Detection and -Correction Techniques
In the previous section, we noted that bit-level error detection and correction—
detecting and correcting the corruption of bits in a link-layer frame sent from one 
node to another physically connected neighboring node—are two services often 
­
provided by the link layer. We saw in Chapter 3 that error-detection and -correction 
services are also often offered at the transport layer as well. In this section, we’ll 
Figure 6.2  ♦  
Network adapter: Its relationship to other host components 
and to protocol stack functionality
Host
Memory
Host bus
(e.g., PCI)
CPU
Controller
Physical
transmission
Network adapter
Link
Physical
Transport
Network
Link
Application
6.2    •    Error-Detection and -Correction Techniques         473
examine a few of the simplest techniques that can be used to detect and, in some 
cases, correct such bit errors. A full treatment of the theory and implementation 
of this topic is itself the topic of many textbooks (for example, [Schwartz 1980] 
or [Bertsekas 1991]), and our treatment here is necessarily brief. Our goal here is 
to develop an intuitive feel for the capabilities that error-detection and -correction 
techniques provide and to see how a few simple techniques work and are used in 
practice in the link layer.
Figure 6.3 illustrates the setting for our study. At the sending node, data, D, to 
be protected against bit errors is augmented with error-detection and -correction bits 
(EDC). Typically, the data to be protected includes not only the datagram passed 
down from the network layer for transmission across the link, but also link-level 
addressing information, sequence numbers, and other fields in the link frame header. 
Both D and EDC are sent to the receiving node in a link-level frame. At the receiv-
ing node, a sequence of bits, D′ and EDC′ is received. Note that D′ and EDC′ may 
differ from the original D and EDC as a result of in-transit bit flips.
The receiver’s challenge is to determine whether or not D′ is the same as the 
original D, given that it has only received D′ and EDC′. The exact wording of the 
receiver’s decision in Figure 6.3 (we ask whether an error is detected, not whether an 
error has occurred!) is important. Error-detection and -correction techniques allow 
the receiver to sometimes, but not always, detect that bit errors have occurred. Even 
with the use of error-detection bits there still may be undetected bit errors; that is, 
the receiver may be unaware that the received information contains bit errors. As a 
Figure 6.3  ♦  Error-detection and -correction scenario
EDC'
D'
Detected error
Datagram
EDC
D
d data bits
Bit error-prone link
all
bits in D'
OK
?
N
Y
Datagram
HI
474         Chapter 6    •    The Link Layer and LANs
consequence, the receiver might deliver a corrupted datagram to the network layer, 
or be unaware that the contents of a field in the frame’s header has been corrupted. 
We thus want to choose an error-detection scheme that keeps the probability of such 
occurrences small. Generally, more sophisticated error-detection and-correction 
techniques (that is, those that have a smaller probability of allowing undetected bit 
errors) incur a larger overhead—more computation is needed to compute and trans-
mit a larger number of error-detection and -correction bits.
Let’s now examine three techniques for detecting errors in the transmitted data—
parity checks (to illustrate the basic ideas behind error detection and correction), check-
summing methods (which are more typically used in the transport layer), and cyclic 
redundancy checks (which are more typically used in the link layer in an adapter).
6.2.1	Parity Checks
Perhaps the simplest form of error detection is the use of a single parity bit. Suppose 
that the information to be sent, D in Figure 6.4, has d bits. In an even parity scheme, 
the sender simply includes one additional bit and chooses its value such that the total 
number of 1s in the d + 1 bits (the original information plus a parity bit) is even. For 
odd parity schemes, the parity bit value is chosen such that there is an odd number 
of 1s. Figure 6.4 illustrates an even parity scheme, with the single parity bit being 
stored in a separate field.
Receiver operation is also simple with a single parity bit. The receiver need only 
count the number of 1s in the received d + 1 bits. If an odd number of 1-valued bits 
are found with an even parity scheme, the receiver knows that at least one bit error has 
occurred. More precisely, it knows that some odd number of bit errors have occurred.
But what happens if an even number of bit errors occur? You should convince 
yourself that this would result in an undetected error. If the probability of bit errors is 
small and errors can be assumed to occur independently from one bit to the next, the 
probability of multiple bit errors in a packet would be extremely small. In this case, 
a single parity bit might suffice. However, measurements have shown that, rather 
than occurring independently, errors are often clustered together in “bursts.” Under 
burst error conditions, the probability of undetected errors in a frame protected by 
single-bit parity can approach 50 percent [Spragins 1991]. Clearly, a more robust 
error-detection scheme is needed (and, fortunately, is used in practice!). But before 
examining error-detection schemes that are used in practice, let’s consider a simple 
Figure 6.4  ♦  One-bit even parity
0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1
1
d data bits
Parity
bit
6.2    •    Error-Detection and -Correction Techniques         475
generalization of one-bit parity that will provide us with insight into error-correction 
techniques.
Figure 6.5 shows a two-dimensional generalization of the single-bit parity 
scheme. Here, the d bits in D are divided into i rows and j columns. A parity value 
is computed for each row and for each column. The resulting i + j + 1 parity bits 
comprise the link-layer frame’s error-detection bits.
Suppose now that a single bit error occurs in the original d bits of information. 
With this two-dimensional parity scheme, the parity of both the column and the row 
containing the flipped bit will be in error. The receiver can thus not only detect the 
fact that a single bit error has occurred, but can use the column and row indices of 
the column and row with parity errors to actually identify the bit that was corrupted 
and correct that error! Figure 6.5 shows an example in which the 1-valued bit in 
position (2,2) is corrupted and switched to a 0—an error that is both detectable and 
correctable at the receiver. Although our discussion has focused on the original d bits 
of information, a single error in the parity bits themselves is also detectable and cor-
rectable. Two-dimensional parity can also detect (but not correct!) any combination 
of two errors in a packet. Other properties of the two-dimensional parity scheme are 
explored in the problems at the end of the chapter.
Figure 6.5  ♦  Two-dimensional even parity
1  0  1  0  1  1
1  1  1  1  0  0
0  1  1  1  0  1
0  0  1  0  1  0
1  0  1  0  1  1
1  0  1  1  0  0
0  1  1  1  0  1
0  0  1  0  1  0
Row parity
Parity 
error
Parity 
error
No errors
Correctable
single-bit error
d1,1
d2,1
. . .
di,1
di+1,1
. . .
. . .
. . .
. . .
. . .
d1,j
d2,j
. . .
di,j
di+1,j
d1,j+1
d2,j+1
. . .
di,j+1
di+1,j+1
Column parity
476         Chapter 6    •    The Link Layer and LANs
The ability of the receiver to both detect and correct errors is known as forward 
error correction (FEC). These techniques are commonly used in audio storage and 
playback devices such as audio CDs. In a network setting, FEC techniques can be 
used by themselves, or in conjunction with link-layer ARQ techniques similar to 
those we examined in Chapter 3. FEC techniques are valuable because they can 
decrease the number of sender retransmissions required. Perhaps more important, 
they allow for immediate correction of errors at the receiver. This avoids having to 
wait for the round-trip propagation delay needed for the sender to receive a NAK 
packet and for the retransmitted packet to propagate back to the receiver—a poten-
tially important advantage for real-time network applications [Rubenstein 1998] or 
links (such as deep-space links) with long propagation delays. Research examining 
the use of FEC in error-control protocols includes [Biersack 1992; Nonnenmacher 
1998; Byers 1998; Shacham 1990].
6.2.2	Checksumming Methods
In checksumming techniques, the d bits of data in Figure 6.4 are treated as a sequence 
of k-bit integers. One simple checksumming method is to simply sum these k-bit inte-
gers and use the resulting sum as the error-detection bits. The Internet checksum is 
based on this approach—bytes of data are treated as 16-bit integers and summed. The 
1s complement of this sum then forms the Internet checksum that is carried in the 
segment header. As discussed in Section 3.3, the receiver checks the checksum by 
taking the 1s complement of the sum of the received data (including the checksum) 
and checking whether the result is all 1 bits. If any of the bits are 0, an error is indi-
cated. RFC 1071 discusses the Internet checksum algorithm and its implementation 
in detail. In the TCP and UDP protocols, the Internet checksum is computed over all 
fields (header and data fields included). In IP the checksum is computed over the IP 
header (since the UDP or TCP segment has its own checksum). In other protocols, 
for example, XTP [Strayer 1992], one checksum is computed over the header and 
another checksum is computed over the entire packet.
Checksumming methods require relatively little packet overhead. For example, 
the checksums in TCP and UDP use only 16 bits. However, they provide relatively 
weak protection against errors as compared with cyclic redundancy check, which is 
discussed below and which is often used in the link layer. A natural question at this 
point is, Why is checksumming used at the transport layer and cyclic redundancy 
check used at the link layer? Recall that the transport layer is typically implemented 
in software in a host as part of the host’s operating system. Because transport-layer 
error detection is implemented in software, it is important to have a simple and fast 
error-detection scheme such as checksumming. On the other hand, error detection at 
the link layer is implemented in dedicated hardware in adapters, which can rapidly 
perform the more complex CRC operations. Feldmeier [Feldmeier 1995] presents 
fast software implementation techniques for not only weighted checksum codes, but 
CRC (see below) and other codes as well.
6.2    •    Error-Detection and -Correction Techniques         477
6.2.3	Cyclic Redundancy Check (CRC)
An error-detection technique used widely in today’s computer networks is based on 
cyclic redundancy check (CRC) codes. CRC codes are also known as polynomial 
codes, since it is possible to view the bit string to be sent as a polynomial whose 
coefficients are the 0 and 1 values in the bit string, with operations on the bit string 
interpreted as polynomial arithmetic.
CRC codes operate as follows. Consider the d-bit piece of data, D, that the send-
ing node wants to send to the receiving node. The sender and receiver must first 
agree on an r + 1 bit pattern, known as a generator, which we will denote as G. 
We will require that the most significant (leftmost) bit of G be a 1. The key idea 
behind CRC codes is shown in Figure 6.6. For a given piece of data, D, the sender 
will choose r additional bits, R, and append them to D such that the resulting d + r  
bit pattern (interpreted as a binary number) is exactly divisible by G (i.e., has no 
remainder) using modulo-2 arithmetic. The process of error checking with CRCs is 
thus simple: The receiver divides the d + r received bits by G. If the remainder is 
nonzero, the receiver knows that an error has occurred; otherwise the data is accepted 
as being correct.
All CRC calculations are done in modulo-2 arithmetic without carries in addi-
tion or borrows in subtraction. This means that addition and subtraction are identical, 
and both are equivalent to the bitwise exclusive-or (XOR) of the operands. Thus, for 
example,
1011 XOR 0101 = 1110
1001 XOR 1101 = 0100
Also, we similarly have
1011 - 0101 = 1110
1001 - 1101 = 0100
Multiplication and division are the same as in base-2 arithmetic, except that any 
required addition or subtraction is done without carries or borrows. As in regular 
Figure 6.6  ♦  CRC
d bits
r bits
D: Data bits to be sent
D • 2r  XOR    R
R: CRC bits
Bit pattern
Mathematical formula
478         Chapter 6    •    The Link Layer and LANs
binary arithmetic, multiplication by 2k left shifts a bit pattern by k places. Thus, given 
D and R, the quantity D #  2r XOR R yields the d + r bit pattern shown in Figure 6.6. 
We’ll use this algebraic characterization of the d + r bit pattern from Figure 6.6 in 
our discussion below.
Let us now turn to the crucial question of how the sender computes R. Recall 
that we want to find R such that there is an n such that
D #  2r XOR R = nG
That is, we want to choose R such that G divides into D #  2r XOR R without 
remainder. If we XOR (that is, add modulo-2, without carry) R to both sides of the 
above equation, we get
D #  2r = nG XOR R
This equation tells us that if we divide D #  2r by G, the value of the remainder 
is precisely R. In other words, we can calculate R as
R = remainder D # 2r
G
Figure 6.7 illustrates this calculation for the case of D = 101110, d = 6, 
G = 1001, and r = 3. The 9 bits transmitted in this case are 101 110  011. 
You should check these calculations for yourself and also check that indeed 
D #  2r = 101011 #  G XOR R.
Figure 6.7  ♦  A sample CRC calculation
1  0  0  1
1  0  1  1  1  0  0  0  0
1  0  1  0  1  1  
1  0  0  1  
1  0  1
0  0  0
1  0  1  0
1  0  0  1
1  1  0
0  0  0
1  1  0  0
1  0  0  1
1  0  1  0
1  0  0  1
 0  1  1
G
D
R
6.3    •    Multiple Access Links and Protocols         479
International standards have been defined for 8-, 12-, 16-, and 32-bit generators, 
G. The CRC-32 32-bit standard, which has been adopted in a number of link-level 
IEEE protocols, uses a generator of
GCRC@32 = 100000100110000010001110110110111
Each of the CRC standards can detect burst errors of fewer than r + 1 bits. (This 
means that all consecutive bit errors of r bits or fewer will be detected.) Furthermore, 
under appropriate assumptions, a burst of length greater than r + 1 bits is detected 
with probability 1 - 0.5r. Also, each of the CRC standards can detect any odd num-
ber of bit errors. See [Williams 1993] for a discussion of implementing CRC checks. 
The theory behind CRC codes and even more powerful codes is beyond the scope of 
this text. The text [Schwartz 1980] provides an excellent introduction to this topic.
6.3	 Multiple Access Links and Protocols
In the introduction to this chapter, we noted that there are two types of network links: 
point-to-point links and broadcast links. A point-to-point link consists of a single 
sender at one end of the link and a single receiver at the other end of the link. Many 
link-layer protocols have been designed for point-to-point links; the point-to-point 
protocol (PPP) and high-level data link control (HDLC) are two such protocols. The 
second type of link, a broadcast link, can have multiple sending and receiving nodes 
all connected to the same, single, shared broadcast channel. The term broadcast is 
used here because when any one node transmits a frame, the channel broadcasts the 
frame and each of the other nodes receives a copy. Ethernet and wireless LANs are 
examples of broadcast link-layer technologies. In this section we’ll take a step back 
from specific link-layer protocols and first examine a problem of central importance 
to the link layer: how to coordinate the access of multiple sending and receiving 
nodes to a shared broadcast channel—the multiple access problem. Broadcast chan-
nels are often used in LANs, networks that are geographically concentrated in a 
single building (or on a corporate or university campus). Thus, we’ll look at how 
multiple access channels are used in LANs at the end of this section.
We are all familiar with the notion of broadcasting—television has been using it 
since its invention. But traditional television is a one-way broadcast (that is, one fixed 
node transmitting to many receiving nodes), while nodes on a computer network 
broadcast channel can both send and receive. Perhaps a more apt human analogy for 
a broadcast channel is a cocktail party, where many people gather in a large room 
(the air providing the broadcast medium) to talk and listen. A second good analogy is 
something many readers will be familiar with—a classroom—where teacher(s) and 
student(s) similarly share the same, single, broadcast medium. A central problem in 
480         Chapter 6    •    The Link Layer and LANs
both scenarios is that of determining who gets to talk (that is, transmit into the chan-
nel) and when. As humans, we’ve evolved an elaborate set of protocols for sharing 
the broadcast channel:
“Give everyone a chance to speak.”
“Don’t speak until you are spoken to.”
“Don’t monopolize the conversation.”
“Raise your hand if you have a question.”
“Don’t interrupt when someone is speaking.”
“Don’t fall asleep when someone is talking.”
Computer networks similarly have protocols—so-called multiple access 
­
protocols—by which nodes regulate their transmission into the shared broadcast 
channel. As shown in Figure 6.8, multiple access protocols are needed in a wide 
variety of network settings, including both wired and wireless access networks, and 
satellite networks. Although technically each node accesses the broadcast chan-
nel through its adapter, in this section we will refer to the node as the sending and 
Figure 6.8  ♦  Various multiple access channels
Shared wire
(for example, cable access network)
Shared wireless
(for example, WiFi)
Satellite
Cocktail party
Head
end
6.3    •    Multiple Access Links and Protocols         481
receiving device. In practice, hundreds or even thousands of nodes can directly com-
municate over a broadcast channel.
Because all nodes are capable of transmitting frames, more than two nodes can 
transmit frames at the same time. When this happens, all of the nodes receive multiple 
frames at the same time; that is, the transmitted frames collide at all of the receiv-
ers. Typically, when there is a collision, none of the receiving nodes can make any 
sense of any of the frames that were transmitted; in a sense, the signals of the col-
liding frames become inextricably tangled together. Thus, all the frames involved in 
the collision are lost, and the broadcast channel is wasted during the collision inter-
val. Clearly, if many nodes want to transmit frames frequently, many transmissions 
will result in collisions, and much of the bandwidth of the broadcast channel will be 
wasted.
In order to ensure that the broadcast channel performs useful work when mul-
tiple nodes are active, it is necessary to somehow coordinate the transmissions of 
the active nodes. This coordination job is the responsibility of the multiple access 
protocol. Over the past 40 years, thousands of papers and hundreds of PhD disserta-
tions have been written on multiple access protocols; a comprehensive survey of the 
first 20 years of this body of work is [Rom 1990]. Furthermore, active research in 
multiple access protocols continues due to the continued emergence of new types of 
links, particularly new wireless links.
Over the years, dozens of multiple access protocols have been implemented in 
a variety of link-layer technologies. Nevertheless, we can classify just about any 
multiple access protocol as belonging to one of three categories: channel partition-
ing protocols, random access protocols, and taking-turns protocols. We’ll cover 
these categories of multiple access protocols in the following three subsections.
Let’s conclude this overview by noting that, ideally, a multiple access protocol 
for a broadcast channel of rate R bits per second should have the following desirable 
characteristics:
	1.	 When only one node has data to send, that node has a throughput of R bps.
	2.	 When M nodes have data to send, each of these nodes has a throughput of R/M 
bps. This need not necessarily imply that each of the M nodes always has an 
instantaneous rate of R/M, but rather that each node should have an average 
transmission rate of R/M over some suitably defined interval of time.
	3.	 The protocol is decentralized; that is, there is no master node that represents a 
single point of failure for the network.
	4.	 The protocol is simple, so that it is inexpensive to implement.
6.3.1	Channel Partitioning Protocols
Recall from our early discussion back in Section 1.3 that time-division ­
multiplexing 
(TDM) and frequency-division multiplexing (FDM) are two techniques that can 
482         Chapter 6    •    The Link Layer and LANs
be used to partition a broadcast channel’s bandwidth among all nodes sharing that 
channel. As an example, suppose the channel supports N nodes and that the trans-
mission rate of the channel is R bps. TDM divides time into time frames and further 
divides each time frame into N time slots. (The TDM time frame should not be 
confused with the link-layer unit of data exchanged between sending and receiving 
adapters, which is also called a frame. In order to reduce confusion, in this subsec-
tion we’ll refer to the link-layer unit of data exchanged as a packet.) Each time slot 
is then assigned to one of the N nodes. Whenever a node has a packet to send, it 
transmits the packet’s bits during its assigned time slot in the revolving TDM frame. 
Typically, slot sizes are chosen so that a single packet can be transmitted during a 
slot time. Figure 6.9 shows a simple four-node TDM example. Returning to our 
cocktail party analogy, a TDM-regulated cocktail party would allow one partygoer 
to speak for a fixed period of time, then allow another partygoer to speak for the 
same amount of time, and so on. Once everyone had had a chance to talk, the ­
pattern 
would repeat.
TDM is appealing because it eliminates collisions and is perfectly fair: Each 
node gets a dedicated transmission rate of R/N bps during each frame time. However, 
it has two major drawbacks. First, a node is limited to an average rate of R/N bps 
even when it is the only node with packets to send. A second drawback is that a node 
must always wait for its turn in the transmission sequence—again, even when it is 
the only node with a frame to send. Imagine the partygoer who is the only one with 
anything to say (and imagine that this is the even rarer circumstance where everyone 
Figure 6.9  ♦  A four-node TDM and FDM example
4KHz
FDM
TDM
Link
4KHz
Slot
All slots labeled “2” are dedicated
to a speciﬁc sender-receiver pair.
Frame
1
2
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
Key:
6.3    •    Multiple Access Links and Protocols         483
wants to hear what that one person has to say). Clearly, TDM would be a poor choice 
for a multiple access protocol for this particular party.
While TDM shares the broadcast channel in time, FDM divides the R bps chan-
nel into different frequencies (each with a bandwidth of R/N) and assigns each fre-
quency to one of the N nodes. FDM thus creates N smaller channels of R/N bps out 
of the single, larger R bps channel. FDM shares both the advantages and drawbacks 
of TDM. It avoids collisions and divides the bandwidth fairly among the N nodes. 
However, FDM also shares a principal disadvantage with TDM—a node is limited to 
a bandwidth of R/N, even when it is the only node with packets to send.
A third channel partitioning protocol is code division multiple access 
(CDMA). While TDM and FDM assign time slots and frequencies, respectively, 
to the nodes, CDMA assigns a different code to each node. Each node then uses 
its unique code to encode the data bits it sends. If the codes are chosen carefully, 
CDMA networks have the wonderful property that different nodes can transmit 
simultaneously and yet have their respective receivers correctly receive a send-
er’s encoded data bits (assuming the receiver knows the sender’s code) in spite 
of interfering transmissions by other nodes. CDMA has been used in military 
systems for some time (due to its anti-jamming properties) and now has wide-
spread civilian use, particularly in cellular telephony. Because CDMA’s use is so 
tightly tied to wireless channels, we’ll save our discussion of the technical details 
of CDMA until Chapter 7. For now, it will suffice to know that CDMA codes, 
like time slots in TDM and frequencies in FDM, can be allocated to the multiple 
access channel users.
6.3.2	Random Access Protocols
The second broad class of multiple access protocols are random access protocols. 
In a random access protocol, a transmitting node always transmits at the full rate 
of the channel, namely, R bps. When there is a collision, each node involved in the 
collision repeatedly retransmits its frame (that is, packet) until its frame gets through 
without a collision. But when a node experiences a collision, it doesn’t necessarily 
retransmit the frame right away. Instead it waits a random delay before retrans-
mitting the frame. Each node involved in a collision chooses independent random 
delays. Because the random delays are independently chosen, it is possible that one 
of the nodes will pick a delay that is sufficiently less than the delays of the other col-
liding nodes and will therefore be able to sneak its frame into the channel without a 
collision.
There are dozens if not hundreds of random access protocols described in the 
literature [Rom 1990; Bertsekas 1991]. In this section we’ll describe a few of the 
most commonly used random access protocols—the ALOHA protocols [Abram-
son 1970; Abramson 1985; Abramson 2009] and the carrier sense multiple access 
(CSMA) protocols [Kleinrock 1975b]. Ethernet [Metcalfe 1976] is a popular and 
widely deployed CSMA protocol.
484         Chapter 6    •    The Link Layer and LANs
Slotted ALOHA
Let’s begin our study of random access protocols with one of the simplest random 
access protocols, the slotted ALOHA protocol. In our description of slotted ALOHA, 
we assume the following:
•	 All frames consist of exactly L bits.
•	 Time is divided into slots of size L/R seconds (that is, a slot equals the time to 
transmit one frame).
•	 Nodes start to transmit frames only at the beginnings of slots.
•	 The nodes are synchronized so that each node knows when the slots begin.
•	 If two or more frames collide in a slot, then all the nodes detect the collision event 
before the slot ends.
Let p be a probability, that is, a number between 0 and 1. The operation of slotted 
ALOHA in each node is simple:
•	 When the node has a fresh frame to send, it waits until the beginning of the next 
slot and transmits the entire frame in the slot.
•	 If there isn’t a collision, the node has successfully transmitted its frame and thus 
need not consider retransmitting the frame. (The node can prepare a new frame 
for transmission, if it has one.)
•	 If there is a collision, the node detects the collision before the end of the slot. The 
node retransmits its frame in each subsequent slot with probability p until the 
frame is transmitted without a collision.
By retransmitting with probability p, we mean that the node effectively tosses 
a biased coin; the event heads corresponds to “retransmit,” which occurs with prob-
ability p. The event tails corresponds to “skip the slot and toss the coin again in the 
next slot”; this occurs with probability (1 - p). All nodes involved in the collision 
toss their coins independently.
Slotted ALOHA would appear to have many advantages. Unlike channel par-
titioning, slotted ALOHA allows a node to transmit continuously at the full rate, R, 
when that node is the only active node. (A node is said to be active if it has frames 
to send.) Slotted ALOHA is also highly decentralized, because each node detects 
collisions and independently decides when to retransmit. (Slotted ALOHA does, 
however, require the slots to be synchronized in the nodes; shortly we’ll discuss 
an unslotted version of the ALOHA protocol, as well as CSMA protocols, none of 
which require such synchronization.) Slotted ALOHA is also an extremely simple 
protocol.
Slotted ALOHA works well when there is only one active node, but how 
­
efficient is it when there are multiple active nodes? There are two possible efficiency 
6.3    •    Multiple Access Links and Protocols         485
concerns here. First, as shown in Figure 6.10, when there are multiple active nodes, 
a certain fraction of the slots will have collisions and will therefore be “wasted.” The 
second concern is that another fraction of the slots will be empty because all active 
nodes refrain from transmitting as a result of the probabilistic transmission policy. 
The only “unwasted” slots will be those in which exactly one node transmits. A slot 
in which exactly one node transmits is said to be a successful slot. The efficiency of 
a slotted multiple access protocol is defined to be the long-run fraction of successful 
slots in the case when there are a large number of active nodes, each always having 
a large number of frames to send. Note that if no form of access control were used, 
and each node were to immediately retransmit after each collision, the efficiency 
would be zero. Slotted ALOHA clearly increases the efficiency beyond zero, but by 
how much?
We now proceed to outline the derivation of the maximum efficiency of slotted 
ALOHA. To keep this derivation simple, let’s modify the protocol a little and assume 
that each node attempts to transmit a frame in each slot with probability p. (That is, 
we assume that each node always has a frame to send and that the node transmits 
with probability p for a fresh frame as well as for a frame that has already suffered a 
collision.) Suppose there are N nodes. Then the probability that a given slot is a suc-
cessful slot is the probability that one of the nodes transmits and that the remaining 
N - 1 nodes do not transmit. The probability that a given node transmits is p; the 
probability that the remaining nodes do not transmit is (1 - p)N-1. Therefore the 
probability a given node has a success is p(1 - p)N-1. Because there are N nodes, 
the probability that any one of the N nodes has a success is Np(1 - p)N-1.
Figure 6.10  ♦  
Nodes 1, 2, and 3 collide in the first slot. Node 2 finally 
succeeds in the fourth slot, node 1 in the eighth slot, and 
node 3 in the ninth slot
Node 3
Key:
C = Collision slot
E = Empty slot
S = Successful slot
Node 2
Node 1
2
2
2
1
1
1
1
3
3
3
Time
C
E
C
S
E
C
E
S
S
486         Chapter 6    •    The Link Layer and LANs
Thus, when there are N active nodes, the efficiency of slotted ALOHA is 
Np(1 - p)N-1. To obtain the maximum efficiency for N active nodes, we have to find the 
p* that maximizes this expression. (See the homework problems for a general outline of 
 
this derivation.) And to obtain the maximum efficiency for a large number of active 
nodes, we take the limit of Np*(1 - p*)N-1 as N approaches infinity. (Again, see the 
homework problems.) After performing these calculations, we’ll find that the maximum 
efficiency of the protocol is given by 1/e = 0.37. That is, when a large number of nodes 
have many frames to transmit, then (at best) only 37 percent of the slots do useful work. 
Thus the effective transmission rate of the channel is not R bps but only 0.37 R bps! 
A similar analysis also shows that 37 percent of the slots go empty and 26 percent 
of slots have collisions. Imagine the poor network administrator who has purchased a 
100-Mbps slotted ALOHA system, expecting to be able to use the network to transmit 
data among a large number of users at an aggregate rate of, say, 80 Mbps! Although the 
channel is capable of transmitting a given frame at the full channel rate of 100 Mbps, in 
the long run, the successful throughput of this channel will be less than 37 Mbps.
ALOHA
The slotted ALOHA protocol required that all nodes synchronize their transmissions 
to start at the beginning of a slot. The first ALOHA protocol [Abramson 1970] was 
actually an unslotted, fully decentralized protocol. In pure ALOHA, when a frame 
first arrives (that is, a network-layer datagram is passed down from the network layer 
at the sending node), the node immediately transmits the frame in its entirety into the 
broadcast channel. If a transmitted frame experiences a collision with one or more 
other transmissions, the node will then immediately (after completely transmitting 
its collided frame) retransmit the frame with probability p. Otherwise, the node waits 
for a frame transmission time. After this wait, it then transmits the frame with prob-
ability p, or waits (remaining idle) for another frame time with probability 1 – p.
To determine the maximum efficiency of pure ALOHA, we focus on an individual 
node. We’ll make the same assumptions as in our slotted ALOHA analysis and take the 
frame transmission time to be the unit of time. At any given time, the probability that a 
node is transmitting a frame is p. Suppose this frame begins transmission at time t0. As 
shown in Figure 6.11, in order for this frame to be successfully transmitted, no other 
nodes can begin their transmission in the interval of time [t0 - 1, t0]. Such a transmis-
sion would overlap with the beginning of the transmission of node i’s frame. The prob-
ability that all other nodes do not begin a transmission in this interval is (1 - p)N-1. 
Similarly, no other node can begin a transmission while node i is transmitting, as such a 
transmission would overlap with the latter part of node i’s transmission. The probabil-
ity that all other nodes do not begin a transmission in this interval is also (1 - p)N-1. 
Thus, the probability that a given node has a successful transmission is p(1 - p)2(N-1). 
By taking limits as in the slotted ALOHA case, we find that the maximum efficiency 
of the pure ALOHA protocol is only 1/(2e)—exactly half that of slotted ALOHA. This 
then is the price to be paid for a fully decentralized ALOHA protocol.
6.3    •    Multiple Access Links and Protocols         487
Carrier Sense Multiple Access (CSMA)
In both slotted and pure ALOHA, a node’s decision to transmit is made indepen-
dently of the activity of the other nodes attached to the broadcast channel. In particu-
lar, a node neither pays attention to whether another node happens to be transmitting 
when it begins to transmit, nor stops transmitting if another node begins to interfere 
with its transmission. In our cocktail party analogy, ALOHA protocols are quite 
like a boorish partygoer who continues to chatter away regardless of whether other 
people are talking. As humans, we have human protocols that allow us not only to 
behave with more civility, but also to decrease the amount of time spent “colliding” 
with each other in conversation and, consequently, to increase the amount of data we 
exchange in our conversations. Specifically, there are two important rules for polite 
human conversation:
•	 Listen before speaking. If someone else is speaking, wait until they are finished. 
In the networking world, this is called carrier sensing—a node listens to the 
channel before transmitting. If a frame from another node is currently being trans-
mitted into the channel, a node then waits until it detects no transmissions for a 
short amount of time and then begins transmission.
•	 If someone else begins talking at the same time, stop talking. In the network-
ing world, this is called collision detection—a transmitting node listens to the 
channel while it is transmitting. If it detects that another node is transmitting an 
interfering frame, it stops transmitting and waits a random amount of time before 
repeating the sense-and-transmit-when-idle cycle.
These two rules are embodied in the family of carrier sense multiple access 
(CSMA) and CSMA with collision detection (CSMA/CD) protocols [Kleinrock 
1975b; Metcalfe 1976; Lam 1980; Rom 1990]. Many variations on CSMA and 
Figure 6.11  ♦  Interfering transmissions in pure ALOHA
Time
Will overlap
with start of 
i’s frame
t0 – 1
t0
t0 + 1
Will overlap
with end of 
i’s frame
Node i frame
488         Chapter 6    •    The Link Layer and LANs
CSMA/CD have been proposed. Here, we’ll consider a few of the most important, 
and fundamental, characteristics of CSMA and CSMA/CD.
The first question that you might ask about CSMA is why, if all nodes perform 
carrier sensing, do collisions occur in the first place? After all, a node will refrain 
from transmitting whenever it senses that another node is transmitting. The answer 
to the question can best be illustrated using space-time diagrams [Molle 1987]. 
­
Figure 6.12 shows a space-time diagram of four nodes (A, B, C, D) attached to a 
linear broadcast bus. The horizontal axis shows the position of each node in space; 
the vertical axis represents time.
At time t0, node B senses the channel is idle, as no other nodes are currently trans-
mitting. Node B thus begins transmitting, with its bits propagating in both directions 
along the broadcast medium. The downward propagation of B’s bits in Figure 6.12 
with increasing time indicates that a nonzero amount of time is needed for B’s bits 
actually to propagate (albeit at near the speed of light) along the broadcast medium. At 
time t1 (t1 7 t0), node D has a frame to send. Although node B is currently transmit-
ting at time t1, the bits being transmitted by B have yet to reach D, and thus D senses 
NORM ABRAMSON AND ALOHANET
Norm Abramson, a PhD engineer, had a passion for surfing and an interest in 
packet switching. This combination of interests brought him to the University of 
Hawaii in 1969. Hawaii consists of many mountainous islands, making it difficult 
to install and operate land-based networks. When not surfing, Abramson thought 
about how to design a network that does packet switching over radio. The network 
he designed had one central host and several secondary nodes scattered over the 
Hawaiian Islands. The network had two channels, each using a different frequency 
band. The downlink channel broadcasted packets from the central host to the sec-
ondary hosts; and the upstream channel sent packets from the secondary hosts to 
the central host. In addition to sending informational packets, the central host also 
sent on the downstream channel an acknowledgment for each packet successfully 
received from the secondary hosts.
Because the secondary hosts transmitted packets in a decentralized fashion, col-
lisions on the upstream channel inevitably occurred. This observation led Abramson 
to devise the pure ALOHA protocol, as described in this chapter. In 1970, with 
continued funding from ARPA, Abramson connected his ALOHAnet to the ARPAnet. 
Abramson’s work is important not only because it was the first example of a radio 
packet network, but also because it inspired Bob Metcalfe. A few years later, 
Metcalfe modified the ALOHA protocol to create the CSMA/CD protocol and the 
Ethernet LAN.
CASE HISTORY
6.3    •    Multiple Access Links and Protocols         489
the channel idle at t1. In accordance with the CSMA protocol, D thus begins transmit-
ting its frame. A short time later, B’s transmission begins to interfere with D’s trans-
mission at D. From Figure 6.12, it is evident that the end-to-end channel propagation 
delay of a broadcast channel—the time it takes for a signal to propagate from one of 
the nodes to another—will play a crucial role in determining its performance. The 
longer this propagation delay, the larger the chance that a carrier-sensing node is not 
yet able to sense a transmission that has already begun at another node in the network.
Carrier Sense Multiple Access with Collision Dection (CSMA/CD)
In Figure 6.12, nodes do not perform collision detection; both B and D continue to 
transmit their frames in their entirety even though a collision has occurred. When a 
node performs collision detection, it ceases transmission as soon as it detects a col-
lision. Figure 6.13 shows the same scenario as in Figure 6.12, except that the two 
Figure 6.12  ♦  
Space-time diagram of two CSMA nodes with colliding 
transmissions
A
Time
Time
Space
t 0
t 1
B
C
D
490         Chapter 6    •    The Link Layer and LANs
nodes each abort their transmission a short time after detecting a collision. Clearly, 
adding collision detection to a multiple access protocol will help protocol perfor-
mance by not transmitting a useless, damaged (by interference with a frame from 
another node) frame in its entirety.
Before analyzing the CSMA/CD protocol, let us now summarize its operation 
from the perspective of an adapter (in a node) attached to a broadcast channel:
	1.	 The adapter obtains a datagram from the network layer, prepares a link-layer 
frame, and puts the frame adapter buffer.
	2.	 If the adapter senses that the channel is idle (that is, there is no signal energy 
entering the adapter from the channel), it starts to transmit the frame. If, on the 
other hand, the adapter senses that the channel is busy, it waits until it senses 
no signal energy and then starts to transmit the frame.
	3.	 While transmitting, the adapter monitors for the presence of signal energy 
coming from other adapters using the broadcast channel.
Figure 6.13  ♦  CSMA with collision detection
A
Time
Time
Collision
detect/abort
time
Space
t 0
t 1
B
C
D
6.3    •    Multiple Access Links and Protocols         491
	4.	 If the adapter transmits the entire frame without detecting signal energy from 
other adapters, the adapter is finished with the frame. If, on the other hand, the 
adapter detects signal energy from other adapters while transmitting, it aborts 
the transmission (that is, it stops transmitting its frame).
	5.	 After aborting, the adapter waits a random amount of time and then returns to 
step 2.
The need to wait a random (rather than fixed) amount of time is hopefully clear—if 
two nodes transmitted frames at the same time and then both waited the same fixed 
amount of time, they’d continue colliding forever. But what is a good interval of 
time from which to choose the random backoff time? If the interval is large and the 
number of colliding nodes is small, nodes are likely to wait a large amount of time 
(with the channel remaining idle) before repeating the sense-and-transmit-when-
idle step. On the other hand, if the interval is small and the number of colliding 
nodes is large, it’s likely that the chosen random values will be nearly the same, 
and transmitting nodes will again collide. What we’d like is an interval that is short 
when the number of colliding nodes is small, and long when the number of colliding 
nodes is large.
The binary exponential backoff algorithm, used in Ethernet as well as in DOC-
SIS cable network multiple access protocols [DOCSIS 2011], elegantly solves this 
problem. Specifically, when transmitting a frame that has already experienced n col-
lisions, a node chooses the value of K at random from {0,1,2, . . . . 2n-1}. Thus, 
the more collisions experienced by a frame, the larger the interval from which K 
is chosen. For Ethernet, the actual amount of time a node waits is K # 512 bit times 
(i.e., K times the amount of time needed to send 512 bits into the Ethernet) and the 
maximum value that n can take is capped at 10.
Let’s look at an example. Suppose that a node attempts to transmit a frame for 
the first time and while transmitting it detects a collision. The node then chooses 
 
K = 0 with probability 0.5 or chooses K = 1 with probability 0.5. If the node 
chooses K = 0, then it immediately begins sensing the channel. If the node chooses 
K = 1, it waits 512 bit times (e.g., 5.12 microseconds for a 100 Mbps Ethernet) 
before beginning the sense-and-transmit-when-idle cycle. After a second collision, 
K is chosen with equal probability from {0,1,2,3}. After three collisions, K is cho-
sen with equal probability from {0,1,2,3,4,5,6,7}. After 10 or more collisions, K is 
chosen with equal probability from {0,1,2,…, 1023}. Thus, the size of the sets from 
which K is chosen grows exponentially with the number of collisions; for this reason 
this algorithm is referred to as binary exponential backoff.
We also note here that each time a node prepares a new frame for transmission, 
it runs the CSMA/CD algorithm, not taking into account any collisions that may 
have occurred in the recent past. So it is possible that a node with a new frame will 
immediately be able to sneak in a successful transmission while several other nodes 
are in the exponential backoff state.
492         Chapter 6    •    The Link Layer and LANs
CSMA/CD Efficiency
When only one node has a frame to send, the node can transmit at the full channel 
rate (e.g., for Ethernet typical rates are 10 Mbps, 100 Mbps, or 1 Gbps). However, if 
many nodes have frames to transmit, the effective transmission rate of the channel 
can be much less. We define the efficiency of CSMA/CD to be the long-run fraction 
of time during which frames are being transmitted on the channel without collisions 
when there is a large number of active nodes, with each node having a large number 
of frames to send. In order to present a closed-form approximation of the efficiency 
of Ethernet, let dprop denote the maximum time it takes signal energy to propagate 
between any two adapters. Let dtrans be the time to transmit a maximum-size frame 
(approximately 1.2 msecs for a 10 Mbps Ethernet). A derivation of the efficiency of 
CSMA/CD is beyond the scope of this book (see [Lam 1980] and [Bertsekas 1991]). 
Here we simply state the following approximation:
Efficiency =
1
1 + 5dprop>dtrans
We see from this formula that as dprop approaches 0, the efficiency approaches 1. 
This matches our intuition that if the propagation delay is zero, colliding nodes will 
abort immediately without wasting the channel. Also, as dtrans becomes very large, 
efficiency approaches 1. This is also intuitive because when a frame grabs the chan-
nel, it will hold on to the channel for a very long time; thus, the channel will be doing 
productive work most of the time.
6.3.3	Taking-Turns Protocols
Recall that two desirable properties of a multiple access protocol are (1) when only 
one node is active, the active node has a throughput of R bps, and (2) when M nodes 
are active, then each active node has a throughput of nearly R/M bps. The ALOHA 
and CSMA protocols have this first property but not the second. This has motivated 
researchers to create another class of protocols—the taking-turns protocols. As with 
random access protocols, there are dozens of taking-turns protocols, and each one of 
these protocols has many variations. We’ll discuss two of the more important protocols 
here. The first one is the polling protocol. The polling protocol requires one of the 
nodes to be designated as a master node. The master node polls each of the nodes in 
a round-robin fashion. In particular, the master node first sends a message to node 1, 
saying that it (node 1) can transmit up to some maximum number of frames. After node 
1 transmits some frames, the master node tells node 2 it (node 2) can transmit up to the 
maximum number of frames. (The master node can determine when a node has finished 
sending its frames by observing the lack of a signal on the channel.) The procedure con-
tinues in this manner, with the master node polling each of the nodes in a cyclic manner.
The polling protocol eliminates the collisions and empty slots that plague ran-
dom access protocols. This allows polling to achieve a much higher efficiency. But 
6.3    •    Multiple Access Links and Protocols         493
it also has a few drawbacks. The first drawback is that the protocol introduces a 
polling delay—the amount of time required to notify a node that it can transmit. If, 
for example, only one node is active, then the node will transmit at a rate less than 
R bps, as the master node must poll each of the inactive nodes in turn each time the 
active node has sent its maximum number of frames. The second drawback, which is 
potentially more serious, is that if the master node fails, the entire channel becomes 
inoperative. The 802.15 protocol and the Bluetooth protocol we will study in Sec-
tion 6.3 are examples of polling protocols.
The second taking-turns protocol is the token-passing protocol. In this pro-
tocol there is no master node. A small, special-purpose frame known as a token is 
exchanged among the nodes in some fixed order. For example, node 1 might always 
send the token to node 2, node 2 might always send the token to node 3, and node N 
might always send the token to node 1. When a node receives a token, it holds onto 
the token only if it has some frames to transmit; otherwise, it immediately forwards 
the token to the next node. If a node does have frames to transmit when it receives 
the token, it sends up to a maximum number of frames and then forwards the token to 
the next node. Token passing is decentralized and highly efficient. But it has its prob-
lems as well. For example, the failure of one node can crash the entire channel. Or if 
a node accidentally neglects to release the token, then some recovery procedure must 
be invoked to get the token back in circulation. Over the years many token-passing 
protocols have been developed, including the fiber distributed data interface (FDDI) 
protocol [Jain 1994] and the IEEE 802.5 token ring protocol [IEEE 802.5 2012], and 
each one had to address these as well as other sticky issues.
6.3.4	
DOCSIS: The Link-Layer Protocol for Cable  
Internet Access
In the previous three subsections, we’ve learned about three broad classes of mul-
tiple access protocols: channel partitioning protocols, random access protocols, and 
taking turns protocols. A cable access network will make for an excellent case study 
here, as we’ll find aspects of each of these three classes of multiple access protocols 
with the cable access network!
Recall from Section 1.2.1 that a cable access network typically connects several 
thousand residential cable modems to a cable modem termination system (CMTS) 
at the cable network headend. The Data-Over-Cable Service Interface Specifica-
tions (DOCSIS) [DOCSIS 2011] specifies the cable data network architecture and 
its protocols. DOCSIS uses FDM to divide the downstream (CMTS to modem) and 
upstream (modem to CMTS) network segments into multiple frequency channels. 
Each downstream channel is 6 MHz wide, with a maximum throughput of approxi-
mately 40 Mbps per channel (although this data rate is seldom seen at a cable modem 
in practice); each upstream channel has a maximum channel width of 6.4 MHz, and 
a maximum upstream throughput of approximately 30 Mbps. Each upstream and 
494         Chapter 6    •    The Link Layer and LANs
downstream channel is a broadcast channel. Frames transmitted on the downstream 
channel by the CMTS are received by all cable modems receiving that channel; since 
there is just a single CMTS transmitting into the downstream channel, however, there 
is no multiple access problem. The upstream direction, however, is more interesting 
and technically challenging, since multiple cable modems share the same upstream 
channel (frequency) to the CMTS, and thus collisions can potentially occur.
As illustrated in Figure 6.14, each upstream channel is divided into intervals 
of time (TDM-like), each containing a sequence of mini-slots during which cable 
modems can transmit to the CMTS. The CMTS explicitly grants permission to indi-
vidual cable modems to transmit during specific mini-slots. The CMTS accomplishes 
this by sending a control message known as a MAP message on a downstream chan-
nel to specify which cable modem (with data to send) can transmit during which 
mini-slot for the interval of time specified in the control message. Since mini-slots 
are explicitly allocated to cable modems, the CMTS can ensure there are no colliding 
transmissions during a mini-slot.
But how does the CMTS know which cable modems have data to send in the 
first place? This is accomplished by having cable modems send mini-slot-request 
frames to the CMTS during a special set of interval mini-slots that are dedicated for 
this purpose, as shown in Figure 6.14. These mini-slot-request frames are transmit-
ted in a random access manner and so may collide with each other. A cable modem 
can neither sense whether the upstream channel is busy nor detect collisions. Instead, 
the cable modem infers that its mini-slot-request frame experienced a collision if it 
does not receive a response to the requested allocation in the next downstream con-
trol message. When a collision is inferred, a cable modem uses binary exponential 
Figure 6.14  ♦  
Upstream and downstream channels between CMTS and 
cable modems
Residences with
cable modems
Minislots
containing
minislot
request frames
Assigned minislots
containing cable
modem upstream
data frames
Cable head end
MAP frame for 
interval [t1,t2]
CMTS
Downstream channel i
Upstream channel j
t1
t2
6.4    •    Switched Local Area Networks         495
backoff to defer the retransmission of its mini-slot-request frame to a future time 
slot. When there is little traffic on the upstream channel, a cable modem may actually 
transmit data frames during slots nominally assigned for mini-slot-request frames 
(and thus avoid having to wait for a mini-slot assignment).
A cable access network thus serves as a terrific example of multiple access pro-
tocols in action—FDM, TDM, random access, and centrally allocated time slots all 
within one network!
6.4	 Switched Local Area Networks
Having covered broadcast networks and multiple access protocols in the previous 
section, let’s turn our attention next to switched local networks. Figure 6.15 shows 
a switched local network connecting three departments, two servers and a router 
with four switches. Because these switches operate at the link layer, they switch 
link-layer frames (rather than network-layer datagrams), don’t recognize network-
layer addresses, and don’t use routing algorithms like RIP or OSPF to determine 
Figure 6.15  ♦  An institutional network connected together by four switches
Mail
server
To external
internet
1 Gbps
1
2
3
4
5
6
1 Gbps
1 Gbps
Electrical Engineering
Computer Science
100 Mbps
(ﬁber)
100 Mbps
(ﬁber)
100 Mbps
(ﬁber)
Mixture of 10 Mbps,
100 Mbps, 1 Gbps,
Cat 5 cable
Web
server
Computer Engineering
496         Chapter 6    •    The Link Layer and LANs
paths through the network of layer-2 switches. Instead of using IP addresses, we will 
soon see that they use link-layer addresses to forward link-layer frames through the 
network of switches. We’ll begin our study of switched LANs by first covering link-
layer addressing (Section 6.4.1). We then examine the celebrated Ethernet protocol 
(Section 6.5.2). After examining link-layer addressing and Ethernet, we’ll look at 
how link-layer switches operate (Section 6.4.3), and then see (Section 6.4.4) how 
these switches are often used to build large-scale LANs.
6.4.1	Link-Layer Addressing and ARP
Hosts and routers have link-layer addresses. Now you might find this surprising, 
recalling from Chapter 4 that hosts and routers have network-layer addresses as well. 
You might be asking, why in the world do we need to have addresses at both the 
network and link layers? In addition to describing the syntax and function of the 
link-layer addresses, in this section we hope to shed some light on why the two lay-
ers of addresses are useful and, in fact, indispensable. We’ll also cover the Address 
Resolution Protocol (ARP), which provides a mechanism to translate IP addresses to 
link-layer addresses.
MAC Addresses
In truth, it is not hosts and routers that have link-layer addresses but rather their 
adapters (that is, network interfaces) that have link-layer addresses. A host or router 
with multiple network interfaces will thus have multiple link-layer addresses associ-
ated with it, just as it would also have multiple IP addresses associated with it. It's 
important to note, however, that link-layer switches do not have link-layer addresses 
associated with their interfaces that connect to hosts and routers. This is because the 
job of the link-layer switch is to carry datagrams between hosts and routers; a switch 
does this job transparently, that is, without the host or router having to explicitly 
address the frame to the intervening switch. This is illustrated in Figure 6.16. A link-
layer address is variously called a LAN address, a physical address, or a MAC 
address. Because MAC address seems to be the most popular term, we’ll henceforth 
refer to link-layer addresses as MAC addresses. For most LANs (including Ethernet 
and 802.11 wireless LANs), the MAC address is 6 bytes long, giving 248 possi-
ble MAC addresses. As shown in Figure 6.16, these 6-byte addresses are typically 
expressed in hexadecimal notation, with each byte of the address expressed as a pair 
of hexadecimal numbers. Although MAC addresses were designed to be permanent, 
it is now possible to change an adapter’s MAC address via software. For the rest of 
this section, however, we’ll assume that an adapter’s MAC address is fixed.
One interesting property of MAC addresses is that no two adapters have the 
same address. This might seem surprising given that adapters are manufactured in 
many countries by many companies. How does a company manufacturing adapters in 
Taiwan make sure that it is using different addresses from a company manufacturing 
6.4    •    Switched Local Area Networks         497
adapters in Belgium? The answer is that the IEEE manages the MAC address space. 
In particular, when a company wants to manufacture adapters, it purchases a chunk 
of the address space consisting of 224 addresses for a nominal fee. IEEE allocates the 
chunk of 224 addresses by fixing the first 24 bits of a MAC address and letting the 
company create unique combinations of the last 24 bits for each adapter.
An adapter’s MAC address has a flat structure (as opposed to a hierarchical 
structure) and doesn’t change no matter where the adapter goes. A laptop with an 
Ethernet interface always has the same MAC address, no matter where the computer 
goes. A smartphone with an 802.11 interface always has the same MAC address, no 
matter where the smartphone goes. Recall that, in contrast, IP addresses have a hier-
archical structure (that is, a network part and a host part), and a host’s IP addresses 
needs to be changed when the host moves, i.e., changes the network to which it is 
attached. An adapter’s MAC address is analogous to a person’s social security num-
ber, which also has a flat addressing structure and which doesn’t change no matter 
where the person goes. An IP address is analogous to a person’s postal address, 
which is hierarchical and which must be changed whenever a person moves. Just as a 
person may find it useful to have both a postal address and a social security number, 
it is useful for a host and router interfaces to have both a network-layer address and 
a MAC address.
When an adapter wants to send a frame to some destination adapter, the sending 
adapter inserts the destination adapter’s MAC address into the frame and then sends the 
frame into the LAN. As we will soon see, a switch occasionally broadcasts an incom-
ing frame onto all of its interfaces. We’ll see in Chapter 7 that 802.11 also broadcasts 
frames. Thus, an adapter may receive a frame that isn’t addressed to it. Thus, when 
an adapter receives a frame, it will check to see whether the destination MAC address 
Figure 6.16  ♦  
Each interface connected to a LAN has a unique MAC 
address
88-B2-2F-54-1A-0F
5C-66-AB-90-75-B1
1A-23-F9-CD-06-9B
49-BD-D2-C7-56-2A
498         Chapter 6    •    The Link Layer and LANs
in the frame matches its own MAC address. If there is a match, the adapter extracts 
the enclosed datagram and passes the datagram up the protocol stack. If there isn’t a 
match, the adapter discards the frame, without passing the network-layer datagram up. 
Thus, the destination only will be interrupted when the frame is received.
However, sometimes a sending adapter does want all the other adapters on the 
LAN to receive and process the frame it is about to send. In this case, the sending 
adapter inserts a special MAC broadcast address into the destination address field 
of the frame. For LANs that use 6-byte addresses (such as Ethernet and 802.11), the 
broadcast address is a string of 48 consecutive 1s (that is, FF-FF-FF-FF-FF-FF in 
hexadecimal notation).
Address Resolution Protocol (ARP)
Because there are both network-layer addresses (for example, Internet IP addresses) 
and link-layer addresses (that is, MAC addresses), there is a need to translate between 
them. For the Internet, this is the job of the Address Resolution Protocol (ARP) 
[RFC 826].
To understand the need for a protocol such as ARP, consider the network 
shown in Figure 6.17. In this simple example, each host and router has a single IP 
address and single MAC address. As usual, IP addresses are shown in dotted-decimal 
KEEPING THE LAYERS INDEPENDENT
There are several reasons why hosts and router interfaces have MAC addresses in 
­
addition to network-layer addresses. First, LANs are designed for arbitrary network-layer 
protocols, not just for IP and the Internet. If adapters were assigned IP addresses rather 
than “neutral” MAC addresses, then adapters would not easily be able to support other 
network-layer protocols (for example, IPX or DECnet). Second, if adapters were to use 
network-layer addresses instead of MAC addresses, the network-layer address would have 
to be stored in the adapter RAM and reconfigured every time the adapter was moved (or 
powered up). Another option is to not use any addresses in the adapters and have each 
adapter pass the data (typically, an IP datagram) of each frame it receives up the protocol 
stack. The network layer could then check for a matching network-layer address. One 
problem with this option is that the host would be interrupted by every frame sent on the 
LAN, including by frames that were destined for other hosts on the same broadcast LAN. 
In summary, in order for the layers to be largely independent building blocks in a network 
architecture, different layers need to have their own addressing scheme. We have now 
seen three types of addresses: host names for the application layer, IP addresses for the 
network layer, and MAC addresses for the link layer.
PRINCIPLES IN PRACTICE
6.4    •    Switched Local Area Networks         499
notation and MAC addresses are shown in hexadecimal notation. For the purposes of 
this discussion, we will assume in this section that the switch broadcasts all frames; 
that is, whenever a switch receives a frame on one interface, it forwards the frame 
on all of its other interfaces. In the next section, we will provide a more accurate 
explanation of how switches operate.
Now suppose that the host with IP address 222.222.222.220 wants to send an IP 
datagram to host 222.222.222.222. In this example, both the source and destination 
are in the same subnet, in the addressing sense of Section 4.3.3. To send a datagram, 
the source must give its adapter not only the IP datagram but also the MAC address 
for destination 222.222.222.222. The sending adapter will then construct a link-layer 
frame containing the destination’s MAC address and send the frame into the LAN.
The important question addressed in this section is, How does the sending host 
determine the MAC address for the destination host with IP address 222.222.222.222? 
As you might have guessed, it uses ARP. An ARP module in the sending host takes 
any IP address on the same LAN as input, and returns the corresponding MAC 
address. In the example at hand, sending host 222.222.222.220 provides its ARP 
module the IP address 222.222.222.222, and the ARP module returns the corre-
sponding MAC address 49-BD-D2-C7-56-2A.
So we see that ARP resolves an IP address to a MAC address. In many ways 
it is analogous to DNS (studied in Section 2.5), which resolves host names to IP 
addresses. However, one important difference between the two resolvers is that DNS 
resolves host names for hosts anywhere in the Internet, whereas ARP resolves IP 
addresses only for hosts and router interfaces on the same subnet. If a node in Cali-
fornia were to try to use ARP to resolve the IP address for a node in Mississippi, ARP 
would return with an error.
Figure 6.17  ♦  
Each interface on a LAN has an IP address and a MAC 
address
IP:222.222.222.221
IP:222.222.222.220
IP:222.222.222.223
IP:222.222.222.222
5C-66-AB-90-75-B1
1A-23-F9-CD-06-9B
49-BD-D2-C7-56-2A
88-B2-2F-54-1A-0F
A
B
C
500         Chapter 6    •    The Link Layer and LANs
Now that we have explained what ARP does, let’s look at how it works. Each host 
and router has an ARP table in its memory, which contains mappings of IP addresses 
to MAC addresses. Figure 6.18 shows what an ARP table in host 222.222.222.220 
might look like. The ARP table also contains a time-to-live (TTL) value, which indi-
cates when each mapping will be deleted from the table. Note that a table does not 
necessarily contain an entry for every host and router on the subnet; some may have 
never been entered into the table, and others may have expired. A typical expiration 
time for an entry is 20 minutes from when an entry is placed in an ARP table.
Now suppose that host 222.222.222.220 wants to send a datagram that is IP-
addressed to another host or router on that subnet. The sending host needs to obtain the 
MAC address of the destination given the IP address. This task is easy if the sender’s 
ARP table has an entry for the destination node. But what if the ARP table doesn’t cur-
rently have an entry for the destination? In particular, suppose 222.222.222.220 wants 
to send a datagram to 222.222.222.222. In this case, the sender uses the ARP protocol 
to resolve the address. First, the sender constructs a special packet called an ARP 
packet. An ARP packet has several fields, including the sending and receiving IP and 
MAC addresses. Both ARP query and response packets have the same format. The pur-
pose of the ARP query packet is to query all the other hosts and routers on the subnet 
to determine the MAC address corresponding to the IP address that is being resolved.
Returning to our example, 222.222.222.220 passes an ARP query packet to 
the adapter along with an indication that the adapter should send the packet to the 
MAC broadcast address, namely, FF-FF-FF-FF-FF-FF. The adapter encapsulates the 
ARP packet in a link-layer frame, uses the broadcast address for the frame’s destina-
tion address, and transmits the frame into the subnet. Recalling our social security 
­
number/postal address analogy, an ARP query is equivalent to a person shouting out 
in a crowded room of cubicles in some company (say, AnyCorp): “What is the social 
security number of the person whose postal address is Cubicle 13, Room 112, Any-
Corp, Palo Alto, California?” The frame containing the ARP query is received by all 
the other adapters on the subnet, and (because of the broadcast address) each adapter 
passes the ARP packet within the frame up to its ARP module. Each of these ARP 
modules checks to see if its IP address matches the destination IP address in the ARP 
packet. The one with a match sends back to the querying host a response ARP packet 
with the desired mapping. The querying host 222.222.222.220 can then update its 
ARP table and send its IP datagram, encapsulated in a link-layer frame whose desti-
nation MAC is that of the host or router responding to the earlier ARP query.
Figure 6.18  ♦  A possible ARP table in 222.222.222.220
IP Address
MAC Address
TTL
222.222.222.221
88-B2-2F-54-1A-0F
13:45:00
222.222.222.223
5C-66-AB-90-75-B1
13:52:00
6.4    •    Switched Local Area Networks         501
There are a couple of interesting things to note about the ARP protocol. First, 
the query ARP message is sent within a broadcast frame, whereas the response 
ARP message is sent within a standard frame. Before reading on you should think 
about why this is so. Second, ARP is plug-and-play; that is, an ARP table gets built 
­
automatically—it doesn’t have to be configured by a system administrator. And if a 
host becomes disconnected from the subnet, its entry is eventually deleted from the 
other ARP tables in the subnet.
Students often wonder if ARP is a link-layer protocol or a network-layer proto-
col. As we’ve seen, an ARP packet is encapsulated within a link-layer frame and thus 
lies architecturally above the link layer. However, an ARP packet has fields contain-
ing link-layer addresses and thus is arguably a link-layer protocol, but it also contains 
network-layer addresses and thus is also arguably a network-layer protocol. In the 
end, ARP is probably best considered a protocol that straddles the boundary between 
the link and network layers—not fitting neatly into the simple layered protocol stack 
we studied in Chapter 1. Such are the complexities of real-world protocols!
Sending a Datagram off the Subnet
It should now be clear how ARP operates when a host wants to send a datagram to 
another host on the same subnet. But now let’s look at the more complicated situ-
ation when a host on a subnet wants to send a network-layer datagram to a host off 
the subnet (that is, across a router onto another subnet). Let’s discuss this issue in 
the context of Figure 6.19, which shows a simple network consisting of two subnets 
interconnected by a router.
There are several interesting things to note about Figure 6.19. Each host has 
exactly one IP address and one adapter. But, as discussed in Chapter 4, a router has 
an IP address for each of its interfaces. For each router interface there is also an ARP 
module (in the router) and an adapter. Because the router in Figure 6.19 has two 
interfaces, it has two IP addresses, two ARP modules, and two adapters. Of course, 
each adapter in the network has its own MAC address.
Figure 6.19  ♦  Two subnets interconnected by a router
IP:111.111.111.110
IP:111.111.111.111
IP:111.111.111.112
IP:222.222.222.221
IP:222.222.222.222
74-29-9C-E8-FF-55
CC-49-DE-D0-AB-7D
E6-E9-00-17-BB-4B
1A-23-F9-CD-06-9B
IP:222.222.222.220
88-B2-2F-54-1A-0F
49-BD-D2-C7-56-2A
502         Chapter 6    •    The Link Layer and LANs
Also note that Subnet 1 has the network address 111.111.111/24 and that Subnet 2 
 
has the network address 222.222.222/24. Thus all of the interfaces connected to Sub-
net 1 have addresses of the form 111.111.111.xxx and all of the interfaces connected 
to Subnet 2 have addresses of the form 222.222.222.xxx.
Now let’s examine how a host on Subnet 1 would send a datagram to a host 
on Subnet 2. Specifically, suppose that host 111.111.111.111 wants to send an IP 
datagram to a host 222.222.222.222. The sending host passes the datagram to its 
adapter, as usual. But the sending host must also indicate to its adapter an appro-
priate destination MAC address. What MAC address should the adapter use? One 
might be tempted to guess that the appropriate MAC address is that of the adapter for 
host 222.222.222.222, namely, 49-BD-D2-C7-56-2A. This guess, however, would 
be wrong! If the sending adapter were to use that MAC address, then none of the 
­
adapters on Subnet 1 would bother to pass the IP datagram up to its network layer, 
since the frame’s destination address would not match the MAC address of any 
adapter on Subnet 1. The datagram would just die and go to datagram heaven.
If we look carefully at Figure 6.19, we see that in order for a datagram to go from 
111.111.111.111 to a host on Subnet 2, the datagram must first be sent to the router 
interface 111.111.111.110, which is the IP address of the first-hop router on the 
path to the final destination. Thus, the appropriate MAC address for the frame is the 
address of the adapter for router interface 111.111.111.110, namely, E6-E9-00-17-
BB-4B. How does the sending host acquire the MAC address for 111.111.111.110? 
By using ARP, of course! Once the sending adapter has this MAC address, it cre-
ates a frame (containing the datagram addressed to 222.222.222.222) and sends the 
frame into Subnet 1. The router adapter on Subnet 1 sees that the link-layer frame 
is addressed to it, and therefore passes the frame to the network layer of the router. 
Hooray—the IP datagram has successfully been moved from source host to the 
router! But we are not finished. We still have to move the datagram from the router 
to the destination. The router now has to determine the correct interface on which the 
datagram is to be forwarded. As discussed in Chapter 4, this is done by consulting a 
forwarding table in the router. The forwarding table tells the router that the datagram 
is to be forwarded via router interface 222.222.222.220. This interface then passes 
the datagram to its adapter, which encapsulates the datagram in a new frame and 
sends the frame into Subnet 2. This time, the destination MAC address of the frame 
is indeed the MAC address of the ultimate destination. And how does the router 
obtain this destination MAC address? From ARP, of course!
ARP for Ethernet is defined in RFC 826. A nice introduction to ARP is given in 
the TCP/IP tutorial, RFC 1180. We’ll explore ARP in more detail in the homework 
problems.
6.4.2	Ethernet
Ethernet has pretty much taken over the wired LAN market. In the 1980s and the 
early 1990s, Ethernet faced many challenges from other LAN technologies, ­
including 
6.4    •    Switched Local Area Networks         503
token ring, FDDI, and ATM. Some of these other technologies succeeded in captur-
ing a part of the LAN market for a few years. But since its invention in the mid-
1970s, Ethernet has continued to evolve and grow and has held on to its dominant 
position. Today, Ethernet is by far the most prevalent wired LAN technology, and it 
is likely to remain so for the foreseeable future. One might say that Ethernet has been 
to local area networking what the Internet has been to global networking.
There are many reasons for Ethernet’s success. First, Ethernet was the first 
widely deployed high-speed LAN. Because it was deployed early, network admin-
istrators became intimately familiar with Ethernet—its wonders and its quirks—and 
were reluctant to switch over to other LAN technologies when they came on the 
scene. Second, token ring, FDDI, and ATM were more complex and expensive than 
Ethernet, which further discouraged network administrators from switching over. 
Third, the most compelling reason to switch to another LAN technology (such as 
FDDI or ATM) was usually the higher data rate of the new technology; however, 
Ethernet always fought back, producing versions that operated at equal data rates 
or higher. Switched Ethernet was also introduced in the early 1990s, which further 
increased its effective data rates. Finally, because Ethernet has been so popular, Eth-
ernet hardware (in particular, adapters and switches) has become a commodity and 
is remarkably cheap.
The original Ethernet LAN was invented in the mid-1970s by Bob Metcalfe 
and David Boggs. The original Ethernet LAN used a coaxial bus to interconnect the 
nodes. Bus topologies for Ethernet actually persisted throughout the 1980s and into 
the mid-1990s. Ethernet with a bus topology is a broadcast LAN—all transmitted 
frames travel to and are processed by all adapters connected to the bus. Recall that 
we covered Ethernet’s CSMA/CD multiple access protocol with binary exponential 
backoff in Section 6.3.2.
By the late 1990s, most companies and universities had replaced their LANs 
with Ethernet installations using a hub-based star topology. In such an installation 
the hosts (and routers) are directly connected to a hub with twisted-pair copper wire. 
A hub is a physical-layer device that acts on individual bits rather than frames. 
When a bit, representing a zero or a one, arrives from one interface, the hub sim-
ply re-creates the bit, boosts its energy strength, and transmits the bit onto all the 
other interfaces. Thus, Ethernet with a hub-based star topology is also a broadcast 
LAN—whenever a hub receives a bit from one of its interfaces, it sends a copy out 
on all of its other interfaces. In particular, if a hub receives frames from two different 
interfaces at the same time, a collision occurs and the nodes that created the frames 
must retransmit.
In the early 2000s Ethernet experienced yet another major evolutionary change. 
Ethernet installations continued to use a star topology, but the hub at the center was 
replaced with a switch. We’ll be examining switched Ethernet in depth later in this 
chapter. For now, we only mention that a switch is not only “collision-less” but is 
also a bona-fide store-and-forward packet switch; but unlike routers, which operate 
up through layer 3, a switch operates only up through layer 2.
504         Chapter 6    •    The Link Layer and LANs
Ethernet Frame Structure
We can learn a lot about Ethernet by examining the Ethernet frame, which is shown 
in Figure 6.20. To give this discussion about Ethernet frames a tangible context, 
let’s consider sending an IP datagram from one host to another host, with both 
hosts on the same Ethernet LAN (for example, the Ethernet LAN in Figure 6.17.) 
(Although the payload of our Ethernet frame is an IP datagram, we note that an 
Ethernet frame can carry other network-layer packets as well.) Let the sending 
adapter, adapter A, have the MAC address AA-AA-AA-AA-AA-AA and the 
receiving adapter, adapter B, have the MAC address BB-BB-BB-BB-BB-BB. The 
sending adapter encapsulates the IP datagram within an Ethernet frame and passes 
the frame to the physical layer. The receiving adapter receives the frame from the 
physical layer, extracts the IP datagram, and passes the IP datagram to the network 
layer. In this context, let’s now examine the six fields of the Ethernet frame, as 
shown in Figure 6.20.
•	 Data field (46 to 1,500 bytes). This field carries the IP datagram. The maxi-
mum transmission unit (MTU) of Ethernet is 1,500 bytes. This means that if the 
IP datagram exceeds 1,500 bytes, then the host has to fragment the datagram, 
as discussed in Section 4.3.2. The minimum size of the data field is 46 bytes. 
This means that if the IP datagram is less than 46 bytes, the data field has to be 
“stuffed” to fill it out to 46 bytes. When stuffing is used, the data passed to the 
network layer contains the stuffing as well as an IP datagram. The network layer 
uses the length field in the IP datagram header to remove the stuffing.
•	 Destination address (6 bytes). This field contains the MAC address of the 
destination adapter, BB-BB-BB-BB-BB-BB. When adapter B receives an Eth-
ernet frame whose destination address is either BB-BB-BB-BB-BB-BB or the 
MAC broadcast address, it passes the contents of the frame’s data field to the 
network layer; if it receives a frame with any other MAC address, it discards 
the frame.
•	 Source address (6 bytes). This field contains the MAC address of the adapter that 
transmits the frame onto the LAN, in this example, AA-AA-AA-AA-AA-AA.
•	 Type field (2 bytes). The type field permits Ethernet to multiplex network-layer 
protocols. To understand this, we need to keep in mind that hosts can use other 
network-layer protocols besides IP. In fact, a given host may support multi-
ple network-layer protocols using different protocols for different applications. 
Figure 6.20  ♦  Ethernet frame structure
Preamble
CRC
Dest.
address
Source
address
Type
Data
6.4    •    Switched Local Area Networks         505
For this reason, when the Ethernet frame arrives at adapter B, adapter B needs 
to know to which network-layer protocol it should pass (that is, demultiplex) 
the contents of the data field. IP and other network-layer protocols (for exam-
ple, Novell IPX or AppleTalk) each have their own, standardized type number. 
Furthermore, the ARP protocol (discussed in the previous section) has its own 
type number, and if the arriving frame contains an ARP packet (i.e., has a type 
field of 0806 hexadecimal), the ARP packet will be demultiplexed up to the 
ARP protocol. Note that the type field is analogous to the protocol field in the 
network-layer datagram and the port-number fields in the transport-layer seg-
ment; all of these fields serve to glue a protocol at one layer to a protocol at the 
layer above.
•	 Cyclic redundancy check (CRC) (4 bytes). As discussed in Section 6.2.3, the pur-
pose of the CRC field is to allow the receiving adapter, adapter B, to detect bit 
errors in the frame.
•	 Preamble (8 bytes). The Ethernet frame begins with an 8-byte preamble field. 
Each of the first 7 bytes of the preamble has a value of 10101010; the last byte 
is 10101011. The first 7 bytes of the preamble serve to “wake up” the receiv-
ing adapters and to synchronize their clocks to that of the sender’s clock. Why 
should the clocks be out of synchronization? Keep in mind that adapter A aims 
to transmit the frame at 10 Mbps, 100 Mbps, or 1 Gbps, depending on the type 
of Ethernet LAN. However, because nothing is absolutely perfect, adapter A will 
not transmit the frame at exactly the target rate; there will always be some drift 
from the target rate, a drift which is not known a priori by the other adapters on 
the LAN. A receiving adapter can lock onto adapter A’s clock simply by locking 
onto the bits in the first 7 bytes of the preamble. The last 2 bits of the eighth byte 
of the preamble (the first two consecutive 1s) alert adapter B that the “important 
stuff” is about to come.
All of the Ethernet technologies provide connectionless service to the network 
layer. That is, when adapter A wants to send a datagram to adapter B, adapter A 
encapsulates the datagram in an Ethernet frame and sends the frame into the LAN, 
without first handshaking with adapter B. This layer-2 connectionless service is anal-
ogous to IP’s layer-3 datagram service and UDP’s layer-4 connectionless service.
Ethernet technologies provide an unreliable service to the network layer. Spe-
cifically, when adapter B receives a frame from adapter A, it runs the frame through 
a CRC check, but neither sends an acknowledgment when a frame passes the CRC 
check nor sends a negative acknowledgment when a frame fails the CRC check. 
When a frame fails the CRC check, adapter B simply discards the frame. Thus, 
adapter A has no idea whether its transmitted frame reached adapter B and passed 
the CRC check. This lack of reliable transport (at the link layer) helps to make Eth-
ernet simple and cheap. But it also means that the stream of datagrams passed to the 
network layer can have gaps.
506         Chapter 6    •    The Link Layer and LANs
If there are gaps due to discarded Ethernet frames, does the application at 
Host B see gaps as well? As we learned in Chapter 3, this depends on whether 
the application is using UDP or TCP. If the application is using UDP, then the 
application in Host B will indeed see gaps in the data. On the other hand, if the 
application is using TCP, then TCP in Host B will not acknowledge the data 
contained in discarded frames, causing TCP in Host A to retransmit. Note that 
when TCP retransmits data, the data will eventually return to the Ethernet adapter 
at which it was discarded. Thus, in this sense, Ethernet does retransmit data, 
although Ethernet is unaware of whether it is transmitting a brand-new datagram 
with brand-new data, or a datagram that contains data that has already been trans-
mitted at least once.
Ethernet Technologies
In our discussion above, we’ve referred to Ethernet as if it were a single protocol 
standard. But in fact, Ethernet comes in many different flavors, with somewhat bewil-
dering acronyms such as 10BASE-T, 10BASE-2, 100BASE-T, 1000BASE-LX, 
BOB METCALFE AND ETHERNET
As a PhD student at Harvard University in the early 1970s, Bob Metcalfe worked 
on the ARPAnet at MIT. During his studies, he also became exposed to Abramson’s 
work on ALOHA and random access protocols. After completing his PhD and just 
before beginning a job at Xerox Palo Alto Research Center (Xerox PARC), he vis-
ited Abramson and his University of Hawaii colleagues for three months, getting a 
firsthand look at ALOHAnet. At Xerox PARC, Metcalfe became exposed to Alto com-
puters, which in many ways were the forerunners of the personal computers of the 
1980s. Metcalfe saw the need to network these computers in an inexpensive man-
ner. So armed with his knowledge about ARPAnet, ALOHAnet, and random access 
protocols, Metcalfe—along with colleague David Boggs—invented Ethernet.
Metcalfe and Boggs’s original Ethernet ran at 2.94 Mbps and linked up to 256 
hosts separated by up to one mile. Metcalfe and Boggs succeeded at getting most of 
the researchers at Xerox PARC to communicate through their Alto computers. Metcalfe 
then forged an alliance between Xerox, Digital, and Intel to establish Ethernet as a 
10 Mbps Ethernet standard, ratified by the IEEE. Xerox did not show much interest in 
commercializing Ethernet. In 1979, Metcalfe formed his own company, 3Com, which 
developed and commercialized networking technology, including Ethernet technol-
ogy. In particular, 3Com developed and marketed Ethernet cards in the early 1980s 
for the immensely popular IBM PCs.
CASE HISTORY
6.4    •    Switched Local Area Networks         507
10GBASE-T and 40GBASE-T. These and many other Ethernet technologies have 
been standardized over the years by the IEEE 802.3 CSMA/CD (Ethernet) working 
group [IEEE 802.3 2012]. While these acronyms may appear bewildering, there is 
actually considerable order here. The first part of the acronym refers to the speed of 
the standard: 10, 100, 1000, or 10G, for 10 Megabit (per second), 100 Megabit, Giga-
bit, 10 Gigabit and 40 Gigibit Ethernet, respectively. “BASE” refers to baseband 
Ethernet, meaning that the physical media only carries Ethernet traffic; almost all of 
the 802.3 standards are for baseband Ethernet. The final part of the acronym refers to 
the physical media itself; Ethernet is both a link-layer and a physical-layer specifica-
tion and is carried over a variety of physical media including coaxial cable, copper 
wire, and fiber. Generally, a “T” refers to twisted-pair copper wires.
Historically, an Ethernet was initially conceived of as a segment of coaxial cable. 
The early 10BASE-2 and 10BASE-5 standards specify 10 Mbps Ethernet over two 
types of coaxial cable, each limited in length to 500 meters. Longer runs could be 
obtained by using a repeater—a physical-layer device that receives a signal on the 
input side, and regenerates the signal on the output side. A coaxial cable corresponds 
nicely to our view of Ethernet as a broadcast medium—all frames transmitted by one 
interface are received at other interfaces, and Ethernet’s CDMA/CD protocol nicely 
solves the multiple access problem. Nodes simply attach to the cable, and voila, we 
have a local area network!
Ethernet has passed through a series of evolutionary steps over the years, and 
today’s Ethernet is very different from the original bus-topology designs using coax-
ial cable. In most installations today, nodes are connected to a switch via point-to-
point segments made of twisted-pair copper wires or fiber-optic cables, as shown in 
Figures 6.15–6.17.
In the mid-1990s, Ethernet was standardized at 100 Mbps, 10 times faster than 
10 Mbps Ethernet. The original Ethernet MAC protocol and frame format were pre-
served, but higher-speed physical layers were defined for copper wire (100BASE-T) 
and fiber (100BASE-FX, 100BASE-SX, 100BASE-BX). Figure 6.21 shows these 
different standards and the common Ethernet MAC protocol and frame format. 
100 Mbps Ethernet is limited to a 100-meter distance over twisted pair, and to 
Physical
Transport
Network
Link
Application
100BASE-TX
100BASE-T4
100BASE-T2
MAC protocol
and frame format
100BASE-SX
100BASE-FX
100BASE-BX
Figure 6.21  ♦  
100 Mbps Ethernet standards: A common link layer, 
­
different physical layers
508         Chapter 6    •    The Link Layer and LANs
several kilometers over fiber, allowing Ethernet switches in different buildings to 
be connected.
Gigabit Ethernet is an extension to the highly successful 10 Mbps and 100 Mbps 
Ethernet standards. Offering a raw data rate of 40,000 Mbps, 40 Gigabit Ethernet 
maintains full compatibility with the huge installed base of Ethernet equipment. The 
standard for Gigabit Ethernet, referred to as IEEE 802.3z, does the following:
•	 Uses the standard Ethernet frame format (Figure 6.20) and is backward com-
patible with 10BASE-T and 100BASE-T technologies. This allows for easy 
integration of Gigabit Ethernet with the existing installed base of Ethernet 
equipment.
•	 Allows for point-to-point links as well as shared broadcast channels. Point-to-
point links use switches while broadcast channels use hubs, as described earlier. 
In Gigabit Ethernet jargon, hubs are called buffered distributors.
•	 Uses CSMA/CD for shared broadcast channels. In order to have acceptable effi-
ciency, the maximum distance between nodes must be severely restricted.
•	 Allows for full-duplex operation at 40 Gbps in both directions for point-to-point 
channels.
Initially operating over optical fiber, Gigabit Ethernet is now able to run over cat-
egory 5 UTP cabling.
Let’s conclude our discussion of Ethernet technology by posing a question 
that may have begun troubling you. In the days of bus topologies and hub-based 
star topologies, Ethernet was clearly a broadcast link (as defined in Section 6.3) in 
which frame collisions occurred when nodes transmitted at the same time. To deal 
with these collisions, the Ethernet standard included the CSMA/CD protocol, which 
is particularly effective for a wired broadcast LAN spanning a small geographical 
region. But if the prevalent use of Ethernet today is a switch-based star topology, 
using store-and-forward packet switching, is there really a need anymore for an Eth-
ernet MAC protocol? As we’ll see shortly, a switch coordinates its transmissions 
and never forwards more than one frame onto the same interface at any time. Fur-
thermore, modern switches are full-duplex, so that a switch and a node can each 
send frames to each other at the same time without interference. In other words, in 
a switch-based Ethernet LAN there are no collisions and, therefore, there is no need 
for a MAC protocol!
As we’ve seen, today’s Ethernets are very different from the original Ethernet 
conceived by Metcalfe and Boggs more than 30 years ago—speeds have increased 
by three orders of magnitude, Ethernet frames are carried over a variety of media, 
switched-Ethernets have become dominant, and now even the MAC protocol is often 
unnecessary! Is all of this really still Ethernet? The answer, of course, is “yes, by 
definition.” It is interesting to note, however, that through all of these changes, there 
6.4    •    Switched Local Area Networks         509
has indeed been one enduring constant that has remained unchanged over 30 years—
Ethernet’s frame format. Perhaps this then is the one true and timeless centerpiece of 
the Ethernet standard.
6.4.3	Link-Layer Switches
Up until this point, we have been purposefully vague about what a switch actually 
does and how it works. The role of the switch is to receive incoming link-layer 
frames and forward them onto outgoing links; we’ll study this forwarding function 
in detail in this subsection. We’ll see that the switch itself is transparent to the 
hosts and routers in the subnet; that is, a host/router addresses a frame to another 
host/router (rather than addressing the frame to the switch) and happily sends the 
frame into the LAN, unaware that a switch will be receiving the frame and forward-
ing it. The rate at which frames arrive to any one of the switch’s output interfaces 
may temporarily exceed the link capacity of that interface. To accommodate this 
problem, switch output interfaces have buffers, in much the same way that router 
output interfaces have buffers for datagrams. Let’s now take a closer look at how 
switches operate.
Forwarding and Filtering
Filtering is the switch function that determines whether a frame should be for-
warded to some interface or should just be dropped. Forwarding is the switch 
function that determines the interfaces to which a frame should be directed, and 
then moves the frame to those interfaces. Switch filtering and forwarding are done 
with a switch table. The switch table contains entries for some, but not necessar-
ily all, of the hosts and routers on a LAN. An entry in the switch table contains (1) 
a MAC address, (2) the switch interface that leads toward that MAC address, and 
(3) the time at which the entry was placed in the table. An example switch table 
 
for the uppermost switch in Figure 6.15 is shown in Figure 6.22. This description 
of frame forwarding may sound similar to our discussion of datagram forwarding 
Figure 6.22  ♦  
Portion of a switch table for the uppermost switch in  
Figure 6.15
Time
Interface
Address
62-FE-F7-11-89-A3
1
9:32
7C-BA-B2-B4-91-10
3
9:36
....
....
....
510         Chapter 6    •    The Link Layer and LANs
in Chapter 4. Indeed, in our discussion of generalized forwarding in Section 4.4, 
we learned that many modern packet switches can be configured to forward on the 
basis of layer-2 destination MAC addresses (i.e., function as a layer-2 switch) or 
layer-3 IP destination addresses (i.e., function as a layer-3 router). Nonetheless, 
 
we’ll make the important distinction that switches forward packets based on MAC 
addresses rather than on IP addresses. We will also see that a traditional (i.e., in a 
non-SDN context) switch table is constructed in a very different manner from a 
router’s forwarding table.
To understand how switch filtering and forwarding work, suppose a frame with 
destination address DD-DD-DD-DD-DD-DD arrives at the switch on interface x. 
The switch indexes its table with the MAC address DD-DD-DD-DD-DD-DD. There 
are three possible cases:
•	 There is no entry in the table for DD-DD-DD-DD-DD-DD. In this case, the switch 
forwards copies of the frame to the output buffers preceding all interfaces except 
for interface x. In other words, if there is no entry for the destination address, the 
switch broadcasts the frame.
•	 There is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface 
x. In this case, the frame is coming from a LAN segment that contains adapter 
DD-DD-DD-DD-DD-DD. There being no need to forward the frame to any of 
the other interfaces, the switch performs the filtering function by discarding the 
frame.
•	 There is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface 
y≠x. In this case, the frame needs to be forwarded to the LAN segment attached 
to interface y. The switch performs its forwarding function by putting the frame 
in an output buffer that precedes interface y.
Let’s walk through these rules for the uppermost switch in Figure 6.15 and its 
switch table in Figure 6.22. Suppose that a frame with destination address 62-FE-
F7-11-89-A3 arrives at the switch from interface 1. The switch examines its table 
and sees that the destination is on the LAN segment connected to interface 1 (that 
is, Electrical Engineering). This means that the frame has already been broadcast on 
the LAN segment that contains the destination. The switch therefore filters (that is, 
discards) the frame. Now suppose a frame with the same destination address arrives 
from interface 2. The switch again examines its table and sees that the destination 
is in the direction of interface 1; it therefore forwards the frame to the output buffer 
preceding interface 1. It should be clear from this example that as long as the switch 
table is complete and accurate, the switch forwards frames toward destinations 
without any broadcasting.
In this sense, a switch is “smarter” than a hub. But how does this switch table get 
configured in the first place? Are there link-layer equivalents to network-layer rout-
ing protocols? Or must an overworked manager manually configure the switch table?
6.4    •    Switched Local Area Networks         511
Self-Learning
A switch has the wonderful property (particularly for the already-overworked network 
administrator) that its table is built automatically, dynamically, and autonomously—
without any intervention from a network administrator or from a configuration pro-
tocol. In other words, switches are self-learning. This capability is accomplished as 
follows:
	1.	 The switch table is initially empty.
	2.	 For each incoming frame received on an interface, the switch stores in its table 
(1) the MAC address in the frame’s source address field, (2) the interface from 
which the frame arrived, and (3) the current time. In this manner the switch 
records in its table the LAN segment on which the sender resides. If every 
host in the LAN eventually sends a frame, then every host will eventually get 
recorded in the table.
	3.	 The switch deletes an address in the table if no frames are received with that 
address as the source address after some period of time (the aging time). In 
this manner, if a PC is replaced by another PC (with a different adapter), the 
MAC address of the original PC will eventually be purged from the switch 
table.
Let’s walk through the self-learning property for the uppermost switch in Fig-
ure 6.15 and its corresponding switch table in Figure 6.22. Suppose at time 9:39 a 
frame with source address 01-12-23-34-45-56 arrives from interface 2. Suppose that 
this address is not in the switch table. Then the switch adds a new entry to the table, 
as shown in Figure 6.23.
Continuing with this same example, suppose that the aging time for this switch 
is 60 minutes, and no frames with source address 62-FE-F7-11-89-A3 arrive to the 
switch between 9:32 and 10:32. Then at time 10:32, the switch removes this address 
from its table.
Figure 6.23  ♦  
Switch learns about the location of an adapter with address 
01-12-23-34-45-56
Address
Interface
Time
01-12-23-34-45-56
2
9:39
62-FE-F7-11-89-A3
1
9:32
7C-BA-B2-B4-91-10
3
9:36
....
....
....
512         Chapter 6    •    The Link Layer and LANs
Switches are plug-and-play devices because they require no intervention 
from a network administrator or user. A network administrator wanting to install 
a switch need do nothing more than connect the LAN segments to the switch 
interfaces. The administrator need not configure the switch tables at the time of 
installation or when a host is removed from one of the LAN segments. Switches 
are also full-duplex, meaning any switch interface can send and receive at the 
same time.
Properties of Link-Layer Switching
Having described the basic operation of a link-layer switch, let’s now consider their 
features and properties. We can identify several advantages of using switches, rather 
than broadcast links such as buses or hub-based star topologies:
•	 Elimination of collisions. In a LAN built from switches (and without hubs), there 
is no wasted bandwidth due to collisions! The switches buffer frames and never 
transmit more than one frame on a segment at any one time. As with a router, the 
maximum aggregate throughput of a switch is the sum of all the switch interface 
rates. Thus, switches provide a significant performance improvement over LANs 
with broadcast links.
•	 Heterogeneous links. Because a switch isolates one link from another, the differ-
ent links in the LAN can operate at different speeds and can run over different 
media. For example, the uppermost switch in Figure 6.15 might have three1 Gbps 
1000BASE-T copper links, two 100 Mbps 100BASE-FX fiber links, and one 
100BASE-T copper link. Thus, a switch is ideal for mixing legacy equipment 
with new equipment.
•	 Management. In addition to providing enhanced security (see sidebar on Focus on 
Security), a switch also eases network management. For example, if an adapter 
malfunctions and continually sends Ethernet frames (called a jabbering adapter), 
a switch can detect the problem and internally disconnect the malfunctioning 
adapter. With this feature, the network administrator need not get out of bed and 
drive back to work in order to correct the problem. Similarly, a cable cut discon-
nects only that host that was using the cut cable to connect to the switch. In the 
days of coaxial cable, many a network manager spent hours “walking the line” (or 
more accurately, “crawling the floor”) to find the cable break that brought down 
the entire network. Switches also gather statistics on bandwidth usage, collision 
rates, and traffic types, and make this information available to the network man-
ager. This information can be used to debug and correct problems, and to plan 
how the LAN should evolve in the future. Researchers are exploring adding yet 
more management functionality into Ethernet LANs in prototype deployments 
[Casado 2007; Koponen 2011].
6.4    •    Switched Local Area Networks         513
Switches Versus Routers
As we learned in Chapter 4, routers are store-and-forward packet switches that for-
ward packets using network-layer addresses. Although a switch is also a store-and-
forward packet switch, it is fundamentally different from a router in that it forwards 
packets using MAC addresses. Whereas a router is a layer-3 packet switch, a switch 
is a layer-2 packet switch. Recall, however, that we learned in Section 4.4 that mod-
ern switches using the “match plus action” operation can be used to forward a layer-2 
frame based on the frame's destination MAC address, as well as a layer-3 datagram 
using the datagram's destination IP address. Indeed, we saw that switches using the 
OpenFlow standard can perform generalized packet forwarding based on any of 
eleven different frame, datagram, and transport-layer header fields.
Even though switches and routers are fundamentally different, network admin-
istrators must often choose between them when installing an interconnection device. 
For example, for the network in Figure 6.15, the network administrator could just as 
easily have used a router instead of a switch to connect the department LANs, servers, 
and internet gateway router. Indeed, a router would permit interdepartmental commu-
nication without creating collisions. Given that both switches and routers are candi-
dates for interconnection devices, what are the pros and cons of the two approaches?
SNIFFING A SWITCHED LAN: SWITCH POISONING
When a host is connected to a switch, it typically only receives frames that are intended 
for it. For example, consider a switched LAN in Figure 6.17. When host A sends a frame 
to host B, and there is an entry for host B in the switch table, then the switch will forward 
the frame only to host B. If host C happens to be running a sniffer, host C will not be able 
to sniff this A-to-B frame. Thus, in a switched-LAN environment (in contrast to a broadcast 
link environment such as 802.11 LANs or hub–based Ethernet LANs), it is more difficult 
for an attacker to sniff frames. However, because the switch broadcasts frames that have 
destination addresses that are not in the switch table, the sniffer at C can still sniff some 
frames that are not intended for C. Furthermore, a sniffer will be able sniff all Ethernet 
broadcast frames with broadcast destination address FF–FF–FF–FF–FF–FF. A well-known 
attack against a switch, called switch poisoning, is to send tons of packets to the 
switch with many different bogus source MAC addresses, thereby filling the switch table 
with bogus entries and leaving no room for the MAC addresses of the legitimate hosts. 
This causes the switch to broadcast most frames, which can then be picked up by the 
sniffer [Skoudis 2006]. As this attack is rather involved even for a sophisticated attacker, 
switches are significantly less vulnerable to sniffing than are hubs and wireless LANs.
FOCUS ON SECURITY
514         Chapter 6    •    The Link Layer and LANs
First consider the pros and cons of switches. As mentioned above, switches are 
plug-and-play, a property that is cherished by all the overworked network adminis-
trators of the world. Switches can also have relatively high filtering and forwarding 
rates—as shown in Figure 6.24, switches have to process frames only up through 
layer 2, whereas routers have to process datagrams up through layer 3. On the other 
hand, to prevent the cycling of broadcast frames, the active topology of a switched 
network is restricted to a spanning tree. Also, a large switched network would require 
large ARP tables in the hosts and routers and would generate substantial ARP traffic 
and processing. Furthermore, switches are susceptible to broadcast storms—if one 
host goes haywire and transmits an endless stream of Ethernet broadcast frames, the 
switches will forward all of these frames, causing the entire network to collapse.
Now consider the pros and cons of routers. Because network addressing is often 
hierarchical (and not flat, as is MAC addressing), packets do not normally cycle 
through routers even when the network has redundant paths. (However, packets can 
cycle when router tables are misconfigured; but as we learned in Chapter 4, IP uses 
a special datagram header field to limit the cycling.) Thus, packets are not restricted 
to a spanning tree and can use the best path between source and destination. Because 
routers do not have the spanning tree restriction, they have allowed the Internet to be 
built with a rich topology that includes, for example, multiple active links between 
Europe and North America. Another feature of routers is that they provide firewall 
protection against layer-2 broadcast storms. Perhaps the most significant drawback 
of routers, though, is that they are not plug-and-play—they and the hosts that connect 
to them need their IP addresses to be configured. Also, routers often have a larger 
per-packet processing time than switches, because they have to process up through 
the layer-3 fields. Finally, there are two different ways to pronounce the word router, 
either as “rootor” or as “rowter,” and people waste a lot of time arguing over the 
proper pronunciation [Perlman 1999].
Given that both switches and routers have their pros and cons (as summarized in 
Table 6.1), when should an institutional network (for example, a university campus 
Figure 6.24  ♦  Packet processing in switches, routers, and hosts
Host
Application
Host
Transport
Network
Link
Physical
Link
Physical
Network
Switch
Router
Link
Physical
Application
Transport
Network
Link
Physical
6.4    •    Switched Local Area Networks         515
network or a corporate campus network) use switches, and when should it use rout-
ers? Typically, small networks consisting of a few hundred hosts have a few LAN 
segments. Switches suffice for these small networks, as they localize traffic and 
increase aggregate throughput without requiring any configuration of IP addresses. 
But larger networks consisting of thousands of hosts typically include routers within 
the network (in addition to switches). The routers provide a more robust isolation of 
traffic, control broadcast storms, and use more “intelligent” routes among the hosts 
in the network.
For more discussion of the pros and cons of switched versus routed networks, 
as well as a discussion of how switched LAN technology can be extended to accom-
modate two orders of magnitude more hosts than today’s Ethernets, see [Meyers 
2004; Kim 2008].
6.4.4	Virtual Local Area Networks (VLANs)
In our earlier discussion of Figure 6.15, we noted that modern institutional LANs 
are often configured hierarchically, with each workgroup (department) having its 
own switched LAN connected to the switched LANs of other groups via a switch 
hierarchy. While such a configuration works well in an ideal world, the real world 
is often far from ideal. Three drawbacks can be identified in the configuration in 
Figure 6.15:
•	 Lack of traffic isolation. Although the hierarchy localizes group traffic to within 
a single switch, broadcast traffic (e.g., frames carrying ARP and DHCP mes-
sages or frames whose destination has not yet been learned by a self-learning 
switch) must still traverse the entire institutional network. Limiting the scope of 
such broadcast traffic would improve LAN performance. Perhaps more impor-
tantly, it also may be desirable to limit LAN broadcast traffic for security/privacy 
reasons. For example, if one group contains the company’s executive manage-
ment team and another group contains disgruntled employees running Wireshark 
packet sniffers, the network manager may well prefer that the executives’ traffic 
never even reaches employee hosts. This type of isolation could be provided by 
Table 6.1  ♦  
Comparison of the typical features of popular interconnection 
devices
Hubs
Routers
Switches
Traffic isolation
No
Yes
Yes
Plug and play
Yes
No
Yes
Optimal routing
No
Yes
No
516         Chapter 6    •    The Link Layer and LANs
replacing the center switch in Figure 6.15 with a router. We’ll see shortly that this 
isolation also can be achieved via a switched (layer 2) solution.
•	 Inefficient use of switches. If instead of three groups, the institution had 10 
groups, then 10 first-level switches would be required. If each group were 
small, say less than 10 people, then a single 96-port switch would likely be large 
enough to accommodate everyone, but this single switch would not provide 
traffic isolation.
•	 Managing users. If an employee moves between groups, the physical cabling 
must be changed to connect the employee to a different switch in Figure 6.15. 
Employees belonging to two groups make the problem even harder.
Fortunately, each of these difficulties can be handled by a switch that supports 
virtual local area networks (VLANs). As the name suggests, a switch that sup-
ports VLANs allows multiple virtual local area networks to be defined over a sin-
gle physical local area network infrastructure. Hosts within a VLAN communicate 
with each other as if they (and no other hosts) were connected to the switch. In a 
port-based VLAN, the switch’s ports (interfaces) are divided into groups by the 
network manager. Each group constitutes a VLAN, with the ports in each VLAN 
forming a broadcast domain (i.e., broadcast traffic from one port can only reach 
other ports in the group). Figure 6.25 shows a single switch with 16 ports. Ports 2 
to 8 belong to the EE VLAN, while ports 9 to 15 belong to the CS VLAN (ports 1 
and 16 are unassigned). This VLAN solves all of the difficulties noted above—EE 
and CS VLAN frames are isolated from each other, the two switches in Figure 6.15 
have been replaced by a single switch, and if the user at switch port 8 joins the CS 
Department, the network operator simply reconfigures the VLAN software so that 
port 8 is now associated with the CS VLAN. One can easily imagine how the VLAN 
switch is configured and operates—the network manager declares a port to belong 
Figure 6.25  ♦  A single switch with two configured VLANs
1
Electrical Engineering
(VLAN ports 2–8)
Computer Science
(VLAN ports 9–15)
9
15
2
4
8
10
16
6.4    •    Switched Local Area Networks         517
to a given VLAN (with undeclared ports belonging to a default VLAN) using switch 
management software, a table of port-to-VLAN mappings is maintained within the 
switch; and switch hardware only delivers frames between ports belonging to the 
same VLAN.
But by completely isolating the two VLANs, we have introduced a new dif-
ficulty! How can traffic from the EE Department be sent to the CS Department? 
One way to handle this would be to connect a VLAN switch port (e.g., port 1 in Fig-
ure 6.25) to an external router and configure that port to belong both the EE and CS 
VLANs. In this case, even though the EE and CS departments share the same physi-
cal switch, the logical configuration would look as if the EE and CS departments 
had separate switches connected via a router. An IP datagram going from the EE to 
the CS department would first cross the EE VLAN to reach the router and then be 
forwarded by the router back over the CS VLAN to the CS host. Fortunately, switch 
vendors make such configurations easy for the network manager by building a single 
device that contains both a VLAN switch and a router, so a separate external router 
is not needed. A homework problem at the end of the chapter explores this scenario 
in more detail.
Returning again to Figure 6.15, let’s now suppose that rather than having a sepa-
rate Computer Engineering department, some EE and CS faculty are housed in a 
separate building, where (of course!) they need network access, and (of course!) 
they’d like to be part of their department’s VLAN. Figure 6.26 shows a second 8-port 
switch, where the switch ports have been defined as belonging to the EE or the 
CS VLAN, as needed. But how should these two switches be interconnected? One 
easy solution would be to define a port belonging to the CS VLAN on each switch 
(similarly for the EE VLAN) and to connect these ports to each other, as shown in 
Figure 6.26(a). This solution doesn’t scale, however, since N VLANS would require 
N ports on each switch simply to interconnect the two switches.
A more scalable approach to interconnecting VLAN switches is known as 
VLAN trunking. In the VLAN trunking approach shown in Figure 6.26(b), a spe-
cial port on each switch (port 16 on the left switch and port 1 on the right switch) is 
configured as a trunk port to interconnect the two VLAN switches. The trunk port 
belongs to all VLANs, and frames sent to any VLAN are forwarded over the trunk 
link to the other switch. But this raises yet another question: How does a switch know 
that a frame arriving on a trunk port belongs to a particular VLAN? The IEEE has 
defined an extended Ethernet frame format, 802.1Q, for frames crossing a VLAN 
trunk. As shown in Figure 6.27, the 802.1Q frame consists of the standard Ethernet 
frame with a four-byte VLAN tag added into the header that carries the identity of 
the VLAN to which the frame belongs. The VLAN tag is added into a frame by the 
switch at the sending side of a VLAN trunk, parsed, and removed by the switch at 
the receiving side of the trunk. The VLAN tag itself consists of a 2-byte Tag Protocol 
Identifier (TPID) field (with a fixed hexadecimal value of 81-00), a 2-byte Tag Con-
trol Information field that contains a 12-bit VLAN identifier field, and a 3-bit priority 
field that is similar in intent to the IP datagram TOS field.
518         Chapter 6    •    The Link Layer and LANs
Figure 6.26  ♦  
Connecting two VLAN switches with two VLANs:  
(a) two cables (b) trunked
1
16
1
8
1
Electrical Engineering
(VLAN ports 2–8)
b.
a.
Electrical Engineering
(VLAN ports 2, 3, 6)
Trunk
link
Computer Science
(VLAN ports 9–15)
9
15
2
4
8
10
16
1
2
3
4
5
6
8
7
Computer Science
(VLAN ports 4, 5, 7)
Figure 6.27  ♦  
Original Ethernet frame (top), 802.1Q-tagged Ethernet 
VLAN frame (below)
Preamble
CRC
Dest.
address
Source
address
Type
Data
Preamble
CRC'
Dest.
address
Source
address
Type
Tag Control Information
Tag Protocol Identiﬁer
Recomputed
CRT
Data
6.5    •    Link Virtualization: A Network as a Link Layer         519
In this discussion, we’ve only briefly touched on VLANs and have focused on port-
based VLANs. We should also mention that VLANs can be defined in several other 
ways. In MAC-based VLANs, the network manager specifies the set of MAC addresses 
that belong to each VLAN; whenever a device attaches to a port, the port is connected 
into the appropriate VLAN based on the MAC address of the device. VLANs can also 
be defined based on network-layer protocols (e.g., IPv4, IPv6, or Appletalk) and other 
criteria. It is also possible for VLANs to be extended across IP routers, allowing islands 
of LANs to be connected together to form a single VLAN that could span the globe 
 
[Yu 2011]. See the 802.1Q standard [IEEE 802.1q 2005] for more details.
6.5	 Link Virtualization: A Network as a Link 
Layer
Because this chapter concerns link-layer protocols, and given that we’re now nearing 
the chapter’s end, let’s reflect on how our understanding of the term link has evolved. 
We began this chapter by viewing the link as a physical wire connecting two com-
municating hosts. In studying multiple access protocols, we saw that multiple hosts 
could be connected by a shared wire and that the “wire” connecting the hosts could 
be radio spectra or other media. This led us to consider the link a bit more abstractly 
as a channel, rather than as a wire. In our study of Ethernet LANs (Figure 6.15) 
we saw that the interconnecting media could actually be a rather complex switched 
infrastructure. Throughout this evolution, however, the hosts themselves maintained 
the view that the interconnecting medium was simply a link-layer channel connect-
ing two or more hosts. We saw, for example, that an Ethernet host can be blissfully 
unaware of whether it is connected to other LAN hosts by a single short LAN seg-
ment (Figure 6.17) or by a geographically dispersed switched LAN (Figure 6.15) or 
by a VLAN (Figure 6.26).
In the case of a dialup modem connection between two hosts, the link connect-
ing the two hosts is actually the telephone network—a logically separate, global tel-
ecommunications network with its own switches, links, and protocol stacks for data 
transfer and signaling. From the Internet link-layer point of view, however, the dial-
up connection through the telephone network is viewed as a simple “wire.” In this 
sense, the Internet virtualizes the telephone network, viewing the telephone network 
as a link-layer technology providing link-layer connectivity between two Internet 
hosts. An overlay network similarly views the Internet as a means for providing con-
nectivity between overlay nodes, seeking to overlay the Internet in the same way that 
the Internet overlays the telephone network.
In this section, we’ll consider Multiprotocol Label Switching (MPLS) net-
works. Unlike the circuit-switched telephone network, MPLS is a packet-switched, 
520         Chapter 6    •    The Link Layer and LANs
virtual-circuit network in its own right. It has its own packet formats and forwarding 
behaviors. Thus, from a pedagogical viewpoint, a discussion of MPLS fits well into a 
study of either the network layer or the link layer. From an Internet viewpoint, how-
ever, we can consider MPLS, like the telephone network and switched-­
Ethernets, 
as a link-layer technology that serves to interconnect IP devices. Thus, we’ll con-
sider MPLS in our discussion of the link layer. Frame-relay and ATM networks 
can also be used to interconnect IP devices, though they represent a slightly older 
(but still deployed) technology and will not be covered here; see the very readable 
book [Goralski 1999] for details. Our treatment of MPLS will be necessarily brief, 
as entire books could be (and have been) written on these networks. We recommend 
[Davie 2000] for details on MPLS. We’ll focus here primarily on how MPLS ­
servers 
interconnect to IP devices, although we’ll dive a bit deeper into the underlying tech-
nologies as well.
6.5.1	Multiprotocol Label Switching (MPLS)
Multiprotocol Label Switching (MPLS) evolved from a number of industry efforts 
in the mid-to-late 1990s to improve the forwarding speed of IP routers by adopting a 
key concept from the world of virtual-circuit networks: a fixed-length label. The goal 
was not to abandon the destination-based IP datagram-forwarding infrastructure for 
one based on fixed-length labels and virtual circuits, but to augment it by selectively 
labeling datagrams and allowing routers to forward datagrams based on fixed-length 
labels (rather than destination IP addresses) when possible. Importantly, these tech-
niques work hand-in-hand with IP, using IP addressing and routing. The IETF uni-
fied these efforts in the MPLS protocol [RFC 3031, RFC 3032], effectively blending 
VC techniques into a routed datagram network.
Let’s begin our study of MPLS by considering the format of a link-layer frame 
that is handled by an MPLS-capable router. Figure 6.28 shows that a link-layer 
frame transmitted between MPLS-capable devices has a small MPLS header added 
between the layer-2 (e.g., Ethernet) header and layer-3 (i.e., IP) header. RFC 3032 
defines the format of the MPLS header for such links; headers are defined for ATM 
and frame-relayed networks as well in other RFCs. Among the fields in the MPLS 
PPP or Ethernet
header
MPLS header
IP header
Remainder of link-layer frame
Label
Exp
S
TTL
Figure 6.28  ♦  
MPLS header: Located between link- and network-layer 
headers
6.5    •    Link Virtualization: A Network as a Link Layer         521
header are the label, 3 bits reserved for experimental use, a single S bit, which is used 
to indicate the end of a series of “stacked” MPLS headers (an advanced topic that 
we’ll not cover here), and a time-to-live field.
It’s immediately evident from Figure 6.28 that an MPLS-enhanced frame can 
only be sent between routers that are both MPLS capable (since a non-MPLS-capable 
router would be quite confused when it found an MPLS header where it had expected 
to find the IP header!). An MPLS-capable router is often referred to as a label-
switched router, since it forwards an MPLS frame by looking up the MPLS label 
in its forwarding table and then immediately passing the datagram to the appropriate 
output interface. Thus, the MPLS-capable router need not extract the destination IP 
address and perform a lookup of the longest prefix match in the forwarding table. But 
how does a router know if its neighbor is indeed MPLS capable, and how does a router 
know what label to associate with the given IP destination? To answer these questions, 
we’ll need to take a look at the interaction among a group of MPLS-capable routers.
In the example in Figure 6.29, routers R1 through R4 are MPLS capable. R5 
and R6 are standard IP routers. R1 has advertised to R2 and R3 that it (R1) can route 
to destination A, and that a received frame with MPLS label 6 will be forwarded to 
destination A. Router R3 has advertised to router R4 that it can route to destinations 
A and D, and that incoming frames with MPLS labels 10 and 12, respectively, will be 
switched toward those destinations. Router R2 has also advertised to router R4 that 
it (R2) can reach destination A, and that a received frame with MPLS label 8 will be 
switched toward A. Note that router R4 is now in the interesting position of having 
Figure 6.29  ♦  MPLS-enhanced forwarding
R4
in
label
out
label
10
12
8
A
D
A
0
0
1
dest
out
interface
R6
R5
R3
R2
D
A
0
0
0
1
1
0
R1
in
label
out
label
6
9
A
D
1
0
10
12
dest
out
interface
in
label
out
label
–
A
0
6
dest
out
interface
in
label
out
label
6
A
0
8
dest
out
interface
522         Chapter 6    •    The Link Layer and LANs
two MPLS paths to reach A: via interface 0 with outbound MPLS label 10, and via 
interface 1 with an MPLS label of 8. The broad picture painted in Figure 6.29 is 
that IP devices R5, R6, A, and D are connected together via an MPLS infrastructure 
(MPLS-capable routers R1, R2, R3, and R4) in much the same way that a switched 
LAN or an ATM network can connect together IP devices. And like a switched 
LAN or ATM network, the MPLS-capable routers R1 through R4 do so without ever 
touching the IP header of a packet.
In our discussion above, we’ve not specified the specific protocol used to dis-
tribute labels among the MPLS-capable routers, as the details of this signaling are 
well beyond the scope of this book. We note, however, that the IETF working group 
on MPLS has specified in [RFC 3468] that an extension of the RSVP protocol, 
known as RSVP-TE [RFC 3209], will be the focus of its efforts for MPLS signaling. 
We’ve also not discussed how MPLS actually computes the paths for packets among 
MPLS capable routers, nor how it gathers link-state information (e.g., amount of link 
bandwidth unreserved by MPLS) to use in these path computations. Existing link-
state routing algorithms (e.g., OSPF) have been extended to flood this information to 
MPLS-capable routers. Interestingly, the actual path computation algorithms are not 
standardized, and are currently vendor-specific.
Thus far, the emphasis of our discussion of MPLS has been on the fact that 
MPLS performs switching based on labels, without needing to consider the IP 
address of a packet. The true advantages of MPLS and the reason for current interest 
in MPLS, however, lie not in the potential increases in switching speeds, but rather in 
the new traffic management capabilities that MPLS enables. As noted above, R4 has 
two MPLS paths to A. If forwarding were performed up at the IP layer on the basis 
of IP address, the IP routing protocols we studied in Chapter 5 would specify only 
a single, least-cost path to A. Thus, MPLS provides the ability to forward packets 
along routes that would not be possible using standard IP routing protocols. This is 
one simple form of traffic engineering using MPLS [RFC 3346; RFC 3272; RFC 
2702; Xiao 2000], in which a network operator can override normal IP routing and 
force some of the traffic headed toward a given destination along one path, and other 
traffic destined toward the same destination along another path (whether for policy, 
performance, or some other reason).
It is also possible to use MPLS for many other purposes as well. It can be used 
to perform fast restoration of MPLS forwarding paths, e.g., to reroute traffic over a 
precomputed failover path in response to link failure [Kar 2000; Huang 2002; RFC 
3469]. Finally, we note that MPLS can, and has, been used to implement so-called 
­
virtual private networks (VPNs). In implementing a VPN for a customer, an ISP uses 
its MPLS-enabled network to connect together the customer’s various networks. MPLS 
can be used to isolate both the resources and addressing used by the customer’s VPN 
from that of other users crossing the ISP’s network; see [DeClercq 2002] for details.
Our discussion of MPLS has been brief, and we encourage you to consult the 
references we’ve mentioned. We note that with so many possible uses for MPLS, it 
appears that it is rapidly becoming the Swiss Army knife of Internet traffic engineering!
6.6    •    Data Center Networking         523
6.6	 Data Center Networking
In recent years, Internet companies such as Google, Microsoft, Facebook, and 
­
Amazon (as well as their counterparts in Asia and Europe) have built massive data 
centers, each housing tens to hundreds of thousands of hosts, and concurrently sup-
porting many distinct cloud applications (e.g., search, e-mail, social networking, and 
e-commerce). Each data center has its own data center network that interconnects its 
hosts with each other and interconnects the data center with the Internet. In this sec-
tion, we provide a brief introduction to data center networking for cloud applications.
The cost of a large data center is huge, exceeding $12 million per month for a 
100,000 host data center [Greenberg 2009a]. Of these costs, about 45 percent can 
be attributed to the hosts themselves (which need to be replaced every 3–4 years); 
25 percent to infrastructure, including transformers, uninterruptable power supplies 
(UPS) systems, generators for long-term outages, and cooling systems; 15 percent 
for electric utility costs for the power draw; and 15 percent for networking, including 
network gear (switches, routers and load balancers), external links, and transit traf-
fic costs. (In these percentages, costs for equipment are amortized so that a common 
cost metric is applied for one-time purchases and ongoing expenses such as power.) 
While networking is not the largest cost, networking innovation is the key to reduc-
ing overall cost and maximizing performance [Greenberg 2009a].
The worker bees in a data center are the hosts: They serve content (e.g., Web 
pages and videos), store e-mails and documents, and collectively perform massively 
distributed computations (e.g., distributed index computations for search engines). 
The hosts in data centers, called blades and resembling pizza boxes, are generally 
commodity hosts that include CPU, memory, and disk storage. The hosts are stacked 
in racks, with each rack typically having 20 to 40 blades. At the top of each rack there 
is a switch, aptly named the Top of Rack (TOR) switch, that interconnects the hosts 
in the rack with each other and with other switches in the data center. Specifically, 
each host in the rack has a network interface card that connects to its TOR switch, 
and each TOR switch has additional ports that can be connected to other switches. 
Today hosts typically have 40 Gbps Ethernet connections to their TOR switches 
[Greenberg 2015]. Each host is also assigned its own data-center-internal IP address.
The data center network supports two types of traffic: traffic flowing between 
external clients and internal hosts and traffic flowing between internal hosts. To handle 
flows between external clients and internal hosts, the data center network includes one 
or more border routers, connecting the data center network to the public Internet. The 
data center network therefore interconnects the racks with each other and connects the 
racks to the border routers. Figure 6.30 shows an example of a data center network. 
Data center network design, the art of designing the interconnection network and pro-
tocols that connect the racks with each other and with the border routers, has become 
an important branch of computer networking research in recent years [Al-Fares 2008; 
Greenberg 2009a; Greenberg 2009b; Mysore 2009; Guo 2009; Wang 2010].
524         Chapter 6    •    The Link Layer and LANs
Load Balancing
A cloud data center, such as a Google or Microsoft data center, provides many 
applications concurrently, such as search, e-mail, and video applications. To sup-
port requests from external clients, each application is associated with a publicly 
visible IP address to which clients send their requests and from which they receive 
responses. Inside the data center, the external requests are first directed to a load 
balancer whose job it is to distribute requests to the hosts, balancing the load across 
the hosts as a function of their current load. A large data center will often have sev-
eral load balancers, each one devoted to a set of specific cloud applications. Such a 
load balancer is sometimes referred to as a “layer-4 switch” since it makes decisions 
based on the destination port number (layer 4) as well as destination IP address in 
the packet. Upon receiving a request for a particular application, the load balancer 
forwards it to one of the hosts that handles the application. (A host may then invoke 
the services of other hosts to help process the request.) When the host finishes pro-
cessing the request, it sends its response back to the load balancer, which in turn 
relays the response back to the external client. The load balancer not only balances 
Figure 6.30  ♦  A data center network with a hierarchical topology
Internet
A
1
2
3
4
5
6
7
8
C
B
Server racks
TOR switches
Tier-2 switches
Tier-1 switches
Access router
Border router
Load
balancer
6.6    •    Data Center Networking         525
the work load across hosts, but also provides a NAT-like function, translating the 
public external IP address to the internal IP address of the appropriate host, and then 
translating back for packets traveling in the reverse direction back to the clients. This 
prevents clients from contacting hosts directly, which has the security benefit of 
hiding the internal network structure and preventing clients from directly interacting 
with the hosts.
Hierarchical Architecture
For a small data center housing only a few thousand hosts, a simple network consist-
ing of a border router, a load balancer, and a few tens of racks all interconnected by 
a single Ethernet switch could possibly suffice. But to scale to tens to hundreds of 
thousands of hosts, a data center often employs a hierarchy of routers and switches, 
such as the topology shown in Figure 6.30. At the top of the hierarchy, the border 
router connects to access routers (only two are shown in Figure 6.30, but there can be 
many more). Below each access router there are three tiers of switches. Each access 
router connects to a top-tier switch, and each top-tier switch connects to multiple 
second-tier switches and a load balancer. Each second-tier switch in turn connects to 
multiple racks via the racks’ TOR switches (third-tier switches). All links typically 
use Ethernet for their link-layer and physical-layer protocols, with a mix of copper 
and fiber cabling. With such a hierarchical design, it is possible to scale a data center 
to hundreds of thousands of hosts.
Because it is critical for a cloud application provider to continually provide appli-
cations with high availability, data centers also include redundant network equip-
ment and redundant links in their designs (not shown in Figure 6.30). For example, 
each TOR switch can connect to two tier-2 switches, and each access router, tier-1 
switch, and tier-2 switch can be duplicated and integrated into the design [Cisco 
2012; Greenberg 2009b]. In the hierarchical design in Figure 6.30, observe that the 
hosts below each access router form a single subnet. In order to localize ARP broad-
cast traffic, each of these subnets is further partitioned into smaller VLAN subnets, 
each comprising a few hundred hosts [Greenberg 2009a].
Although the conventional hierarchical architecture just described solves the 
problem of scale, it suffers from limited host-to-host capacity [Greenberg 2009b]. 
To understand this limitation, consider again Figure 6.30, and suppose each host 
connects to its TOR switch with a 1 Gbps link, whereas the links between switches 
are 10 Gbps Ethernet links. Two hosts in the same rack can always communicate at 
a full 1 Gbps, limited only by the rate of the hosts’ network interface cards. How-
ever, if there are many simultaneous flows in the data center network, the maximum 
rate between two hosts in different racks can be much less. To gain insight into 
this issue, consider a traffic pattern consisting of 40 simultaneous flows between 
 
40 pairs of hosts in different racks. Specifically, suppose each of 10 hosts in rack 1 
in Figure 6.30 sends a flow to a corresponding host in rack 5. Similarly, there are ten 
simultaneous flows between pairs of hosts in racks 2 and 6, ten simultaneous flows 
 
526         Chapter 6    •    The Link Layer and LANs
between racks 3 and 7, and ten simultaneous flows between racks 4 and 8. If each 
flow evenly shares a link’s capacity with other flows traversing that link, then the 
40 flows crossing the 10 Gbps A-to-B link (as well as the 10 Gbps B-to-C link) will 
each only receive 10 Gbps / 40 = 250 Mbps, which is significantly less than the 
 
1 Gbps network interface card rate. The problem becomes even more acute for flows 
between hosts that need to travel higher up the hierarchy. One possible solution to 
this limitation is to deploy higher-rate switches and routers. But this would signifi-
cantly increase the cost of the data center, because switches and routers with high 
port speeds are very expensive.
Supporting high-bandwidth host-to-host communication is important because a 
key requirement in data centers is flexibility in placement of computation and ser-
vices [Greenberg 2009b; Farrington 2010]. For example, a large-scale Internet search 
engine may run on thousands of hosts spread across multiple racks with significant 
bandwidth requirements between all pairs of hosts. Similarly, a cloud computing 
service such as EC2 may wish to place the multiple virtual machines comprising a 
customer’s service on the physical hosts with the most capacity irrespective of their 
location in the data center. If these physical hosts are spread across multiple racks, 
network bottlenecks as described above may result in poor performance.
Trends in Data Center Networking
In order to reduce the cost of data centers, and at the same time improve their delay 
and throughput performance, Internet cloud giants such as Google, Facebook, 
­
Amazon, and Microsoft are continually deploying new data center network designs. 
Although these designs are proprietary, many important trends can nevertheless be 
identified.
One such trend is to deploy new interconnection architectures and network 
protocols that overcome the drawbacks of the traditional hierarchical designs. One 
such approach is to replace the hierarchy of switches and routers with a fully con-
nected topology [Facebook 2014; Al-Fares 2008; Greenberg 2009b; Guo 2009], such 
as the topology shown in Figure 6.31. In this design, each tier-1 switch connects to 
all of the tier-2 switches so that (1) host-to-host traffic never has to rise above the 
switch tiers, and (2) with n tier-1 switches, between any two tier-2 switches there are 
n disjoint paths. Such a design can significantly improve the host-to-host capacity. 
To see this, consider again our example of 40 flows. The topology in Figure 6.31 
can handle such a flow pattern since there are four distinct paths between the first 
tier-2 switch and the second tier-2 switch, together providing an aggregate capacity of 
 
40 Gbps between the first two tier-2 switches. Such a design not only alleviates the 
host-to-host capacity limitation, but also creates a more flexible computation and ser-
vice environment in which communication between any two racks not connected to 
the same switch is logically equivalent, irrespective of their locations in the data center.
Another major trend is to employ shipping container–based modular data cent-
ers (MDCs) [YouTube 2009; Waldrop 2007]. In an MDC, a factory builds, within a 
6.6    •    Data Center Networking         527
standard 12-meter shipping container, a “mini data center” and ships the container 
to the data center location. Each container has up to a few thousand hosts, stacked 
in tens of racks, which are packed closely together. At the data center location, mul-
tiple containers are interconnected with each other and also with the Internet. Once 
a prefabricated container is deployed at a data center, it is often difficult to service. 
Thus, each container is designed for graceful performance degradation: as compo-
nents (servers and switches) fail over time, the container continues to operate but 
with degraded performance. When many components have failed and performance 
has dropped below a threshold, the entire container is removed and replaced with a 
fresh one.
Building a data center out of containers creates new networking challenges. 
With an MDC, there are two types of networks: the container-internal networks 
within each of the containers and the core network connecting each container 
 
[Guo 2009; Farrington 2010]. Within each container, at the scale of up to a few 
thousand hosts, it is possible to build a fully connected network (as described above) 
using inexpensive commodity Gigabit Ethernet switches. However, the design of the 
core network, interconnecting hundreds to thousands of containers while providing 
high host-to-host bandwidth across containers for typical workloads, remains a chal-
lenging problem. A hybrid electrical/optical switch architecture for interconnecting 
the containers is proposed in [Farrington 2010].
When using highly interconnected topologies, one of the major issues is design-
ing routing algorithms among the switches. One possibility [Greenberg 2009b] is 
to use a form of random routing. Another possibility [Guo 2009] is to deploy mul-
tiple network interface cards in each host, connect each host to multiple low-cost 
commodity switches, and allow the hosts themselves to intelligently route traffic 
among the switches. Variations and extensions of these approaches are currently 
being deployed in contemporary data centers.
Figure 6.31  ♦  Highly interconnected data network topology
1
2
3
4
5
6
7
8
Server racks
TOR switches
Tier-2 switches
Tier-1 switches
528         Chapter 6    •    The Link Layer and LANs
Another important trend is that large cloud providers are increasingly building 
or customizing just about everything that is in their data centers, including network 
adapters, switches routers, TORs, software, and networking protocols [Greenberg 
2015, Singh 2015]. Another trend, pioneered by Amazon, is to improve reliability 
with “availability zones,” which essentially replicate distinct data centers in different 
nearby buildings. By having the buildings nearby (a few kilometers apart), trans-
actional data can be synchronized across the data centers in the same availability 
zone while providing fault tolerance [Amazon 2014]. Many more innovations in data 
center design are likely to continue to come; interested readers are encouraged to see 
the recent papers and videos on data center network design.
6.7	 Retrospective: A Day in the Life of a Web 
Page Request
Now that we’ve covered the link layer in this chapter, and the network, transport and 
application layers in earlier chapters, our journey down the protocol stack is com-
plete! In the very beginning of this book (Section 1.1), we wrote “much of this book 
is concerned with computer network protocols,” and in the first five chapters, we’ve 
certainly seen that this is indeed the case! Before heading into the topical chapters in 
second part of this book, we’d like to wrap up our journey down the protocol stack by 
taking an integrated, holistic view of the protocols we’ve learned about so far. One 
way then to take this “big picture” view is to identify the many (many!) protocols 
that are involved in satisfying even the simplest request: downloading a Web page. 
Figure 6.32 illustrates our setting: a student, Bob, connects a laptop to his school’s 
Ethernet switch and downloads a Web page (say the home page of www.google 
.com). As we now know, there’s a lot going on “under the hood” to satisfy this seem-
ingly simple request. A Wireshark lab at the end of this chapter examines trace files 
containing a number of the packets involved in similar scenarios in more detail.
6.7.1	Getting Started: DHCP, UDP, IP, and Ethernet
Let’s suppose that Bob boots up his laptop and then connects it to an Ethernet cable 
connected to the school’s Ethernet switch, which in turn is connected to the school’s 
router, as shown in Figure 6.32. The school’s router is connected to an ISP, in this 
example, comcast.net. In this example, comcast.net is providing the DNS service 
for the school; thus, the DNS server resides in the Comcast network rather than the 
school network. We’ll assume that the DHCP server is running within the router, as 
is often the case.
When Bob first connects his laptop to the network, he can’t do anything 
 
(e.g., download a Web page) without an IP address. Thus, the first network-related 
6.7    •    Retrospective: A Day in the Life of a Web Page Request         529
action taken by Bob’s laptop is to run the DHCP protocol to obtain an IP address, as 
well as other information, from the local DHCP server:
	1.	 The operating system on Bob’s laptop creates a DHCP request message 
­
(Section 4.3.3) and puts this message within a UDP segment (Section 3.3) 
with destination port 67 (DHCP server) and source port 68 (DHCP client). The 
UDP segment is then placed within an IP datagram (Section 4.3.1) with a 
broadcast IP destination address (255.255.255.255) and a source IP address of 
0.0.0.0, since Bob’s laptop doesn’t yet have an IP address.
	2.	 The IP datagram containing the DHCP request message is then placed within 
an Ethernet frame (Section 6.4.2). The Ethernet frame has a destina-
tion MAC addresses of FF:FF:FF:FF:FF:FF so that the frame will be 
broadcast to all devices connected to the switch (hopefully including a 
DHCP server); the frame’s source MAC address is that of Bob’s laptop, 
00:16:D3:23:68:8A.
	3.	 The broadcast Ethernet frame containing the DHCP request is the first frame 
sent by Bob’s laptop to the Ethernet switch. The switch broadcasts the 
incoming frame on all outgoing ports, including the port connected to the 
router.
00:22:6B:45:1F:1B
68.85.2.1
00:16:D3:23:68:8A
68.85.2.101
comcast.net
DNS server
68.87.71.226
www.google.com
Web server
64.233.169.105
School network
68.80.2.0/24
Comcast’s network
68.80.0.0/13
Google’s network
64.233.160.0/19
1–7
8–13
18–24
14–17
Figure 6.32  ♦  
A day in the life of a Web page request: Network setting 
and actions
530         Chapter 6    •    The Link Layer and LANs
	4.	 The router receives the broadcast Ethernet frame containing the DHCP request 
on its interface with MAC address 00:22:6B:45:1F:1B and the IP datagram 
is extracted from the Ethernet frame. The datagram’s broadcast IP destina-
tion address indicates that this IP datagram should be processed by upper 
layer protocols at this node, so the datagram’s payload (a UDP segment) is 
thus demultiplexed (Section 3.2) up to UDP, and the DHCP request message 
is extracted from the UDP segment. The DHCP server now has the DHCP 
request message.
	5.	 Let’s suppose that the DHCP server running within the router can allocate IP 
addresses in the CIDR (Section 4.3.3) block 68.85.2.0/24. In this example, all 
IP addresses used within the school are thus within Comcast’s address block. 
Let’s suppose the DHCP server allocates address 68.85.2.101 to Bob’s laptop. 
The DHCP server creates a DHCP ACK message (Section 4.3.3) containing 
this IP address, as well as the IP address of the DNS server (68.87.71.226), 
the IP address for the default gateway router (68.85.2.1), and the subnet block 
(68.85.2.0/24) (equivalently, the “network mask”). The DHCP message is 
put inside a UDP segment, which is put inside an IP datagram, which is put 
inside an Ethernet frame. The Ethernet frame has a source MAC address of the 
router’s interface to the home network (00:22:6B:45:1F:1B) and a destination 
MAC address of Bob’s laptop (00:16:D3:23:68:8A).
	6.	 The Ethernet frame containing the DHCP ACK is sent (unicast) by the router 
to the switch. Because the switch is self-learning (Section 6.4.3) and previ-
ously received an Ethernet frame (containing the DHCP request) from Bob’s 
laptop, the switch knows to forward a frame addressed to 00:16:D3:23:68:8A 
only to the output port leading to Bob’s laptop.
	7.	 Bob’s laptop receives the Ethernet frame containing the DHCP ACK, extracts 
the IP datagram from the Ethernet frame, extracts the UDP segment from the 
IP datagram, and extracts the DHCP ACK message from the UDP segment. 
Bob’s DHCP client then records its IP address and the IP address of its DNS 
server. It also installs the address of the default gateway into its IP forward-
ing table (Section 4.1). Bob’s laptop will send all datagrams with destination 
address outside of its subnet 68.85.2.0/24 to the default gateway. At this point, 
Bob’s laptop has initialized its networking components and is ready to begin 
processing the Web page fetch. (Note that only the last two DHCP steps of the 
four presented in Chapter 4 are actually necessary.)
6.7.2	Still Getting Started: DNS and ARP
When Bob types the URL for www.google.com into his Web browser, he begins 
the long chain of events that will eventually result in Google’s home page being 
displayed by his Web browser. Bob’s Web browser begins the process by creating 
a TCP socket (Section 2.7) that will be used to send the HTTP request (Section 
2.2) to www.google.com. In order to create the socket, Bob’s laptop will need to 
6.7    •    Retrospective: A Day in the Life of a Web Page Request         531
know the IP address of www.google.com. We learned in Section 2.5, that the DNS 
­
protocol is used to provide this name-to-IP-address translation service.
	8.	 The operating system on Bob’s laptop thus creates a DNS query message 
(Section 2.5.3), putting the string “www.google.com” in the question section 
of the DNS message. This DNS message is then placed within a UDP segment 
with a destination port of 53 (DNS server). The UDP segment is then placed 
within an IP datagram with an IP destination address of 68.87.71.226 (the 
address of the DNS server returned in the DHCP ACK in step 5) and a source 
IP address of 68.85.2.101.
	9.	 Bob’s laptop then places the datagram containing the DNS query message in 
an Ethernet frame. This frame will be sent (addressed, at the link layer) to the 
gateway router in Bob’s school’s network. However, even though Bob’s laptop 
knows the IP address of the school’s gateway router (68.85.2.1) via the DHCP 
ACK message in step 5 above, it doesn’t know the gateway router’s MAC 
address. In order to obtain the MAC address of the gateway router, Bob’s 
­
laptop will need to use the ARP protocol (Section 6.4.1).
	
10.	 Bob’s laptop creates an ARP query message with a target IP address of 
68.85.2.1 (the default gateway), places the ARP message within an Ethernet 
frame with a broadcast destination address (FF:FF:FF:FF:FF:FF) and sends the 
Ethernet frame to the switch, which delivers the frame to all connected devices, 
including the gateway router.
	
11.	 The gateway router receives the frame containing the ARP query message on the 
interface to the school network, and finds that the target IP address of 68.85.2.1 in 
the ARP message matches the IP address of its interface. The gateway router thus 
prepares an ARP reply, indicating that its MAC address of 00:22:6B:45:1F:1B 
corresponds to IP address 68.85.2.1. It places the ARP reply message in an Eth-
ernet frame, with a destination address of 00:16:D3:23:68:8A (Bob’s laptop) and 
sends the frame to the switch, which delivers the frame to Bob’s laptop.
	
12.	 Bob’s laptop receives the frame containing the ARP reply message and 
extracts the MAC address of the gateway router (00:22:6B:45:1F:1B) from the 
ARP reply message.
	
13.	 Bob’s laptop can now (finally!) address the Ethernet frame containing the DNS 
query to the gateway router’s MAC address. Note that the IP datagram in this frame 
has an IP destination address of 68.87.71.226 (the DNS server), while the frame 
has a destination address of 00:22:6B:45:1F:1B (the gateway router). Bob’s laptop 
sends this frame to the switch, which delivers the frame to the gateway router.
6.7.3	Still Getting Started: Intra-Domain Routing to the 
DNS Server
	
14.	 The gateway router receives the frame and extracts the IP datagram containing 
the DNS query. The router looks up the destination address of this datagram 
532         Chapter 6    •    The Link Layer and LANs
(68.87.71.226) and determines from its forwarding table that the datagram 
should be sent to the leftmost router in the Comcast network in Figure 6.32. 
The IP datagram is placed inside a link-layer frame appropriate for the link 
connecting the school’s router to the leftmost Comcast router and the frame is 
sent over this link.
	
15.	 The leftmost router in the Comcast network receives the frame, extracts the 
IP datagram, examines the datagram’s destination address (68.87.71.226) and 
determines the outgoing interface on which to forward the datagram toward the 
DNS server from its forwarding table, which has been filled in by ­
Comcast’s 
intra-domain protocol (such as RIP, OSPF or IS-IS, Section 5.3) as well as the 
Internet’s inter-domain protocol, BGP (Section 5.4).
	
16.	 Eventually the IP datagram containing the DNS query arrives at the DNS server. 
The DNS server extracts the DNS query message, looks up the name www 
.google.com in its DNS database (Section 2.5), and finds the DNS resource 
record that contains the IP address (64.233.169.105) for www.google.com. 
(assuming that it is currently cached in the DNS server). Recall that this cached 
data originated in the authoritative DNS server (Section 2.5.2) for googlecom. 
The DNS server forms a DNS reply message containing this hostname-to-IP-
address mapping, and places the DNS reply message in a UDP segment, and the 
segment within an IP datagram addressed to Bob’s laptop (68.85.2.101). This 
datagram will be forwarded back through the Comcast network to the school’s 
router and from there, via the Ethernet switch to Bob’s laptop.
	
17.	 Bob’s laptop extracts the IP address of the server www.google.com from the 
DNS message. Finally, after a lot of work, Bob’s laptop is now ready to con-
tact the www.google.com server!
6.7.4	Web Client-Server Interaction: TCP and HTTP
	
18.	 Now that Bob’s laptop has the IP address of www.google.com, it can create the 
TCP socket (Section 2.7) that will be used to send the HTTP GET message 
(Section 2.2.3) to www.google.com. When Bob creates the TCP socket, the 
TCP in Bob’s laptop must first perform a three-way handshake (Section 3.5.6) 
with the TCP in www.google.com. Bob’s laptop thus first creates a TCP SYN 
segment with destination port 80 (for HTTP), places the TCP segment inside an 
IP datagram with a destination IP address of 64.233.169.105 (www.google 
.com), places the datagram inside a frame with a destination MAC address of 
00:22:6B:45:1F:1B (the gateway router) and sends the frame to the switch.
	
19.	 The routers in the school network, Comcast’s network, and Google’s network 
forward the datagram containing the TCP SYN toward www.google.com, 
using the forwarding table in each router, as in steps 14–16 above. Recall that 
the router forwarding table entries governing forwarding of packets over the 
inter-domain link between the Comcast and Google networks are determined 
by the BGP protocol (Chapter 5).
6.7    •    Retrospective: A Day in the Life of a Web Page Request         533
	
20.	 Eventually, the datagram containing the TCP SYN arrives at www.google 
.com. The TCP SYN message is extracted from the datagram and demulti-
plexed to the welcome socket associated with port 80. A connection socket 
(Section 2.7) is created for the TCP connection between the Google HTTP 
server and Bob’s laptop. A TCP SYNACK (Section 3.5.6) segment is gener-
ated, placed inside a datagram addressed to Bob’s laptop, and finally placed 
inside a link-layer frame appropriate for the link connecting www.google.com 
to its first-hop router.
	
21.	 The datagram containing the TCP SYNACK segment is forwarded through the 
Google, Comcast, and school networks, eventually arriving at the Ethernet card 
in Bob’s laptop. The datagram is demultiplexed within the operating system to 
the TCP socket created in step 18, which enters the connected state.
	
22.	 With the socket on Bob’s laptop now (finally!) ready to send bytes to www 
.google.com, Bob’s browser creates the HTTP GET message (Section 2.2.3) 
containing the URL to be fetched. The HTTP GET message is then written into 
the socket, with the GET message becoming the payload of a TCP segment. 
The TCP segment is placed in a datagram and sent and delivered to www 
.google.com as in steps 18–20 above.
	
23.	 The HTTP server at www.google.com reads the HTTP GET message from 
the TCP socket, creates an HTTP response message (Section 2.2), places the 
requested Web page content in the body of the HTTP response message, and 
sends the message into the TCP socket.
	
24.	 The datagram containing the HTTP reply message is forwarded through the 
Google, Comcast, and school networks, and arrives at Bob’s laptop. Bob’s 
Web browser program reads the HTTP response from the socket, extracts 
the html for the Web page from the body of the HTTP response, and finally 
(finally!) displays the Web page!
Our scenario above has covered a lot of networking ground! If you’ve understood 
most or all of the above example, then you’ve also covered a lot of ground since you 
first read Section 1.1, where we wrote “much of this book is concerned with computer 
network protocols” and you may have wondered what a protocol actually was! As 
detailed as the above example might seem, we’ve omitted a number of possible addi-
tional protocols (e.g., NAT running in the school’s gateway router, wireless access to 
the school’s network, security protocols for accessing the school network or encrypt-
ing segments or datagrams, network management protocols), and considerations 
(Web caching, the DNS hierarchy) that one would encounter in the public ­
Internet. 
We’ll cover a number of these topics and more in the second part of this book.
Lastly, we note that our example above was an integrated and holistic, but also 
very “nuts and bolts,” view of many of the protocols that we’ve studied in the first 
part of this book. The example focused more on the “how” than the “why.” For a 
broader, more reflective view on the design of network protocols in general, see 
[Clark 1988, RFC 5218].
534         Chapter 6    •    The Link Layer and LANs
6.8	 Summary
In this chapter, we’ve examined the link layer—its services, the principles underly-
ing its operation, and a number of important specific protocols that use these princi-
ples in implementing link-layer services.
We saw that the basic service of the link layer is to move a network-layer data-
gram from one node (host, switch, router, WiFi access point) to an adjacent node. We 
saw that all link-layer protocols operate by encapsulating a network-layer datagram 
within a link-layer frame before transmitting the frame over the link to the adjacent 
node. Beyond this common framing function, however, we learned that different 
link-layer protocols provide very different link access, delivery, and transmission 
services. These differences are due in part to the wide variety of link types over 
which link-layer protocols must operate. A simple point-to-point link has a single 
sender and receiver communicating over a single “wire.” A multiple access link is 
shared among many senders and receivers; consequently, the link-layer protocol for 
a multiple access channel has a protocol (its multiple access protocol) for coordinat-
ing link access. In the case of MPLS, the “link” connecting two adjacent nodes (for 
example, two IP routers that are adjacent in an IP sense—that they are next-hop 
IP routers toward some destination) may actually be a network in and of itself. In 
one sense, the idea of a network being considered as a link should not seem odd. A 
telephone link connecting a home modem/computer to a remote modem/router, for 
example, is actually a path through a sophisticated and complex telephone network.
Among the principles underlying link-layer communication, we examined error-
detection and -correction techniques, multiple access protocols, link-layer address-
ing, virtualization (VLANs), and the construction of extended switched LANs and 
data center networks. Much of the focus today at the link layer is on these switched 
networks. In the case of error detection/correction, we examined how it is possible 
to add additional bits to a frame’s header in order to detect, and in some cases cor-
rect, bit-flip errors that might occur when the frame is transmitted over the link. We 
covered simple parity and checksumming schemes, as well as the more robust cyclic 
redundancy check. We then moved on to the topic of multiple access protocols. We 
identified and studied three broad approaches for coordinating access to a broadcast 
channel: channel partitioning approaches (TDM, FDM), random access approaches 
(the ALOHA protocols and CSMA protocols), and taking-turns approaches (poll-
ing and token passing). We studied the cable access network and found that it 
uses many of these multiple access methods. We saw that a consequence of hav-
ing multiple nodes share a single broadcast channel was the need to provide node 
addresses at the link layer. We learned that link-layer addresses were quite different 
from ­
network-layer addresses and that, in the case of the Internet, a special proto-
col (ARP—the Address Resolution Protocol) is used to translate between these two 
forms of addressing and studied the hugely successful Ethernet protocol in detail. We 
then examined how nodes sharing a broadcast channel form a LAN and how multiple 
LANs can be connected together to form larger LANs—all without the intervention 
Homework Problems and Questions         535
of network-layer routing to interconnect these local nodes. We also learned how 
­
multiple virtual LANs can be created on a single physical LAN infrastructure.
We ended our study of the link layer by focusing on how MPLS networks pro-
vide link-layer services when they interconnect IP routers and an overview of the 
network designs for today’s massive data centers. We wrapped up this chapter (and 
indeed the first five chapters) by identifying the many protocols that are needed to 
fetch a simple Web page. Having covered the link layer, our journey down the pro-
tocol stack is now over! Certainly, the physical layer lies below the link layer, but 
the details of the physical layer are probably best left for another course (for exam-
ple, in communication theory, rather than computer networking). We have, however, 
touched upon several aspects of the physical layer in this chapter and in Chapter 1 
(our discussion of physical media in Section 1.2). We’ll consider the physical layer 
again when we study wireless link characteristics in the next chapter.
Although our journey down the protocol stack is over, our study of computer 
networking is not yet at an end. In the following three chapters we cover wireless 
networking, network security, and multimedia networking. These four topics do 
not fit conveniently into any one layer; indeed, each topic crosscuts many layers. 
Understanding these topics (billed as advanced topics in some networking texts) thus 
requires a firm foundation in all layers of the protocol stack—a foundation that our 
study of the link layer has now completed!
Homework Problems and Questions
Chapter 6 Review Questions
SECTIONS 6.1–6.2
	R1.	 What is framing in link layer?
	R2.	 If all the links in the Internet were to provide reliable delivery service, would 
the TCP reliable delivery service be redundant? Why or why not?
	R3.	 Name three error-detection strategies employed by link layer.
SECTION 6.3
	R4.	 Suppose two nodes start to transmit at the same time a packet of length L 
over a broadcast channel of rate R. Denote the propagation delay between the 
two nodes as dprop. Will there be a collision if dprop 6 L/R? Why or why not?
	R5.	 In Section 6.3, we listed four desirable characteristics of a broadcast channel. 
Which of these characteristics does slotted ALOHA have? Which of these 
characteristics does token passing have?
536         Chapter 6    •    The Link Layer and LANs
	R6.	 In CSMA/CD, after the fifth collision, what is the probability that a node 
chooses K = 4? The result K = 4 corresponds to a delay of how many 
­
seconds on a 10 Mbps Ethernet?
	R7.	 While TDM and FDM assign time slots and frequencies, CDMA assigns a dif-
ferent code to each node. Explain the basic principle in which CDMA works.
	R8.	 Why does collision occur in CSMA, if all nodes perform carrier sensing 
before transmission?
SECTION 6.4
	R9.	 How big is the MAC address space? The IPv4 address space? The IPv6 
address space?
	
R10.	 Suppose nodes A, B, and C each attach to the same broadcast LAN (through 
their adapters). If A sends thousands of IP datagrams to B with each encap-
sulating frame addressed to the MAC address of B, will C’s adapter process 
these frames? If so, will C’s adapter pass the IP datagrams in these frames 
to the network layer C? How would your answers change if A sends frames 
with the MAC broadcast address?
	
R11.	 Why is an ARP query sent within a broadcast frame? Why is an ARP 
response sent within a frame with a specific destination MAC address?
	
R12.	 For the network in Figure 6.19, the router has two ARP modules, each with its 
own ARP table. Is it possible that the same MAC address appears in both tables?
	
R13.	 What is a hub used for?
	
R14.	 Consider Figure 6.15. How many subnetworks are there, in the addressing 
sense of Section 4.3?
	
R15.	 Each host and router has an ARP table in its memory. What are the contents 
of this table?
	
R16.	 The Ethernet frame begins with an 8-byte preamble field. The purpose of the 
first 7 bytes is to “wake up” the receiving adapters and to synchronize their 
clocks to that of the sender’s clock. What are the contents of the 8 bytes? 
What is the purpose of the last byte?
Problems
	 P1.	 Suppose the information content of a packet is the bit pattern 1010 0111 0101 
1001 and an even parity scheme is being used. What would the value of the field 
containing the parity bits be for the case of a two-dimensional parity scheme? 
Your answer should be such that a minimum-length checksum field is used.
	 P2.	 Show (give an example other than the one in Figure 6.5) that two-dimen-
sional parity checks can correct and detect a single bit error. Show (give an 
example of) a double-bit error that can be detected but not corrected.
Problems         537
	 P3.	 Suppose the information portion of a packet contains six bytes consisting 
of the 8-bit unsigned binary ASCII representation of string “CHKSUM”; 
compute the Internet checksum for this data.
	 P4.	 Compute the Internet checksum for each of the following:
a.	 the binary representation of the numbers 1 through 6.
b.	 the ASCII representation of the letters C through H (uppercase).
c.	 the ASCII representation of the letters c through h (lowercase).
	 P5.	 Consider the generator, G 5 1001, and suppose that D has the 
value 11000111010. What is the value of R?
	 P6.	 Rework the previous problem, but suppose that D has the value
a.	 01101010101.
b.	 11111010101.
c.	 10001100001.
	 P7.	 In this problem, we explore some of the properties of the CRC. For  
the ­
generator G (= 1001) given in Section 6.2.3, answer the following  
questions.
a.	 Why can it detect any single bit error in data D?
b.	 Can the above G detect any odd number of bit errors? Why?
	 P8.	 In Section 6.3, we provided an outline of the derivation of the efficiency of 
slotted ALOHA. In this problem we’ll complete the derivation.
a.	 Recall that when there are N active nodes, the efficiency of slotted 
ALOHA is Np(1 - p)N-1. Find the value of p that maximizes this expres-
sion.
b.	 Using the value of p found in (a), find the efficiency of slotted ALOHA 
by letting N approach infinity. Hint: (1 - 1/N)N approaches 1/e as N 
approaches infinity.
	 P9.	 Show that the maximum efficiency of pure ALOHA is 1/(2e). Note: This 
problem is easy if you have completed the problem above!
P	
10.	 Consider two nodes, A and B, that use the slotted ALOHA protocol to con-
tend for a channel. Suppose node A has more data to transmit than node B, 
and node A’s retransmission probability pA is greater than node B’s retrans-
mission probability, pB.
a.	 Provide a formula for node A’s average throughput. What is the total 
efficiency of the protocol with these two nodes?
b.	 If pA = 2pB, is node A’s average throughput twice as large as that of node 
B? Why or why not? If not, how can you choose pA and pB to make that 
happen?
538         Chapter 6    •    The Link Layer and LANs
c.	 In general, suppose there are N nodes, among which node A has retrans-
mission probability 2p and all other nodes have retransmission probability 
p. Provide expressions to compute the average throughputs of node A and 
of any other node.
	
P11.	 Suppose four active nodes—nodes A, B, C and D—are competing for access 
to a channel using slotted ALOHA. Assume each node has an infinite number 
of packets to send. Each node attempts to transmit in each slot with probabil-
ity p. The first slot is numbered slot 1, the second slot is numbered slot 2, and 
so on.
a.	 What is the probability that node A succeeds for the first time in slot 5?
b.	 What is the probability that some node (either A, B, C or D) succeeds in 
slot 4?
c.	 What is the probability that the first success occurs in slot 3?
d.	 What is the efficiency of this four-node system?
	
P12.	 Graph the efficiency of slotted ALOHA and pure ALOHA as a function of  
p for the following values of N:
a.	 N = 15.
b.	 N = 25.
c.	 N = 35.
	
P13.	 Consider a broadcast channel with N nodes and a transmission rate of R bps. 
Suppose the broadcast channel uses polling (with an additional polling node) 
for multiple access. Suppose the amount of time from when a node completes 
transmission until the subsequent node is permitted to transmit (that is, the 
polling delay) is dpoll. Suppose that within a polling round, a given node is 
allowed to transmit at most Q bits. What is the maximum throughput of the 
broadcast channel?
	
P14.	 Consider three LANs interconnected by two routers, as shown in Figure 6.33.
a.	 Assign IP addresses to all of the interfaces. For Subnet 1 use 
addresses of the form 192.168.1.xxx; for Subnet 2 uses addresses of 
the form 192.168.2.xxx; and for Subnet 3 use addresses of the form 
192.168.3.xxx.
b.	 Assign MAC addresses to all of the adapters.
c.	 Consider sending an IP datagram from Host E to Host B. Suppose all of 
the ARP tables are up to date. Enumerate all the steps, as done for the 
single-router example in Section 6.4.1.
d.	 Repeat (c), now assuming that the ARP table in the sending host is empty 
(and the other tables are up to date).
	
P15.	 Consider Figure 6.33. Now we replace the router between subnets 1 and 2 
with a switch S1, and label the router between subnets 2 and 3 as R1.
Problems         539
a.	 Consider sending an IP datagram from Host E to Host F. Will Host E ask router 
R1 to help forward the datagram? Why? In the Ethernet frame containing the 
IP datagram, what are the source and destination IP and MAC addresses?
b.	 Suppose E would like to send an IP datagram to B, and assume that E’s 
ARP cache does not contain B’s MAC address. Will E perform an ARP 
query to find B’s MAC address? Why? In the Ethernet frame (containing 
the IP datagram destined to B) that is delivered to router R1, what are the 
source and destination IP and MAC addresses?
c.	 Suppose Host A would like to send an IP datagram to Host B, and neither A’s 
ARP cache contains B’s MAC address nor does B’s ARP cache contain A’s 
MAC address. Further suppose that the switch S1’s forwarding table contains 
entries for Host B and router R1 only. Thus, A will broadcast an ARP request 
message. What actions will switch S1 perform once it receives the ARP 
request message? Will router R1 also receive this ARP request message? If 
so, will R1 forward the message to Subnet 3? Once Host B receives this ARP 
request message, it will send back to Host A an ARP response message. But 
will it send an ARP query message to ask for A’s MAC address? Why? What 
will switch S1 do once it receives an ARP response message from Host B?
	
P16.	 Consider the previous problem, but suppose now that the router between sub-
nets 2 and 3 is replaced by a switch. Answer questions (a)–(c) in the previous 
problem in this new context.
Figure 6.33  ♦  Three subnets, interconnected by routers
Subnet 3
E
F
C
Subnet 2
D
A
B
Subnet 1
540         Chapter 6    •    The Link Layer and LANs
	
P17.	 Recall that with the CSMA/CD protocol, the adapter waits 536K bit times 
after a collision, where K is drawn randomly. For K 5 115, how long does 
the adapter wait until returning to Step 2 for a 10 Mbps broadcast channel? 
For a 100 Mbps broadcast channel?
	
P18.	 Suppose nodes Aand B are on the same 12 Mbps broadcast channel, and the 
propagation delay between the two nodes is 316 bit times. Suppose CSMA/CD  
and Ethernet packets are used for this broadcast channel. Suppose node A begins 
transmitting a frame and, before it finishes, node B begins transmitting a 
frame. Can A finish transmitting before it detects that B has transmitted? 
Why or why not? If the answer is yes, then A incorrectly believes that its 
frame was successful transmitted without a collision. Hint: Suppose at time 
t 5 0 bits, A begins transmitting a frame. In the worst case, Atransmits a 
minimum-sized frame of 512 1 64 bit times. So A would finish transmit-
ting the frame at t 5 512 1 64 bit times. Thus, the answer is no, if B’s signal 
reaches A before bit time t 5 512 1 64 bits. In the worst case, when does B’s 
signal reach A?
	
P19.	 Suppose nodes A and B are on the same 10 Mbps broadcast channel, and the 
propagation delay between the two nodes is 245 bit times. Suppose A and 
B send Ethernet frames at the same time, the frames collide, and then A and 
B choose different values of K in the CSMA/CD algorithm. Assuming no 
other nodes are active, can the retransmissions from A and B collide? For our 
purposes, it suffices to work out the following example. Suppose A and B 
begin transmission at t = 0 bit times. They both detect collisions at t = 245
t bit times. Suppose KA = 0 and KB = 1. At what time does B schedule its 
retransmission? At what time does A begin transmission? (Note: The nodes 
must wait for an idle channel after returning to Step 2—see protocol.) At 
what time does A’s signal reach B? Does B refrain from transmitting at its 
scheduled time?
	
P20.	 In this problem, you will derive the efficiency of a CSMA/CD-like multiple 
access protocol. In this protocol, time is slotted and all adapters are synchro-
nized to the slots. Unlike slotted ALOHA, however, the length of a slot (in 
seconds) is much less than a frame time (the time to transmit a frame). Let S 
be the length of a slot. Suppose all frames are of constant length L = kRS,  
where R is the transmission rate of the channel and k is a large integer. Sup-
pose there are N nodes, each with an infinite number of frames to send. We 
also assume that dprop 6 S, so that all nodes can detect a collision before the 
end of a slot time. The protocol is as follows:
•	 If, for a given slot, no node has possession of the channel, all nodes 
contend for the channel; in particular, each node transmits in the slot with 
probability p. If exactly one node transmits in the slot, that node takes 
possession of the channel for the subsequent k - 1 slots and transmits its 
entire frame.
•	 If some node has possession of the channel, all other nodes refrain 
from transmitting until the node that possesses the channel has finished 
transmitting its frame. Once this node has transmitted its frame, all nodes 
contend for the channel.
	
	 Note that the channel alternates between two states: the productive state, 
which lasts exactly k slots, and the nonproductive state, which lasts for a ran-
dom number of slots. Clearly, the channel efficiency is the ratio of k/(k + x), 
where x is the expected number of consecutive unproductive slots.
a.	 For fixed N and p, determine the efficiency of this protocol.
b.	 For fixed N, determine the p that maximizes the efficiency.
c.	 Using the p (which is a function of N) found in (b), determine the effi-
ciency as N approaches infinity.
d.	 Show that this efficiency approaches 1 as the frame length becomes large.
	
P21.	 Consider Figure 6.33 in problem P14. Provide MAC addresses and IP 
addresses for the interfaces at Host A, both routers, and Host F. Suppose 
Host A sends a datagram to Host F. Give the source and destination MAC 
addresses in the frame encapsulating this IP datagram as the frame is trans-
mitted (i) from A to the left router, (ii) from the left router to the right router, 
(iii) from the right router to F. Also give the source and destination IP 
addresses in the IP datagram encapsulated within the frame at each of these 
points in time.
	
P22.	 Suppose now that the leftmost router in Figure 6.33 is replaced by a switch. 
Hosts A, B, C, and D and the right router are all star-connected into this 
switch. Give the source and destination MAC addresses in the frame encap-
sulating this IP datagram as the frame is transmitted (i) from A to the switch, 
(ii) from the switch to the right router, (iii) from the right router to F. Also 
give the source and destination IP addresses in the IP datagram encapsulated 
within the frame at each of these points in time.
	
P23.	 Consider Figure 5.15. Suppose that all links are 120 Mbps. What is the 
maximum total aggregate throughput that can be achieved among 12 hosts  
(4 in each department) and 2 servers in this network? You can assume that 
any host or server can send to any other host or server. Why?
	
P24.	 Suppose the three departmental switches in Figure 5.15 are replaced by hubs. 
All links are 120 Mbps. Now answer the questions posed in Problem P23.
	
P25.	 Suppose that all the switches in Figure 5.15 are replaced by hubs. All links 
are 120 Mbps. Now answer the questions posed in Problem P23
	
P26.	 Let’s consider the operation of a learning switch in the context of a network 
in which 6 nodes labeled A through F are star connected into an Ethernet 
switch. Suppose that (i) B sends a frame to E, (ii) E replies with a frame to B, 
(iii) A sends a frame to B, (iv) B replies with a frame to A. The switch table 
Problems         541
542         Chapter 6    •    The Link Layer and LANs
is initially empty. Show the state of the switch table before and after each of 
these events. For each of these events, identify the link(s) on which the trans-
mitted frame will be forwarded, and briefly justify your answers.
	
P27.	 In this problem, we explore the use of small packets for Voice-over-IP appli-
cations. One of the drawbacks of a small packet size is that a large fraction of 
link bandwidth is consumed by overhead bytes. To this end, suppose that the 
packet consists of P bytes and 5 bytes of header.
a.	 Consider sending a digitally encoded voice source directly. Suppose the 
source is encoded at a constant rate of 128 kbps. Assume each packet is 
entirely filled before the source sends the packet into the network. The 
time required to fill a packet is the packetization delay. In terms of L, 
determine the packetization delay in milliseconds.
b.	 Packetization delays greater than 20 msec can cause a noticeable and 
unpleasant echo. Determine the packetization delay for L = 1,500 bytes 
(roughly corresponding to a maximum-sized Ethernet packet) and for 
L = 50 (corresponding to an ATM packet).
c.	 Calculate the store-and-forward delay at a single switch for a link rate of 
R = 622 Mbps for L = 1,500 bytes, and for L = 50 bytes.
d.	 Comment on the advantages of using a small packet size.
	
P28.	 Consider the single switch VLAN in Figure 6.25, and assume an external 
router is connected to switch port 1. Assign IP addresses to the EE and CS 
hosts and router interface. Trace the steps taken at both the network layer  
and the link layer to transfer an IP datagram from an EE host to a CS host  
(Hint: Reread the discussion of Figure 6.19 in the text).
	
P29.	 Consider the MPLS network shown in Figure 6.29, and suppose that rout-
ers R5 and R6 are now MPLS enabled. Suppose that we want to perform 
traffic engineering so that packets from R6 destined for A are switched to 
A via R6-R4-R3-R1, and packets from R5 destined for A are switched via 
R5-R4-R2-R1. Show the MPLS tables in R5 and R6, as well as the modified 
table in R4, that would make this possible.
	
P30.	 Consider again the same scenario as in the previous problem, but suppose 
that packets from R6 destined for D are switched via R6-R4-R3, while pack-
ets from R5 destined to D are switched via R4-R2-R1-R3. Show the MPLS 
tables in all routers that would make this possible.
	
P31.	 In this problem, you will put together much of what you have learned about 
Internet protocols. Suppose you walk into a room, connect to Ethernet, and 
want to download a Web page. What are all the protocol steps that take place, 
starting from powering on your PC to getting the Web page? Assume there  
is nothing in our DNS or browser caches when you power on your PC.  
(Hint: The steps include the use of Ethernet, DHCP, ARP, DNS, TCP, and 
Wireshark Labs         543
HTTP protocols.) Explicitly indicate in your steps how you obtain the IP and 
MAC addresses of a gateway router.
	
P32.	 Consider the data center network with hierarchical topology in Figure 6.30. 
Suppose now there are 80 pairs of flows, with ten flows between the first 
and ninth rack, ten flows between the second and tenth rack, and so on. 
Further suppose that all links in the network are 10 Gbps, except for the links 
between hosts and TOR switches, which are 1 Gbps.
a.	 Each flow has the same data rate; determine the maximum rate of a flow.
b.	 For the same traffic pattern, determine the maximum rate of a flow for the 
highly interconnected topology in Figure 6.31.
c.	 Now suppose there is a similar traffic pattern, but involving 20 hosts on 
each rack and 160 pairs of flows. Determine the maximum flow rates for 
the two topologies.
	
P33.	 Consider the hierarchical network in Figure 6.30 and suppose that the data 
center needs to support e-mail and video distribution among other applica-
tions. Suppose four racks of servers are reserved for e-mail and four racks are 
reserved for video. For each of the applications, all four racks must lie below 
a single tier-2 switch since the tier-2 to tier-1 links do not have sufficient 
bandwidth to support the intra-application traffic. For the e-mail application, 
suppose that for 99.9 percent of the time only three racks are used, and that 
the video application has identical usage patterns.
a.	 For what fraction of time does the e-mail application need to use a fourth 
rack? How about for the video application?
b.	 Assuming e-mail usage and video usage are independent, for what fraction 
of time do (equivalently, what is the probability that) both applications 
need their fourth rack?
c.	 Suppose that it is acceptable for an application to have a shortage of serv-
ers for 0.001 percent of time or less (causing rare periods of performance 
degradation for users). Discuss how the topology in Figure 6.31 can be 
used so that only seven racks are collectively assigned to the two applica-
tions (assuming that the topology can support all the traffic).
Wireshark Labs
At the Companion website for this textbook, http://www.pearsonglobaleditions.com/
kurose, you’ll find a Wireshark lab that examines the operation of the IEEE 802.3 
protocol and the Wireshark frame format. A second Wireshark lab examines packet 
traces taken in a home network scenario.
544
Why did you decide to specialize in networking?
When I arrived at UCLA as a new graduate student in Fall 1969, my intention was to study 
control theory. Then I took the queuing theory classes of Leonard Kleinrock and was very 
impressed by him. For a while, I was working on adaptive control of queuing systems as a 
possible thesis topic. In early 1972, Larry Roberts initiated the ARPAnet Satellite System 
project (later called Packet Satellite). Professor Kleinrock asked me to join the project. The 
first thing we did was to introduce a simple, yet realistic, backoff algorithm to the slotted 
ALOHA protocol. Shortly thereafter, I found many interesting research problems, such as 
ALOHA’s instability problem and need for adaptive backoff, which would form the core of 
my thesis.
You were active in the early days of the Internet in the 1970s, beginning with your  
student days at UCLA. What was it like then? Did people have any inkling of what the 
Internet would become?
The atmosphere was really no different from other system-building projects I have seen in 
industry and academia. The initially stated goal of the ARPAnet was fairly modest, that 
is, to provide access to expensive computers from remote locations so that many more 
scientists could use them. However, with the startup of the Packet Satellite project in 1972 
and the Packet Radio project in 1973, ARPA’s goal had expanded substantially. By 1973, 
ARPA was building three different packet networks at the same time, and it became neces-
sary for Vint Cerf and Bob Kahn to develop an interconnection strategy.
Back then, all of these progressive developments in networking were viewed  
(I believe) as logical rather than magical. No one could have envisioned the scale of the 
Internet and power of personal computers today. It was a decade before appearance of the 
first PCs. To put things in perspective, most students submitted their computer programs 
Simon S. Lam
Simon S. Lam is Professor and Regents Chair in Computer Sciences 
at the University of Texas at Austin. From 1971 to 1974, he was 
with the ARPA Network Measurement Center at UCLA, where he 
worked on satellite and radio packet switching. He led a research 
group that invented secure sockets and prototyped, in 1993, the 
first secure sockets layer named Secure Network Programming, 
which won the 2004 ACM Software System Award. His research 
interests are in design and analysis of network protocols and security 
services. He received his BSEE from Washington State University 
and his MS and PhD from UCLA. He was elected to the National 
Academy of Engineering in 2007.
AN INTERVIEW WITH…
545
as decks of punched cards for batch processing. Only some students had direct access to 
computers, which were typically housed in a restricted area. Modems were slow and still a 
rarity. As a graduate student, I had only a phone on my desk, and I used pencil and paper to 
do most of my work.
Where do you see the field of networking and the Internet heading in the future?
In the past, the simplicity of the Internet’s IP protocol was its greatest strength in vanquish-
ing competition and becoming the de facto standard for internetworking. Unlike competi-
tors, such as X.25 in the 1980s and ATM in the 1990s, IP can run on top of any link-layer 
networking technology, because it offers only a best-effort datagram service. Thus, any 
packet network can connect to the Internet.
Today, IP’s greatest strength is actually a shortcoming. IP is like a straitjacket that 
confines the Internet’s development to specific directions. In recent years, many research-
ers have redirected their efforts to the application layer only. There is also a great deal of 
research on wireless ad hoc networks, sensor networks, and satellite networks. These net-
works can be viewed either as stand-alone systems or link-layer systems, which can flourish 
because they are outside of the IP straitjacket.
Many people are excited about the possibility of P2P systems as a platform for novel 
Internet applications. However, P2P systems are highly inefficient in their use of Internet 
resources. A concern of mine is whether the transmission and switching capacity of the 
Internet core will continue to increase faster than the traffic demand on the Internet as it 
grows to interconnect all kinds of devices and support future P2P-enabled applications. 
Without substantial overprovisioning of capacity, ensuring network stability in the presence 
of malicious attacks and congestion will continue to be a significant challenge.
The Internet’s phenomenal growth also requires the allocation of new IP addresses at 
a rapid rate to network operators and enterprises worldwide. At the current rate, the pool of 
unallocated IPv4 addresses would be depleted in a few years. When that happens, large con-
tiguous blocks of address space can only be allocated from the IPv6 address space. Since 
adoption of IPv6 is off to a slow start, due to lack of incentives for early adopters, IPv4 and 
IPv6 will most likely co-exist on the Internet for many years to come. Successful migra-
tion from an IPv4-dominant Internet to an IPv6-dominant Internet will require a substantial 
global effort.
What is the most challenging part of your job?
The most challenging part of my job as a professor is teaching and motivating every stu-
dent in my class, and every doctoral student under my supervision, rather than just the high 
achievers. The very bright and motivated may require a little guidance but not much else.  
546
I often learn more from these students than they learn from me. Educating and motivating 
the underachievers present a major challenge.
What impacts do you foresee technology having on learning in the future?
Eventually, almost all human knowledge will be accessible through the Internet, which will 
be the most powerful tool for learning. This vast knowledge base will have the potential of 
leveling the playing field for students all over the world. For example, motivated students in 
any country will be able to access the best-class Web sites, multimedia lectures, and teach-
ing materials. Already, it was said that the IEEE and ACM digital libraries have accelerated 
the development of computer science researchers in China. In time, the Internet will tran-
scend all geographic barriers to learning.
547
In the telephony world, the past 20 years have arguably been the golden years of 
cellular telephony. The number of worldwide mobile cellular subscribers increased 
from 34 million in 1993 to nearly 7.0 billion subscribers by 2014, with the number 
of cellular subscribers now surpassing the number of wired telephone lines. There 
are now a larger number of mobile phone subscriptions than there are people on our 
planet. The many advantages of cell phones are evident to all—anywhere, anytime, 
untethered access to the global telephone network via a highly portable lightweight 
device. More recently, laptops, smartphones, and tablets  are wirelessly connected 
to the Internet via a cellular  or  WiFi network.  And increasingly, devices such as 
gaming consoles, thermostats, home security systems, home appliances, watches, 
eye glasses, cars, traffic control systems and more are being wirelessly connected to 
the Internet.
From a networking standpoint, the challenges posed by networking these wire-
less and mobile devices, particularly at the link layer and the network layer, are so 
different from traditional wired computer networks that an individual chapter devoted 
to the study of wireless and mobile networks (i.e., this chapter) is appropriate.
We’ll begin this chapter with a discussion of mobile users, wireless links, and 
networks, and their relationship to the larger (typically wired) networks to which 
they connect. We’ll draw a distinction between the challenges posed by the ­
wireless 
nature of the communication links in such networks, and by the mobility that these 
wireless links enable. Making this important distinction—between wireless and 
mobility—will allow us to better isolate, identify, and master the key concepts in 
each area. Note that there are indeed many networked environments in which the net-
work nodes are wireless but not mobile (e.g., wireless home or office networks with 
7
CHAPTER
Wireless 
and Mobile 
Networks
548         Chapter 7    •    Wireless and Mobile Networks
stationary workstations and large displays), and that there are limited forms of mobil-
ity that do not require wireless links (e.g., a worker who uses a wired laptop at home, 
shuts down the laptop, drives to work, and attaches the laptop to the company’s 
wired network). Of course, many of the most exciting networked environments are 
those in which users are both wireless and mobile—for example, a scenario in which 
a mobile user (say in the back seat of car) maintains a Voice-over-IP call and multi-
ple ongoing TCP connections while racing down the autobahn at 160 kilometers per 
hour, soon in an autonomous vehicle. It is here, at the intersection of wireless and 
mobility, that we’ll find the most interesting technical challenges!
We’ll begin by illustrating the setting in which we’ll consider wireless commu-
nication and mobility—a network in which wireless (and possibly mobile) users are 
connected into the larger network infrastructure by a wireless link at the network’s 
edge. We’ll then consider the characteristics of this wireless link in Section 7.2. We 
include a brief introduction to code division multiple access (CDMA), a shared-
medium access protocol that is often used in wireless networks, in Section 7.2. In 
Section 7.3, we’ll examine the link-level aspects of the IEEE 802.11 (WiFi) wireless 
LAN standard in some depth; we’ll also say a few words about Bluetooth and other 
wireless personal area networks. In Section 7.4, we’ll provide an overview of cellular 
Internet access, including 3G and emerging 4G cellular technologies that provide 
both voice and high-speed Internet access. In Section 7.5, we’ll turn our attention to 
mobility, focusing on the problems of locating a mobile user, routing to the mobile 
user, and “handing off” the mobile user who dynamically moves from one point of 
attachment to the network to another. We’ll examine how these mobility services are 
implemented in the mobile IP standard in enterprise 802.11 networks, and in LTE 
cellular networks in Sections 7.6 and 7.7, respectively. Finally, we’ll consider the 
impact of wireless links and mobility on transport-layer protocols and networked 
applications in Section 7.8.
7.1	 Introduction
Figure 7.1 shows the setting in which we’ll consider the topics of wireless data com-
munication and mobility. We’ll begin by keeping our discussion general enough to 
cover a wide range of networks, including both wireless LANs such as IEEE 802.11 
and cellular networks such as a 4G network; we’ll drill down into a more detailed 
discussion of specific wireless architectures in later sections. We can identify the 
following elements in a wireless network:
•	 Wireless hosts. As in the case of wired networks, hosts are the end-system devices 
that run applications. A wireless host might be a laptop, tablet, smartphone, or 
desktop computer. The hosts themselves may or may not be mobile.
7.1    •    Introduction         549
•	 Wireless links. A host connects to a base station (defined below) or to another 
wireless host through a wireless communication link. Different wireless link 
technologies have different transmission rates and can transmit over differ-
ent distances. Figure 7.2 shows two key characteristics (coverage area and 
link rate) of the more popular wireless network standards. (The figure is only 
meant to provide a rough idea of these characteristics. For example, some of 
these types of networks are only now being deployed, and some link rates can 
increase or decrease beyond the values shown depending on distance, channel 
conditions, and the number of users in the wireless network.) We’ll cover these 
standards later in the first half of this chapter; we’ll also consider other wireless 
link characteristics (such as their bit error rates and the causes of bit errors) in 
Section 7.2.
	
In Figure 7.1, wireless links connect wireless hosts located at the edge of the 
network into the larger network infrastructure. We hasten to add that wireless 
links are also sometimes used within a network to connect routers, switches, and 
Figure 7.1  ♦  Elements of a wireless network
Network
infrastructure
Key:
Wireless access point
Coverage area
Wireless host
Wireless host in motion
550         Chapter 7    •    Wireless and Mobile Networks
other network equipment. However, our focus in this chapter will be on the use of 
wireless communication at the network edge, as it is here that many of the most 
exciting technical challenges, and most of the growth, are occurring.
•	 Base station. The base station is a key part of the wireless network infrastructure. 
Unlike the wireless host and wireless link, a base station has no obvious counter-
part in a wired network. A base station is responsible for sending and receiving 
data (e.g., packets) to and from a wireless host that is associated with that base 
station. A base station will often be responsible for coordinating the transmission 
of multiple wireless hosts with which it is associated. When we say a wireless 
host is “associated” with a base station, we mean that (1) the host is within the 
wireless communication distance of the base station, and (2) the host uses that 
base station to relay data between it (the host) and the larger network. Cell towers 
in cellular networks and access points in 802.11 wireless LANs are examples of 
base stations.
	
In Figure 7.1, the base station is connected to the larger network (e.g., the ­
Internet, 
corporate or home network, or telephone network), thus functioning as a link-
layer relay between the wireless host and the rest of the world with which the host 
communicates.
	
Hosts associated with a base station are often referred to as operating in 
­
infrastructure mode, since all traditional network services (e.g., address assign-
ment and routing) are provided by the network to which a host is connected via 
Figure 7.2  ♦  Link characteristics of selected wireless network standards
802.11ac
802.11a,g
802.11n
802.11b
802.15.1
3G: UMTS/WCDMA, CDMA2000
2G: IS-95, CDMA, GSM
Indoor
Outdoor
Mid range
outdoor
Long range
outdoor
10–30m
50–200m
200m–4Km
5Km–20Km
54 Mbps
4 Mbps
5–11 Mbps
450 Mbps
1 Mbps
384 Kbps
Enhanced 3G: HSPA
4G: LTE
802.11a,g point-to-point
1300 Mbps
7.1    •    Introduction         551
the base station. In ad hoc networks, wireless hosts have no such infrastructure 
with which to connect. In the absence of such infrastructure, the hosts themselves 
must provide for services such as routing, address assignment, DNS-like name 
translation, and more.
	
When a mobile host moves beyond the range of one base station and into the 
range of another, it will change its point of attachment into the larger network 
Public WIFI Access: Coming Soon to A Lamp Post Near You?
WiFi hotspots—public locations where users can find 802.11 wireless access—are 
becoming increasingly common in hotels, airports, and cafés around the world. Most 
college campuses offer ubiquitous wireless access, and it’s hard to find a hotel that 
doesn’t offer wireless Internet access.
Over the past decade a number of cities have designed, deployed, and oper-
ated municipal WiFi networks. The vision of providing ubiquitous WiFi access to the 
community as a public service (much like streetlights)—helping to bridge the digital 
divide by providing Internet access to all citizens and to promote economic develop-
ment—is compelling. Many cities around the world, including Philadelphia, Toronto, 
Hong Kong, Minneapolis, London, and Auckland, have plans to provide ubiquitous 
wireless within the city, or have already done so to varying degrees. The goal in 
Philadelphia was to “turn Philadelphia into the nation’s largest WiFi hotspot and help 
to improve education, bridge the digital divide, enhance neighborhood develop-
ment, and reduce the costs of government.” The ambitious program—an agreement 
between the city, Wireless Philadelphia (a nonprofit entity), and the Internet Service 
Provider Earthlink—built an operational network of 802.11b hotspots on streetlamp 
pole arms and traffic control devices that covered 80 percent of the city. But financial 
and operational concerns caused the network to be sold to a group of private inves-
tors in 2008, who later sold the network back to the city in 2010. Other cities, such 
as Minneapolis, Toronto, Hong Kong, and Auckland, have had success with smaller-
scale efforts.
The fact that 802.11 networks operate in the unlicensed spectrum (and hence 
can be deployed without purchasing expensive spectrum use rights) would seem to 
make them financially attractive. However, 802.11 access points (see Section 7.3) 
have much shorter ranges than 4G cellular base stations (see Section 7.4), requir-
ing a larger number of deployed endpoints to cover the same geographic region. 
Cellular data networks providing Internet access, on the other hand, operate in the 
licensed spectrum. Cellular providers pay billions of dollars for spectrum access 
rights for their networks, making cellular data networks a business rather than munic-
ipal undertaking.
Case History
552         Chapter 7    •    Wireless and Mobile Networks
(i.e., change the base station with which it is associated)—a process referred to 
as handoff. Such mobility raises many challenging questions. If a host can move, 
how does one find the mobile host’s current location in the network so that data 
can be forwarded to that mobile host? How is addressing performed, given that 
a host can be in one of many possible locations? If the host moves during a 
TCP connection or phone call, how is data routed so that the connection contin-
ues uninterrupted? These and many (many!) other questions make wireless and 
mobile networking an area of exciting networking research.
•	 Network infrastructure. This is the larger network with which a wireless host may 
wish to communicate.
Having discussed the “pieces” of a wireless network, we note that these pieces 
can be combined in many different ways to form different types of wireless net-
works. You may find a taxonomy of these types of wireless networks useful as you 
read on in this chapter, or read/learn more about wireless networks beyond this book. 
At the highest level we can classify wireless networks according to two criteria: (i) 
whether a packet in the wireless network crosses exactly one wireless hop or multiple 
wireless hops, and (ii) whether there is infrastructure such as a base station in the 
network:
•	 Single-hop, infrastructure-based. These networks have a base station that is con-
nected to a larger wired network (e.g., the Internet). Furthermore, all commu-
nication is between this base station and a wireless host over a single wireless 
hop. The 802.11 networks you use in the classroom, café, or library; and the 4G 
LTE data networks that we will learn about shortly all fall in this category. The 
vast majority of our daily interactions are with single-hop, infrastructure-based 
­
wireless networks.
•	 Single-hop, infrastructure-less. In these networks, there is no base station that 
is connected to a wireless network. However, as we will see, one of the nodes 
in this single-hop network may coordinate the transmissions of the other nodes. 
­
Bluetooth networks (that connect small wireless devices such as keyboards, 
speakers, and headsets, and which we will study in Section 7.3.6) and 802.11 
networks in ad hoc mode are single-hop, infrastructure-less networks.
•	 Multi-hop, infrastructure-based. In these networks, a base station is present that 
is wired to the larger network. However, some wireless nodes may have to relay 
their communication through other wireless nodes in order to communicate via 
the base station. Some wireless sensor networks and so-called wireless mesh 
networks fall in this category.
•	 Multi-hop, infrastructure-less. There is no base station in these networks, and 
nodes may have to relay messages among several other nodes in order to reach 
a destination. Nodes may also be mobile, with connectivity changing among 
nodes—a class of networks known as mobile ad hoc networks (MANETs). 
7.2    •    Wireless Links and Network Characteristics         553
If the mobile nodes are vehicles, the network is a vehicular ad hoc network 
(VANET). As you might imagine, the development of protocols for such net-
works is challenging and is the subject of much ongoing research.
In this chapter, we’ll mostly confine ourselves to single-hop networks, and then 
mostly to infrastructure-based networks.
Let’s now dig deeper into the technical challenges that arise in wireless and 
mobile networks. We’ll begin by first considering the individual wireless link, defer-
ring our discussion of mobility until later in this chapter.
7.2	 Wireless Links and Network Characteristics
Let’s begin by considering a simple wired network, say a home network, with a 
wired Ethernet switch (see Section 6.4) interconnecting the hosts. If we replace 
the wired Ethernet with a wireless 802.11 network, a wireless network interface 
would replace the host’s wired Ethernet interface, and an access point would 
replace the Ethernet switch, but virtually no changes would be needed at the net-
work layer or above. This suggests that we focus our attention on the link layer 
when looking for important differences between wired and wireless networks. 
Indeed, we can find a number of important differences between a wired link and 
a wireless link:
•	 Decreasing signal strength. Electromagnetic radiation attenuates as it passes 
through matter (e.g., a radio signal passing through a wall). Even in free space, 
the signal will disperse, resulting in decreased signal strength (sometimes referred 
to as path loss) as the distance between sender and receiver increases.
•	 Interference from other sources. Radio sources transmitting in the same frequency 
band will interfere with each other. For example, 2.4 GHz wireless phones and 
802.11b wireless LANs transmit in the same frequency band. Thus, the 802.11b 
wireless LAN user talking on a 2.4 GHz wireless phone can expect that neither 
the network nor the phone will perform particularly well. In addition to interfer-
ence from transmitting sources, electromagnetic noise within the environment 
(e.g., a nearby motor, a microwave) can result in interference.
•	 Multipath propagation. Multipath propagation occurs when portions of the 
electromagnetic wave reflect off objects and the ground, taking paths of different 
lengths between a sender and receiver. This results in the blurring of the received 
signal at the receiver. Moving objects between the sender and receiver can cause 
multipath propagation to change over time.
For a detailed discussion of wireless channel characteristics, models, and measure-
ments, see [Anderson 1995].
554         Chapter 7    •    Wireless and Mobile Networks
The discussion above suggests that bit errors will be more common in wireless 
links than in wired links. For this reason, it is perhaps not surprising that wireless 
link protocols (such as the 802.11 protocol we’ll examine in the following section) 
employ not only powerful CRC error detection codes, but also link-level relia-
ble-data-transfer protocols that retransmit corrupted frames.
Having considered the impairments that can occur on a wireless channel, let’s 
next turn our attention to the host receiving the wireless signal. This host receives an 
electromagnetic signal that is a combination of a degraded form of the original signal 
transmitted by the sender (degraded due to the attenuation and multipath propagation 
effects that we discussed above, among others) and background noise in the environ-
ment. The signal-to-noise ratio (SNR) is a relative measure of the strength of the 
received signal (i.e., the information being transmitted) and this noise. The SNR 
is typically measured in units of decibels (dB), a unit of measure that some think 
is used by electrical engineers primarily to confuse computer scientists. The SNR, 
measured in dB, is twenty times the ratio of the base-10 logarithm of the amplitude 
of the received signal to the amplitude of the noise. For our purposes here, we need 
only know that a larger SNR makes it easier for the receiver to extract the transmitted 
signal from the background noise.
Figure 7.3 (adapted from [Holland 2001]) shows the bit error rate (BER)—
roughly speaking, the probability that a transmitted bit is received in error at the 
receiver—versus the SNR for three different modulation techniques for encoding 
information for transmission on an idealized wireless channel. The theory of modu-
lation and coding, as well as signal extraction and BER, is well beyond the scope of 
Figure 7.3  ♦  Bit error rate, transmission rate, and SNR
10–7
10–6
10–5
10–4
10–3
10–2
10–1
10
20
30
40
0
SNR (dB)
BER
QAM16
(4 Mbps)
QAM256
(8 Mbps)
BPSK
(1 Mbps)
7.2    •    Wireless Links and Network Characteristics         555
this text (see [Schwartz 1980] for a discussion of these topics). Nonetheless, Figure 
7.3 illustrates several physical-layer characteristics that are important in understand-
ing higher-layer wireless communication protocols:
•	 For a given modulation scheme, the higher the SNR, the lower the BER. Since 
a sender can increase the SNR by increasing its transmission power, a sender 
can decrease the probability that a frame is received in error by increasing its 
transmission power. Note, however, that there is arguably little practical gain in 
increasing the power beyond a certain threshold, say to decrease the BER from 
10-12 to 10-13. There are also disadvantages associated with increasing the trans-
mission power: More energy must be expended by the sender (an important con-
cern for battery-powered mobile users), and the sender’s transmissions are more 
likely to interfere with the transmissions of another sender (see Figure 7.4(b)).
•	 For a given SNR, a modulation technique with a higher bit transmission rate 
(whether in error or not) will have a higher BER. For example, in Figure 7.3, 
with an SNR of 10 dB, BPSK modulation with a transmission rate of 1 Mbps has 
a BER of less than 10-7, while with QAM16 modulation with a transmission rate 
of 4 Mbps, the BER is 10-1, far too high to be practically useful. However, with 
an SNR of 20 dB, QAM16 modulation has a transmission rate of 4 Mbps and a 
BER of 10-7, while BPSK modulation has a transmission rate of only 1 Mbps 
and a BER that is so low as to be (literally) “off the charts.” If one can tolerate a 
BER of 10-7, the higher transmission rate offered by QAM16 would make it the 
preferred modulation technique in this situation. These considerations give rise to 
the final characteristic, described next.
•	 Dynamic selection of the physical-layer modulation technique can be used to 
adapt the modulation technique to channel conditions. The SNR (and hence 
Figure 7.4  ♦  Hidden terminal problem caused by obstacle (a) and fading (b)
A
A
C
B
C
Location
b.
a.
0
Signal strength
B
556         Chapter 7    •    Wireless and Mobile Networks
the BER) may change as a result of mobility or due to changes in the environ-
ment. Adaptive modulation and coding are used in cellular data systems and in 
the 802.11 WiFi and 4G cellular data networks that we’ll study in Sections 7.3 
and 7.4. This allows, for example, the selection of a modulation technique that 
provides the highest transmission rate possible subject to a constraint on the BER, 
for given channel characteristics.
A higher and time-varying bit error rate is not the only difference between a 
wired and wireless link. Recall that in the case of wired broadcast links, all nodes 
receive the transmissions from all other nodes. In the case of wireless links, the situ-
ation is not as simple, as shown in Figure 7.4. Suppose that Station A is transmit-
ting to Station B. Suppose also that Station C is transmitting to Station B. With the 
so-called hidden terminal problem, physical obstructions in the environment (for 
example, a mountain or a building) may prevent A and C from hearing each other’s 
transmissions, even though A’s and C’s transmissions are indeed interfering at the 
destination, B. This is shown in Figure 7.4(a). A second scenario that results in unde-
tectable collisions at the receiver results from the fading of a signal’s strength as it 
propagates through the wireless medium. Figure 7.4(b) illustrates the case where A 
and C are placed such that their signals are not strong enough to detect each other’s 
transmissions, yet their signals are strong enough to interfere with each other at sta-
tion B. As we’ll see in Section 7.3, the hidden terminal problem and fading make 
multiple access in a wireless network considerably more complex than in a wired 
network.
7.2.1	CDMA
Recall from Chapter 6 that when hosts communicate over a shared medium, a pro-
tocol is needed so that the signals sent by multiple senders do not interfere at the 
receivers. In Chapter 6 we described three classes of medium access protocols: chan-
nel partitioning, random access, and taking turns. Code division multiple access 
(CDMA) belongs to the family of channel partitioning protocols. It is prevalent in 
wireless LAN and cellular technologies. Because CDMA is so important in the wire-
less world, we’ll take a quick look at CDMA now, before getting into specific wire-
less access technologies in the subsequent sections.
In a CDMA protocol, each bit being sent is encoded by multiplying the bit by 
a signal (the code) that changes at a much faster rate (known as the chipping rate) 
than the original sequence of data bits. Figure 7.5 shows a simple, idealized CDMA 
encoding/decoding scenario. Suppose that the rate at which original data bits reach 
the CDMA encoder defines the unit of time; that is, each original data bit to be 
transmitted requires a one-bit slot time. Let di be the value of the data bit for the 
ith bit slot. For mathematical convenience, we represent a data bit with a 0 value 
as -1. Each bit slot is further subdivided into M mini-slots; in Figure 7.5, M = 8, 
7.2    •    Wireless Links and Network Characteristics         557
although in practice M is much larger. The CDMA code used by the sender con-
sists of a sequence of M values, cm, m = 1, . . . , M, each taking a +1 or -1 value. 
In the example in Figure 7.5, the M-bit CDMA code being used by the sender is 
(1, 1, 1, -1, 1, -1, -1, -1).
To illustrate how CDMA works, let us focus on the ith data bit, di. For the mth 
mini-slot of the bit-transmission time of di, the output of the CDMA encoder, Zi,m, is 
the value of di multiplied by the mth bit in the assigned CDMA code, cm:
	
Zi,m = di # cm
(7.1)
Figure 7.5  ♦  A simple CDMA example: Sender encoding, receiver decoding
1
1 1 1
-1
-1 -1 -1
1
1 1 1
-1
-1 -1 -1
1
-1
-1
-1 -1
1 1 1
1
-1
-1 -1 -1
1 1 1
Time slot 1
received input
Time slot 0
received input
Code
1
-1
-1
-1 -1
1 1 1
1
-1
-1 -1 -1
1 1 1
Data bits
Code
1
1 1 1
-1
-1 -1 -1
1
1 1 1
-1
-1 -1 -1
d1 = -1
d0 = 1
Time slot 1
Sender
Channel output Zi,m
Receiver
Zi,m
di • cm
=
Zi,m • cm
d
M
i
m=1
M
5
S
Time slot 1
channel output
Time slot 0
channel output
Time slot 0
d1 = -1
d0 = 1
558         Chapter 7    •    Wireless and Mobile Networks
In a simple world, with no interfering senders, the receiver would receive the encoded 
bits, Zi,m, and recover the original data bit, di, by computing:
	
di = 1
M a
M
m=1
Zi,m # cm
(7.2)
The reader might want to work through the details of the example in Figure 7.5 to 
see that the original data bits are indeed correctly recovered at the receiver using 
Equation 7.2.
The world is far from ideal, however, and as noted above, CDMA must work in 
the presence of interfering senders that are encoding and transmitting their data using 
a different assigned code. But how can a CDMA receiver recover a sender’s original 
data bits when those data bits are being tangled with bits being transmitted by other 
senders? CDMA works under the assumption that the interfering transmitted bit sig-
nals are additive. This means, for example, that if three senders send a 1 value, and a 
fourth sender sends a -1 value during the same mini-slot, then the received signal at 
all receivers during that mini-slot is a 2 (since 1 + 1 + 1 - 1 = 2). In the presence 
of multiple senders, sender s computes its encoded transmissions, Zs
i,m, in exactly 
the same manner as in Equation 7.1. The value received at a receiver during the mth 
mini-slot of the ith bit slot, however, is now the sum of the transmitted bits from all 
N senders during that mini-slot:
Z*
i, m = a
N
s=1
Z s
i,m
Amazingly, if the senders’ codes are chosen carefully, each receiver can recover the 
data sent by a given sender out of the aggregate signal simply by using the sender’s 
code in exactly the same manner as in Equation 7.2:
	
di = 1
M a
M
m=1
Zi,m
* # cm
(7.3)
as shown in Figure 7.6, for a two-sender CDMA example. The M-bit CDMA code 
being used by the upper sender is (1, 1, 1, -1, 1, -1, -1, -1), while the CDMA code 
being used by the lower sender is (1, -1, 1, 1, 1, -1, 1, 1). Figure 7.6 illustrates a 
receiver recovering the original data bits from the upper sender. Note that the receiver 
is able to extract the data from sender 1 in spite of the interfering transmission from 
 
sender 2.
Recall our cocktail analogy from Chapter 6. A CDMA protocol is similar to 
having partygoers speaking in multiple languages; in such circumstances humans are 
actually quite good at locking into the conversation in the language they understand, 
while filtering out the remaining conversations. We see here that CDMA is a parti-
tioning protocol in that it partitions the codespace (as opposed to time or frequency) 
and assigns each node a dedicated piece of the codespace.
Our discussion here of CDMA is necessarily brief; in practice a number of dif-
ficult issues must be addressed. First, in order for the CDMA receivers to be able 
7.2    •    Wireless Links and Network Characteristics         559
to extract a particular sender’s signal, the CDMA codes must be carefully chosen. 
­
Second, our discussion has assumed that the received signal strengths from various 
senders are the same; in reality this can be difficult to achieve. There is a consid-
erable body of literature addressing these and other issues related to CDMA; see 
­
[Pickholtz 1982; Viterbi 1995] for details.
Figure 7.6  ♦  A two-sender CDMA example
Receiver 1
1
1 1 1
-1
-1 -1 -1
1
1 1 1
-1
-1 -1 -1
Time slot 1
received input
Time slot 0
received input
Data bits
Data bits
1
1 1 1
-1
-1 -1 -1
1
1 1 1
-1
-1 -1 -1
Code
Senders
1 1 1
-1
1 1 1
-1
1
-1
-1
1 1 1
1 1
Code
Code
+
-2
2
2 2 2
2
-2
2
-2
2
2 2 2
2
-2
2
Channel, Zi,m
*
Zi,m
di  • cm
=
Zi,m • cm
d
M
i
m=1
M
5
S
d1 = -1
d0 = 1
d1 = 1
2
1
1
*
2
2
2
Zi,m
di  • cm
=
1
1
1
d0 = 1
2
1
1
d1 = -1
d0 = 1
1
1
560         Chapter 7    •    Wireless and Mobile Networks
7.3	 WiFi: 802.11 Wireless LANs
Pervasive in the workplace, the home, educational institutions, cafés, airports, and 
street corners, wireless LANs are now one of the most important access network 
technologies in the Internet today. Although many technologies and standards for 
wireless LANs were developed in the 1990s, one particular class of standards has 
clearly emerged as the winner: the IEEE 802.11 wireless LAN, also known as WiFi. 
In this section, we’ll take a close look at 802.11 wireless LANs, examining its frame 
structure, its medium access protocol, and its internetworking of 802.11 LANs with 
wired Ethernet LANs.
There are several 802.11 standards for wireless LAN technology in the IEEE 
802.11 (“WiFi”) family, as summarized in Table 7.1. The different 802.11 standards 
all share some common characteristics. They all use the same medium access proto-
col, CSMA/CA, which we’ll discuss shortly. All three use the same frame structure 
for their link-layer frames as well. All three standards have the ability to reduce 
their transmission rate in order to reach out over greater distances. And, importantly, 
802.11 products are also all backwards compatible, meaning, for example, that a 
mobile capable only of 802.11g may still interact with a newer 802.11ac base station.
Table 7.1  ♦  Summary of IEEE 802.11 standards
Standard
Frequency Range
Data Rate
802.11b
2.4 GHz
up to 11 Mbps
802.11a
5 GHz
up to 54 Mbps
802.11g
2.4 GHz
up to 54 Mbps
802.11n
2.5 GHz and 5 GHz
up to 450 Mbps
802.11ac
5 GHz
up to 1300 Mbps
However, as shown in Table 7.1, the standards have some major differences 
at the physical layer. 802.11 devices operate in two difference frequency ranges: 
2.4–2.485 GHz (referred to as the 2.4 GHz range) and 5.1 – 5.8 GHz (referred to 
as the 5 GHz range). The 2.4 GHz range is an unlicensed frequency band, where 
802.11 devices may compete for frequency spectrum with 2.4 GHz phones and 
microwave ovens. At 5 GHz, 802.11 LANs have a shorter transmission distance 
for a given power level and suffer more from multipath propagation. The two most 
recent standards, 802.11n [IEEE 802.11n 2012] and 802.11ac [IEEE 802.11ac 2013; 
Cisco 802.11ac 2015] uses multiple input multiple-output (MIMO) antennas; i.e., 
two or more antennas on the sending side and two or more antennas on the receiving 
side that are transmitting/receiving different signals [Diggavi 2004]. 802.11ac base 
7.3    •    WiFi: 802.11 Wireless LANs         561
stations may transmit to multiple stations simultaneously, and use “smart” antennas 
to adaptively beamform to target transmissions in the direction of a receiver. This 
decreases interference and increases the distance reached at a given data rate. The data 
rates shown in Table 7.1 are for an idealized environment, e.g., a receiver placed 1 
 
meter away from the base station, with no interference—a scenario that we’re 
unlikely to experience in practice! So as the saying goes, YMMV: Your Mileage (or 
in this case your wireless data rate) May Vary.
7.3.1	The 802.11 Architecture
Figure 7.7 illustrates the principal components of the 802.11 wireless LAN architec-
ture. The fundamental building block of the 802.11 architecture is the basic service 
set (BSS). A BSS contains one or more wireless stations and a central base station, 
known as an access point (AP) in 802.11 parlance. Figure 7.7 shows the AP in each 
of two BSSs connecting to an interconnection device (such as a switch or router), 
which in turn leads to the Internet. In a typical home network, there is one AP and one 
router (typically integrated together as one unit) that connects the BSS to the Internet.
As with Ethernet devices, each 802.11 wireless station has a 6-byte MAC 
address that is stored in the firmware of the station’s adapter (that is, 802.11 net-
work interface card). Each AP also has a MAC address for its wireless interface. As 
with Ethernet, these MAC addresses are administered by IEEE and are (in theory) 
­
globally unique.
Figure 7.7  ♦  IEEE 802.11 LAN architecture
Internet
Switch or router
AP
BSS 1
BSS 2
AP
562         Chapter 7    •    Wireless and Mobile Networks
As noted in Section 7.1, wireless LANs that deploy APs are often referred to 
as infrastructure wireless LANs, with the “infrastructure” being the APs along 
with the wired Ethernet infrastructure that interconnects the APs and a router. Figure 
7.8 shows that IEEE 802.11 stations can also group themselves together to form an 
ad hoc network—a network with no central control and with no connections to the 
­
“outside world.” Here, the network is formed “on the fly,” by mobile devices that 
have found themselves in proximity to each other, that have a need to communi-
cate, and that find no preexisting network infrastructure in their location. An ad hoc 
network might be formed when people with laptops get together (for example, in 
a conference room, a train, or a car) and want to exchange data in the absence of a 
centralized AP. There has been tremendous interest in ad hoc networking, as com-
municating portable devices continue to proliferate. In this section, though, we’ll 
focus our attention on infrastructure wireless LANs.
Channels and Association
In 802.11, each wireless station needs to associate with an AP before it can send or 
receive network-layer data. Although all of the 802.11 standards use association, 
we’ll discuss this topic specifically in the context of IEEE 802.11b/g.
When a network administrator installs an AP, the administrator assigns a one- 
or two-word Service Set Identifier (SSID) to the access point. (When you choose 
Wi-Fi under Setting on your iPhone, for example, a list is displayed showing the 
SSID of each AP in range.) The administrator must also assign a channel number 
to the AP. To understand channel numbers, recall that 802.11 operates in the fre-
quency range of 2.4 GHz to 2.4835 GHz. Within this 85 MHz band, 802.11 defines 
11 partially overlapping channels. Any two channels are non-overlapping if and 
only if they are separated by four or more channels. In particular, the set of channels 
Figure 7.8  ♦  An IEEE 802.11 ad hoc network
BSS
7.3    •    WiFi: 802.11 Wireless LANs         563
1, 6, and 11 is the only set of three non-overlapping channels. This means that an 
administrator could create a wireless LAN with an aggregate maximum transmis-
sion rate of 33 Mbps by installing three 802.11b APs at the same physical location, 
assigning channels 1, 6, and 11 to the APs, and interconnecting each of the APs 
with a switch.
Now that we have a basic understanding of 802.11 channels, let’s describe an 
interesting (and not completely uncommon) situation—that of a WiFi jungle. A WiFi 
jungle is any physical location where a wireless station receives a sufficiently strong 
signal from two or more APs. For example, in many cafés in New York City, a wire-
less station can pick up a signal from numerous nearby APs. One of the APs might be 
managed by the café, while the other APs might be in residential apartments near the 
café. Each of these APs would likely be located in a different IP subnet and would 
have been independently assigned a channel.
Now suppose you enter such a WiFi jungle with your phone, tablet, or ­
laptop, 
seeking wireless Internet access and a blueberry muffin. Suppose there are five 
APs in the WiFi jungle. To gain Internet access, your wireless device needs to join 
exactly one of the subnets and hence needs to associate with exactly one of the APs. 
­
Associating means the wireless device creates a virtual wire between itself and the 
AP. Specifically, only the associated AP will send data frames (that is, frames con-
taining data, such as a datagram) to your wireless device, and your wireless device 
will send data frames into the Internet only through the associated AP. But how does 
your wireless device associate with a particular AP? And more fundamentally, how 
does your wireless device know which APs, if any, are out there in the jungle?
The 802.11 standard requires that an AP periodically send beacon frames, each 
of which includes the AP’s SSID and MAC address. Your wireless device, know-
ing that APs are sending out beacon frames, scans the 11 channels, seeking beacon 
frames from any APs that may be out there (some of which may be transmitting 
on the same channel—it’s a jungle out there!). Having learned about available APs 
from the beacon frames, you (or your wireless device) select one of the APs for 
association.
The 802.11 standard does not specify an algorithm for selecting which of 
the available APs to associate with; that algorithm is left up to the designers of 
the 802.11 firmware and software in your wireless device. Typically, the device 
chooses the AP whose beacon frame is received with the highest signal strength. 
While a high signal strength is good (see, e.g., Figure 7.3), signal strength is not 
the only AP characteristic that will determine the performance a device receives. 
In particular, it’s possible that the selected AP may have a strong signal, but may 
be overloaded with other affiliated devices (that will need to share the wireless 
bandwidth at that AP), while an unloaded AP is not selected due to a slightly 
weaker signal. A number of alternative ways of choosing APs have thus recently 
been proposed [Vasudevan 2005; Nicholson 2006; Sundaresan 2006]. For an 
interesting and down-to-earth discussion of how signal strength is measured, see 
[Bardwell 2004].
564         Chapter 7    •    Wireless and Mobile Networks
The process of scanning channels and listening for beacon frames is known 
as passive scanning (see Figure 7.9a). A wireless device can also perform active 
scanning, by broadcasting a probe frame that will be received by all APs within the 
wireless device’s range, as shown in Figure 7.9b. APs respond to the probe request 
frame with a probe response frame. The wireless device can then choose the AP with 
which to associate from among the responding APs.
After selecting the AP with which to associate, the wireless device sends an asso-
ciation request frame to the AP, and the AP responds with an association response 
frame. Note that this second request/response handshake is needed with active scan-
ning, since an AP responding to the initial probe request frame doesn’t know which 
of the (possibly many) responding APs the device will choose to associate with, in 
much the same way that a DHCP client can choose from among multiple DHCP 
servers (see Figure 4.21). Once associated with an AP, the device will want to join 
the subnet (in the IP addressing sense of Section 4.3.3) to which the AP belongs. 
Thus, the device will typically send a DHCP discovery message (see Figure 4.21) 
into the subnet via the AP in order to obtain an IP address on the subnet. Once the 
address is obtained, the rest of the world then views that device simply as another 
host with an IP address in that subnet.
In order to create an association with a particular AP, the wireless device may 
be required to authenticate itself to the AP. 802.11 wireless LANs provide a number 
of alternatives for authentication and access. One approach, used by many compa-
nies, is to permit access to a wireless network based on a device’s MAC address. A 
second approach, used by many Internet cafés, employs usernames and passwords. 
Figure 7.9  ♦  Active and passive scanning for access points
1
1
3
2
H1
AP 2
AP 1
BBS 1
a. Passive scanning
 
1. Beacon frames sent from APs
 
2. Association Request frame sent:
 
 
H1 to selected AP
 
3. Association Response frame sent:
 
 
Selected AP to H1
a. Active scanning
 
1. Probe Request frame broadcast from H1
 
2. Probes Response frame sent from APs
 
3. Association Request frame sent:
 
 
H1 to selected AP
 
4. Association Response frame sent:
 
 
Selected AP to H1
 
 
BBS 2
2
2
4
3
H1
AP 2
AP 1
BBS 1
BBS 2
1
7.3    •    WiFi: 802.11 Wireless LANs         565
In both cases, the AP typically communicates with an authentication server, relay-
ing information between the wireless device and the authentication server using a 
protocol such as RADIUS [RFC 2865] or DIAMETER [RFC 3588]. Separating the 
authentication server from the AP allows one authentication server to serve many 
APs, centralizing the (often sensitive) decisions of authentication and access within 
the single server, and keeping AP costs and complexity low. We’ll see in chapter 8 
that the new IEEE 802.11i protocol defining security aspects of the 802.11 protocol 
family takes precisely this approach.
7.3.2	The 802.11 MAC Protocol
Once a wireless device is associated with an AP, it can start sending and receiving 
data frames to and from the access point. But because multiple wireless devices, 
or the AP itself may want to transmit data frames at the same time over the same 
channel, a multiple access protocol is needed to coordinate the transmissions. In 
the following, we'll refer to the devices or the AP as wireless “stations” that share 
the multiple access channel. As discussed in Chapter 6 and Section 7.2.1, broadly 
speaking there are three classes of multiple access protocols: channel partitioning 
(including CDMA), random access, and taking turns. Inspired by the huge suc-
cess of Ethernet and its random access protocol, the designers of 802.11 chose a 
random access protocol for 802.11 wireless LANs. This random access protocol 
is referred to as CSMA with collision avoidance, or more succinctly as CSMA/
CA. As with Ethernet’s CSMA/CD, the “CSMA” in CSMA/CA stands for “carrier 
sense multiple access,” meaning that each station senses the channel before trans-
mitting, and refrains from transmitting when the channel is sensed busy. Although 
both ­
Ethernet and 802.11 use carrier-sensing random access, the two MAC protocols 
have important differences. First, instead of using collision detection, 802.11 uses 
collision-avoidance techniques. Second, because of the relatively high bit error rates 
of wireless channels, 802.11 (unlike Ethernet) uses a link-layer acknowledgment/
retransmission (ARQ) scheme. We’ll describe 802.11’s collision-avoidance and 
link-layer acknowledgment schemes below.
Recall from Sections 6.3.2 and 6.4.2 that with Ethernet’s collision-detection 
algorithm, an Ethernet station listens to the channel as it transmits. If, while transmit-
ting, it detects that another station is also transmitting, it aborts its transmission and 
tries to transmit again after waiting a small, random amount of time. Unlike the 802.3 
Ethernet protocol, the 802.11 MAC protocol does not implement collision detection. 
There are two important reasons for this:
•	 The ability to detect collisions requires the ability to send (the station’s own 
­
signal) and receive (to determine whether another station is also transmitting) at 
the same time. Because the strength of the received signal is typically very small 
compared to the strength of the transmitted signal at the 802.11 adapter, it is 
costly to build hardware that can detect a collision.
566         Chapter 7    •    Wireless and Mobile Networks
•	 More importantly, even if the adapter could transmit and listen at the same time 
(and presumably abort transmission when it senses a busy channel), the adapter 
would still not be able to detect all collisions, due to the hidden terminal problem 
and fading, as discussed in Section 7.2.
Because 802.11wireless LANs do not use collision detection, once a station 
begins to transmit a frame, it transmits the frame in its entirety; that is, once a station 
gets started, there is no turning back. As one might expect, transmitting entire frames 
(particularly long frames) when collisions are prevalent can significantly degrade a 
multiple access protocol’s performance. In order to reduce the likelihood of collisions, 
802.11 employs several collision-avoidance techniques, which we’ll shortly discuss.
Before considering collision avoidance, however, we’ll first need to examine 
802.11’s link-layer acknowledgment scheme. Recall from Section 7.2 that when a 
station in a wireless LAN sends a frame, the frame may not reach the destination sta-
tion intact for a variety of reasons. To deal with this non-negligible chance of failure, 
the 802.11 MAC protocol uses link-layer acknowledgments. As shown in Figure 7.10, 
when the destination station receives a frame that passes the CRC, it waits a short 
period of time known as the Short Inter-frame Spacing (SIFS) and then sends back 
Figure 7.10  ♦  802.11 uses link-layer acknowledgments
Destination
DIFS
SIFS
data
ack
Source
7.3    •    WiFi: 802.11 Wireless LANs         567
an acknowledgment frame. If the transmitting station does not receive an acknowl-
edgment within a given amount of time, it assumes that an error has occurred and 
retransmits the frame, using the CSMA/CA protocol to access the channel. If an 
acknowledgment is not received after some fixed number of retransmissions, the trans-
mitting station gives up and discards the frame.
Having discussed how 802.11 uses link-layer acknowledgments, we’re now in a 
position to describe the 802.11 CSMA/CA protocol. Suppose that a station (wireless 
device or an AP) has a frame to transmit.
	1.	 If initially the station senses the channel idle, it transmits its frame after a  
short period of time known as the Distributed Inter-frame Space (DIFS);  
see ­
Figure 7.10.
	2.	 Otherwise, the station chooses a random backoff value using binary exponen-
tial backoff (as we encountered in Section 6.3.2) and counts down this value 
after DIFS when the channel is sensed idle. While the channel is sensed busy, 
the counter value remains frozen.
	3.	 When the counter reaches zero (note that this can only occur while the chan-
nel is sensed idle), the station transmits the entire frame and then waits for an 
acknowledgment.
	4.	 If an acknowledgment is received, the transmitting station knows that its frame 
has been correctly received at the destination station. If the station has another 
frame to send, it begins the CSMA/CA protocol at step 2. If the acknowledg-
ment isn’t received, the transmitting station reenters the backoff phase in step 2, 
 
with the random value chosen from a larger interval.
Recall that under Ethernet’s CSMA/CD, multiple access protocol (Section 6.3.2), 
a station begins transmitting as soon as the channel is sensed idle. With CSMA/CA, 
however, the station refrains from transmitting while counting down, even when it 
senses the channel to be idle. Why do CSMA/CD and CDMA/CA take such different 
approaches here?
To answer this question, let’s consider a scenario in which two stations each 
have a data frame to transmit, but neither station transmits immediately because each 
senses that a third station is already transmitting. With Ethernet’s CSMA/CD, the 
two stations would each transmit as soon as they detect that the third station has 
finished transmitting. This would cause a collision, which isn’t a serious issue in 
CSMA/CD, since both stations would abort their transmissions and thus avoid the 
useless transmissions of the remainders of their frames. In 802.11, however, the situ-
ation is quite different. Because 802.11 does not detect a collision and abort trans-
mission, a frame suffering a collision will be transmitted in its entirety. The goal 
in 802.11 is thus to avoid collisions whenever possible. In 802.11, if the two sta-
tions sense the channel busy, they both immediately enter random backoff, hopefully 
choosing different backoff values. If these values are indeed different, once the chan-
nel becomes idle, one of the two stations will begin transmitting before the other, and 
(if the two stations are not hidden from each other) the “losing station” will hear the 
 
568         Chapter 7    •    Wireless and Mobile Networks
“winning station’s” signal, freeze its counter, and refrain from transmitting until the 
winning station has completed its transmission. In this manner, a costly collision is 
avoided. Of course, collisions can still occur with 802.11 in this scenario: The two 
stations could be hidden from each other, or the two stations could choose random 
backoff values that are close enough that the transmission from the station starting 
first have yet to reach the second station. Recall that we encountered this problem 
earlier in our discussion of random access algorithms in the context of Figure 6.12.
Dealing with Hidden Terminals: RTS and CTS
The 802.11 MAC protocol also includes a nifty (but optional) reservation scheme 
that helps avoid collisions even in the presence of hidden terminals. Let’s investi-
gate this scheme in the context of Figure 7.11, which shows two wireless ­
stations 
and one access point. Both of the wireless stations are within range of the AP 
(whose ­
coverage is shown as a shaded circle) and both have associated with the AP. 
­
However, due to fading, the signal ranges of wireless stations are limited to the inte-
riors of the shaded circles shown in Figure 7.11. Thus, each of the wireless stations 
is hidden from the other, although neither is hidden from the AP.
Let’s now consider why hidden terminals can be problematic. Suppose Station H1 is 
transmitting a frame and halfway through H1’s transmission, Station H2 wants to send a 
frame to the AP. H2, not hearing the transmission from H1, will first wait a DIFS interval 
and then transmit the frame, resulting in a collision. The channel will therefore be wasted 
during the entire period of H1’s transmission as well as during H2’s transmission.
In order to avoid this problem, the IEEE 802.11 protocol allows a station to 
use a short Request to Send (RTS) control frame and a short Clear to Send (CTS) 
control frame to reserve access to the channel. When a sender wants to send a DATA 
Figure 7.11  ♦  
Hidden terminal example: H1 is hidden from H2, and vice 
versa
AP
H1
H2
7.3    •    WiFi: 802.11 Wireless LANs         569
frame, it can first send an RTS frame to the AP, indicating the total time required 
to transmit the DATA frame and the acknowledgment (ACK) frame. When the AP 
receives the RTS frame, it responds by broadcasting a CTS frame. This CTS frame 
serves two purposes: It gives the sender explicit permission to send and also instructs 
the other stations not to send for the reserved duration.
Thus, in Figure 7.12, before transmitting a DATA frame, H1 first broadcasts an RTS 
frame, which is heard by all stations in its circle, including the AP. The AP then responds 
Figure 7.12  ♦  Collision avoidance using the RTS and CTS frames
Destination
All other nodes
Defer access
Source
DIFS
ACK
SIFS
SIFS
SIFS
DATA
CTS
CTS
ACK
RTS
570         Chapter 7    •    Wireless and Mobile Networks
with a CTS frame, which is heard by all stations within its range, including H1 and H2. 
Station H2, having heard the CTS, refrains from transmitting for the time specified in the 
CTS frame. The RTS, CTS, DATA, and ACK frames are shown in Figure 7.12.
The use of the RTS and CTS frames can improve performance in two important 
ways:
•	 The hidden station problem is mitigated, since a long DATA frame is transmitted 
only after the channel has been reserved.
•	 Because the RTS and CTS frames are short, a collision involving an RTS or CTS 
frame will last only for the duration of the short RTS or CTS frame. Once the RTS 
and CTS frames are correctly transmitted, the following DATA and ACK frames 
should be transmitted without collisions.
You are encouraged to check out the 802.11 applet in the textbook’s Web site. 
This interactive applet illustrates the CSMA/CA protocol, including the RTS/CTS 
exchange sequence.
Although the RTS/CTS exchange can help reduce collisions, it also introduces 
delay and consumes channel resources. For this reason, the RTS/CTS exchange is 
only used (if at all) to reserve the channel for the transmission of a long DATA 
frame. In practice, each wireless station can set an RTS threshold such that the RTS/
CTS sequence is used only when the frame is longer than the threshold. For many 
wireless stations, the default RTS threshold value is larger than the maximum frame 
length, so the RTS/CTS sequence is skipped for all DATA frames sent.
Using 802.11 as a Point-to-Point Link
Our discussion so far has focused on the use of 802.11 in a multiple access setting. 
We should mention that if two nodes each have a directional antenna, they can point 
their directional antennas at each other and run the 802.11 protocol over what is essen-
tially a point-to-point link. Given the low cost of commodity 802.11 hardware, the use 
of directional antennas and an increased transmission power allow 802.11 to be used 
as an inexpensive means of providing wireless point-to-point connections over tens of 
kilometers distance. [Raman 2007] describes one of the first such multi-hop wireless 
networks, operating in the rural Ganges plains in India using point-to-point 802.11 links.
7.3.3	The IEEE 802.11 Frame
Although the 802.11 frame shares many similarities with an Ethernet frame, it also con-
tains a number of fields that are specific to its use for wireless links. The 802.11 frame 
is shown in Figure 7.13. The numbers above each of the fields in the frame represent 
the lengths of the fields in bytes; the numbers above each of the subfields in the frame 
control field represent the lengths of the subfields in bits. Let’s now examine the fields 
in the frame as well as some of the more important subfields in the frame’s control field.
7.3    •    WiFi: 802.11 Wireless LANs         571
Payload and CRC Fields
At the heart of the frame is the payload, which typically consists of an IP datagram 
or an ARP packet. Although the field is permitted to be as long as 2,312 bytes, it is 
typically fewer than 1,500 bytes, holding an IP datagram or an ARP packet. As with 
an Ethernet frame, an 802.11 frame includes a 32-bit cyclic redundancy check (CRC) 
so that the receiver can detect bit errors in the received frame. As we’ve seen, bit 
errors are much more common in wireless LANs than in wired LANs, so the CRC is 
even more useful here.
Address Fields
Perhaps the most striking difference in the 802.11 frame is that it has four address 
fields, each of which can hold a 6-byte MAC address. But why four address 
fields? Doesn’t a source MAC field and destination MAC field suffice, as they do 
for ­
Ethernet? It turns out that three address fields are needed for internetworking 
­
purposes—specifically, for moving the network-layer datagram from a wireless sta-
tion through an AP to a router interface. The fourth address field is used when APs 
­
forward frames to each other in ad hoc mode. Since we are only considering infra-
structure networks here, let’s focus our attention on the first three address fields. The 
802.11 standard defines these fields as follows:
•	 Address 2 is the MAC address of the station that transmits the frame. Thus, if a 
wireless station transmits the frame, that station’s MAC address is inserted in the 
address 2 field. Similarly, if an AP transmits the frame, the AP’s MAC address is 
inserted in the address 2 field.
•	 Address 1 is the MAC address of the wireless station that is to receive the frame. 
Thus if a mobile wireless station transmits the frame, address 1 contains the MAC 
address of the destination AP. Similarly, if an AP transmits the frame, address 1 
contains the MAC address of the destination wireless station.
Figure 7.13  ♦  The 802.11 frame
Frame
control
2
2
2
4
1
1
1
1
1
1
1
1
2
6
6
6
2
6
0-2312
4
Frame (numbers indicate ﬁeld length in bytes):
Address
1
Duration
Payload
CRC
Protocol
version
To
AP
From
AP
More
frag
Power
mgt
More
data
Address
2
Address
3
Address
4
Seq
control
Type
Subtype
Retry
WEP
Rsvd
Frame control ﬁeld expanded (numbers indicate ﬁeld length in bits):
572         Chapter 7    •    Wireless and Mobile Networks
•	 To understand address 3, recall that the BSS (consisting of the AP and wire-
less stations) is part of a subnet, and that this subnet connects to other subnets 
via some router interface. Address 3 contains the MAC address of this router 
­
interface.
To gain further insight into the purpose of address 3, let’s walk through an inter-
networking example in the context of Figure 7.14. In this figure, there are two APs, 
each of which is responsible for a number of wireless stations. Each of the APs has a 
direct connection to a router, which in turn connects to the global Internet. We should 
keep in mind that an AP is a link-layer device, and thus neither “speaks” IP nor 
understands IP addresses. Consider now moving a datagram from the router interface 
R1 to the wireless Station H1. The router is not aware that there is an AP between it 
and H1; from the router’s perspective, H1 is just a host in one of the subnets to which 
it (the router) is connected.
•	 The router, which knows the IP address of H1 (from the destination address of 
the datagram), uses ARP to determine the MAC address of H1, just as in an 
ordinary Ethernet LAN. After obtaining H1’s MAC address, router interface R1 
encapsulates the datagram within an Ethernet frame. The source address field of 
this frame contains R1’s MAC address, and the destination address field contains 
H1’s MAC address.
Figure 7.14  ♦  
The use of address fields in 802.11 frames: Sending frames 
between H1 and R1
Internet
Router
AP
H1
R1
BSS 1
BSS 2
AP
7.3    •    WiFi: 802.11 Wireless LANs         573
•	 When the Ethernet frame arrives at the AP, the AP converts the 802.3 Ethernet 
frame to an 802.11 frame before transmitting the frame into the wireless chan-
nel. The AP fills in address 1 and address 2 with H1’s MAC address and its own 
MAC address, respectively, as described above. For address 3, the AP inserts the 
MAC address of R1. In this manner, H1 can determine (from address 3) the MAC 
address of the router interface that sent the datagram into the subnet.
Now consider what happens when the wireless station H1 responds by moving a 
datagram from H1 to R1.
•	 H1 creates an 802.11 frame, filling the fields for address 1 and address 2 with the 
AP’s MAC address and H1’s MAC address, respectively, as described above. For 
address 3, H1 inserts R1’s MAC address.
•	 When the AP receives the 802.11 frame, it converts the frame to an Ethernet frame. 
The source address field for this frame is H1’s MAC address, and the destination 
address field is R1’s MAC address. Thus, address 3 allows the AP to determine 
the appropriate destination MAC address when constructing the Ethernet frame.
In summary, address 3 plays a crucial role for internetworking the BSS with a wired 
LAN.
Sequence Number, Duration, and Frame Control Fields
Recall that in 802.11, whenever a station correctly receives a frame from another sta-
tion, it sends back an acknowledgment. Because acknowledgments can get lost, the 
sending station may send multiple copies of a given frame. As we saw in our discus-
sion of the rdt2.1 protocol (Section 3.4.1), the use of sequence numbers allows the 
receiver to distinguish between a newly transmitted frame and the retransmission of 
a previous frame. The sequence number field in the 802.11 frame thus serves exactly 
the same purpose here at the link layer as it did in the transport layer in Chapter 3.
Recall that the 802.11 protocol allows a transmitting station to reserve the chan-
nel for a period of time that includes the time to transmit its data frame and the time 
to transmit an acknowledgment. This duration value is included in the frame’s dura-
tion field (both for data frames and for the RTS and CTS frames).
As shown in Figure 7.13, the frame control field includes many subfields. We’ll 
say just a few words about some of the more important subfields; for a more complete 
discussion, you are encouraged to consult the 802.11 specification [Held 2001; Crow 
1997; IEEE 802.11 1999]. The type and subtype fields are used to distinguish the asso-
ciation, RTS, CTS, ACK, and data frames. The to and from fields are used to define 
the meanings of the different address fields. (These meanings change depending on 
whether ad hoc or infrastructure modes are used and, in the case of infrastructure 
mode, whether a wireless station or an AP is sending the frame.) Finally the WEP field 
indicates whether encryption is being used or not (WEP is discussed in Chapter 8).
574         Chapter 7    •    Wireless and Mobile Networks
7.3.4	Mobility in the Same IP Subnet
In order to increase the physical range of a wireless LAN, companies and universities 
will often deploy multiple BSSs within the same IP subnet. This naturally raises the 
issue of mobility among the BSSs—how do wireless stations seamlessly move from one 
BSS to another while maintaining ongoing TCP sessions? As we’ll see in this subsec-
tion, mobility can be handled in a relatively straightforward manner when the BSSs are 
part of the subnet. When stations move between subnets, more sophisticated mobility 
management protocols will be needed, such as those we’ll study in Sections 7.5 and 7.6.
Let’s now look at a specific example of mobility between BSSs in the same sub-
net. Figure 7.15 shows two interconnected BSSs with a host, H1, moving from BSS1 
to BSS2. Because in this example the interconnection device that connects the two 
BSSs is not a router, all of the stations in the two BSSs, including the APs, belong 
to the same IP subnet. Thus, when H1 moves from BSS1 to BSS2, it may keep its IP 
address and all of its ongoing TCP connections. If the interconnection device were a 
router, then H1 would have to obtain a new IP address in the subnet in which it was 
moving. This address change would disrupt (and eventually terminate) any on-going 
TCP connections at H1. In Section 7.6, we’ll see how a network-layer mobility pro-
tocol, such as mobile IP, can be used to avoid this problem.
But what specifically happens when H1 moves from BSS1 to BSS2? As H1 
wanders away from AP1, H1 detects a weakening signal from AP1 and starts to scan 
for a stronger signal. H1 receives beacon frames from AP2 (which in many corporate 
and university settings will have the same SSID as AP1). H1 then disassociates with 
AP1 and associates with AP2, while keeping its IP address and maintaining its ongo-
ing TCP sessions.
This addresses the handoff problem from the host and AP viewpoint. But what 
about the switch in Figure 7.15? How does it know that the host has moved from one 
AP to another? As you may recall from Chapter 6, switches are “self-learning” and 
automatically build their forwarding tables. This self-learning feature nicely handles 
Figure 7.15  ♦  Mobility in the same subnet
BSS 1
BSS 2
H1
Switch
AP 1
AP 2
7.3    •    WiFi: 802.11 Wireless LANs         575
occasional moves (for example, when an employee gets transferred from one depart-
ment to another); however, switches were not designed to support highly mobile 
users who want to maintain TCP connections while moving between BSSs. To 
appreciate the problem here, recall that before the move, the switch has an entry in 
its forwarding table that pairs H1’s MAC address with the outgoing switch interface 
through which H1 can be reached. If H1 is initially in BSS1, then a datagram des-
tined to H1 will be directed to H1 via AP1. Once H1 associates with BSS2, however, 
its frames should be directed to AP2. One solution (a bit of a hack, really) is for AP2 
to send a broadcast Ethernet frame with H1’s source address to the switch just after 
the new association. When the switch receives the frame, it updates its forwarding 
table, allowing H1 to be reached via AP2. The 802.11f standards group is developing 
an inter-AP protocol to handle these and related issues.
Our discussion above has focused on mobility with the same LAN subnet. Recall 
that VLANs, which we studied in Section 6.4.4, can be used to connect together 
islands of LANs into a large virtual LAN that can span a large geographical region. 
Mobility among base stations within such a VLAN can be handled in exactly the 
same manner as above [Yu 2011].
7.3.5	Advanced Features in 802.11
We’ll wrap up our coverage of 802.11 with a short discussion of two advanced capabili-
ties found in 802.11 networks. As we’ll see, these capabilities are not completely speci-
fied in the 802.11 standard, but rather are made possible by mechanisms specified in 
the standard. This allows different vendors to implement these capabilities using their 
own (proprietary) approaches, presumably giving them an edge over the competition.
802.11 Rate Adaptation
We saw earlier in Figure 7.3 that different modulation techniques (with the different 
transmission rates that they provide) are appropriate for different SNR scenarios. 
Consider for example a mobile 802.11 user who is initially 20 meters away from 
the base station, with a high signal-to-noise ratio. Given the high SNR, the user can 
communicate with the base station using a physical-layer modulation technique that 
provides high transmission rates while maintaining a low BER. This is one happy 
user! Suppose now that the user becomes mobile, walking away from the base sta-
tion, with the SNR falling as the distance from the base station increases. In this case, 
if the modulation technique used in the 802.11 protocol operating between the base 
station and the user does not change, the BER will become unacceptably high as the 
SNR decreases, and eventually no transmitted frames will be received correctly.
For this reason, some 802.11 implementations have a rate adaptation capability 
that adaptively selects the underlying physical-layer modulation technique to use 
based on current or recent channel characteristics. If a node sends two frames in a 
row without receiving an acknowledgment (an implicit indication of bit errors on 
576         Chapter 7    •    Wireless and Mobile Networks
the channel), the transmission rate falls back to the next lower rate. If 10 frames 
in a row are acknowledged, or if a timer that tracks the time since the last fallback 
expires, the transmission rate increases to the next higher rate. This rate adapta-
tion mechanism shares the same “probing” philosophy as TCP’s congestion-control 
mechanism—when conditions are good (reflected by ACK receipts), the transmis-
sion rate is increased until something “bad” happens (the lack of ACK receipts); 
when something “bad” happens, the transmission rate is reduced. 802.11 rate adapta-
tion and TCP congestion control are thus similar to the young child who is constantly 
pushing his/her parents for more and more (say candy for a young child, later curfew 
hours for the teenager) until the parents finally say “Enough!” and the child backs 
off (only to try again later after conditions have hopefully improved!). A number 
of other schemes have also been proposed to improve on this basic automatic rate-
adjustment scheme [Kamerman 1997; Holland 2001; Lacage 2004].
Power Management
Power is a precious resource in mobile devices, and thus the 802.11 standard pro-
vides power-management capabilities that allow 802.11 nodes to minimize the 
amount of time that their sense, transmit, and receive functions and other circuitry 
need to be “on.” 802.11 power management operates as follows. A node is able to 
explicitly alternate between sleep and wake states (not unlike a sleepy student in a 
classroom!). A node indicates to the access point that it will be going to sleep by set-
ting the power-management bit in the header of an 802.11 frame to 1. A timer in the 
node is then set to wake up the node just before the AP is scheduled to send its bea-
con frame (recall that an AP typically sends a beacon frame every 100 msec). Since 
the AP knows from the set power-transmission bit that the node is going to sleep, it 
(the AP) knows that it should not send any frames to that node, and will buffer any 
frames destined for the sleeping host for later transmission.
A node will wake up just before the AP sends a beacon frame, and quickly enter 
the fully active state (unlike the sleepy student, this wakeup requires only 250 micro-
seconds [Kamerman 1997]!). The beacon frames sent out by the AP contain a list of 
nodes whose frames have been buffered at the AP. If there are no buffered frames 
for the node, it can go back to sleep. Otherwise, the node can explicitly request that 
the buffered frames be sent by sending a polling message to the AP. With an inter-
beacon time of 100 msec, a wakeup time of 250 microseconds, and a similarly small 
time to receive a beacon frame and check to ensure that there are no buffered frames, 
a node that has no frames to send or receive can be asleep 99% of the time, resulting 
in a significant energy savings.
7.3.6	Personal Area Networks: Bluetooth and Zigbee
As illustrated in Figure 7.2, the IEEE 802.11 WiFi standard is aimed at commu-
nication among devices separated by up to 100 meters (except when 802.11 is 
7.3    •    WiFi: 802.11 Wireless LANs         577
used in a point-to-point configuration with a directional antenna). Two other wire-
less protocols in the IEEE 802 family are Bluetooth and Zigbee (defined in the IEEE 
802.15.1 and IEEE 802.15.4 standards [IEEE 802.15 2012]).
Bluetooth
An IEEE 802.15.1 network operates over a short range, at low power, and at low cost. 
It is essentially a low-power, short-range, low-rate “cable replacement” technology 
for interconnecting a computer with its wireless keyboard, mouse or other periph-
eral device; cellular phones, speakers, headphones, and many other devices, whereas 
802.11 is a higher-power, medium-range, higher-rate “access” technology. For this 
reason, 802.15.1 networks are sometimes referred to as wireless personal area net-
works (WPANs). The link and physical layers of 802.15.1 are based on the earlier 
Bluetooth specification for personal area networks [Held 2001, Bisdikian 2001]. 
802.15.1 networks operate in the 2.4 GHz unlicensed radio band in a TDM manner, 
with time slots of 625 microseconds. During each time slot, a sender transmits on 
one of 79 channels, with the channel changing in a known but pseudo-random man-
ner from slot to slot. This form of channel hopping, known as frequency-hopping 
spread spectrum (FHSS), spreads transmissions in time over the frequency spec-
trum. 802.15.1 can provide data rates up to 4 Mbps.
802.15.1 networks are ad hoc networks: No network infrastructure (e.g., an access 
point) is needed to interconnect 802.15.1 devices. Thus, 802.15.1 devices must organ-
ize themselves. 802.15.1 devices are first organized into a piconet of up to eight active 
devices, as shown in Figure 7.16. One of these devices is designated as the master, with 
the remaining devices acting as slaves. The master node truly rules the piconet—its 
clock determines time in the piconet, it can transmit in each odd-numbered slot, and a 
Figure 7.16  ♦  A Bluetooth piconet
Radius of
coverage
Master device
Slave device
Parked device
Key:
M
M
S
S
S
S
P
P
P
P
P
578         Chapter 7    •    Wireless and Mobile Networks
slave can transmit only after the master has communicated with it in the previous slot 
and even then the slave can only transmit to the master. In addition to the slave devices, 
there can also be up to 255 parked devices in the network. These devices cannot com-
municate until their status has been changed from parked to active by the master node.
For more information about WPANs, the interested reader should consult the 
Bluetooth references [Held 2001, Bisdikian 2001] or the official IEEE 802.15 Web 
site [IEEE 802.15 2012].
Zigbee
A second personal area network standardized by the IEEE is the 802.15.4 standard 
[IEEE 802.15 2012] known as Zigbee. While Bluetooth networks provide a “cable 
replacement” data rate of over a Megabit per second, Zigbee is targeted at lower-
powered, lower-data-rate, lower-duty-cycle applications than Bluetooth. While we 
may tend to think that “bigger and faster is better,” not all network applications 
need high bandwidth and the consequent higher costs (both economic and power 
costs). For example, home temperature and light sensors, security devices, and wall-
mounted switches are all very simple, low-power, low-duty-cycle, low-cost devices. 
Zigbee is thus well-suited for these devices. Zigbee defines channel rates of 20, 40, 
100, and 250 Kbps, depending on the channel frequency.
Nodes in a Zigbee network come in two flavors. So-called “reduced- 
function devices” operate as slave devices under the control of a single “full-function 
device,” much as Bluetooth slave devices. A full-function device can operate as a 
master device as in Bluetooth by controlling multiple slave devices, and multiple 
full-function devices can additionally be configured into a mesh network in which 
full-function devices route frames amongst themselves. Zigbee shares many protocol 
mechanisms that we’ve already encountered in other link-layer protocols: beacon 
frames and link-layer acknowledgments (similar to 802.11), carrier-sense random 
access protocols with binary exponential backoff (similar to 802.11 and Ethernet), 
and fixed, guaranteed allocation of time slots (similar to DOCSIS).
Zigbee networks can be configured in many different ways. Let’s consider the 
simple case of a single full-function device controlling multiple reduced-function 
devices in a time-slotted manner using beacon frames. Figure 7.17 shows the case 
Figure 7.17  ♦  Zigbee 802.15.4 super-frame structure
Beacon
Guaranteed slots
Contention slots
Inactive period
Super frame
7.4    •    Cellular Internet Access         579
where the Zigbee network divides time into recurring super frames, each of which 
begins with a beacon frame. Each beacon frame divides the super frame into an active 
period (during which devices may transmit) and an inactive period (during which all 
devices, including the controller, can sleep and thus conserve power). The active 
period consists of 16 time slots, some of which are used by devices in a CSMA/CA 
random access manner, and some of which are allocated by the controller to specific 
devices, thus providing guaranteed channel access for those devices. More details 
about Zigbee networks can be found at [Baronti 2007, IEEE 802.15.4 2012].
7.4	 Cellular Internet Access
In the previous section we examined how an Internet host can access the Internet 
when inside a WiFi hotspot—that is, when it is within the vicinity of an 802.11 
access point. But most WiFi hotspots have a small coverage area of between 10 and 
100 meters in diameter. What do we do then when we have a desperate need for wire-
less Internet access and we cannot access a WiFi hotspot?
Given that cellular telephony is now ubiquitous in many areas throughout the 
world, a natural strategy is to extend cellular networks so that they support not only 
voice telephony but wireless Internet access as well. Ideally, this Internet access would 
be at a reasonably high speed and would provide for seamless mobility, allowing users 
to maintain their TCP sessions while traveling, for example, on a bus or a train. With 
sufficiently high upstream and downstream bit rates, the user could even maintain 
video-conferencing sessions while roaming about. This scenario is not that far-fetched. 
Data rates of several megabits per second are becoming available as broadband data 
services such as those we will cover here become more widely deployed.
In this section, we provide a brief overview of current and emerging cellular 
Internet access technologies. Our focus here will be on both the wireless first hop as 
well as the network that connects the wireless first hop into the larger telephone net-
work and/or the Internet; in Section 7.7 we’ll consider how calls are routed to a user 
moving between base stations. Our brief discussion will necessarily provide only a 
simplified and high-level description of cellular technologies. Modern cellular com-
munications, of course, has great breadth and depth, with many universities offering 
several courses on the topic. Readers seeking a deeper understanding are encouraged 
to see [Goodman 1997; Kaaranen 2001; Lin 2001; Korhonen 2003; Schiller 2003; 
Palat 2009; Scourias 2012; Turner 2012; Akyildiz 2010], as well as the particularly 
excellent and exhaustive references [Mouly 1992; Sauter 2014].
7.4.1	An Overview of Cellular Network Architecture
In our description of cellular network architecture in this section, we’ll adopt the 
terminology of the Global System for Mobile Communications (GSM) standards. 
580         Chapter 7    •    Wireless and Mobile Networks
(For history buffs, the GSM acronym was originally derived from Groupe Spécial 
Mobile, until the more anglicized name was adopted, preserving the original acro-
nym letters.) In the 1980s, Europeans recognized the need for a pan-European digi-
tal cellular telephony system that would replace the numerous incompatible analog 
cellular telephony systems, leading to the GSM standard [Mouly 1992]. Europeans 
deployed GSM technology with great success in the early 1990s, and since then 
GSM has grown to be the 800-pound gorilla of the cellular telephone world, with 
more than 80% of all cellular subscribers worldwide using GSM.
4G Cellular Mobile Versus Wireless LANs
Many cellular mobile phone operators are deploying 4G cellular mobile systems. In 
some countries (e.g., Korea and Japan), 4G LTE coverage is higher than 90%—nearly 
ubiquitous. In 2015, average download rates over deployed LTE systems range from 
10Mbps in the US and India to close to 40 Mbps in New Zealand. These 4G systems 
are being deployed in licensed radio-frequency bands, with some operators paying 
considerable sums to governments for spectrum-use licenses. 4G systems allow users 
to access the Internet from remote outdoor locations while on the move, in a manner 
similar to today’s cellular phone-only access. In many cases, a user may have simulta-
neous access to both wireless LANs and 4G. With the capacity of 4G systems being 
both more constrained and more expensive, many mobile devices default to the use  
of WiFi rather than 4G, when both are avilable. The question of whether wireless 
edge network access will be primarily over wireless LANs or cellular systems remains 
an open question:
•	

The emerging wireless LAN infrastructure may become nearly ubiquitous. IEEE 
802.11 wireless LANs, operating at 54 Mbps and higher, are enjoying widespread 
deployment. Essentially all laptops, tablets and smartphones are factory-equipped with 
802.11 LAN capabilities. Furthermore, emerging Internet appliances—such as wire-
less cameras and picture frames—also have low-powered wireless LAN capabilities.
•	

Wireless LAN base stations can also handle mobile phone appliances. Many 
phones are already capable of connecting to the cellular phone network or to an IP 
network either natively or using a Skype-like Voice-over-IP service, thus bypassing 
the operator’s cellular voice and 4G data services.
Of course, many other experts believe that 4G not only will be a major ­
success, 
but will also dramatically revolutionize the way we work and live. Most likely, 
both WiFi and 4G will both become prevalent wireless technologies, with roaming 
­
wireless devices automatically selecting the access technology that provides the best 
service at their current physical location.
Case History
7.4    •    Cellular Internet Access         581
When people talk about cellular technology, they often classify the technology 
as belonging to one of several “generations.” The earliest generations were designed 
primarily for voice traffic. First generation (1G) systems were analog FDMA systems 
designed exclusively for voice-only communication. These 1G systems are almost 
extinct now, having been replaced by digital 2G systems. The original 2G systems 
were also designed for voice, but later extended (2.5G) to support data (i.e., Internet) 
as well as voice service. 3G systems also support voice and data, but with an empha-
sis on data capabilities and higher-speed radio access links. The 4G systems being 
deployed today are based on LTE technology, feature an all-IP core network, and 
provide integrated voice and data at multi-Megabit speeds.
Cellular Network Architecture, 2G: Voice Connections to the 
­
Telephone Network
The term cellular refers to the fact that the region covered by a cellular network 
is partitioned into a number of geographic coverage areas, known as cells, shown 
as hexagons on the left side of Figure 7.18. As with the 802.11WiFi standard we 
­
studied in Section 7.3.1, GSM has its own particular nomenclature. Each cell 
 
Figure 7.18  ♦  Components of the GSM 2G cellular network architecture
BSC
BSC
MSC
Key:
Base transceiver station
(BTS)
Base station controller
(BSC)
Mobile switching center
(MSC)
Mobile subscribers
Gateway
MSC
Base Station System
(BSS)
Base Station System (BSS)
Public telephone
network
G
582         Chapter 7    •    Wireless and Mobile Networks
contains a base transceiver station (BTS) that transmits signals to and receives sig-
nals from the mobile stations in its cell. The coverage area of a cell depends on many 
factors, including the transmitting power of the BTS, the transmitting power of the 
user devices, obstructing buildings in the cell, and the height of base station antennas. 
Although Figure 7.18 shows each cell containing one base transceiver station residing 
in the middle of the cell, many systems today place the BTS at corners where three 
cells intersect, so that a single BTS with directional antennas can service three cells.
The GSM standard for 2G cellular systems uses combined FDM/TDM (radio) 
for the air interface. Recall from Chapter 1 that, with pure FDM, the channel is parti-
tioned into a number of frequency bands with each band devoted to a call. Also recall 
from Chapter 1 that, with pure TDM, time is partitioned into frames with each frame 
further partitioned into slots and each call being assigned the use of a particular slot 
in the revolving frame. In combined FDM/TDM systems, the channel is partitioned 
into a number of frequency sub-bands; within each sub-band, time is partitioned into 
frames and slots. Thus, for a combined FDM/TDM system, if the channel is parti-
tioned into F sub-bands and time is partitioned into T slots, then the channel will be 
able to support F.T simultaneous calls. Recall that we saw in Section 6.3.4 that cable 
access networks also use a combined FDM/TDM approach. GSM systems consist of 
200-kHz frequency bands with each band supporting eight TDM calls. GSM encodes 
speech at 13 kbps and 12.2 kbps.
A GSM network’s base station controller (BSC) will typically service several 
tens of base transceiver stations. The role of the BSC is to allocate BTS radio chan-
nels to mobile subscribers, perform paging (finding the cell in which a mobile user 
is resident), and perform handoff of mobile users—a topic we’ll cover shortly in 
Section 7.7.2. The base station controller and its controlled base transceiver stations 
collectively constitute a GSM base station subsystem (BSS).
As we’ll see in Section 7.7, the mobile switching center (MSC) plays the cen-
tral role in user authorization and accounting (e.g., determining whether a mobile 
device is allowed to connect to the cellular network), call establishment and tear-
down, and handoff. A single MSC will typically contain up to five BSCs, resulting in 
approximately 200K subscribers per MSC. A cellular provider’s network will have 
a number of MSCs, with special MSCs known as gateway MSCs connecting the 
provider’s cellular network to the larger public telephone network.
7.4.2	
3G Cellular Data Networks: Extending the Internet  
to Cellular Subscribers
Our discussion in Section 7.4.1 focused on connecting cellular voice users to the pub-
lic telephone network. But, of course, when we’re on the go, we’d also like to read 
e-mail, access the Web, get location-dependent services (e.g., maps and restaurant 
recommendations) and perhaps even watch streaming video. To do this, our smart-
phone will need to run a full TCP/IP protocol stack (including the physical link, net-
work, transport, and application layers) and connect into the Internet via the cellular 
7.4    •    Cellular Internet Access         583
data network. The topic of cellular data networks is a rather bewildering collection of 
competing and ever-evolving standards as one generation (and half-generation) suc-
ceeds the former and introduces new technologies and services with new acronyms. 
To make matters worse, there’s no single official body that sets requirements for 
2.5G, 3G, 3.5G, or 4G technologies, making it hard to sort out the differences among 
competing standards. In our discussion below, we’ll focus on the UMTS (Universal 
Mobile Telecommunications Service) 3G and 4G standards developed by the 3rd 
Generation Partnership project (3GPP) [3GPP 2016].
Let’s first take a top-down look at 3G cellular data network architecture shown 
in Figure 7.19.
Figure 7.19  ♦  3G system architecture
Gateway
MSC
G
Key:
Serving GPRS
Support Node
(SGSN)
Gateway GPRS
Support Node
(GGSN)
Radio Network
Controller (RNC)
GGSN
SGSN
G
G
MSC
Public telephone
network
Radio Interface
(WCDMA, HSPA)
Radio Access Network
Universal Terrestrial Radio
Access Network (UTRAN)
Core Network
General Packet Radio Service
(GPRS) Core Network
Public
Internet
Public
Internet
584         Chapter 7    •    Wireless and Mobile Networks
3G Core Network
The 3G core cellular data network connects radio access networks to the public Inter-
net. The core network interoperates with components of the existing cellular voice 
network (in particular, the MSC) that we previously encountered in Figure 7.18. 
Given the considerable amount of existing infrastructure (and profitable services!) 
in the existing cellular voice network, the approach taken by the designers of 3G 
data services is clear: leave the existing core GSM cellular voice network untouched, 
adding additional cellular data functionality in parallel to the existing cellular voice 
network. The alternative—integrating new data services directly into the core of the 
existing cellular voice network—would have raised the same challenges encountered 
in Section 4.3, where we discussed integrating new (IPv6) and legacy (IPv4) tech-
nologies in the Internet.
There are two types of nodes in the 3G core network: Serving GPRS Support 
Nodes (SGSNs) and Gateway GPRS Support Nodes (GGSNs). (GPRS stands for 
Generalized Packet Radio Service, an early cellular data service in 2G networks; 
here we discuss the evolved version of GPRS in 3G networks). An SGSN is respon-
sible for delivering datagrams to/from the mobile nodes in the radio access network 
to which the SGSN is attached. The SGSN interacts with the cellular voice network’s 
MSC for that area, providing user authorization and handoff, maintaining location 
(cell) information about active mobile nodes, and performing datagram forwarding 
between mobile nodes in the radio access network and a GGSN. The GGSN acts as 
a gateway, connecting multiple SGSNs into the larger Internet. A GGSN is thus the 
last piece of 3G infrastructure that a datagram originating at a mobile node encoun-
ters before entering the larger Internet. To the outside world, the GGSN looks like 
any other gateway router; the mobility of the 3G nodes within the GGSN’s network 
is hidden from the outside world behind the GGSN.
3G Radio Access Network: The Wireless Edge
The 3G radio access network is the wireless first-hop network that we see as a 3G 
user. The Radio Network Controller (RNC) typically controls several cell base 
transceiver stations similar to the base stations that we encountered in 2G systems 
(but officially known in 3G UMTS parlance as a “Node Bs”—a rather non-descrip-
tive name!). Each cell’s wireless link operates between the mobile nodes and a base 
transceiver station, just as in 2G networks. The RNC connects to both the circuit-
switched cellular voice network via an MSC, and to the packet-switched Internet via 
an SGSN. Thus, while 3G cellular voice and cellular data services use different core 
networks, they share a common first/last-hop radio access network.
A significant change in 3G UMTS over 2G networks is that rather than using 
GSM’s FDMA/TDMA scheme, UMTS uses a CDMA technique known as Direct 
Sequence Wideband CDMA (DS-WCDMA) [Dahlman 1998] within TDMA slots; 
TDMA slots, in turn, are available on multiple frequencies—an interesting use of 
7.4    •    Cellular Internet Access         585
all three dedicated channel-sharing approaches that we earlier identified in Chapter 
6 and similar to the approach taken in wired cable access networks (see Section 
6.3.4). This change requires a new 3G cellular wireless-access network operating 
in parallel with the 2G BSS radio network shown in Figure 7.19. The data service 
associated with the WCDMA specification is known as HSPA (High Speed Packet 
Access) and promises downlink data rates of up to 14 Mbps. Details regarding 3G 
networks can be found at the 3rd Generation Partnership Project (3GPP) Web site 
[3GPP 2016].
7.4.3	On to 4G: LTE
Fourth generation (4G) cellular systems are becoming widely deployed. In 2015, 
more than 50 countries had 4G coverage exceeding 50%. The 4G Long-Term 
­
Evolution (LTE) standard [Sauter 2014] put forward by the 3GPP has two important 
innovations over 3G systems an all-IP core network and an enhanced radio access 
network, as discussed below.
4G System Architecture: An All-IP Core Network
Figure 7.20 shows the overall 4G network architecture, which (unfortunately) intro-
duces yet another (rather impenetrable) new vocabulary and set of acronyms for 
Figure 7.20  ♦  4G network architecture
E-UTRAN
radio access
network 
all-IP Enhanced Packet Core (EPC)
Control plane
UE
eNodeB
MME
HHS
S-GW
P-GW
Data plane
586         Chapter 7    •    Wireless and Mobile Networks
­
network ­
components. But let’s not get lost in these acronyms! There are two impor-
tant high-level observations about the 4G architecture:
•	 A unified, all-IP network architecture. Unlike the 3G network shown in Figure 
7.19, which has separate network components and paths for voice and data traffic, 
the 4G architecture shown in Figure 7.20 is “all-IP”—both voice and data are car-
ried in IP datagrams to/from the wireless device (the User Equipment, UE in 4G 
parlance) to the gateway to the packet gateway (P-GW) that connects the 4G edge 
network to the rest of the network. With 4G, the last vestiges of cellular networks’ 
roots in the telephony have disappeared, giving way to universal IP service!
•	 A clear separation of the 4G data plane and 4G control plane. Mirroring our dis-
tinction between the data and control planes for IP’s network layer in Chapters 4 
 
and 5 respectively, the 4G network architecture also clearly separates the data and 
control planes. We’ll discuss their functionality below.
•	 A clear separation between the radio access network, and the all-IP-core ­
network. 
IP datagrams carrying user data are forwarded between the user (UE) and the 
gateway (P-GW in Figure 7.20) over a 4G-internal IP network to the external 
Internet. Control packets are exchanged over this same internal network among 
the 4G’s control services components, whose roles are described below.
The principal components of the 4G architecture are as follows.
•	 The eNodeB is the logical descendant of the 2G base station and the 3G Radio 
Network Controller (a.k.a Node B) and again plays a central role here. Its data-
plane role is to forward datagrams between UE (over the LTE radio access 
­
network) and the P-GW.
	
UE datagrams are encapsulated at the eNodeB and tunneled to the P-GW through 
the 4G network’s all-IP enhanced packet core (EPC). This tunneling between 
the eNodeB and P-GW is similar the tunneling we saw in Section 4.3 of IPv6 
datagrams between two IPv6 endpoints through a network of IPv4 routers. These 
tunnels may have associated quality of service (QoS) guarantees. For example, 
a 4G network may guarantee that voice traffic experiences no more than a 100 
msec delay between UE and P-GW, and has a packet loss rate of less than 1%; 
TCP traffic might have a guarantee of 300 msec and a packet loss rate of less than 
.0001% [Palat 2009]. We’ll cover QoS in Chapter 9.
	
In the control plane, the eNodeB handles registration and mobility signaling traf-
fic on behalf of the UE.
•	 The Packet Data Network Gateway (P-GW) allocates IP addresses to the UEs 
and performs QoS enforcement. As a tunnel endpoint it also performs datagram 
encapsulation/decapsulation when forwarding a datagram to/from a UE.
•	 The Serving Gateway (S-GW) is the data-plane mobility anchor point—all UE 
traffic will pass through the S-GW. The S-GW also performs charging/billing 
functions and lawful traffic interception.
7.4    •    Cellular Internet Access         587
•	 The Mobility Management Entity (MME) performs connection and mobility 
management on behalf of the UEs resident in the cell it controls. It receives UE 
subscription information from the HHS. We cover mobility in cellular networks 
in detail in Section 7.7.
•	 The Home Subscriber Server (HSS) contains UE information including roam-
ing access capabilities, quality of service profiles, and authentication information. 
As we’ll see in Section 7.7, the HSS obtains this information from the UE’s home 
cellular provider.
Very readable introductions to 4G network architecture and its EPC are [Motorola 
2007; Palat 2009; Sauter 2014].
LTE Radio Access Network
LTE uses a combination of frequency division multiplexing and time division multi-
plexing on the downstream channel, known as orthogonal frequency division multi-
plexing (OFDM) [Rohde 2008; Ericsson 2011]. (The term “orthogonal” comes from 
the fact the signals being sent on different frequency channels are created so that 
they interfere very little with each other, even when channel frequencies are tightly 
spaced). In LTE, each active mobile node is allocated one or more 0.5 ms time slots 
in one or more of the channel frequencies. Figure 7.21 shows an allocation of eight 
time slots over four frequencies. By being allocated increasingly more time slots 
(whether on the same frequency or on different frequencies), a mobile node is able 
to achieve increasingly higher transmission rates. Slot (re)allocation among mobile 
Figure 7.21  ♦  
Twenty 0.5 ms slots organized into 10 ms frames at each 
frequency. An eight-slot allocation is shown shaded.
f1
f2
f3
f4
f5
f6
0
0.5
1.0
1.5
2.0
2.5
9.0
9.5
10.0
588         Chapter 7    •    Wireless and Mobile Networks
nodes can be performed as often as once every millisecond. Different modulation 
schemes can also be used to change the transmission rate; see our earlier discussion 
of Figure 7.3 and dynamic selection of modulation schemes in WiFi networks.
The particular allocation of time slots to mobile nodes is not mandated by the 
LTE standard. Instead, the decision of which mobile nodes will be allowed to transmit 
in a given time slot on a given frequency is determined by the scheduling algorithms 
provided by the LTE equipment vendor and/or the network operator. With opportun-
istic scheduling [Bender 2000; Kolding 2003; Kulkarni 2005], matching the physical-
layer protocol to the channel conditions between the sender and receiver and choosing 
the receivers to which packets will be sent based on channel conditions allow the 
radio network controller to make best use of the wireless medium. In addition, user 
priorities and contracted levels of service (e.g., silver, gold, or platinum) can be used 
in scheduling downstream packet transmissions. In addition to the LTE capabilities 
described above, LTE-Advanced allows for downstream bandwidths of hundreds of 
Mbps by allocating aggregated channels to a mobile node [Akyildiz 2010].
An additional 4G wireless technology—WiMAX (World Interoperability for 
Microwave Access)—is a family of IEEE 802.16 standards that differ significantly 
from LTE. WiMAX has not yet been able to enjoy the widespread deployment of 
LTE. A detailed discussion of WiMAX can be found on this book’s Web site.
7.5	 Mobility Management: Principles
Having covered the wireless nature of the communication links in a wireless net-
work, it’s now time to turn our attention to the mobility that these wireless links 
enable. In the broadest sense, a mobile node is one that changes its point of attach-
ment into the network over time. Because the term mobility has taken on many mean-
ings in both the computer and telephony worlds, it will serve us well first to consider 
several dimensions of mobility in some detail.
•	 From the network layer’s standpoint, how mobile is a user? A physically mobile 
user will present a very different set of challenges to the network layer, depending 
on how he or she moves between points of attachment to the network. At one end 
of the spectrum in Figure 7.22, a user may carry a laptop with a wireless network 
interface card around in a building. As we saw in Section 7.3.4, this user is not 
mobile from a network-layer perspective. Moreover, if the user associates with 
the same access point regardless of location, the user is not even mobile from the 
perspective of the link layer.
	
At the other end of the spectrum, consider the user zooming along the autobahn 
in a BMW or Tesla at 150 kilometers per hour, passing through multiple wireless 
access networks and wanting to maintain an uninterrupted TCP connection to a 
remote application throughout the trip. This user is definitely mobile! In between 
7.5    •    Mobility Management: Principles         589
these extremes is a user who takes a laptop from one location (e.g., office or 
dormitory) into another (e.g., coffeeshop, classroom) and wants to connect into 
the-network in the new location. This user is also mobile (although less so than 
the BMW driver!) but does not need to maintain an ongoing connection while 
moving between points of attachment to the network. Figure 7.22 illustrates this 
spectrum of user mobility from the network layer’s perspective.
•	 How important is it for the mobile node’s address to always remain the same? 
With mobile telephony, your phone number—essentially the network-layer 
address of your phone—remains the same as you travel from one provider’s 
mobile phone network to another. Must a laptop similarly maintain the same IP 
address while moving between IP networks?
	
The answer to this question will depend strongly on the applications being run. 
For the BMW or Tesla driver who wants to maintain an uninterrupted TCP con-
nection to a remote application while zipping along the autobahn, it would be 
convenient to maintain the same IP address. Recall from Chapter 3 that an Internet 
application needs to know the IP address and port number of the remote entity 
with which it is communicating. If a mobile entity is able to maintain its IP address 
as it moves, mobility becomes invisible from the application standpoint. There is 
great value to this transparency—an application need not be concerned with a 
potentially changing IP address, and the same application code serves mobile 
and nonmobile connections alike. We’ll see in the following section that mobile 
IP provides this transparency, allowing a mobile node to maintain its permanent 
IP address while moving among networks.
	
On the other hand, a less glamorous mobile user might simply want to turn off 
an office laptop, bring that laptop home, power up, and work from home. If the 
laptop functions primarily as a client in client-server applications (e.g., send/read 
e-mail, browse the Web, Telnet to a remote host) from home, the particular IP 
address used by the laptop is not that important. In particular, one could get by 
fine with an address that is temporarily allocated to the laptop by the ISP serving 
the home. We saw in Section 4.3 that DHCP already provides this functionality.
Figure 7.22  ♦  
Various degrees of mobility, from the network layer’s point 
of view
User moves only
within same wireless
access network
No mobility
High mobility
User moves between
access networks,
shutting down while
moving between
networks
User moves between
access networks,
while maintaining
ongoing connections
590         Chapter 7    •    Wireless and Mobile Networks
•	 What supporting wired infrastructure is available? In all of our scenarios above, 
we’ve implicitly assumed that there is a fixed infrastructure to which the mobile 
user can connect—for example, the home’s ISP network, the wireless access net-
work in the office, or the wireless access networks lining the autobahn. What if 
no such infrastructure exists? If two users are within communication proximity 
of each other, can they establish a network connection in the absence of any 
other network-layer infrastructure? Ad hoc networking provides precisely these 
capabilities. This rapidly developing area is at the cutting edge of mobile net-
working research and is beyond the scope of this book. [Perkins 2000] and the 
IETF Mobile Ad Hoc Network (manet) working group Web pages [manet 2016] 
provide thorough treatments of the subject.
In order to illustrate the issues involved in allowing a mobile user to maintain 
ongoing connections while moving between networks, let’s consider a human anal-
ogy. A twenty-something adult moving out of the family home becomes mobile, 
living in a series of dormitories and/or apartments, and often changing addresses. If 
an old friend wants to get in touch, how can that friend find the address of her mobile 
friend? One common way is to contact the family, since a mobile adult will often reg-
ister his or her current address with the family (if for no other reason than so that the 
parents can send money to help pay the rent!). The family home, with its permanent 
address, becomes that one place that others can go as a first step in communicating 
with the mobile adult. Later communication from the friend may be either indirect 
(for example, with mail being sent first to the parents’ home and then forwarded to 
the mobile adult) or direct (for example, with the friend using the address obtained 
from the parents to send mail directly to her mobile friend).
In a network setting, the permanent home of a mobile node (such as a laptop or 
smartphone) is known as the home network, and the entity within the home network 
that performs the mobility management functions discussed below on behalf of the 
mobile node is known as the home agent. The network in which the mobile node is 
currently residing is known as the foreign (or visited) network, and the entity within 
the foreign network that helps the mobile node with the mobility management func-
tions discussed below is known as a foreign agent. For mobile professionals, their 
home network might likely be their company network, while the visited network 
might be the network of a colleague they are visiting. A correspondent is the entity 
wishing to communicate with the mobile node. Figure 7.23 illustrates these concepts, 
as well as addressing concepts considered below. In Figure 7.23, note that agents are 
shown as being collocated with routers (e.g., as processes running on routers), but 
alternatively they could be executing on other hosts or servers in the network.
7.5.1	Addressing
We noted above that in order for user mobility to be transparent to network applica-
tions, it is desirable for a mobile node to keep its address as it moves from one network 
7.5    •    Mobility Management: Principles         591
to another. When a mobile node is resident in a foreign network, all traffic addressed 
to the node’s permanent address now needs to be routed to the foreign network. How 
can this be done? One option is for the foreign network to advertise to all other net-
works that the mobile node is resident in its network. This could be via the usual 
exchange of intradomain and interdomain routing information and would require 
few changes to the existing routing infrastructure. The foreign network could simply 
advertise to its neighbors that it has a highly specific route to the mobile node’s per-
manent address (that is, essentially inform other networks that it has the correct path 
for routing datagrams to the mobile node’s permanent address; see Section 4.3). These 
neighbors would then propagate this routing information throughout the network as 
part of the normal procedure of updating routing information and forwarding tables. 
When the mobile node leaves one foreign network and joins another, the new foreign 
network would advertise a new, highly specific route to the mobile node, and the old 
foreign network would withdraw its routing information regarding the mobile node.
This solves two problems at once, and it does so without making significant 
changes to the network-layer infrastructure. Other networks know the location of 
Figure 7.23  ♦  Initial elements of a mobile network architecture
Home agent
Home network:
128.119.40/24
Visited network:
79.129.13/24
Mobile node
Permanent address:
128.119.40.186
Permanent address:
128.119.40.186
Foreign agent
Care-of address:
79.129.13.2
Correspondent
Wide area
network
592         Chapter 7    •    Wireless and Mobile Networks
the mobile node, and it is easy to route datagrams to the mobile node, since the for-
warding tables will direct datagrams to the foreign network. A significant drawback, 
however, is that of scalability. If mobility management were to be the responsibility 
of network routers, the routers would have to maintain forwarding table entries for 
potentially millions of mobile nodes, and update these entries as nodes move. Some 
additional drawbacks are explored in the problems at the end of this chapter.
An alternative approach (and one that has been adopted in practice) is to push 
mobility functionality from the network core to the network edge—a recurring theme 
in our study of Internet architecture. A natural way to do this is via the mobile node’s 
home network. In much the same way that parents of the mobile twenty-something 
track their child’s location, the home agent in the mobile node’s home network can 
track the foreign network in which the mobile node resides. A protocol between the 
mobile node (or a foreign agent representing the mobile node) and the home agent 
will certainly be needed to update the mobile node’s location.
Let’s now consider the foreign agent in more detail. The conceptually simplest 
approach, shown in Figure 7.23, is to locate foreign agents at the edge routers in the 
foreign network. One role of the foreign agent is to create a so-called care-of address 
(COA) for the mobile node, with the network portion of the COA matching that of 
the foreign network. There are thus two addresses associated with a mobile node, 
its permanent address (analogous to our mobile youth’s family’s home address) 
and its COA, sometimes known as a foreign address (analogous to the address of 
the house in which our mobile youth is currently residing). In the example in Figure 
7.23, the permanent address of the mobile node is 128.119.40.186. When visiting 
network 79.129.13/24, the mobile node has a COA of 79.129.13.2. A second role of 
the foreign agent is to inform the home agent that the mobile node is resident in its 
(the foreign agent’s) network and has the given COA. We’ll see shortly that the COA 
will be used to “reroute” datagrams to the mobile node via its foreign agent.
Although we have separated the functionality of the mobile node and the foreign 
agent, it is worth noting that the mobile node can also assume the responsibilities of 
the foreign agent. For example, the mobile node could obtain a COA in the foreign 
network (for example, using a protocol such as DHCP) and itself inform the home 
agent of its COA.
7.5.2	Routing to a Mobile Node
We have now seen how a mobile node obtains a COA and how the home agent 
can be informed of that address. But having the home agent know the COA solves 
only part of the problem. How should datagrams be addressed and forwarded to the 
mobile node? Since only the home agent (and not network-wide routers) knows the 
location of the mobile node, it will no longer suffice to simply address a datagram to 
the mobile node’s permanent address and send it into the network-layer infrastruc-
ture. Something more must be done. Two approaches can be identified, which we 
will refer to as indirect and direct routing.
7.5    •    Mobility Management: Principles         593
Indirect Routing to a Mobile Node
Let’s first consider a correspondent that wants to send a datagram to a mobile node. 
In the indirect routing approach, the correspondent simply addresses the datagram 
to the mobile node’s permanent address and sends the datagram into the network, 
blissfully unaware of whether the mobile node is resident in its home network or is 
visiting a foreign network; mobility is thus completely transparent to the correspond-
ent. Such datagrams are first routed, as usual, to the mobile node’s home network. 
This is illustrated in step 1 in Figure 7.24.
Let’s now turn our attention to the home agent. In addition to being responsible 
for interacting with a foreign agent to track the mobile node’s COA, the home agent 
has another very important function. Its second job is to be on the lookout for arriving 
datagrams addressed to nodes whose home network is that of the home agent but that 
are currently resident in a foreign network. The home agent intercepts these datagrams 
and then forwards them to a mobile node in a two-step process. The datagram is first 
forwarded to the foreign agent, using the mobile node’s COA (step 2 in Figure 7.24), 
and then forwarded from the foreign agent to the mobile node (step 3 in Figure 7.24).
Figure 7.24  ♦  Indirect routing to a mobile node
Home
agent
Home network:
128.119.40/24
Visited network:
79.129.13/24
Mobile node
Permanent address:
128.119.40.186
Permanent address:
128.119.40.186
Foreign
agent
Care-of
address:
79.129.13.2
Wide area
network
Correspondent
1
2
4
3
594         Chapter 7    •    Wireless and Mobile Networks
It is instructive to consider this rerouting in more detail. The home agent will 
need to address the datagram using the mobile node’s COA, so that the network layer 
will route the datagram to the foreign network. On the other hand, it is desirable to 
leave the correspondent’s datagram intact, since the application receiving the data-
gram should be unaware that the datagram was forwarded via the home agent. Both 
goals can be satisfied by having the home agent encapsulate the correspondent’s 
original complete datagram within a new (larger) datagram. This larger datagram is 
addressed and delivered to the mobile node’s COA. The foreign agent, who “owns” 
the COA, will receive and decapsulate the datagram—that is, remove the correspond-
ent’s original datagram from within the larger encapsulating datagram and forward 
(step 3 in Figure 7.24) the original datagram to the mobile node. Figure 7.25 shows a 
correspondent’s original datagram being sent to the home network, an encapsulated 
datagram being sent to the foreign agent, and the original datagram being delivered 
to the mobile node. The sharp reader will note that the encapsulation/decapsulation 
described here is identical to the notion of tunneling, discussed in Section 4.3 in the 
context of IP multicast and IPv6.
Let’s next consider how a mobile node sends datagrams to a correspondent. 
This is quite simple, as the mobile node can address its datagram directly to the 
correspondent (using its own permanent address as the source address, and the 
Figure 7.25  ♦  Encapsulation and decapsulation
Home
agent
Permanent address:
128.119.40.186
Permanent address:
128.119.40.186
Foreign
agent
Correspondent
dest: 128.119.40.186
dest: 79.129.13.2
dest: 128.119.40.186
dest: 128.119.40.186
Care-of address:
79.129.13.2
7.5    •    Mobility Management: Principles         595
correspondent’s address as the destination address). Since the mobile node knows 
the correspondent’s address, there is no need to route the datagram back through the 
home agent. This is shown as step 4 in Figure 7.24.
Let’s summarize our discussion of indirect routing by listing the new network-
layer functionality required to support mobility.
•	 A mobile-node–to–foreign-agent protocol. The mobile node will register with the 
foreign agent when attaching to the foreign network. Similarly, a mobile node 
will deregister with the foreign agent when it leaves the foreign network.
•	 A foreign-agent–to–home-agent registration protocol. The foreign agent will 
register the mobile node’s COA with the home agent. A foreign agent need not 
explicitly deregister a COA when a mobile node leaves its network, because the 
subsequent registration of a new COA, when the mobile node moves to a new 
network, will take care of this.
•	 A home-agent datagram encapsulation protocol. Encapsulation and forward-
ing of the correspondent’s original datagram within a datagram addressed to the 
COA.
•	 A foreign-agent decapsulation protocol. Extraction of the correspondent’s origi-
nal datagram from the encapsulating datagram, and the forwarding of the original 
datagram to the mobile node.
The previous discussion provides all the pieces—foreign agents, the home 
agent, and indirect forwarding—needed for a mobile node to maintain an ongoing 
connection while moving among networks. As an example of how these pieces fit 
together, assume the mobile node is attached to foreign network A, has registered a 
COA in network A with its home agent, and is receiving datagrams that are being 
indirectly routed through its home agent. The mobile node now moves to foreign 
network B and registers with the foreign agent in network B, which informs the 
home agent of the mobile node’s new COA. From this point on, the home agent will 
reroute datagrams to foreign network B. As far as a correspondent is concerned, 
mobility is transparent—datagrams are routed via the same home agent both before 
and after the move. As far as the home agent is concerned, there is no disruption in 
the flow of datagrams—arriving datagrams are first forwarded to foreign network 
A; after the change in COA, datagrams are forwarded to foreign network B. But 
will the mobile node see an interrupted flow of datagrams as it moves between net-
works? As long as the time between the mobile node’s disconnection from network 
A (at which point it can no longer receive datagrams via A) and its attachment to 
network B (at which point it will register a new COA with its home agent) is small, 
few datagrams will be lost. Recall from Chapter 3 that end-to-end connections can 
suffer datagram loss due to network congestion. Hence occasional datagram loss 
within a connection when a node moves between networks is by no means a cata-
strophic problem. If loss-free communication is required, upper-layer mechanisms 
596         Chapter 7    •    Wireless and Mobile Networks
will recover from datagram loss, whether such loss results from network congestion 
or from user mobility.
An indirect routing approach is used in the mobile IP standard [RFC 5944], as 
discussed in Section 7.6.
Direct Routing to a Mobile Node
The indirect routing approach illustrated in Figure 7.24 suffers from an inef-
ficiency known as the triangle routing problem—datagrams addressed to the 
mobile node must be routed first to the home agent and then to the foreign net-
work, even when a much more efficient route exists between the correspondent 
and the mobile node. In the worst case, imagine a mobile user who is visiting the 
foreign network of a colleague. The two are sitting side by side and exchanging 
data over the network. Datagrams from the correspondent (in this case the col-
league of the visitor) are routed to the mobile user’s home agent and then back 
again to the foreign network!
Direct routing overcomes the inefficiency of triangle routing, but does so at 
the cost of additional complexity. In the direct routing approach, a correspondent 
agent in the correspondent’s network first learns the COA of the mobile node. This 
can be done by having the correspondent agent query the home agent, assuming that 
(as in the case of indirect routing) the mobile node has an up-to-date value for its 
COA registered with its home agent. It is also possible for the correspondent itself to 
perform the function of the correspondent agent, just as a mobile node could perform 
the function of the foreign agent. This is shown as steps 1 and 2 in Figure 7.26. The 
correspondent agent then tunnels datagrams directly to the mobile node’s COA, in 
a manner analogous to the tunneling performed by the home agent, steps 3 and 4 in 
Figure 7.26.
While direct routing overcomes the triangle routing problem, it introduces two 
important additional challenges:
•	 A mobile-user location protocol is needed for the correspondent agent to query 
the home agent to obtain the mobile node’s COA (steps 1 and 2 in Figure 7.26).
•	 When the mobile node moves from one foreign network to another, how will data 
now be forwarded to the new foreign network? In the case of indirect routing, this 
problem was easily solved by updating the COA maintained by the home agent. 
However, with direct routing, the home agent is queried for the COA by the cor-
respondent agent only once, at the beginning of the session. Thus, updating the 
COA at the home agent, while necessary, will not be enough to solve the problem 
of routing data to the mobile node’s new foreign network.
One solution would be to create a new protocol to notify the correspondent of 
the changing COA. An alternate solution, and one that we’ll see adopted in practice 
7.5    •    Mobility Management: Principles         597
in GSM networks, works as follows. Suppose data is currently being forwarded to 
the mobile node in the foreign network where the mobile node was located when 
the session first started (step 1 in Figure 7.27). We’ll identify the foreign agent 
 
in that foreign network where the mobile node was first found as the anchor 
­
foreign agent. When the mobile node moves to a new foreign network (step 2 in 
Figure 7.27), the mobile node registers with the new foreign agent (step 3), and the 
new foreign agent provides the anchor foreign agent with the mobile node’s new 
COA (step 4). When the anchor foreign agent receives an encapsulated datagram 
for a departed mobile node, it can then re-encapsulate the datagram and forward 
it to the mobile node (step 5) using the new COA. If the mobile node later moves 
yet again to a new foreign network, the foreign agent in that new visited network 
would then contact the anchor foreign agent in order to set up forwarding to this 
new foreign network.
Figure 7.26  ♦  Direct routing to a mobile user
Home
agent
Home network:
128.119.40/24
Visited network:
79.129.13/24
Mobile node
Permanent address:
128.119.40.186
Key:
Permanent address:
128.119.40.186
Foreign
agent
Care-of address:
79.129.13.2
Wide area
network
Correspondent
Control messages
Correspondent
agent
1
2
3
Data ﬂow
4
598         Chapter 7    •    Wireless and Mobile Networks
Figure 7.27  ♦  Mobile transfer between networks with direct routing
Home
agent
Home network:
Foreign network
being visited at
session start:
New foreign
network:
Anchor
foreign
agent
New foreign agent
Wide area
network
Correspondent
Correspondent
agent
1
4
2
3
5
7.6	 Mobile IP
The Internet architecture and protocols for supporting mobility, collectively known as 
mobile IP, are defined primarily in RFC 5944 for IPv4. Mobile IP is a flexible standard, 
supporting many different modes of operation (for example, operation with or without 
a foreign agent), multiple ways for agents and mobile nodes to discover each other, use 
of single or multiple COAs, and multiple forms of encapsulation. As such, mobile IP is 
a complex standard, and would require an entire book to describe in detail; indeed one 
such book is [Perkins 1998b]. Our modest goal here is to provide an overview of the most 
important aspects of mobile IP and to illustrate its use in a few common-case scenarios.
The mobile IP architecture contains many of the elements we have considered 
above, including the concepts of home agents, foreign agents, care-of addresses, and 
encapsulation/decapsulation. The current standard [RFC 5944] specifies the use of 
indirect routing to the mobile node.
The mobile IP standard consists of three main pieces:
•	 Agent discovery. Mobile IP defines the protocols used by a home or foreign agent 
to advertise its services to mobile nodes, and protocols for mobile nodes to solicit 
the services of a foreign or home agent.
7.6    •    Mobile IP         599
•	 Registration with the home agent. Mobile IP defines the protocols used by the 
mobile node and/or foreign agent to register and deregister COAs with a mobile 
node’s home agent.
•	 Indirect routing of datagrams. The standard also defines the manner in which 
datagrams are forwarded to mobile nodes by a home agent, including rules for 
forwarding datagrams, rules for handling error conditions, and several forms of 
encapsulation [RFC 2003, RFC 2004].
Security considerations are prominent throughout the mobile IP standard. 
For example, authentication of a mobile node is clearly needed to ensure that a 
­
malicious user does not register a bogus care-of address with a home agent, which 
could cause all datagrams addressed to an IP address to be redirected to the mali-
cious user. Mobile IP achieves security using many of the mechanisms that we 
will examine in Chapter 8, so we will not address security considerations in our 
discussion below.
Agent Discovery
A mobile IP node arriving to a new network, whether attaching to a foreign network 
or returning to its home network, must learn the identity of the corresponding for-
eign or home agent. Indeed it is the discovery of a new foreign agent, with a new 
network address, that allows the network layer in a mobile node to learn that it has 
moved into a new foreign network. This process is known as agent discovery. Agent 
discovery can be accomplished in one of two ways: via agent advertisement or via 
agent solicitation.
With agent advertisement, a foreign or home agent advertises its services using 
an extension to the existing router discovery protocol [RFC 1256]. The agent peri-
odically broadcasts an ICMP message with a type field of 9 (router discovery) on all 
links to which it is connected. The router discovery message contains the IP address 
of the router (that is, the agent), thus allowing a mobile node to learn the agent’s IP 
address. The router discovery message also contains a mobility agent advertisement 
extension that contains additional information needed by the mobile node. Among 
the more important fields in the extension are the following:
•	 Home agent bit (H). Indicates that the agent is a home agent for the network in 
which it resides.
•	 Foreign agent bit (F). Indicates that the agent is a foreign agent for the network 
in which it resides.
•	 Registration required bit (R). Indicates that a mobile user in this network must 
register with a foreign agent. In particular, a mobile user cannot obtain a care-of 
address in the foreign network (for example, using DHCP) and assume the func-
tionality of the foreign agent for itself, without registering with the foreign agent.
600         Chapter 7    •    Wireless and Mobile Networks
•	 M, G encapsulation bits. Indicate whether a form of encapsulation other than IP-
in-IP encapsulation will be used.
•	 Care-of address (COA) fields. A list of one or more care-of addresses provided 
by the foreign agent. In our example below, the COA will be associated with the 
foreign agent, who will receive datagrams sent to the COA and then forward them 
to the appropriate mobile node. The mobile user will select one of these addresses 
as its COA when registering with its home agent.
Figure 7.28 illustrates some of the key fields in the agent advertisement message.
With agent solicitation, a mobile node wanting to learn about agents without 
waiting to receive an agent advertisement can broadcast an agent solicitation mes-
sage, which is simply an ICMP message with type value 10. An agent receiving the 
solicitation will unicast an agent advertisement directly to the mobile node, which 
can then proceed as if it had received an unsolicited advertisement.
Registration with the Home Agent
Once a mobile IP node has received a COA, that address must be registered with the 
home agent. This can be done either via the foreign agent (who then registers the 
Type = 9
Code = 0
Type = 16
Length
Sequence number
Registration lifetime
Reserved
RBHFMGrT
bits
Checksum
Standard
ICMP ﬁelds
0
8
16
24
Router address
0 or more care-of addresses
Mobility agent
advertisement
extension
Figure 7.28  ♦  
ICMP router discovery message with mobility agent 
­
advertisement extension
7.6    •    Mobile IP         601
COA with the home agent) or directly by the mobile IP node itself. We consider the 
former case below. Four steps are involved.
	1.	 Following the receipt of a foreign agent advertisement, a mobile node sends a 
mobile IP registration message to the foreign agent. The registration message is 
carried within a UDP datagram and sent to port 434. The registration message 
carries a COA advertised by the foreign agent, the address of the home agent 
(HA), the permanent address of the mobile node (MA), the requested life-
time of the registration, and a 64-bit registration identification. The requested 
registration lifetime is the number of seconds that the registration is to be 
valid. If the registration is not renewed at the home agent within the specified 
lifetime, the registration will become invalid. The registration identifier acts 
like a sequence number and serves to match a received registration reply with a 
registration request, as discussed below.
	2.	 The foreign agent receives the registration message and records the mobile 
node’s permanent IP address. The foreign agent now knows that it should be 
looking for datagrams containing an encapsulated datagram whose destination 
address matches the permanent address of the mobile node. The foreign agent 
then sends a mobile IP registration message (again, within a UDP datagram) 
to port 434 of the home agent. The message contains the COA, HA, MA, 
encapsulation format requested, requested registration lifetime, and registration 
identification.
	3.	 The home agent receives the registration request and checks for authentic-
ity and correctness. The home agent binds the mobile node’s permanent IP 
address with the COA; in the future, datagrams arriving at the home agent 
and addressed to the mobile node will now be encapsulated and tunneled to 
the COA. The home agent sends a mobile IP registration reply containing the 
HA, MA, actual registration lifetime, and the registration identification of the 
request that is being satisfied with this reply.
	4.	 The foreign agent receives the registration reply and then forwards it to the 
mobile node.
At this point, registration is complete, and the mobile node can receive data-
grams sent to its permanent address. Figure 7.29 illustrates these steps. Note that 
the home agent specifies a lifetime that is smaller than the lifetime requested by the 
mobile node.
A foreign agent need not explicitly deregister a COA when a mobile node 
leaves its network. This will occur automatically, when the mobile node moves to a 
new network (whether another foreign network or its home network) and registers 
a new COA.
The mobile IP standard allows many additional scenarios and capabilities in 
addition to those described previously. The interested reader should consult [Perkins 
1998b; RFC 5944].
602         Chapter 7    •    Wireless and Mobile Networks
7.7	 Managing Mobility in Cellular Networks
Having examined how mobility is managed in IP networks, let’s now turn our 
attention to networks with an even longer history of supporting mobility—cellular 
telephony networks. Whereas we focused on the first-hop wireless link in cellular 
 
Figure 7.29  ♦  Agent advertisement and mobile IP registration
Home agent
HA: 128.119.40.7
Mobile agent
MA: 128.119.40.186
Visited network:
79.129.13/24
ICMP agent adv.
COA: 79.129.13.2
. . .
COA: 79.129.13.2
HA:128.119.40.7
MA: 128.119.40.186
Lifetime: 9999
identiﬁcation: 714
. . .
Registration req.
COA: 79.129.13.2
HA:128.119.40.7
MA: 128.119.40.186
Lifetime: 9999
identiﬁcation: 714
encapsulation format
. . .
Registration req.
Time
Time
Time
HA: 128.119.40.7
MA: 128.119.40.186
Lifetime: 4999
identiﬁcation: 714
encapsulation format
. . .
Registration reply
HA: 128.119.40.7
MA: 128.119.40.186
Lifetime: 4999
identiﬁcation: 714
. . .
Registration reply
Foreign agent
COA: 79.129.13.2
7.7    •    Managing Mobility in Cellular Networks         603
networks in Section 7.4, we’ll focus here on mobility, using the GSM cellular net-
work [Goodman 1997; Mouly 1992; Scourias 2012; Kaaranen 2001; Korhonen 2003; 
Turner 2012] as our case study, since it is a mature and widely deployed technology. 
Mobility in 3G and 4G networks is similar in principle to that used in GSM. As in the 
case of mobile IP, we’ll see that a number of the fundamental principles we identified 
in Section 7.5 are embodied in GSM’s network architecture.
Like mobile IP, GSM adopts an indirect routing approach (see Section 7.5.2), 
first routing the correspondent’s call to the mobile user’s home network and 
from there to the visited network. In GSM terminology, the mobile users’s home 
network is referred to as the mobile user’s home public land mobile network 
(home PLMN). Since the PLMN acronym is a bit of a mouthful, and mindful of 
our quest to avoid an alphabet soup of acronyms, we’ll refer to the GSM home 
PLMN simply as the home network. The home network is the cellular provider 
with which the mobile user has a subscription (i.e., the provider that bills the 
user for monthly cellular service). The visited PLMN, which we’ll refer to sim-
ply as the visited network, is the network in which the mobile user is currently 
residing.
As in the case of mobile IP, the responsibilities of the home and visited networks 
are quite different.
•	 The home network maintains a database known as the home location register 
(HLR), which contains the permanent cell phone number and subscriber pro-
file information for each of its subscribers. Importantly, the HLR also contains 
information about the current locations of these subscribers. That is, if a mobile 
user is currently roaming in another provider’s cellular network, the HLR 
contains enough information to obtain (via a process we’ll describe shortly) 
an address in the visited network to which a call to the mobile user should 
be routed. As we’ll see, a special switch in the home network, known as the 
Gateway Mobile services Switching Center (GMSC) is contacted by a corre-
spondent when a call is placed to a mobile user. Again, in our quest to avoid an 
alphabet soup of acronyms, we’ll refer to the GMSC here by a more descriptive 
term, home MSC.
•	 The visited network maintains a database known as the visitor location register 
(VLR). The VLR contains an entry for each mobile user that is currently in the 
portion of the network served by the VLR. VLR entries thus come and go as 
mobile users enter and leave the network. A VLR is usually co-located with the 
mobile switching center (MSC) that coordinates the setup of a call to and from the 
visited network.
In practice, a provider’s cellular network will serve as a home network for its 
subscribers and as a visited network for mobile users whose subscription is with a 
different cellular provider.
604         Chapter 7    •    Wireless and Mobile Networks
7.7.1	Routing Calls to a Mobile User
We’re now in a position to describe how a call is placed to a mobile GSM user in a 
visited network. We’ll consider a simple example below; more complex scenarios 
are described in [Mouly 1992]. The steps, as illustrated in Figure 7.30, are as follows:
	1.	 The correspondent dials the mobile user’s phone number. This number itself 
does not refer to a particular telephone line or location (after all, the phone 
number is fixed and the user is mobile!). The leading digits in the number are 
sufficient to globally identify the mobile’s home network. The call is routed 
from the correspondent through the PSTN to the home MSC in the mobile’s 
home network. This is the first leg of the call.
	2.	 The home MSC receives the call and interrogates the HLR to determine the 
location of the mobile user. In the simplest case, the HLR returns the mobile 
station roaming number (MSRN), which we will refer to as the roaming 
number. Note that this number is different from the mobile’s permanent phone 
number, which is associated with the mobile’s home network. The roaming 
number is ephemeral: It is temporarily assigned to a mobile when it enters a 
visited network. The roaming number serves a role similar to that of the care-of  
Figure 7.30  ♦  Placing a call to a mobile user: Indirect routing
Mobile
user
Visited
network
Home
network
Public switched 
telephone
network
1
3
Correspondent
VLR
HLR
2
7.7    •    Managing Mobility in Cellular Networks         605
address in mobile IP and, like the COA, is invisible to the correspondent and  
the mobile. If HLR does not have the roaming number, it returns the address of 
the VLR in the visited network. In this case (not shown in Figure 7.30), the home 
MSC will need to query the VLR to obtain the roaming number of the mobile 
node. But how does the HLR get the roaming number or the VLR address in  
the first place? What happens to these values when the mobile user moves to 
another visited network? We’ll consider these important questions shortly.
	3.	 Given the roaming number, the home MSC sets up the second leg of the call 
through the network to the MSC in the visited network. The call is completed, 
being routed from the correspondent to the home MSC, and from there to the 
visited MSC, and from there to the base station serving the mobile user.
An unresolved question in step 2 is how the HLR obtains information about the 
location of the mobile user. When a mobile telephone is switched on or enters a part 
of a visited network that is covered by a new VLR, the mobile must register with the 
visited network. This is done through the exchange of signaling messages between 
the mobile and the VLR. The visited VLR, in turn, sends a location update request 
message to the mobile’s HLR. This message informs the HLR of either the roaming 
number at which the mobile can be contacted, or the address of the VLR (which can 
then later be queried to obtain the mobile number). As part of this exchange, the VLR 
also obtains subscriber information from the HLR about the mobile and determines 
what services (if any) should be accorded the mobile user by the visited network.
7.7.2	Handoffs in GSM
A handoff occurs when a mobile station changes its association from one base sta-
tion to another during a call. As shown in Figure 7.31, a mobile’s call is initially 
(before handoff) routed to the mobile through one base station (which we’ll refer to 
as the old base station), and after handoff is routed to the mobile through another base 
Figure 7.31  ♦  
Handoff scenario between base stations with a common MSC
Old BS
New BS
Old
routing
New
routing
VLR
606         Chapter 7    •    Wireless and Mobile Networks
station (which we’ll refer to as the new base station). Note that a handoff between 
base stations results not only in the mobile transmitting/receiving to/from a new base 
station, but also in the rerouting of the ongoing call from a switching point within 
the network to the new base station. Let’s initially assume that the old and new base 
stations share the same MSC, and that the rerouting occurs at this MSC.
There may be several reasons for handoff to occur, including (1) the signal 
between the current base station and the mobile may have deteriorated to such an 
extent that the call is in danger of being dropped, and (2) a cell may have become 
overloaded, handling a large number of calls. This congestion may be alleviated by 
handing off mobiles to less congested nearby cells.
While it is associated with a base station, a mobile periodically measures the 
strength of a beacon signal from its current base station as well as beacon signals 
from nearby base stations that it can “hear.” These measurements are reported once or 
twice a second to the mobile’s current base station. Handoff in GSM is initiated by the 
old base station based on these measurements, the current loads of mobiles in nearby 
cells, and other factors [Mouly 1992]. The GSM standard does not specify the specific 
algorithm to be used by a base station to determine whether or not to perform handoff.
Figure 7.32 illustrates the steps involved when a base station does decide to hand 
off a mobile user:
	1.	 The old base station (BS) informs the visited MSC that a handoff is to be per-
formed and the BS (or possible set of BSs) to which the mobile is to be handed off.
	2.	 The visited MSC initiates path setup to the new BS, allocating the resources 
needed to carry the rerouted call, and signaling the new BS that a handoff is 
about to occur.
	3.	 The new BS allocates and activates a radio channel for use by the mobile.
	4.	 The new BS signals back to the visited MSC and the old BS that the visited-
MSC-to-new-BS path has been established and that the mobile should be 
Figure 7.32  ♦  
Steps in accomplishing a handoff between base stations 
with a common MSC
Old
BS
New
BS
1
5
7
8
2
3
6
4
VLR
7.7    •    Managing Mobility in Cellular Networks         607
informed of the impending handoff. The new BS provides all of the informa-
tion that the mobile will need to associate with the new BS.
	5.	 The mobile is informed that it should perform a handoff. Note that up until this 
point, the mobile has been blissfully unaware that the network has been laying 
the groundwork (e.g., allocating a channel in the new BS and allocating a path 
from the visited MSC to the new BS) for a handoff.
	6.	 The mobile and the new BS exchange one or more messages to fully activate 
the new channel in the new BS.
	7.	 The mobile sends a handoff complete message to the new BS, which is for-
warded up to the visited MSC. The visited MSC then reroutes the ongoing call 
to the mobile via the new BS.
	8.	 The resources allocated along the path to the old BS are then released.
Let’s conclude our discussion of handoff by considering what happens when the 
mobile moves to a BS that is associated with a different MSC than the old BS, and what 
happens when this inter-MSC handoff occurs more than once. As shown in Figure 7.33, 
GSM defines the notion of an anchor MSC. The anchor MSC is the MSC visited by 
the mobile when a call first begins; the anchor MSC thus remains unchanged during 
the call. Throughout the call’s duration and regardless of the number of inter-MSC 
Figure 7.33  ♦  Rerouting via the anchor MSC
Home network
Correspondent
a.  Before handoff
Anchor
MSC
PSTN
b.  After handoff
Correspondent
Anchor
MSC
PSTN
Home network
608         Chapter 7    •    Wireless and Mobile Networks
transfers performed by the mobile, the call is routed from the home MSC to the anchor 
MSC, and then from the anchor MSC to the visited MSC where the mobile is cur-
rently located. When a mobile moves from the coverage area of one MSC to another, 
the ongoing call is rerouted from the anchor MSC to the new visited MSC containing 
 
the new base station. Thus, at all times there are at most three MSCs (the home MSC, 
the anchor MSC, and the visited MSC) between the correspondent and the mobile. 
Figure 7.33 illustrates the routing of a call among the MSCs visited by a mobile user.
Rather than maintaining a single MSC hop from the anchor MSC to the current 
MSC, an alternative approach would have been to simply chain the MSCs visited by 
the mobile, having an old MSC forward the ongoing call to the new MSC each time 
the mobile moves to a new MSC. Such MSC chaining can in fact occur in IS-41 cel-
lular networks, with an optional path minimization step to remove MSCs between 
the anchor MSC and the current visited MSC [Lin 2001].
Let’s wrap up our discussion of GSM mobility management with a compari-
son of mobility management in GSM and Mobile IP. The comparison in Table 7.2 
indicates that although IP and cellular networks are fundamentally different in many 
ways, they share a surprising number of common functional elements and overall 
approaches in handling mobility.
7.8	 Wireless and Mobility: Impact on ­
Higher-
Layer Protocols
In this chapter, we’ve seen that wireless networks differ significantly from their 
wired counterparts at both the link layer (as a result of wireless channel charac-
teristics such as fading, multipath, and hidden terminals) and at the network layer 
 
Table 7.2  ♦  Commonalities between mobile IP and GSM mobility
GSM element
Comment on GSM element
Mobile IP element
Home system
Network to which the mobile user's permanent phone number belongs.
Home network
Gateway mobile switching center or 
simply home MSC, Home location 
register (HLR)
Home MSC: point of contact to obtain routable address of mobile user. HLR: 
database in home system containing permanent phone number, profile 
information, current location of mobile user, subscription information.
Home agent
Visited system
Network other than home system where mobile user is currently residing.
Visited network
Visited mobile services switching  
center, Visitor location register (VLR)
Visited MSC: responsible for setting up calls to/from mobile nodes in cells 
associated with MSC. VLR: temporary database entry in visited system, 
containing subscription information for each visiting mobile user.
Foreign agent
Mobile station roaming number 
(MSRN) or simply roaming number
Routable address for telephone call segment between home MSC and visited 
MSC, visible to neither the mobile nor the correspondent.
Care-of address
7.8    •    Wireless and Mobility: Impact on ­
Higher-Layer Protocols         609
(as a result of mobile users who change their points of attachment to the network). 
But are there important differences at the transport and application layers? It’s tempt-
ing to think that these differences will be minor, since the network layer provides the 
same best-effort delivery service model to upper layers in both wired and wireless 
networks. Similarly, if protocols such as TCP or UDP are used to provide transport-
layer services to applications in both wired and wireless networks, then the applica-
tion layer should remain unchanged as well. In one sense our intuition is right—TCP 
and UDP can (and do) operate in networks with wireless links. On the other hand, 
transport protocols in general, and TCP in particular, can sometimes have very dif-
ferent performance in wired and wireless networks, and it is here, in terms of perfor-
mance, that differences are manifested. Let’s see why.
Recall that TCP retransmits a segment that is either lost or corrupted on the path 
between sender and receiver. In the case of mobile users, loss can result from either 
network congestion (router buffer overflow) or from handoff (e.g., from delays in 
rerouting segments to a mobile’s new point of attachment to the network). In all 
cases, TCP’s receiver-to-sender ACK indicates only that a segment was not received 
intact; the sender is unaware of whether the segment was lost due to congestion, 
during handoff, or due to detected bit errors. In all cases, the sender’s response is 
the same—to retransmit the segment. TCP’s congestion-control response is also the 
same in all cases—TCP decreases its congestion window, as discussed in Section 
3.7. By unconditionally decreasing its congestion window, TCP implicitly assumes 
that segment loss results from congestion rather than corruption or handoff. We saw 
in Section 7.2 that bit errors are much more common in wireless networks than in 
wired networks. When such bit errors occur or when handoff loss occurs, there’s 
really no reason for the TCP sender to decrease its congestion window (and thus 
decrease its sending rate). Indeed, it may well be the case that router buffers are 
empty and packets are flowing along the end-to-end path unimpeded by congestion.
Researchers realized in the early to mid 1990s that given high bit error rates on 
wireless links and the possibility of handoff loss, TCP’s congestion-control response 
could be problematic in a wireless setting. Three broad classes of approaches are 
possible for dealing with this problem:
•	 Local recovery. Local recovery protocols recover from bit errors when and where 
(e.g., at the wireless link) they occur, e.g., the 802.11 ARQ protocol we studied 
in Section 7.3, or more sophisticated approaches that use both ARQ and FEC 
[Ayanoglu 1995].
•	 TCP sender awareness of wireless links. In the local recovery approaches, the 
TCP sender is blissfully unaware that its segments are traversing a wireless link. 
An alternative approach is for the TCP sender and receiver to be aware of the 
existence of a wireless link, to distinguish between congestive losses occurring 
in the wired network and corruption/loss occurring at the wireless link, and to 
invoke congestion control only in response to congestive wired-network losses. 
[Balakrishnan 1997] investigates various types of TCP, assuming that end ­
systems 
610         Chapter 7    •    Wireless and Mobile Networks
can make this distinction. [Liu 2003] investigates techniques for distinguishing 
between losses on the wired and wireless segments of an end-to-end path.
•	 Split-connection approaches. In a split-connection approach [Bakre 1995], the 
end-to-end connection between the mobile user and the other end point is broken 
into two transport-layer connections: one from the mobile host to the wireless 
access point, and one from the wireless access point to the other communication 
end point (which we’ll assume here is a wired host). The end-to-end connection 
is thus formed by the concatenation of a wireless part and a wired part. The trans-
port layer over the wireless segment can be a standard TCP connection [Bakre 
1995], or a specially tailored error recovery protocol on top of UDP. [Yavatkar 
1994] investigates the use of a transport-layer selective repeat protocol over the 
wireless connection. Measurements reported in [Wei 2006] indicate that split 
TCP connections are widely used in cellular data networks, and that significant 
improvements can indeed be made through the use of split TCP connections.
Our treatment of TCP over wireless links has been necessarily brief here. 
­
In-depth surveys of TCP challenges and solutions in wireless networks can be found 
in [Hanabali 2005; Leung 2006]. We encourage you to consult the references for 
details of this ongoing area of research.
Having considered transport-layer protocols, let us next consider the effect of 
wireless and mobility on application-layer protocols. Here, an important consideration 
is that wireless links often have relatively low bandwidths, as we saw in Figure 7.2. As 
a result, applications that operate over wireless links, particularly over cellular wireless 
links, must treat bandwidth as a scarce commodity. For example, a Web server serving 
content to a Web browser executing on a 4G phone will likely not be able to provide the 
same image-rich content that it gives to a browser operating over a wired connection. 
Although wireless links do provide challenges at the application layer, the mobility they 
enable also makes possible a rich set of location-aware and context-aware applications 
[Chen 2000; Baldauf 2007]. More generally, wireless and mobile networks will play 
a key role in realizing the ubiquitous computing environments of the future [Weiser 
1991]. It’s fair to say that we’ve only seen the tip of the iceberg when it comes to the 
impact of wireless and mobile networks on networked applications and their protocols!
7.9	 Summary
Wireless and mobile networks have revolutionized telephony and are having an 
increasingly profound impact in the world of computer networks as well. With their 
anytime, anywhere, untethered access into the global network infrastructure, they are 
not only making network access more ubiquitous, they are also enabling an exciting 
new set of location-dependent services. Given the growing importance of wireless and 
Homework Problems and Questions         611
mobile networks, this chapter has focused on the principles, common link technolo-
gies, and network architectures for supporting wireless and mobile communication.
We began this chapter with an introduction to wireless and mobile networks, 
drawing an important distinction between the challenges posed by the wireless nature 
of the communication links in such networks, and by the mobility that these wireless 
links enable. This allowed us to better isolate, identify, and master the key concepts 
in each area. We focused first on wireless communication, considering the char-
acteristics of a wireless link in Section 7.2. In Sections 7.3 and 7.4, we examined 
the link-level aspects of the IEEE 802.11 (WiFi) wireless LAN standard, two IEEE 
802.15 personal area networks (Bluetooth and Zigbee), and 3G and 4G cellular Inter-
net access. We then turned our attention to the issue of mobility. In Section 7.5, we 
identified several forms of mobility, with points along this spectrum posing different 
challenges and admitting different solutions. We considered the problems of locating 
and routing to a mobile user, as well as approaches for handing off the mobile user 
who dynamically moves from one point of attachment to the network to another. We 
examined how these issues were addressed in the mobile IP standard and in GSM, in 
Sections 7.6 and 7.7, respectively. Finally, we considered the impact of wireless links 
and mobility on transport-layer protocols and networked applications in ­
Section 7.8.
Although we have devoted an entire chapter to the study of wireless and mobile 
networks, an entire book (or more) would be required to fully explore this exciting 
and rapidly expanding field. We encourage you to delve more deeply into this field 
by consulting the many references provided in this chapter.
Homework Problems and Questions
Chapter 7 Review Questions
Section 7.1
	R1.	 What does it mean for a wireless network to be operating in “infrastructure 
mode”? If the network is not in infrastructure mode, what mode of operation 
is it in, and what is the difference between that mode of operation and infra-
structure mode?
	R2.	 Both MANET and VANET are multi-hop infrastructure-less wireless networks. 
What is the difference between them?
Section 7.2
	R3.	 What are the differences between the following types of wireless channel 
impairments: path loss, multipath propagation, interference from other sources?
	R4.	 As a mobile node gets farther and farther away from a base station, what are 
two actions that a base station could take to ensure that the loss probability of 
a transmitted frame does not increase?
612         Chapter 7    •    Wireless and Mobile Networks
Sections 7.3 and 7.4
	
  R5.	 Describe the role of the beacon frames in 802.11.
	
  R6.	 An access point periodically sends beacon frames. What are the contents of 
the beacon frames?
	
  R7.	 Why are acknowledgments used in 802.11 but not in wired Ethernet?
	
  R8.	 What is the difference between passive scanning and active scanning?
	
  R9.	 What are the two main purposes of a CTS frame?
	
R10.	 Suppose the IEEE 802.11 RTS and CTS frames were as long as the standard 
DATA and ACK frames. Would there be any advantage to using the CTS and 
RTS frames? Why or why not?
	
R11.	 Section 7.3.4 discusses 802.11 mobility, in which a wireless station moves 
from one BSS to another within the same subnet. When the APs are intercon-
nected with a switch, an AP may need to send a frame with a spoofed MAC 
address to get the switch to forward the frame properly. Why?
	
R12.	 What is the difference between Bluetooth and Zigbee in terms of data rate?
	
R13.	 What is meant by a super frame in the 802.15.4 Zigbee standard?
	
R14.	 What is the role of the “core network” in the 3G cellular data architecture?
	
R15.	 What is the role of the RNC in the 3G cellular data network architecture? 
What role does the RNC play in the cellular voice network?
	
R16.	 What is the role of the eNodeB, MME, P-GW, and S-GW in 4G architecture?
	
R17.	 What are three important differences between the 3G and 4G cellular 
­
architectures?
Sections 7.5 and 7.6
	
R18.	 If a node has a wireless connection to the Internet, does that node have to be 
mobile? Explain. Suppose that a user with a laptop walks around her house 
with her laptop, and always accesses the Internet through the same access 
point. Is this user mobile from a network standpoint? Explain.
	
R19.	 What is the difference between a permanent address and a care-of address? 
Who assigns a care-of address?
	
R20.	 Consider a TCP connection going over Mobile IP. True or false: The TCP 
connection phase between the correspondent and the mobile host goes 
through the mobile’s home network, but the data transfer phase is directly 
between the correspondent and the mobile host, bypassing the home  
network.
Section 7.7
	
R21.	 What is the role of a GSM network’s base station controller (BSC)?
R	
22.	 What is the role of the anchor MSC in GSM networks?
Problems         613
Section 7.8
	
R23.	 What are three approaches that can be taken to avoid having a single ­
wireless 
link degrade the performance of an end-to-end transport-layer TCP ­
connection?
Problems
	 P1.	 Consider the single-sender CDMA example in Figure 7.5. What would be the 
sender’s output (for the 2 data bits shown) if the sender’s CDMA code were 
(1, 1, 21, 1, 1, 21, 21, 1)?
	 P2.	 Consider sender 2 in Figure 7.6. Assume that both the first two bits sent by 
sender 2 are 21. What are the sender’s outputs to the channel (before being 
added to the signal from sender 1)?
	 P3.	 After selecting the AP with which to associate, a wireless host sends an 
association request frame to the AP, and the AP responds with an associa-
tion response frame. Once associated with an AP, the host will want to join 
the subnet (in the IP addressing sense of Section 4.4.2) to which the AP 
belongs. What does the host do next?
	 P4.	 If two CDMA senders have codes (1, 1, 1, 21, 1, 21, 21, 21) and (1, 21, 
1, 1, 1, 1, 1, 1), would the corresponding receivers be able to decode the data 
correctly? Justify.
	 P5.	 Suppose there are two ISPs providing WiFi access in a particular café, with 
each ISP operating its own AP and having its own IP address block.
a.	 Further suppose that by accident, each ISP has configured its AP to oper-
ate over channel 11. Will the 802.11 protocol completely break down in 
this situation? Discuss what happens when two stations, each associated 
with a different ISP, attempt to transmit at the same time.
b.	 Now suppose that one AP operates over channel 1 and the other over 
channel 11. How do your answers change?
	 P6.	 In step 4 of the CSMA/CA protocol, a station that successfully transmits a 
frame begins the CSMA/CA protocol for a second frame at step 2, rather than 
at step 1. What rationale might the designers of CSMA/CA have had in mind 
by having such a station not transmit the second frame immediately (if the 
channel is sensed idle)?
	 P7.	 Suppose an 802.11b station is configured to always reserve the channel with 
the RTS/CTS sequence. Suppose this station suddenly wants to transmit 
1,000 bytes of data, and all other stations are idle at this time. Assume a 
transmission rate of 12 Mbps. As a function of SIFS and DIFS, and ignoring 
propagation delay and assuming no bit errors, calculate the time required to 
transmit the frame and receive the acknowledgment.
	 P8.	 Consider the scenario shown in Figure 7.34, in which there are four wireless 
nodes, A, B, C, and D. The radio coverage of the four nodes is shown via 
the shaded ovals; all nodes share the same frequency. When A transmits, it 
614         Chapter 7    •    Wireless and Mobile Networks
can only be heard/received by B; when B transmits, both A and C can hear/
receive from B; when C transmits, both B and D can hear/receive from C; 
when D transmits, only C can hear/receive from D.
	
	 Suppose now that each node has an infinite supply of messages that it wants 
to send to each of the other nodes. If a message’s destination is not an imme-
diate neighbor, then the message must be relayed. For example, if A wants 
to send to D, a message from A must first be sent to B, which then sends 
the message to C, which then sends the message to D. Time is slotted, with 
a message transmission time taking exactly one time slot, e.g., as in slotted 
Aloha. During a slot, a node can do one of the following: (i) send a message, 
(ii) receive a message (if exactly one message is being sent to it), (iii) remain 
silent. As always, if a node hears two or more simultaneous transmissions, 
a collision occurs and none of the transmitted messages are received suc-
cessfully. You can assume here that there are no bit-level errors, and thus if 
exactly one message is sent, it will be received correctly by those within the 
transmission radius of the sender.
a.	 Suppose now that an omniscient controller (i.e., a controller that knows 
the state of every node in the network) can command each node to do 
whatever it (the omniscient controller) wishes, i.e., to send a message, 
to receive a message, or to remain silent. Given this omniscient control-
ler, what is the maximum rate at which a data message can be transferred 
from C to A, given that there are no other messages between any other 
source/destination pairs?
b.	 Suppose now that A sends messages to B, and D sends messages to C. 
What is the combined maximum rate at which data messages can flow 
from A to B and from D to C?
c.	 Suppose now that A sends messages to B, and C sends messages to D. 
What is the combined maximum rate at which data messages can flow 
from A to B and from C to D?
d.	 Suppose now that the wireless links are replaced by wired links. Repeat 
questions (a) through (c) again in this wired scenario.
Figure 7.34  ♦  Scenario for problem P8
A
B
C
D
Problems         615
e.	 Now suppose we are again in the wireless scenario, and that for every data 
message sent from source to destination, the destination will send an ACK 
message back to the source (e.g., as in TCP). Also suppose that each ACK 
message takes up one slot. Repeat questions (a)–(c) above for this scenario.
	 P9.	 Power is a precious resource in mobile devices, and thus the 802.11 standard 
provides power-management capabilities that allow 802.11 nodes to minimize 
the amount of time that their sense, transmit, and receive functions and other  
circuitry need to be “on.” In 802.11, a node is able to explicitly alternate 
between sleep and wake states. Explain in brief how a node communicates with 
the AP to perform power management.
	
P10.	 Consider the following idealized LTE scenario. The downstream channel  
(see Figure 7.21) is slotted in time, across F frequencies. There are four 
nodes, A, B, C, and D, reachable from the base station at rates of 10 Mbps, 
5 Mbps, 2.5 Mbps, and 1 Mbps, respectively, on the downstream channel. 
These rates assume that the base station utilizes all time slots available on 
all F frequencies to send to just one station. The base station has an infinite 
amount of data to send to each of the nodes, and can send to any one of 
these four nodes using any of the F frequencies during any time slot in the 
­
downstream sub-frame.
a.	 What is the maximum rate at which the base station can send to the nodes, 
assuming it can send to any node it chooses during each time slot? Is your 
solution fair? Explain and define what you mean by “fair.”
b.	 If there is a fairness requirement that each node must receive an equal 
amount of data during each one second interval, what is the average 
transmission rate by the base station (to all nodes) during the downstream 
sub-frame? Explain how you arrived at your answer.
c.	 Suppose that the fairness criterion is that any node can receive at most 
twice as much data as any other node during the sub-frame. What is the 
average transmission rate by the base station (to all nodes) during the sub-
frame? Explain how you arrived at your answer.
	
P11.	 In Section 7.5, one proposed solution that allowed mobile users to maintain 
their IP addresses as they moved among foreign networks was to have a foreign 
network advertise a highly specific route to the mobile user and use the existing 
routing infrastructure to propagate this information throughout the network. We 
identified scalability as one concern. Suppose that when a mobile user moves 
from one network to another, the new foreign network advertises a specific route 
to the mobile user, and the old foreign network withdraws its route. Consider 
how routing information propagates in a distance-vector algorithm (particularly 
for the case of interdomain routing among networks that span the globe).
a.	 Will other routers be able to route datagrams immediately to the new for-
eign network as soon as the foreign network begins advertising its route?
616         Chapter 7    •    Wireless and Mobile Networks
b.	 Is it possible for different routers to believe that different foreign networks 
contain the mobile user?
c.	 Discuss the timescale over which other routers in the network will eventu-
ally learn the path to the mobile users.
	
P12.	 Suppose the correspondent in Figure 7.23 were mobile. Sketch the additional 
network-layer infrastructure that would be needed to route the datagram from 
the original mobile user to the (now mobile) correspondent. Show the struc-
ture of the datagram(s) between the original mobile user and the (now mobile) 
correspondent, as in Figure 7.24.
	
P13.	 In mobile IP, what effect will mobility have on end-to-end delays of data-
grams between the source and destination?
	
P14.	 Consider the chaining example discussed at the end of Section 7.7.2. Suppose 
a mobile user visits foreign networks A, B, and C, and that a correspondent 
begins a connection to the mobile user when it is resident in foreign ­
network 
A. List the sequence of messages between foreign agents, and between 
foreign agents and the home agent as the mobile user moves from network A 
to network B to network C. Next, suppose chaining is not performed, and the 
correspondent (as well as the home agent) must be explicitly notified of the 
changes in the mobile user’s care-of address. List the sequence of messages 
that would need to be exchanged in this second scenario.
	
P15.	 Consider two mobile nodes in a foreign network having a foreign agent. Is it 
possible for the two mobile nodes to use the same care-of address in mobile 
IP? Explain your answer.
	
P16.	 In our discussion of how the VLR updated the HLR with information about 
the mobile’s current location, what are the advantages and disadvantages of 
providing the MSRN as opposed to the address of the VLR to the HLR?
Wireshark Lab
At the Web site for this textbook, www.pearsonglobaleditions.com/kurose, you’ll 
find a Wireshark lab for this chapter that captures and studies the 802.11 frames 
exchanged between a wireless laptop and an access point.
617
Please describe a few of the most exciting projects you have worked on during your 
career. What were the biggest challenges?
In the mid-90s at USC and ISI, I had the great fortune to work with the likes of Steve 
Deering, Mark Handley, and Van Jacobson on the design of multicast routing protocols  
(in particular, PIM). I tried to carry many of the architectural design lessons from multicast 
into the design of ecological monitoring arrays, where for the first time I really began to 
take applications and multidisciplinary research seriously. That interest in jointly innovat-
ing in the social and technological space is what interests me so much about my latest area 
of research, mobile health. The challenges in these projects were as diverse as the problem 
AN INTERVIEW WITH…
Deborah Estrin
Deborah Estrin is a Professor of Computer Science at Cornell Tech 
in New York City and a Professor of Public Health at Weill Cornell 
Medical College. She is founder of the Health Tech Hub 
at Cornell Tech and co-founder of the non-profit startup Open 
mHealth. She received her Ph.D. (1985) in Computer Science 
from M.I.T. and her B.S. (1980) from UC Berkeley. Estrin’s early 
research focused on the design of network protocols, including 
multicast and inter-domain routing. In 2002 Estrin founded the 
NSF-funded Science and Technology Center at UCLA, Center for 
Embedded Networked Sensing (CENS http://cens.ucla.edu.). 
CENS launched new areas of multi-disciplinary computer systems 
research from sensor networks for environmental monitoring, to par-
ticipatory sensing for citizen science. Her current focus is on mobile 
health and small data, leveraging the pervasiveness of mobile 
devices and digital interactions for health and life management, as 
described in her 2013 TEDMED talk. Professor Estrin is an elected 
member of the American Academy of Arts and Sciences (2007) and 
the National Academy of Engineering (2009). She is a fellow of 
the IEEE, ACM, and AAAS. She was selected as the first ACM-W 
Athena Lecturer (2006), awarded the Anita Borg Institute’s Women 
of Vision Award for Innovation (2007), inducted into the WITI hall of 
fame (2008) and awarded Doctor Honoris Causa from EPFL (2008) 
and Uppsala University (2011).
618
domains, but what they all had in common was the need to keep our eyes open to whether 
we had the problem definition right as we iterated between design and deployment, proto-
type and pilot. None of them were problems that could be solved analytically, with simula-
tion or even in constructed laboratory experiments. They all challenged our ability to retain 
clean architectures in the presence of messy problems and contexts, and they all called for 
extensive collaboration.
What changes and innovations do you see happening in wireless networks and mobility 
in the future?
In a prior edition of this interview I said that I have never put much faith into predicting the 
future, but I did go on to speculate that we might see the end of feature phones (i.e., those 
that are not programmable and are used only for voice and text messaging) as smart phones 
become more and more powerful and the primary point of Internet access for many—and 
now not so many years later that is clearly the case. I also predicted that we would see the 
continued proliferation of embedded SIMs by which all sorts of devices have the ability 
to communicate via the cellular network at low data rates. While that has occurred, we see 
many devices and “Internet of Things” that use embedded WiFi and other lower power, 
shorter range, forms of connectivity to local hubs. I did not anticipate at that time the emer-
gence of a large consumer wearables market. By the time the next edition is published I 
expect broad proliferation of personal applications that leverage data from IoT and other 
digital traces.
Where do you see the future of networking and the Internet?
Again I think its useful to look both back and forward. Previously I observed that the efforts 
in named data and software-defined networking would emerge to create a more manageable, 
evolvable, and richer infrastructure and more generally represent moving the role of archi-
tecture higher up in the stack. In the beginnings of the Internet, architecture was layer 4 and 
below, with applications being more siloed/monolithic, sitting on top. Now data and analyt-
ics dominate transport. The adoption of SDN (which I’m really happy to see is featured 
in this 7th edition of this book) has been well beyond what I ever anticipated. However, 
looking up the stack, our dominant applications increasingly live in walled gardens, whether 
mobile apps or large consumer platforms such as Facebook. As Data Science and Big Data 
techniques develop, they might help to lure these applications out of their silos because of 
the value in connecting with other apps and platforms.
619
What people inspired you professionally?
There are three people who come to mind. First, Dave Clark, the secret sauce and under-
sung hero of the Internet community. I was lucky to be around in the early days to see him 
act as the “organizing principle” of the IAB and Internet governance; the priest of rough 
consensus and running code. Second, Scott Shenker, for his intellectual brilliance, integrity, 
and persistence. I strive for, but rarely attain, his clarity in defining problems and solutions. 
He is always the first person I e-mail for advice on matters large and small. Third, my sister 
Judy Estrin, who had the creativity and courage to spend her career bringing ideas and con-
cepts to market. Without the Judys of the world the Internet technologies would never have 
transformed our lives.
What are your recommendations for students who want careers in computer science  
and networking?
First, build a strong foundation in your academic work, balanced with any and every real-
world work experience you can get. As you look for a working environment, seek opportu-
nities in problem areas you really care about and with smart teams that you can learn from.
This page intentionally left blank
621
Way back in Section 1.6 we described some of the more prevalent and damaging 
classes of Internet attacks, including malware attacks, denial of service, sniffing, 
source masquerading, and message modification and deletion. Although we have 
since learned a tremendous amount about computer networks, we still haven’t exam-
ined how to secure networks from those attacks. Equipped with our newly acquired 
expertise in computer networking and Internet protocols, we’ll now study in-depth 
secure communication and, in particular, how computer networks can be defended 
from those nasty bad guys.
Let us introduce Alice and Bob, two people who want to communicate and wish 
to do so “securely.” This being a networking text, we should remark that Alice and 
Bob could be two routers that want to exchange routing tables securely, a client and 
server that want to establish a secure transport connection, or two e-mail applications 
that want to exchange secure e-mail—all case studies that we will consider later in 
this chapter. Alice and Bob are well-known fixtures in the security community, per-
haps because their names are more fun than a generic entity named “A” that wants 
to communicate securely with a generic entity named “B.” Love affairs, wartime 
communication, and business transactions are the commonly cited human needs for 
secure communications; preferring the first to the latter two, we’re happy to use 
Alice and Bob as our sender and receiver, and imagine them in this first scenario.
We said that Alice and Bob want to communicate and wish to do so “securely,” 
but what precisely does this mean? As we will see, security (like love) is a many-
splendored thing; that is, there are many facets to security. Certainly, Alice and 
Bob would like for the contents of their communication to remain secret from 
an eavesdropper. They probably would also like to make sure that when they are 
8
CHAPTER
Security in 
Computer 
Networks
622         Chapter 8    •    Security in Computer Networks
communicating, they are indeed communicating with each other, and that if their 
communication is tampered with by an eavesdropper, that this tampering is detected. 
In the first part of this chapter, we’ll cover the fundamental cryptography techniques 
that allow for encrypting communication, authenticating the party with whom one is 
communicating, and ensuring message integrity.
In the second part of this chapter, we’ll examine how the fundamental 
­
cryptography principles can be used to create secure networking protocols. Once 
again taking a top-down approach, we’ll examine secure protocols in each of the 
(top four) layers, beginning with the application layer. We’ll examine how to secure 
e-mail, how to secure a TCP connection, how to provide blanket security at the net-
work layer, and how to secure a wireless LAN. In the third part of this chapter we’ll 
consider operational security, which is about protecting organizational networks 
from attacks. In particular, we’ll take a careful look at how firewalls and intrusion 
detection systems can enhance the security of an organizational network.
8.1	 What Is Network Security?
Let’s begin our study of network security by returning to our lovers, Alice and Bob, 
who want to communicate “securely.” What precisely does this mean? Certainly, 
Alice wants only Bob to be able to understand a message that she has sent, even 
though they are communicating over an insecure medium where an intruder (Trudy, 
the intruder) may intercept whatever is transmitted from Alice to Bob. Bob also 
wants to be sure that the message he receives from Alice was indeed sent by Alice, 
and Alice wants to make sure that the person with whom she is communicating is 
indeed Bob. Alice and Bob also want to make sure that the contents of their messages 
have not been altered in transit. They also want to be assured that they can communi-
cate in the first place (i.e., that no one denies them access to the resources needed to 
communicate). Given these considerations, we can identify the following desirable 
properties of secure communication.
•	 Confidentiality. Only the sender and intended receiver should be able to under-
stand the contents of the transmitted message. Because eavesdroppers may 
intercept the message, this necessarily requires that the message be somehow 
encrypted so that an intercepted message cannot be understood by an interceptor. 
This aspect of confidentiality is probably the most commonly perceived mean-
ing of the term secure communication. We’ll study cryptographic techniques for 
encrypting and decrypting data in Section 8.2.
•	 Message integrity. Alice and Bob want to ensure that the content of their 
­
communication is not altered, either maliciously or by accident, in transit. Exten-
sions to the checksumming techniques that we encountered in reliable transport 
8.1    •    What Is Network Security?         623
and data link protocols can be used to provide such message integrity. We will 
study message integrity in Section 8.3.
•	 End-point authentication. Both the sender and receiver should be able to confirm 
the identity of the other party involved in the communication—to confirm that the 
other party is indeed who or what they claim to be. Face-to-face human commu-
nication solves this problem easily by visual recognition. When communicating 
entities exchange messages over a medium where they cannot see the other party, 
authentication is not so simple. When a user wants to access an inbox, how does 
the mail server verify that the user is the person he or she claims to be? We study 
end-point authentication in Section 8.4.
•	 Operational security. Almost all organizations (companies, universities, and so 
on) today have networks that are attached to the public Internet. These networks 
therefore can potentially be compromised. Attackers can attempt to deposit worms 
into the hosts in the network, obtain corporate secrets, map the internal network 
configurations, and launch DoS attacks. We’ll see in Section 8.9 that operational 
devices such as firewalls and intrusion detection systems are used to counter 
attacks against an organization’s network. A firewall sits between the organiza-
tion’s network and the public network, controlling packet access to and from 
the network. An intrusion detection system performs “deep packet ­
inspection,” 
­
alerting the network administrators about suspicious activity.
Having established what we mean by network security, let’s next consider 
exactly what information an intruder may have access to, and what actions can be 
taken by the intruder. Figure 8.1 illustrates the scenario. Alice, the sender, wants to 
send data to Bob, the receiver. In order to exchange data securely, while meeting 
the requirements of confidentiality, end-point authentication, and message integrity, 
Alice and Bob will exchange control messages and data messages (in much the same 
way that TCP senders and receivers exchange control segments and data segments). 
Secure
sender
Alice
Trudy
Channel
Control, data messages
Secure
receiver
Bob
Data
Data
Figure 8.1  ♦  Sender, receiver, and intruder (Alice, Bob, and Trudy)
624         Chapter 8    •    Security in Computer Networks
All or some of these messages will typically be encrypted. As discussed in Section 
1.6, an intruder can potentially perform
•	 eavesdropping—sniffing and recording control and data messages on the ­
channel.
•	 modification, insertion, or deletion of messages or message content.
As we’ll see, unless appropriate countermeasures are taken, these capabilities 
allow an intruder to mount a wide variety of security attacks: snooping on communi-
cation (possibly stealing passwords and data), impersonating another entity, hijack-
ing an ongoing session, denying service to legitimate network users by overloading 
system resources, and so on. A summary of reported attacks is maintained at the 
CERT Coordination Center [CERT 2016].
Having established that there are indeed real threats loose in the Internet, what 
are the Internet equivalents of Alice and Bob, our friends who need to communicate 
securely? Certainly, Bob and Alice might be human users at two end systems, for 
example, a real Alice and a real Bob who really do want to exchange secure e-mail. 
They might also be participants in an electronic commerce transaction. For example, 
a real Bob might want to transfer his credit card number securely to a Web server 
to purchase an item online. Similarly, a real Alice might want to interact with her 
bank online. The parties needing secure communication might themselves also be 
part of the network infrastructure. Recall that the domain name system (DNS, see 
Section 2.4) or routing daemons that exchange routing information (see Chapter 5) 
require secure communication between two parties. The same is true for network 
management applications, a topic we examined in Chapter 5). An intruder that could 
actively interfere with DNS lookups (as discussed in Section 2.4), routing computa-
tions [RFC 4272], or network management functions [RFC 3414] could wreak havoc 
in the Internet.
Having now established the framework, a few of the most important definitions, 
and the need for network security, let us next delve into cryptography. While the use 
of cryptography in providing confidentiality is self-evident, we’ll see shortly that it 
is also central to providing end-point authentication and message integrity—making 
cryptography a cornerstone of network security.
8.2	 Principles of Cryptography
Although cryptography has a long history dating back at least as far as Julius Caesar, 
modern cryptographic techniques, including many of those used in the Internet, are 
based on advances made in the past 30 years. Kahn’s book, The Codebreakers [Kahn 
1967], and Singh’s book, The Code Book: The Science of Secrecy from Ancient 
Egypt to Quantum Cryptography [Singh 1999], provide a fascinating look at the 
8.2    •    Principles of Cryptography         625
long history of cryptography. A complete discussion of cryptography itself requires a 
complete book [Kaufman 1995; Schneier 1995] and so we only touch on the essential 
aspects of cryptography, particularly as they are practiced on the Internet. We also 
note that while our focus in this section will be on the use of cryptography for con-
fidentiality, we’ll see shortly that cryptographic techniques are inextricably woven 
into authentication, message integrity, nonrepudiation, and more.
Cryptographic techniques allow a sender to disguise data so that an intruder can 
gain no information from the intercepted data. The receiver, of course, must be able 
to recover the original data from the disguised data. Figure 8.2 illustrates some of the 
important terminology.
Suppose now that Alice wants to send a message to Bob. Alice’s message in 
its original form (for example, “Bob, I love you. Alice”) is known as 
­
plaintext, or cleartext. Alice encrypts her plaintext message using an encryption 
algorithm so that the encrypted message, known as ciphertext, looks unintelligi-
ble to any intruder. Interestingly, in many modern cryptographic systems, including 
those used in the Internet, the encryption technique itself is known—published, stand-
ardized, and available to everyone (for example, [RFC 1321; RFC 3447; RFC 2420; 
NIST 2001]), even a potential intruder! Clearly, if everyone knows the method for 
encoding data, then there must be some secret information that prevents an intruder 
from decrypting the transmitted data. This is where keys come in.
In Figure 8.2, Alice provides a key, KA, a string of numbers or characters, as 
input to the encryption algorithm. The encryption algorithm takes the key and the 
plaintext message, m, as input and produces ciphertext as output. The notation 
KA(m) refers to the ciphertext form (encrypted using the key KA) of the plaintext 
message, m. The actual encryption algorithm that uses key KA will be evident from 
the context. Similarly, Bob will provide a key, KB, to the decryption algorithm 
Figure 8.2  ♦  Cryptographic components
Encryption
algorithm
Ciphertext
Channel
Trudy
Alice
Bob
Decryption
algorithm
Plaintext
Key:
Key
Plaintext
KA
KB
626         Chapter 8    •    Security in Computer Networks
that takes the ciphertext and Bob’s key as input and produces the original plain-
text as output. That is, if Bob receives an encrypted message KA(m), he decrypts it 
by computing KB(KA(m)) = m. In symmetric key systems, Alice’s and Bob’s keys 
are identical and are secret. In public key systems, a pair of keys is used. One of 
the keys is known to both Bob and Alice (indeed, it is known to the whole world). 
 
The other key is known only by either Bob or Alice (but not both). In the following 
two subsections, we consider symmetric key and public key systems in more detail.
8.2.1	Symmetric Key Cryptography
All cryptographic algorithms involve substituting one thing for another, for exam-
ple, taking a piece of plaintext and then computing and substituting the appropriate 
ciphertext to create the encrypted message. Before studying a modern key-based 
cryptographic system, let us first get our feet wet by studying a very old, very simple 
symmetric key algorithm attributed to Julius Caesar, known as the Caesar cipher  
(a cipher is a method for encrypting data).
For English text, the Caesar cipher would work by taking each letter in the plain-
text message and substituting the letter that is k letters later (allowing wraparound; 
that is, having the letter z followed by the letter a) in the alphabet. For example if 
k = 3, then the letter a in plaintext becomes d in ciphertext; b in plaintext becomes 
e in ciphertext, and so on. Here, the value of k serves as the key. As an example, the 
plaintext message “bob, i love you. Alice” becomes “ere, l oryh 
brx. dolfh” in ciphertext. While the ciphertext does indeed look like gibberish, 
it wouldn’t take long to break the code if you knew that the Caesar cipher was being 
used, as there are only 25 possible key values.
An improvement on the Caesar cipher is the monoalphabetic cipher, which also 
substitutes one letter of the alphabet with another letter of the alphabet. ­
However, 
rather than substituting according to a regular pattern (for example, substitution with 
an offset of k for all letters), any letter can be substituted for any other letter, as long 
as each letter has a unique substitute letter, and vice versa. The substitution rule in 
Figure 8.3 shows one possible rule for encoding plaintext.
The plaintext message “bob, i love you. Alice” becomes “nkn, s 
gktc wky. Mgsbc.” Thus, as in the case of the Caesar cipher, this looks like 
gibberish. A monoalphabetic cipher would also appear to be better than the Caesar 
cipher in that there are 26! (on the order of 1026) possible pairings of letters rather 
than 25 possible pairings. A brute-force approach of trying all 1026 possible pairings 
Figure 8.3  ♦  A monoalphabetic cipher
Plaintext letter:
a b c d e f g h i j k l m n o p q r s t u v w x y z
Ciphertext letter:
m n b v c x z a s d f g h j k l p o i u y t r e w q
8.2    •    Principles of Cryptography         627
would require far too much work to be a feasible way of breaking the encryption 
algorithm and decoding the message. However, by statistical analysis of the plain-
text language, for example, knowing that the letters e and t are the most frequently 
occurring letters in typical English text (accounting for 13 percent and 9 percent of 
letter occurrences), and knowing that particular two-and three-letter occurrences of 
letters appear quite often together (for example, “in,” “it,” “the,” “ion,” “ing,” and so 
forth) make it relatively easy to break this code. If the intruder has some knowledge 
about the possible contents of the message, then it is even easier to break the code. 
For example, if Trudy the intruder is Bob’s wife and suspects Bob of having an 
affair with Alice, then she might suspect that the names “bob” and “alice” appear in 
the text. If Trudy knew for certain that those two names appeared in the ciphertext 
and had a copy of the example ciphertext message above, then she could immedi-
ately determine seven of the 26 letter pairings, requiring 109 fewer possibilities to 
be checked by a brute-force method. Indeed, if Trudy suspected Bob of having an 
affair, she might well expect to find some other choice words in the message as well.
When considering how easy it might be for Trudy to break Bob and Alice’s 
encryption scheme, one can distinguish three different scenarios, depending on what 
information the intruder has.
•	 Ciphertext-only attack. In some cases, the intruder may have access only to the 
intercepted ciphertext, with no certain information about the contents of the plain-
text message. We have seen how statistical analysis can help in a ciphertext-only 
attack on an encryption scheme.
•	 Known-plaintext attack. We saw above that if Trudy somehow knew for sure 
that “bob” and “alice” appeared in the ciphertext message, then she could have 
determined the (plaintext, ciphertext) pairings for the letters a, l, i, c, e, b, and o. 
Trudy might also have been fortunate enough to have recorded all of the cipher-
text transmissions and then found Bob’s own decrypted version of one of the 
transmissions scribbled on a piece of paper. When an intruder knows some of the 
(plaintext, ciphertext) pairings, we refer to this as a known-plaintext attack on 
the encryption scheme.
•	 Chosen-plaintext attack. In a chosen-plaintext attack, the intruder is able to 
choose the plaintext message and obtain its corresponding ciphertext form. For 
the simple encryption algorithms we’ve seen so far, if Trudy could get Alice to 
send the message, “The quick brown fox jumps over the lazy 
dog,” she could completely break the encryption scheme. We’ll see shortly that 
for more sophisticated encryption techniques, a chosen-plaintext attack does not 
necessarily mean that the encryption technique can be broken.
Five hundred years ago, techniques improving on monoalphabetic encryp-
tion, known as polyalphabetic encryption, were invented. The idea behind pol-
yalphabetic encryption is to use multiple monoalphabetic ciphers, with a specific 
628         Chapter 8    •    Security in Computer Networks
monoalphabetic cipher to encode a letter in a specific position in the plaintext mes-
sage. Thus, the same letter, appearing in different positions in the plaintext message, 
might be encoded differently. An example of a polyalphabetic encryption scheme is 
shown in Figure 8.4. It has two Caesar ciphers (with k = 5 and k = 19), shown as 
rows. We might choose to use these two Caesar ciphers, C1 and C2, in the repeating 
pattern C1, C2, C2, C1, C2. That is, the first letter of plaintext is to be encoded using 
C1, the second and third using C2, the fourth using C1, and the fifth using C2. The 
pattern then repeats, with the sixth letter being encoded using C1, the seventh with 
C2, and so on. The plaintext message “bob, i love you.” is thus encrypted 
“ghu, n etox dhz.” Note that the first b in the plaintext message is encrypted 
using C1, while the second b is encrypted using C2. In this example, the encryption 
and decryption “key” is the knowledge of the two Caesar keys (k = 5, k = 19) and 
the pattern C1, C2, C2, C1, C2.
Block Ciphers
Let us now move forward to modern times and examine how symmetric key encryp-
tion is done today. There are two broad classes of symmetric encryption tech-
niques: stream ciphers and block ciphers. We’ll briefly examine stream ciphers in 
­
Section 8.7 when we investigate security for wireless LANs. In this section, we focus 
on block ciphers, which are used in many secure Internet protocols, including PGP 
(for secure e-mail), SSL (for securing TCP connections), and IPsec (for securing the 
network-layer transport).
In a block cipher, the message to be encrypted is processed in blocks of k bits. 
For example, if k = 64, then the message is broken into 64-bit blocks, and each block 
is encrypted independently. To encode a block, the cipher uses a one-to-one map-
ping to map the k-bit block of cleartext to a k-bit block of ciphertext. Let’s look at an 
example. Suppose that k = 3, so that the block cipher maps 3-bit inputs ­
(cleartext) 
to 3-bit outputs (ciphertext). One possible mapping is given in Table 8.1. Notice that 
this is a one-to-one mapping; that is, there is a different output for each input. This 
block cipher breaks the message up into 3-bit blocks and encrypts each block accord-
ing to the above mapping. You should verify that the message 010110001111 gets 
encrypted into 101000111001.
Continuing with this 3-bit block example, note that the mapping in Table 8.1 
is just one mapping of many possible mappings. How many possible mappings are 
Figure 8.4  ♦  A polyalphabetic cipher using two Caesar ciphers
Plaintext letter:
a b c d e f g h i j k l m n o p q r s t u v w x y z
C1(k = 5): 
C2(k = 19): 
f g h i j k l m n o p q r s t u v w x y z a b c d e
t u v w x y z a b c d e f g h i j k l m n o p q r s
8.2    •    Principles of Cryptography         629
there? To answer this question, observe that a mapping is nothing more than a permu-
tation of all the possible inputs. There are 23 (= 8) possible inputs (listed under the 
input columns). These eight inputs can be permuted in 8! = 40,320 different ways. 
Since each of these permutations specifies a mapping, there are 40,320 possible map-
pings. We can view each of these mappings as a key—if Alice and Bob both know 
the mapping (the key), they can encrypt and decrypt the messages sent between them.
The brute-force attack for this cipher is to try to decrypt ciphtertext by using all 
mappings. With only 40,320 mappings (when k = 3), this can quickly be accom-
plished on a desktop PC. To thwart brute-force attacks, block ciphers typically use 
much larger blocks, consisting of k = 64 bits or even larger. Note that the number of 
possible mappings for a general k-block cipher is 2k!, which is astronomical for even 
moderate values of k (such as k = 64).
Although full-table block ciphers, as just described, with moderate values of k 
can produce robust symmetric key encryption schemes, they are unfortunately dif-
ficult to implement. For k = 64 and for a given mapping, Alice and Bob would 
need to maintain a table with 264 input values, which is an infeasible task. Moreo-
ver, if Alice and Bob were to change keys, they would have to each regenerate the 
table. Thus, a full-table block cipher, providing predetermined mappings between all 
inputs and outputs (as in the example above), is simply out of the question.
Instead, block ciphers typically use functions that simulate randomly permuted 
tables. An example (adapted from [Kaufman 1995]) of such a function for k = 64 
bits is shown in Figure 8.5. The function first breaks a 64-bit block into 8 chunks, 
with each chunk consisting of 8 bits. Each 8-bit chunk is processed by an 8-bit to 
8-bit table, which is of manageable size. For example, the first chunk is processed 
by the table denoted by T
1. Next, the 8 output chunks are reassembled into a 64-bit 
block. The positions of the 64 bits in the block are then scrambled (permuted) to 
produce a 64-bit output. This output is fed back to the 64-bit input, where another 
cycle begins. After n such cycles, the function provides a 64-bit block of ciphertext. 
The purpose of the rounds is to make each input bit affect most (if not all) of the final 
output bits. (If only one round were used, a given input bit would affect only 8 of the 
64 output bits.) The key for this block cipher algorithm would be the eight permuta-
tion tables (assuming the scramble function is publicly known).
Table 8.1  ♦  A specific 3-bit block cipher
input
output
input
output
000
110
100
011
001
111
101
010
010
101
110
000
011
100
111
001
630         Chapter 8    •    Security in Computer Networks
Today there are a number of popular block ciphers, including DES (standing 
for Data Encryption Standard), 3DES, and AES (standing for Advanced Encryption 
Standard). Each of these standards uses functions, rather than predetermined tables, 
along the lines of Figure 8.5 (albeit more complicated and specific to each cipher). 
Each of these algorithms also uses a string of bits for a key. For example, DES uses 
64-bit blocks with a 56-bit key. AES uses 128-bit blocks and can operate with keys 
that are 128, 192, and 256 bits long. An algorithm’s key determines the specific 
“mini-table” mappings and permutations within the algorithm’s internals. The brute-
force attack for each of these ciphers is to cycle through all the keys, applying the 
decryption algorithm with each key. Observe that with a key length of n, there are 2n 
possible keys. NIST [NIST 2001] estimates that a machine that could crack 56-bit 
DES in one second (that is, try all 256 keys in one second) would take approximately 
149 trillion years to crack a 128-bit AES key.
Cipher-Block Chaining
In computer networking applications, we typically need to encrypt long messages 
 
(or long streams of data). If we apply a block cipher as described by simply chopping 
up the message into k-bit blocks and independently encrypting each block, a subtle 
but important problem occurs. To see this, observe that two or more of the cleartext 
blocks can be identical. For example, the cleartext in two or more blocks could be 
“HTTP/1.1”. For these identical blocks, a block cipher would, of course, produce 
the same ciphertext. An attacker could potentially guess the cleartext when it sees 
identical ciphertext blocks and may even be able to decrypt the entire message by 
Figure 8.5  ♦  An example of a block cipher
64-bit output
Loop
for n
rounds
8 bits
8 bits
T1
8 bits
8 bits
T2
8 bits
8 bits
T3
8 bits
64-bit input
8 bits
T4
8 bits
8 bits
T5
8 bits
8 bits
T6
8 bits
8 bits
T7
8 bits
8 bits
T8
64-bit scrambler
8.2    •    Principles of Cryptography         631
identifying identical ciphtertext blocks and using knowledge about the underlying 
protocol structure [Kaufman 1995].
To address this problem, we can mix some randomness into the ciphertext so 
that identical plaintext blocks produce different ciphertext blocks. To explain this 
idea, let m(i) denote the ith plaintext block, c(i) denote the ith ciphertext block, and 
a ⊕b denote the exclusive-or (XOR) of two bit strings, a and b. (Recall that the 
 
0 ⊕0 = 1 ⊕1 = 0 and 0 ⊕1 = 1 ⊕0 = 1, and the XOR of two bit strings is 
 
done on a bit-by-bit basis. So, for example, 10101010 ⊕11110000 = 01011010.) 
Also, denote the block-cipher encryption algorithm with key S as KS. The basic idea 
is as follows. The sender creates a random k-bit number r(i) for the ith block and 
calculates c(i) = KS(m(i) ⊕r(i 
)). Note that a new k-bit random number is chosen 
for each block. The sender then sends c(1), r(1), c(2), r(2), c(3), r(3), and so on. 
Since the receiver receives c(i) and r(i), it can recover each block of the plaintext by 
computing m(i) = KS(c(i)) ⊕r(i 
). It is important to note that, although r(i) is sent 
in the clear and thus can be sniffed by Trudy, she cannot obtain the plaintext m(i), 
since she does not know the key KS. Also note that if two plaintext blocks m(i) and 
m(j) are the same, the corresponding ciphertext blocks c(i) and c(j) will be different 
(as long as the random numbers r(i) and r(j) are different, which occurs with very 
high probability).
As an example, consider the 3-bit block cipher in Table 8.1. Suppose the plain-
text is 010010010. If Alice encrypts this directly, without including the randomness, 
the resulting ciphertext becomes 101101101. If Trudy sniffs this ciphertext, because 
each of the three cipher blocks is the same, she can correctly surmise that each of the 
three plaintext blocks are the same. Now suppose instead Alice generates the ran-
dom blocks r(1) = 001, r(2) = 111, and r(3) = 100 and uses the above technique 
to generate the ciphertext c(1) = 100, c(2) = 010, and c(3) = 000. Note that the 
three ciphertext blocks are different even though the plaintext blocks are the same. 
Alice then sends c(1), r(1), c(2), and r(2). You should verify that Bob can obtain the 
original plaintext using the shared key KS.
The astute reader will note that introducing randomness solves one problem but 
creates another: namely, Alice must transmit twice as many bits as before. Indeed, 
for each cipher bit, she must now also send a random bit, doubling the required band-
width. In order to have our cake and eat it too, block ciphers typically use a technique 
called Cipher Block Chaining (CBC). The basic idea is to send only one random 
value along with the very first message, and then have the sender and receiver use 
the computed coded blocks in place of the subsequent random number. Specifically, 
CBC operates as follows:
	1.	 Before encrypting the message (or the stream of data), the sender generates a 
random k-bit string, called the Initialization Vector (IV). Denote this initial-
ization vector by c(0). The sender sends the IV to the receiver in cleartext.
	2.	 For the first block, the sender calculates m(1) ⊕c(0), that is, calculates the 
exclusive-or of the first block of cleartext with the IV. It then runs the result 
632         Chapter 8    •    Security in Computer Networks
through the block-cipher algorithm to get the corresponding ciphertext block; 
that is, c(1) = KS(m(1) ⊕c(0)). The sender sends the encrypted block c(1) to 
the receiver.
	3.	 For the ith block, the sender generates the ith ciphertext block from c(i) =  
KS(m(i) ⊕c(i - 1)).
Let’s now examine some of the consequences of this approach. First, the receiver 
will still be able to recover the original message. Indeed, when the receiver receives 
c(i), it decrypts it with KS to obtain s(i) = m(i) ⊕c(i - 1); since the receiver also 
knows c(i - 1), it then obtains the cleartext block from m(i) = s(i) ⊕c(i - 1). 
Second, even if two cleartext blocks are identical, the corresponding ciphtertexts 
(almost always) will be different. Third, although the sender sends the IV in the 
clear, an intruder will still not be able to decrypt the ciphertext blocks, since the 
intruder does not know the secret key, S. Finally, the sender only sends one overhead 
block (the IV), thereby negligibly increasing the bandwidth usage for long messages 
 
(consisting of hundreds of blocks).
As an example, let’s now determine the ciphertext for the 3-bit block cipher in 
Table 8.1 with plaintext 010010010 and IV = c(0) = 001. The sender first uses the 
 
IV to calculate c(1) = KS(m(1) ⊕c(0)) = 100. The sender then calculates c(2) = 
KS(m(2) ⊕c(1)) = KS(010 ⊕100) = 000, and c(3) = KS(m(3) ⊕c(2)) = KS(010 ⊕ 
000) = 101. The reader should verify that the receiver, knowing the IV and KS can 
recover the original plaintext.
CBC has an important consequence when designing secure network protocols: 
we’ll need to provide a mechanism within the protocol to distribute the IV from sender 
to receiver. We’ll see how this is done for several protocols later in this chapter.
8.2.2	Public Key Encryption
For more than 2,000 years (since the time of the Caesar cipher and up to the 1970s), 
encrypted communication required that the two communicating parties share a com-
mon secret—the symmetric key used for encryption and decryption. One difficulty 
with this approach is that the two parties must somehow agree on the shared key; 
but to do so requires (presumably secure) communication! Perhaps the parties could 
first meet and agree on the key in person (for example, two of Caesar’s centurions 
might meet at the Roman baths) and thereafter communicate with encryption. In a 
networked world, however, communicating parties may never meet and may never 
converse except over the network. Is it possible for two parties to communicate with 
encryption without having a shared secret key that is known in advance? In 1976, 
Diffie and Hellman [Diffie 1976] demonstrated an algorithm (known now as Dif-
fie-Hellman Key Exchange) to do just that—a radically different and marvelously 
elegant approach toward secure communication that has led to the development of 
today’s public key cryptography systems. We’ll see shortly that public key cryptog-
raphy systems also have several wonderful properties that make them useful not only 
8.2    •    Principles of Cryptography         633
for encryption, but for authentication and digital signatures as well. Interestingly, it 
has recently come to light that ideas similar to those in [Diffie 1976] and [RSA 1978] 
had been independently developed in the early 1970s in a series of secret reports 
by researchers at the Communications-Electronics Security Group in the United 
­
Kingdom [Ellis 1987]. As is often the case, great ideas can spring up independently 
in many places; fortunately, public key advances took place not only in private, but 
also in the public view, as well.
The use of public key cryptography is conceptually quite simple. Suppose Alice 
wants to communicate with Bob. As shown in Figure 8.6, rather than Bob and Alice 
sharing a single secret key (as in the case of symmetric key systems), Bob (the recipi-
ent of Alice’s messages) instead has two keys—a public key that is available to 
everyone in the world (including Trudy the intruder) and a private key that is known 
only to Bob. We will use the notation K+
B and K-
B to refer to Bob’s public and pri-
vate keys, respectively. In order to communicate with Bob, Alice first fetches Bob’s 
public key. Alice then encrypts her message, m, to Bob using Bob’s public key and 
a known (for example, standardized) encryption algorithm; that is, Alice computes 
K+
B(m). Bob receives Alice’s encrypted message and uses his private key and a known 
(for example, standardized) decryption algorithm to decrypt Alice’s encrypted mes-
sage. That is, Bob computes K-
B(K+
B(m)). We will see below that there are encryption/
decryption algorithms and techniques for choosing public and private keys such that 
K-
B(K+
B(m)) = m; that is, applying Bob’s public key, K+
B, to a message, m (to get 
K+
B(m)), and then applying Bob’s private key, K-
B, to the encrypted version of m (that 
is, computing K-
B(K+
B(m))) gives back m. This is a remarkable result! In this manner, 
Alice can use Bob’s publicly available key to send a secret message to Bob without 
either of them having to distribute any secret keys! We will see shortly that we can 
interchange the public key and private key encryption and get the same remarkable 
result––that is, K-
B (B 
+(m)) = K+
B (K-
B(m)) = m.
Figure 8.6  ♦  Public key cryptography
Encryption
algorithm
Ciphertext
Decryption
algorithm
Plaintext
message, m
Plaintext
message, m
Private decryption key
m = KB
–(KB
+(m))
KB
–
KB
+(m)
Public encryption key
KB
+
634         Chapter 8    •    Security in Computer Networks
The use of public key cryptography is thus conceptually simple. But two immedi-
ate worries may spring to mind. A first concern is that although an intruder intercept-
ing Alice’s encrypted message will see only gibberish, the intruder knows both the 
key (Bob’s public key, which is available for all the world to see) and the algorithm 
that Alice used for encryption. Trudy can thus mount a chosen-plaintext attack, using 
the known standardized encryption algorithm and Bob’s publicly available encryption 
key to encode any message she chooses! Trudy might well try, for example, to encode 
messages, or parts of messages, that she suspects that Alice might send. Clearly, if 
public key cryptography is to work, key selection and encryption/decryption must be 
done in such a way that it is impossible (or at least so hard as to be nearly impossible) 
for an intruder to either determine Bob’s private key or somehow otherwise decrypt 
or guess Alice’s message to Bob. A second concern is that since Bob’s encryption key 
is public, anyone can send an encrypted message to Bob, including Alice or someone 
claiming to be Alice. In the case of a single shared secret key, the fact that the sender 
knows the secret key implicitly identifies the sender to the receiver. In the case of 
public key cryptography, however, this is no longer the case since anyone can send 
an encrypted message to Bob using Bob’s publicly available key. A digital signature, 
a topic we will study in Section 8.3, is needed to bind a sender to a message.
RSA
While there may be many algorithms that address these concerns, the RSA ­
algorithm 
(named after its founders, Ron Rivest, Adi Shamir, and Leonard Adleman) has 
become almost synonymous with public key cryptography. Let’s first see how RSA 
works and then examine why it works.
RSA makes extensive use of arithmetic operations using modulo-n arithmetic. 
So let’s briefly review modular arithmetic. Recall that x mod n simply means the 
remainder of x when divided by n; so, for example, 19 mod 5 = 4. In modular arith-
metic, one performs the usual operations of addition, multiplication, and exponen-
tiation. However, the result of each operation is replaced by the integer remainder 
that is left when the result is divided by n. Adding and multiplying with modular 
arithmetic is facilitated with the following handy facts:
[(a mod n) + (b mod n)] mod n = (a + b) mod n
[(a mod n) - (b mod n)] mod n = (a - b) mod n
[(a mod n) # (b mod n)] mod n = (a # b) mod n
It follows from the third fact that (a mod n)d mod n = ad mod n, which is an identity 
that we will soon find very useful.
Now suppose that Alice wants to send to Bob an RSA-encrypted message, as 
shown in Figure 8.6. In our discussion of RSA, let’s always keep in mind that a mes-
sage is nothing but a bit pattern, and every bit pattern can be uniquely represented by 
8.2    •    Principles of Cryptography         635
an integer number (along with the length of the bit pattern). For example, suppose 
a message is the bit pattern 1001; this message can be represented by the decimal 
integer 9. Thus, when encrypting a message with RSA, it is equivalent to encrypting 
the unique integer number that represents the message.
There are two interrelated components of RSA:
•	 The choice of the public key and the private key
•	 The encryption and decryption algorithm
To generate the public and private RSA keys, Bob performs the following steps:
	1.	 Choose two large prime numbers, p and q. How large should p and q be? The 
larger the values, the more difficult it is to break RSA, but the longer it takes 
to perform the encoding and decoding. RSA Laboratories recommends that 
the product of p and q be on the order of 1,024 bits. For a discussion of how to 
find large prime numbers, see [Caldwell 2012].
	2.	 Compute n = pq and z = (p - 1)(q - 1).
	3.	 Choose a number, e, less than n, that has no common factors (other than 1) 
with z. (In this case, e and z are said to be relatively prime.) The letter e is used 
since this value will be used in encryption.
	4.	 Find a number, d, such that ed - 1 is exactly divisible (that is, with no ­
remainder) 
by z. The letter d is used because this value will be used in decryption. Put another 
way, given e, we choose d such that
ed mod z = 1
	5.	 The public key that Bob makes available to the world, K+
B, is the pair of num-
bers (n, e); his private key, K-
B, is the pair of numbers (n, d).
The encryption by Alice and the decryption by Bob are done as follows:
•	 Suppose Alice wants to send Bob a bit pattern represented by the integer num-
ber m (with m 6 n). To encode, Alice performs the exponentiation me, and then 
computes the integer remainder when me is divided by n. In other words, the 
encrypted value, c, of Alice’s plaintext message, m, is
c = me mod n
	
The bit pattern corresponding to this ciphertext c is sent to Bob.
•	 To decrypt the received ciphertext message, c, Bob computes
m = cd mod n
which requires the use of his private key (n, d).
636         Chapter 8    •    Security in Computer Networks
As a simple example of RSA, suppose Bob chooses p = 5 and q = 7. ­
(Admittedly, 
these values are far too small to be secure.) Then n = 35 and z = 24. Bob chooses 
e = 5, since 5 and 24 have no common factors. Finally, Bob chooses d = 29, since 
5 # 29 - 1 (that is, ed - 1) is exactly divisible by 24. Bob makes the two values, n = 35 
 
and e = 5, public and keeps the value d = 29 secret. Observing these two public 
values, suppose Alice now wants to send the letters l, o, v, and e to Bob. Interpreting 
each letter as a number between 1 and 26 (with a being 1, and z being 26), Alice and 
Bob perform the encryption and decryption shown in Tables 8.2 and 8.3, respectively. 
Note that in this example, we consider each of the four letters as a distinct message. 
A more realistic example would be to convert the four letters into their 8-bit ASCII 
representations and then encrypt the integer corresponding to the resulting 32-bit bit 
pattern. (Such a realistic example generates numbers that are much too long to print 
in a textbook!)
Given that the “toy” example in Tables 8.2 and 8.3 has already produced some 
extremely large numbers, and given that we saw earlier that p and q should each be 
several hundred bits long, several practical issues regarding RSA come to mind. 
How does one choose large prime numbers? How does one then choose e and d? 
How does one perform exponentiation with large numbers? A discussion of these 
important issues is beyond the scope of this book; see [Kaufman 1995] and the refer-
ences therein for details.
Table 8.2  ♦  Alice’s RSA encryption, e = 5, n = 35
Plaintext Letter
m: numeric representation
me
Ciphertext c = me mod n
l
12
248832
17
o
15
759375
15
v
22
5153632
22
e
5
3125
10
Table 8.3  ♦  Bob’s RSA decryption, d = 29, n = 35
Ciphertext c
cd
m = cd mod n
Plaintext Letter
17
4819685721067509150915091411825223071697
12
l
15
127834039403948858939111232757568359375
15
o
22
851643319086537701956194499721106030592
22
v
10
1000000000000000000000000000000
5
e
8.2    •    Principles of Cryptography         637
Session Keys
We note here that the exponentiation required by RSA is a rather time-consuming process. 
By contrast, DES is at least 100 times faster in software and between 1,000 and 10,000 
times faster in hardware [RSA Fast 2012]. As a result, RSA is often used in practice 
in combination with symmetric key cryptography. For example, if Alice wants to send 
Bob a large amount of encrypted data, she could do the following. First Alice chooses 
a key that will be used to encode the data itself; this key is referred to as a session key, 
and is denoted by KS. Alice must inform Bob of the session key, since this is the shared 
­
symmetric key they will use with a symmetric key cipher (e.g., with DES or AES). Alice 
encrypts the session key using Bob’s public key, that is, computes c = (KS)e mod n. Bob 
receives the RSA-encrypted session key, c, and decrypts it to obtain the session key, KS. 
Bob now knows the session key that Alice will use for her encrypted data transfer.
Why Does RSA Work?
RSA encryption/decryption appears rather magical. Why should it be that by apply-
ing the encryption algorithm and then the decryption algorithm, one recovers the 
original message? In order to understand why RSA works, again denote n = pq, 
where p and q are the large prime numbers used in the RSA algorithm.
Recall that, under RSA encryption, a message (uniquely represented by an ­
integer), 
m, is exponentiated to the power e using modulo-n arithmetic, that is,
c = me mod n
Decryption is performed by raising this value to the power d, again using modulo-n 
arithmetic. The result of an encryption step followed by a decryption step is thus 
 
(me mod n)d mod n. Let’s now see what we can say about this quantity. As mentioned 
earlier, one important property of modulo arithmetic is (a mod n)d mod n = ad mod n 
for any values a, n, and d. Thus, using a = me in this property, we have
(me mod n)d mod n = med mod n
It therefore remains to show that med mod n = m. Although we’re trying to 
remove some of the magic about why RSA works, to establish this, we’ll need to use a 
rather magical result from number theory here. Specifically, we’ll need the result that 
says if p and q are prime, n = pq, and z = (p - 1)(q - 1), then xy mod n is the same as 
x(y mod z) mod n [Kaufman 1995]. Applying this result with x = m and y = ed we have
med mod n = m(ed mod z) mod n
But remember that we have chosen e and d such that ed mod z = 1. This gives us
med mod n = m1 mod n = m
638         Chapter 8    •    Security in Computer Networks
which is exactly the result we are looking for! By first exponentiating to the power of 
e (that is, encrypting) and then exponentiating to the power of d (that is, ­
decrypting), 
we obtain the original value, m. Even more wonderful is the fact that if we first 
exponentiate to the power of d and then exponentiate to the power of e—that is, we 
reverse the order of encryption and decryption, performing the decryption operation 
first and then applying the encryption operation—we also obtain the original value, 
m. This wonderful result follows immediately from the modular arithmetic:
(md mod n)e mod n = mde mod n = med mod n = (me mod n)d mod n
The security of RSA relies on the fact that there are no known algorithms for 
quickly factoring a number, in this case the public value n, into the primes p and q. If 
one knew p and q, then given the public value e, one could easily compute the secret 
key, d. On the other hand, it is not known whether or not there exist fast algorithms 
for factoring a number, and in this sense, the security of RSA is not guaranteed.
Another popular public-key encryption algorithm is the Diffie-Hellman algo-
rithm, which we will briefly explore in the homework problems. Diffie-Hellman 
is not as versatile as RSA in that it cannot be used to encrypt messages of arbitrary 
length; it can be used, however, to establish a symmetric session key, which is in turn 
used to encrypt messages.
8.3	 Message Integrity and Digital Signatures
In the previous section we saw how encryption can be used to provide confidenti-
ality to two communicating entities. In this section we turn to the equally impor-
tant cryptography topic of providing message integrity (also known as message 
­
authentication). Along with message integrity, we will discuss two related topics in 
this section: digital signatures and end-point authentication.
We define the message integrity problem using, once again, Alice and Bob. 
Suppose Bob receives a message (which may be encrypted or may be in plaintext) 
and he believes this message was sent by Alice. To authenticate this message, Bob 
needs to verify:
	1.	 The message indeed originated from Alice.
	2.	 The message was not tampered with on its way to Bob.
We’ll see in Sections 8.4 through 8.7 that this problem of message integrity is a criti-
cal concern in just about all secure networking protocols.
As a specific example, consider a computer network using a link-state routing 
algorithm (such as OSPF) for determining routes between each pair of routers in the 
8.3    •    Message Integrity and Digital Signatures         639
network (see Chapter 5). In a link-state algorithm, each router needs to broadcast a 
link-state message to all other routers in the network. A router’s link-state message 
includes a list of its directly connected neighbors and the direct costs to these neigh-
bors. Once a router receives link-state messages from all of the other routers, it can 
create a complete map of the network, run its least-cost routing algorithm, and con-
figure its forwarding table. One relatively easy attack on the routing algorithm is for 
Trudy to distribute bogus link-state messages with incorrect link-state information. 
Thus the need for message integrity—when router B receives a link-state message 
from router A, router B should verify that router A actually created the message and, 
further, that no one tampered with the message in transit.
In this section, we describe a popular message integrity technique that is used 
by many secure networking protocols. But before doing so, we need to cover another 
important topic in cryptography—cryptographic hash functions.
8.3.1	Cryptographic Hash Functions
As shown in Figure 8.7, a hash function takes an input, m, and computes a fixed-size 
string H(m) known as a hash. The Internet checksum (Chapter 3) and CRCs (Chapter 6) 
meet this definition. A cryptographic hash function is required to have the follow-
ing additional property:
•	 It is computationally infeasible to find any two different messages x and y such 
that H(x) = H(y).
Informally, this property means that it is computationally infeasible for an 
intruder to substitute one message for another message that is protected by the hash 
Figure 8.7  ♦  Hash functions
Many-to-one
hash function
Long message: m
Dear Alice:
This is a VERY long letter
since there is so much to
say.....
..........
..........
Bob
Fixed-length
hash: H(m)
Opgmdvboijrtnsd
gghPPdogm;lcvkb
640         Chapter 8    •    Security in Computer Networks
function. That is, if (m, H(m)) are the message and the hash of the message created 
by the sender, then an intruder cannot forge the contents of another message, y, that 
has the same hash value as the original message.
Let’s convince ourselves that a simple checksum, such as the Internet checksum, 
would make a poor cryptographic hash function. Rather than performing 1s comple-
ment arithmetic (as in the Internet checksum), let us compute a checksum by treating 
each character as a byte and adding the bytes together using 4-byte chunks at a time. 
Suppose Bob owes Alice $100.99 and sends an IOU to Alice consisting of the text 
string “IOU100.99BOB.” The ASCII representation (in hexadecimal notation) for 
these letters is 49,4F,55,31,30,30,2E,39,39,42,4F,42.
Figure 8.8 (top) shows that the 4-byte checksum for this message is B2 
C1 D2 AC. A slightly different message (and a much more costly one for Bob) 
is shown in the bottom half of Figure 8.8. The messages “IOU100.99BOB” and 
“IOU900.19BOB” have the same checksum. Thus, this simple checksum algorithm 
violates the requirement above. Given the original data, it is simple to find another 
set of data with the same checksum. Clearly, for security purposes, we are going to 
need a more powerful hash function than a checksum.
The MD5 hash algorithm of Ron Rivest [RFC 1321] is in wide use today. It 
computes a 128-bit hash in a four-step process consisting of a padding step (adding 
a one followed by enough zeros so that the length of the message satisfies certain 
conditions), an append step (appending a 64-bit representation of the message length 
before padding), an initialization of an accumulator, and a final looping step in which 
the message’s 16-word blocks are processed (mangled) in four rounds. For a descrip-
tion of MD5 (including a C source code implementation) see [RFC 1321].
Figure 8.8  ♦  
Initial message and fraudulent message have the same 
­
checksum!
Message
I O U 1
0 0 . 9
9 B O B
ASCII
Representation
49
4F
55
31
30
30
2E
39
39
42
4F
42
B2
C1
D2
AC
Checksum
Message
I O U 9
0 0 . 1
9 B O B
ASCII
Representation
49
4F
55
39
30
30
2E
31
39
42
4F
42
B2
C1
D2
AC
Checksum
8.3    •    Message Integrity and Digital Signatures         641
The second major hash algorithm in use today is the Secure Hash Algorithm 
(SHA-1) [FIPS 1995]. This algorithm is based on principles similar to those used 
in the design of MD4 [RFC 1320], the predecessor to MD5. SHA-1, a US federal 
standard, is required for use whenever a cryptographic hash algorithm is needed for 
federal applications. It produces a 160-bit message digest. The longer output length 
makes SHA-1 more secure.
8.3.2	Message Authentication Code
Let’s now return to the problem of message integrity. Now that we understand hash 
functions, let’s take a first stab at how we might perform message integrity:
	1.	 Alice creates message m and calculates the hash H(m) (for example with  
SHA-1).
	2.	 Alice then appends H(m) to the message m, creating an extended message  
(m, H(m)), and sends the extended message to Bob.
	3.	 Bob receives an extended message (m, h) and calculates H(m). If H(m) = h, 
Bob concludes that everything is fine.
This approach is obviously flawed. Trudy can create a bogus message m´ in which 
she says she is Alice, calculate H(m´), and send Bob (m´, H(m´)). When Bob receives 
the message, everything checks out in step 3, so Bob doesn’t suspect any funny 
­
business.
To perform message integrity, in addition to using cryptographic hash functions, 
Alice and Bob will need a shared secret s. This shared secret, which is nothing more 
than a string of bits, is called the authentication key. Using this shared secret, mes-
sage integrity can be performed as follows:
	1.	 Alice creates message m, concatenates s with m to create m + s, and calculates 
the hash H(m + s) (for example with SHA-1). H(m + s) is called the message 
authentication code (MAC).
	2.	 Alice then appends the MAC to the message m, creating an extended message 
(m, H(m + s)), and sends the extended message to Bob.
	3.	 Bob receives an extended message (m, h) and knowing s, calculates the MAC 
H(m + s). If H(m + s) = h, Bob concludes that everything is fine.
A summary of the procedure is shown in Figure 8.9. Readers should note that the 
MAC here (standing for “message authentication code”) is not the same MAC used 
in link-layer protocols (standing for “medium access control”)!
One nice feature of a MAC is that it does not require an encryption algorithm. 
Indeed, in many applications, including the link-state routing algorithm described 
earlier, communicating entities are only concerned with message integrity and are not 
concerned with message confidentiality. Using a MAC, the entities can authenticate 
642         Chapter 8    •    Security in Computer Networks
the messages they send to each other without having to integrate complex encryption 
algorithms into the integrity process.
As you might expect, a number of different standards for MACs have been pro-
posed over the years. The most popular standard today is HMAC, which can be used 
either with MD5 or SHA-1. HMAC actually runs data and the authentication key 
through the hash function twice [Kaufman 1995; RFC 2104].
There still remains an important issue. How do we distribute the shared authen-
tication key to the communicating entities? For example, in the link-state routing 
algorithm, we would somehow need to distribute the secret authentication key to 
each of the routers in the autonomous system. (Note that the routers can all use the 
same authentication key.) A network administrator could actually accomplish this by 
physically visiting each of the routers. Or, if the network administrator is a lazy guy, 
and if each router has its own public key, the network administrator could distribute 
the authentication key to any one of the routers by encrypting it with the router’s 
public key and then sending the encrypted key over the network to the router.
8.3.3	Digital Signatures
Think of the number of the times you’ve signed your name to a piece of paper dur-
ing the last week. You sign checks, credit card receipts, legal documents, and let-
ters. Your signature attests to the fact that you (as opposed to someone else) have 
acknowledged and/or agreed with the document’s contents. In a digital world, one 
often wants to indicate the owner or creator of a document, or to signify one’s agree-
ment with a document’s content. A digital signature is a cryptographic technique 
for achieving these goals in a digital world.
Just as with handwritten signatures, digital signing should be done in a way that 
is verifiable and nonforgeable. That is, it must be possible to prove that a document 
Figure 8.9  ♦  Message authentication code (MAC)
H(.)
H(.)
m
m
m
m
s
s
s
+
Internet
Compare
Key:
= Message
= Shared secret
H(m+s)
H(m+s)
8.3    •    Message Integrity and Digital Signatures         643
signed by an individual was indeed signed by that individual (the signature must be 
verifiable) and that only that individual could have signed the document (the signa-
ture cannot be forged).
Let’s now consider how we might design a digital signature scheme. Observe 
that when Bob signs a message, Bob must put something on the message that is 
unique to him. Bob could consider attaching a MAC for the signature, where the 
MAC is created by appending his key (unique to him) to the message, and then tak-
ing the hash. But for Alice to verify the signature, she must also have a copy of the 
key, in which case the key would not be unique to Bob. Thus, MACs are not going 
to get the job done here.
Recall that with public-key cryptography, Bob has both a public and private 
key, with both of these keys being unique to Bob. Thus, public-key cryptography is 
an excellent candidate for providing digital signatures. Let us now examine how it 
is done.
Suppose that Bob wants to digitally sign a document, m. We can think of the 
document as a file or a message that Bob is going to sign and send. As shown in 
Figure 8.10, to sign this document, Bob simply uses his private key, K-
B, to compute 
K-
B(m). At first, it might seem odd that Bob is using his private key (which, as we 
saw in Section 8.2, was used to decrypt a message that had been encrypted with his 
public key) to sign a document. But recall that encryption and decryption are nothing 
more than mathematical operations (exponentiation to the power of e or d in RSA; 
see Section 8.2) and recall that Bob’s goal is not to scramble or obscure the contents 
of the document, but rather to sign the document in a manner that is verifiable and 
nonforgeable. Bob’s digital signature of the document is K-
B(m).
Does the digital signature K-
B(m) meet our requirements of being verifiable and 
nonforgeable? Suppose Alice has m and K-
B(m). She wants to prove in court (being 
Figure 8.10  ♦  Creating a digital signature for a document
Encryption
algorithm
Message: m
Bob’s private
key, KB
–
Dear Alice:
Sorry I have been unable
to write for so long. Since
we.....
..........
..........
Bob
Signed message:
KB
– (m)
fadfg54986fgnzmcnv
T98734ngldskg02j
ser09tugkjdﬂg
..........
644         Chapter 8    •    Security in Computer Networks
litigious) that Bob had indeed signed the document and was the only person who 
could have possibly signed the document. Alice takes Bob’s public key, K+
B, and 
applies it to the digital signature, K-
B(m), associated with the document, m. That is, 
she computes K+
B(K-
B(m)), and voilà, with a dramatic flurry, she produces m, which 
exactly matches the original document! Alice then argues that only Bob could have 
signed the document, for the following reasons:
•	 Whoever signed the message must have used the private key, K-
B, in computing 
the signature K-
B(m), such that K+
B(K-
B(m)) = m.
•	 The only person who could have known the private key, K-
B, is Bob. Recall from 
our discussion of RSA in Section 8.2 that knowing the public key, K+
B, is of no 
help in learning the private key, K-
B. Therefore, the only person who could know 
K-
B is the person who generated the pair of keys, (K+
B, K-
B), in the first place, Bob. 
(Note that this assumes, though, that Bob has not given K-
B to anyone, nor has 
anyone stolen K-
B from Bob.)
It is also important to note that if the original document, m, is ever modified to 
some alternate form, m´, the signature that Bob created for m will not be valid for m´, 
since K+
B(K-
B(m)) does not equal m´. Thus we see that digital signatures also provide 
message integrity, allowing the receiver to verify that the message was unaltered as 
well as the source of the message.
One concern with signing data by encryption is that encryption and decryption 
are computationally expensive. Given the overheads of encryption and decryption, 
signing data via complete encryption/decryption can be overkill. A more efficient 
approach is to introduce hash functions into the digital signature. Recall from 
­
Section 8.3.2 that a hash algorithm takes a message, m, of arbitrary length and com-
putes a fixed-length “fingerprint” of the message, denoted by H(m). Using a hash 
function, Bob signs the hash of a message rather than the message itself, that is, 
Bob calculates K-
B(H(m)). Since H(m) is generally much smaller than the original 
message m, the computational effort required to create the digital signature is sub-
stantially reduced.
In the context of Bob sending a message to Alice, Figure 8.11 provides a sum-
mary of the operational procedure of creating a digital signature. Bob puts his origi-
nal long message through a hash function. He then digitally signs the resulting hash 
with his private key. The original message (in cleartext) along with the digitally 
signed message digest (henceforth referred to as the digital signature) is then sent 
 
to Alice. Figure 8.12 provides a summary of the operational procedure of the sig-
nature. Alice applies the sender’s public key to the message to obtain a hash result. 
Alice also applies the hash function to the cleartext message to obtain a second hash 
result. If the two hashes match, then Alice can be sure about the integrity and author 
of the message.
Before moving on, let’s briefly compare digital signatures with MACs, since they 
have parallels, but also have important subtle differences. Both digital signatures and 
8.3    •    Message Integrity and Digital Signatures         645
MACs start with a message (or a document). To create a MAC out of the message, 
we append an authentication key to the message, and then take the hash of the result. 
Note that neither public key nor symmetric key encryption is involved in creating the 
MAC. To create a digital signature, we first take the hash of the message and then 
encrypt the message with our private key (using public key cryptography). Thus, a 
digital signature is a “heavier” technique, since it requires an underlying Public Key 
Infrastructure (PKI) with certification authorities as described below. We’ll see in 
Section 8.4 that PGP—a popular secure e-mail system—uses digital signatures for 
message integrity. We’ve seen already that OSPF uses MACs for message integrity. 
We’ll see in Sections 8.5 and 8.6 that MACs are also used for popular transport-layer 
and network-layer security protocols.
Public Key Certification
An important application of digital signatures is public key certification, that is, 
certifying that a public key belongs to a specific entity. Public key certification is 
used in many popular secure networking protocols, including IPsec and SSL.
To gain insight into this problem, let’s consider an Internet-commerce version of 
the classic “pizza prank.” Alice is in the pizza delivery business and accepts orders 
Figure 8.11  ♦  Sending a digitally signed message
Bob’s private
key, KB
–
Many-to-one
hash function
Long message
Dear Alice:
This is a VERY long letter
since there is so much to
say.....
..........
..........
Bob
Fixed-length
hash
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Signed
hash
Package to send
to Alice
Fgkopdgoo69cmxw
54psdterma[asofmz
Encryption
algorithm
646         Chapter 8    •    Security in Computer Networks
over the Internet. Bob, a pizza lover, sends Alice a plaintext message that includes 
his home address and the type of pizza he wants. In this message, Bob also includes 
a digital signature (that is, a signed hash of the original plaintext message) to prove to 
Alice that he is the true source of the message. To verify the signature, Alice obtains 
Bob’s public key (perhaps from a public key server or from the e-mail message) 
and checks the digital signature. In this manner she makes sure that Bob, rather than 
some adolescent prankster, placed the order.
This all sounds fine until clever Trudy comes along. As shown in Figure 8.13, 
Trudy is indulging in a prank. She sends a message to Alice in which she says she is 
Bob, gives Bob’s home address, and orders a pizza. In this message she also includes 
her (Trudy’s) public key, although Alice naturally assumes it is Bob’s public key. 
Trudy also attaches a digital signature, which was created with her own (Trudy’s) 
private key. After receiving the message, Alice applies Trudy’s public key (thinking 
that it is Bob’s) to the digital signature and concludes that the plaintext message was 
Bob’s public
key, KB
+
Long message
Dear Alice:
This is a VERY long letter
since there is so much to
say.....
..........
..........
Bob
Fixed-length
hash
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Signed
hash
Fgkopdgoo69cmxw
54psdterma[asofmz
Many-to-one
hash function
Compare
Fixed-length
hash
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Encryption
algorithm
Figure 8.12  ♦  Verifying a signed message
8.3    •    Message Integrity and Digital Signatures         647
indeed created by Bob. Bob will be very surprised when the delivery person brings a 
pizza with pepperoni and anchovies to his home!
We see from this example that for public key cryptography to be useful, you 
need to be able to verify that you have the actual public key of the entity (person, 
router, browser, and so on) with whom you want to communicate. For example, 
when Alice wants to communicate with Bob using public key cryptography, she 
needs to verify that the public key that is supposed to be Bob’s is indeed Bob’s.
Binding a public key to a particular entity is typically done by a Certification 
Authority (CA), whose job is to validate identities and issue certificates. A CA has 
the following roles:
	1.	 A CA verifies that an entity (a person, a router, and so on) is who it says it is. 
There are no mandated procedures for how certification is done. When dealing 
with a CA, one must trust the CA to have performed a suitably rigorous iden-
tity verification. For example, if Trudy were able to walk into the Fly-by-Night 
Figure 8.13  ♦  Trudy masquerades as Bob using public key cryptography
Trudy’s private
key, KT
–
Trudy’s public
key, KT
+
Signed (using
Trudy's private key)
message digest
Fgkopdgoo69cmxw
54psdterma[asofmz
Message
Alice,
Deliver a pizza to me.
                               Bob
Many-to-one
hash function
Alice uses Trudy’s
public key, thinking
it is Bob’s, and
concludes the
message is from Bob
PIZZA
Encryption
algorithm
648         Chapter 8    •    Security in Computer Networks
CA and simply announce “I am Alice” and receive certificates associated  
with the identity of Alice, then one shouldn’t put much faith in public keys 
certified by the Fly-by-Night CA. On the other hand, one might (or might not!) 
be more willing to trust a CA that is part of a federal or state program. You  
can trust the identity associated with a public key only to the extent to which 
you can trust a CA and its identity verification techniques. What a tangled  
web of trust we spin!
	2.	 Once the CA verifies the identity of the entity, the CA creates a certificate 
that binds the public key of the entity to the identity. The certificate contains 
the public key and globally unique identifying information about the owner of 
the public key (for example, a human name or an IP address). The certificate is 
digitally signed by the CA. These steps are shown in Figure 8.14.
Let us now see how certificates can be used to combat pizza-ordering prank-
sters, like Trudy, and other undesirables. When Bob places his order he also sends his 
CA-signed certificate. Alice uses the CA’s public key to check the validity of Bob’s 
certificate and extract Bob’s public key.
Both the International Telecommunication Union (ITU) and the IETF have 
developed standards for CAs. ITU X.509 [ITU 2005a] specifies an authentication 
service as well as a specific syntax for certificates. [RFC 1422] describes CA-based 
key management for use with secure Internet e-mail. It is compatible with X.509 but 
goes beyond X.509 by establishing procedures and conventions for a key manage-
ment architecture. Table 8.4 describes some of the important fields in a certificate.
Figure 8.14  ♦  Bob has his public key certified by the CA
Bob’s CA-signed
certiﬁcate containing
his public key, KB
+
Certiﬁcation
Authority (CA)
(KB
+, B)
CA’s private
key, KCA
–
Encryption
algorithm
8.4    •    End-Point Authentication         649
8.4	 End-Point Authentication
End-point authentication is the process of one entity proving its identity to another 
entity over a computer network, for example, a user proving its identity to an e-mail 
server. As humans, we authenticate each other in many ways: We recognize each 
­
other’s faces when we meet, we recognize each other’s voices on the telephone, we are 
authenticated by the customs official who checks us against the picture on our passport.
In this section, we consider how one party can authenticate another party when 
the two are communicating over a network. We focus here on authenticating a “live” 
party, at the point in time when communication is actually occurring. A concrete 
example is a user authenticating him or herself to an e-mail server. This is a subtly 
different problem from proving that a message received at some point in the past did 
indeed come from that claimed sender, as studied in Section 8.3.
When performing authentication over the network, the communicating parties 
cannot rely on biometric information, such as a visual appearance or a voiceprint. 
Indeed, we will see in our later case studies that it is often network elements such as 
routers and client/server processes that must authenticate each other. Here, authen-
tication must be done solely on the basis of messages and data exchanged as part of 
an authentication protocol. Typically, an authentication protocol would run before 
the two communicating parties run some other protocol (for example, a reliable data 
transfer protocol, a routing information exchange protocol, or an e-mail protocol). 
The authentication protocol first establishes the identities of the parties to each oth-
er’s satisfaction; only after authentication do the parties get down to the work at hand.
As in the case of our development of a reliable data transfer (rdt) protocol in Chapter 
3, we will find it instructive here to develop various versions of an authentication pro-
tocol, which we will call ap (authentication protocol), and poke holes in each version 
Table 8.4  ♦  Selected fields in an X.509 and RFC 1422 public key
Field Name
Description
Version
Version number of X.509 specification
Serial number
CA-issued unique identifier for a certificate
Signature
Specifies the algorithm used by CA to sign this certificate
Issuer name
Identity of CA issuing this certificate, in distinguished name (DN) [RFC 4514] format
Validity period
Start and end of period of validity for certificate
Subject name
Identity of entity whose public key is associated with this certificate, in DN format
Subject public key
The subject’s public key as well indication of the public key algorithm (and algorithm 
parameters) to be used with this key
650         Chapter 8    •    Security in Computer Networks
as we proceed. (If you enjoy this stepwise evolution of a design, you might also enjoy 
[Bryant 1988], which recounts a fictitious narrative between designers of an open- 
network authentication system, and their discovery of the many subtle issues involved.)
Let’s assume that Alice needs to authenticate herself to Bob.
8.4.1	Authentication Protocol ap1.0
Perhaps the simplest authentication protocol we can imagine is one where Alice sim-
ply sends a message to Bob saying she is Alice. This protocol is shown in Figure 8.15. 
The flaw here is obvious—there is no way for Bob actually to know that the person 
sending the message “I am Alice” is indeed Alice. For example, Trudy (the intruder) 
could just as well send such a message.
8.4.2	Authentication Protocol ap2.0
If Alice has a well-known network address (e.g., an IP address) from which she 
always communicates, Bob could attempt to authenticate Alice by verifying that 
the source address on the IP datagram carrying the authentication message matches 
Alice’s well-known address. In this case, Alice would be authenticated. This might 
stop a very network-naive intruder from impersonating Alice, but it wouldn’t stop 
the determined student studying this book, or many others!
From our study of the network and data link layers, we know that it is not that 
hard (for example, if one had access to the operating system code and could build 
one’s own operating system kernel, as is the case with Linux and several other 
freely available operating systems) to create an IP datagram, put whatever IP source 
address we want (for example, Alice’s well-known IP address) into the IP datagram, 
and send the datagram over the link-layer protocol to the first-hop router. From then 
Figure 8.15  ♦  Protocol ap1.0 and a failure scenario
Alice
I am Alice
Bob
Trudy
Trudy
Alice
I am Alice
Bob
8.4    •    End-Point Authentication         651
on, the incorrectly source-addressed datagram would be dutifully forwarded to Bob. 
This approach, shown in Figure 8.16, is a form of IP spoofing. IP spoofing can be 
avoided if Trudy’s first-hop router is configured to forward only datagrams con-
taining Trudy’s IP source address [RFC 2827]. However, this capability is not uni-
versally deployed or enforced. Bob would thus be foolish to assume that Trudy’s 
network manager (who might be Trudy herself) had configured Trudy’s first-hop 
router to forward only appropriately addressed datagrams.
8.4.3	Authentication Protocol ap3.0
One classic approach to authentication is to use a secret password. The password is 
a shared secret between the authenticator and the person being authenticated. Gmail, 
Facebook, telnet, FTP, and many other services use password authentication. In pro-
tocol ap3.0, Alice thus sends her secret password to Bob, as shown in Figure 8.17.
Since passwords are so widely used, we might suspect that protocol ap3.0 is 
fairly secure. If so, we’d be wrong! The security flaw here is clear. If Trudy eaves-
drops on Alice’s communication, then she can learn Alice’s password. Lest you think 
this is unlikely, consider the fact that when you Telnet to another machine and log 
 
in, the login password is sent unencrypted to the Telnet server. Someone connected 
to the Telnet client or server’s LAN can possibly sniff (read and store) all packets 
transmitted on the LAN and thus steal the login password. In fact, this is a well-
known approach for stealing passwords (see, for example, [Jimenez 1997]). Such a 
threat is obviously very real, so ap3.0 clearly won’t do.
8.4.4	Authentication Protocol ap3.1
Our next idea for fixing ap3.0 is naturally to encrypt the password. By encrypting 
the password, we can prevent Trudy from learning Alice’s password. If we assume 
Figure 8.16  ♦  Protocol ap2.0 and a failure scenario
Alice
I am Alice
Alice’s IP addr.
Bob
Trudy
Alice
I am Alice
Alice’s IP addr.
Bob
Trudy
652         Chapter 8    •    Security in Computer Networks
that Alice and Bob share a symmetric secret key, KA-B, then Alice can encrypt the 
password and send her identification message, “I am Alice,” and her encrypted 
password to Bob. Bob then decrypts the password and, assuming the password is cor-
rect, authenticates Alice. Bob feels comfortable in authenticating Alice since Alice 
not only knows the password, but also knows the shared secret key value needed to 
encrypt the password. Let’s call this protocol ap3.1.
While it is true that ap3.1 prevents Trudy from learning Alice’s password, the 
use of cryptography here does not solve the authentication problem. Bob is subject 
to a playback attack: Trudy need only eavesdrop on Alice’s communication, record 
the encrypted version of the password, and play back the encrypted version of the 
password to Bob to pretend that she is Alice. The use of an encrypted password in 
ap3.1 doesn’t make the situation manifestly different from that of protocol ap3.0 in 
Figure 8.17.
8.4.5	Authentication Protocol ap4.0
The failure scenario in Figure 8.17 resulted from the fact that Bob could not distin-
guish between the original authentication of Alice and the later playback of Alice’s 
original authentication. That is, Bob could not tell if Alice was live (that is, was 
currently really on the other end of the connection) or whether the messages he was 
receiving were a recorded playback of a previous authentication of Alice. The very 
(very) observant reader will recall that the three-way TCP handshake protocol needed 
Figure 8.17  ♦  Protocol ap3.0 and a failure scenario
Alice
I am Alice,
password
OK
Bob
Trudy
Alice
I am Alice,
password
OK
Bob
Trudy
Tape recorder
Key:
8.4    •    End-Point Authentication         653
to address the same problem—the server side of a TCP connection did not want to 
accept a connection if the received SYN segment was an old copy (retransmission) 
of a SYN segment from an earlier connection. How did the TCP server side solve 
the problem of determining whether the client was really live? It chose an initial 
sequence number that had not been used in a very long time, sent that number to the 
client, and then waited for the client to respond with an ACK segment containing that 
number. We can adopt the same idea here for authentication purposes.
A nonce is a number that a protocol will use only once in a lifetime. That is, 
once a protocol uses a nonce, it will never use that number again. Our ap4.0 protocol 
uses a nonce as follows:
	1.	 Alice sends the message “I am Alice” to Bob.
	2.	 Bob chooses a nonce, R, and sends it to Alice.
	3.	 Alice encrypts the nonce using Alice and Bob’s symmetric secret key, KA-B, 
and sends the encrypted nonce, KA-B (R), back to Bob. As in protocol ap3.1, 
it is the fact that Alice knows KA-B and uses it to encrypt a value that lets Bob 
know that the message he receives was generated by Alice. The nonce is used 
to ensure that Alice is live.
	4.	 Bob decrypts the received message. If the decrypted nonce equals the nonce he 
sent Alice, then Alice is authenticated.
Protocol ap4.0 is illustrated in Figure 8.18. By using the once-in-a-lifetime 
value, R, and then checking the returned value, KA-B (R), Bob can be sure that Alice 
is both who she says she is (since she knows the secret key value needed to encrypt 
R) and live (since she has encrypted the nonce, R, that Bob just created).
The use of a nonce and symmetric key cryptography forms the basis of ap4.0. A 
natural question is whether we can use a nonce and public key cryptography (rather 
than symmetric key cryptography) to solve the authentication problem. This issue is 
explored in the problems at the end of the chapter.
Figure 8.18  ♦  Protocol ap4.0 and a failure scenario
Alice
R
KA–B(R)
I am Alice
Bob
654         Chapter 8    •    Security in Computer Networks
8.5	 Securing E-Mail
In previous sections, we examined fundamental issues in network security, including 
symmetric key and public key cryptography, end-point authentication, key distribu-
tion, message integrity, and digital signatures. We are now going to examine how 
these tools are being used to provide security in the Internet.
Interestingly, it is possible to provide security services in any of the top four 
layers of the Internet protocol stack. When security is provided for a specific applica-
tion-layer protocol, the application using the protocol will enjoy one or more security 
services, such as confidentiality, authentication, or integrity. When security is pro-
vided for a transport-layer protocol, all applications that use that protocol enjoy the 
security services of the transport protocol. When security is provided at the network 
layer on a host-to-host basis, all transport-layer segments (and hence all application-
layer data) enjoy the security services of the network layer. When security is pro-
vided on a link basis, then the data in all frames traveling over the link receive the 
security services of the link.
In Sections 8.5 through 8.8, we examine how security tools are being used in 
the application, transport, network, and link layers. Being consistent with the general 
structure of this book, we begin at the top of the protocol stack and discuss security at 
the application layer. Our approach is to use a specific application, e-mail, as a case 
study for application-layer security. We then move down the protocol stack. We’ll 
examine the SSL protocol (which provides security at the transport layer), IPsec 
(which provides security at the network layer), and the security of the IEEE 802.11 
wireless LAN protocol.
You might be wondering why security functionality is being provided at more 
than one layer in the Internet. Wouldn’t it suffice simply to provide the security 
functionality at the network layer and be done with it? There are two answers to this 
question. First, although security at the network layer can offer “blanket coverage” 
by encrypting all the data in the datagrams (that is, all the transport-layer segments) 
and by authenticating all the source IP addresses, it can’t provide user-level secu-
rity. For example, a commerce site cannot rely on IP-layer security to authenticate 
a customer who is purchasing goods at the commerce site. Thus, there is a need 
for security functionality at higher layers as well as blanket coverage at lower lay-
ers. Second, it is generally easier to deploy new Internet services, including security 
services, at the higher layers of the protocol stack. While waiting for security to be 
broadly deployed at the network layer, which is probably still many years in the 
future, many application developers “just do it” and introduce security functional-
ity into their favorite applications. A classic example is Pretty Good Privacy (PGP), 
which provides secure e-mail (discussed later in this section). Requiring only client 
and server application code, PGP was one of the first security technologies to be 
broadly used in the Internet.
8.5    •    Securing E-Mail         655
8.5.1	Secure E-Mail
We now use the cryptographic principles of Sections 8.2 through 8.3 to create a 
secure e-mail system. We create this high-level design in an incremental manner, 
at each step introducing new security services. When designing a secure e-mail sys-
tem, let us keep in mind the racy example introduced in Section 8.1—the love affair 
between Alice and Bob. Imagine that Alice wants to send an e-mail message to Bob, 
and Trudy wants to intrude.
Before plowing ahead and designing a secure e-mail system for Alice and Bob, 
we should consider which security features would be most desirable for them. First 
and foremost is confidentiality. As discussed in Section 8.1, neither Alice nor Bob 
wants Trudy to read Alice’s e-mail message. The second feature that Alice and Bob 
would most likely want to see in the secure e-mail system is sender authentication. 
In particular, when Bob receives the message “I don’t love you anymore. 
I never want to see you again. Formerly yours, Alice,” 
he would naturally want to be sure that the message came from Alice and not from 
Trudy. Another feature that the two lovers would appreciate is message integrity, 
that is, assurance that the message Alice sends is not modified while en route to 
Bob. Finally, the e-mail system should provide receiver authentication; that is, Alice 
wants to make sure that she is indeed sending the letter to Bob and not to someone 
else (for example, Trudy) who is impersonating Bob.
So let’s begin by addressing the foremost concern, confidentiality. The most 
straightforward way to provide confidentiality is for Alice to encrypt the message 
with symmetric key technology (such as DES or AES) and for Bob to decrypt the 
message on receipt. As discussed in Section 8.2, if the symmetric key is long enough, 
and if only Alice and Bob have the key, then it is extremely difficult for anyone else 
(including Trudy) to read the message. Although this approach is straightforward, it 
has the fundamental difficulty that we discussed in Section 8.2—distributing a sym-
metric key so that only Alice and Bob have copies of it. So we naturally consider an 
alternative approach—public key cryptography (using, for example, RSA). In the 
public key approach, Bob makes his public key publicly available (e.g., in a public 
key server or on his personal Web page), Alice encrypts her message with Bob’s 
public key, and she sends the encrypted message to Bob’s e-mail address. When Bob 
receives the message, he simply decrypts it with his private key. Assuming that Alice 
knows for sure that the public key is Bob’s public key, this approach is an excellent 
means to provide the desired confidentiality. One problem, however, is that public 
key encryption is relatively inefficient, particularly for long messages.
To overcome the efficiency problem, let’s make use of a session key (discussed 
in Section 8.2.2). In particular, Alice (1) selects a random symmetric session key, KS, 
(2) encrypts her message, m, with the symmetric key, (3) encrypts the symmetric 
key with Bob’s public key, KB 
+, (4) concatenates the encrypted message and the 
encrypted symmetric key to form a “package,” and (5) sends the package to Bob’s 
656         Chapter 8    •    Security in Computer Networks
e-mail address. The steps are illustrated in Figure 8.19. (In this and the subsequent 
figures, the circled “+” represents concatenation and the circled “-” represents 
deconcatenation.) When Bob receives the package, he (1) uses his private key, K-
B, 
to obtain the symmetric key, KS, and (2) uses the symmetric key KS to decrypt the 
message m.
Having designed a secure e-mail system that provides confidentiality, let’s now 
design another system that provides both sender authentication and message integ-
rity. We’ll suppose, for the moment, that Alice and Bob are no longer concerned with 
confidentiality (they want to share their feelings with everyone!), and are concerned 
only about sender authentication and message integrity. To accomplish this task, we 
use digital signatures and message digests, as described in Section 8.3. Specifically, 
Alice (1) applies a hash function, H (for example, MD5), to her message, m, to obtain 
a message digest, (2) signs the result of the hash function with her private key, K-
A, to 
create a digital signature, (3) concatenates the original (unencrypted) message with 
the signature to create a package, and (4) sends the package to Bob’s e-mail address. 
When Bob receives the package, he (1) applies Alice’s public key, K+
A, to the signed 
message digest and (2) compares the result of this operation with his own hash, H, 
of the message. The steps are illustrated in Figure 8.20. As discussed in Section 8.3, 
if the two results are the same, Bob can be pretty confident that the message came 
from Alice and is unaltered.
Now let’s consider designing an e-mail system that provides confidentiality, 
sender authentication, and message integrity. This can be done by combining the 
procedures in Figures 8.19 and 8.20. Alice first creates a preliminary package, 
exactly as in Figure 8.20, that consists of her original message along with a digitally 
signed hash of the message. She then treats this preliminary package as a message in 
itself and sends this new message through the sender steps in Figure 8.19, creating a 
new package that is sent to Bob. The steps applied by Alice are shown in Figure 8.21. 
When Bob receives the package, he first applies his side of Figure 8.19 and then his 
Figure 8.19  ♦  
Alice used a symmetric session key, KS, to send a secret 
e-mail to Bob
KS(.)
KS(.)
KS(m)
KS(m)
KS
KS
KB
+(.)
KB
+(KS)
KB
+(KS)
m
m
+
–
Internet
KB
–(.)
Alice sends e-mail message m
Bob receives e-mail message m
8.5    •    Securing E-Mail         657
side of Figure 8.20. It should be clear that this design achieves the goal of provid-
ing confidentiality, sender authentication, and message integrity. Note that, in this 
scheme, Alice uses public key cryptography twice: once with her own private key 
and once with Bob’s public key. Similarly, Bob also uses public key cryptography 
twice—once with his private key and once with Alice’s public key.
The secure e-mail design outlined in Figure 8.21 probably provides satisfactory 
security for most e-mail users for most occasions. But there is still one important 
issue that remains to be addressed. The design in Figure 8.21 requires Alice to obtain 
Bob’s public key, and requires Bob to obtain Alice’s public key. The distribution 
of these public keys is a nontrivial problem. For example, Trudy might masquerade 
as Bob and give Alice her own public key while saying that it is Bob’s public key, 
Figure 8.20  ♦  
Using hash functions and digital signatures to provide 
­
sender authentication and message integrity
H(.)
KA
–(.)
KA
+(.)
KA
–(H(m))
KA
–(H(m))
m
m
m
+
–
Internet
Alice sends e-mail message m
Bob receives e-mail message m
H(.)
Compare
Figure 8.21  ♦  
Alice uses symmetric key cyptography, public key 
­
cryptography, a hash function, and a digital signature to 
­
provide secrecy, sender authentication, and message integrity
H(.)
KA
–(.)
KS(.)
KS
KA
–(H(m))
m
m
+
+
to Internet
KB
+(.)
658         Chapter 8    •    Security in Computer Networks
enabling her to receive the message meant for Bob. As we learned in Section 8.3, a 
popular approach for securely distributing public keys is to certify the public keys 
using a CA.
8.5.2	PGP
Written by Phil Zimmermann in 1991, Pretty Good Privacy (PGP) is a nice exam-
ple of an e-mail encryption scheme [PGPI 2016]. Versions of PGP are available in 
the public domain; for example, you can find the PGP software for your favorite plat-
form as well as lots of interesting reading at the International PGP Home Page [PGPI 
2016]. The PGP design is, in essence, the same as the design shown in Figure 8.21. 
Depending on the version, the PGP software uses MD5 or SHA for calculating the 
message digest; CAST, triple-DES, or IDEA for symmetric key encryption; and 
RSA for the public key encryption.
When PGP is installed, the software creates a public key pair for the user. The 
public key can be posted on the user’s Web site or placed in a public key server. The 
private key is protected by the use of a password. The password has to be entered 
every time the user accesses the private key. PGP gives the user the option of dig-
itally signing the message, encrypting the message, or both digitally signing and 
encrypting. Figure 8.22 shows a PGP signed message. This message appears after the 
MIME header. The encoded data in the message is K-
A (H(m)), that is, the digitally 
signed message digest. As we discussed above, in order for Bob to verify the integ-
rity of the message, he needs to have access to Alice’s public key.
Figure 8.23 shows a secret PGP message. This message also appears after the 
MIME header. Of course, the plaintext message is not included within the secret e-mail 
message. When a sender (such as Alice) wants both confidentiality and integrity, PGP 
contains a message like that of Figure 8.23 within the message of Figure 8.22.
PGP also provides a mechanism for public key certification, but the mechanism 
is quite different from the more conventional CA. PGP public keys are certified by 
Figure 8.22  ♦  A PGP signed message
-----BEGIN PGP SIGNED MESSAGE-----
Hash:  SHA1
Bob:
Can I see you tonight?
Passionately yours, Alice
-----BEGIN PGP SIGNATURE-----
Version: PGP for Personal Privacy 5.0
Charset:
noconv
yhHJRHhGJGhgg/12EpJ+lo8gE4vB3mqJhFEvZP9t6n7G6m5Gw2
-----END PGP SIGNATURE-----
8.6    •    Securing TCP Connections: SSL         659
a web of trust. Alice herself can certify any key/username pair when she believes 
the pair really belong together. In addition, PGP permits Alice to say that she trusts 
another user to vouch for the authenticity of more keys. Some PGP users sign each 
other’s keys by holding key-signing parties. Users physically gather, exchange 
­
public keys, and certify each other’s keys by signing them with their private keys.
8.6	 Securing TCP Connections: SSL
In the previous section, we saw how cryptographic techniques can provide confiden-
tiality, data integrity, and end-point authentication to a specific application, namely, 
e-mail. In this section, we’ll drop down a layer in the protocol stack and examine 
how cryptography can enhance TCP with security services, including confidential-
ity, data integrity, and end-point authentication. This enhanced version of TCP is 
commonly known as Secure Sockets Layer (SSL). A slightly modified version of 
SSL version 3, called Transport Layer Security (TLS), has been standardized by 
the IETF [RFC 4346].
The SSL protocol was originally designed by Netscape, but the basic ideas behind 
securing TCP had predated Netscape’s work (for example, see Woo [Woo 1994]). 
Since its inception, SSL has enjoyed broad deployment. SSL is supported by all popu-
lar Web browsers and Web servers, and it is used by Gmail and essentially all Internet 
commerce sites (including Amazon, eBay, and TaoBao). Hundreds of billions of dol-
lars are spent over SSL every year. In fact, if you have ever purchased anything over 
the Internet with your credit card, the communication between your browser and the 
server for this purchase almost certainly went over SSL. (You can identify that SSL is 
being used by your browser when the URL begins with https: rather than http.)
To understand the need for SSL, let’s walk through a typical Internet commerce 
scenario. Bob is surfing the Web and arrives at the Alice Incorporated site, which is 
selling perfume. The Alice Incorporated site displays a form in which Bob is sup-
posed to enter the type of perfume and quantity desired, his address, and his pay-
ment card number. Bob enters this information, clicks on Submit, and expects to 
receive (via ordinary postal mail) the purchased perfumes; he also expects to receive 
Figure 8.23  ♦  A secret PGP message
-----BEGIN PGP MESSAGE-----
Version: PGP for Personal Privacy 5.0
u2R4d+/jKmn8Bc5+hgDsqAewsDfrGdszX68liKm5F6Gc4sDfcXyt
RfdS10juHgbcfDssWe7/K=lKhnMikLo0+1/BvcX4t==Ujk9PbcD4
Thdf2awQfgHbnmKlok8iy6gThlp
-----END PGP MESSAGE
660         Chapter 8    •    Security in Computer Networks
a charge for his order in his next payment card statement. This all sounds good, but 
if no security measures are taken, Bob could be in for a few surprises.
•	 If no confidentiality (encryption) is used, an intruder could intercept Bob’s order 
and obtain his payment card information. The intruder could then make purchases 
at Bob’s expense.
•	 If no data integrity is used, an intruder could modify Bob’s order, having him 
purchase ten times more bottles of perfume than desired.
•	 Finally, if no server authentication is used, a server could display Alice Incor-
porated’s famous logo when in actuality the site maintained by Trudy, who is 
masquerading as Alice Incorporated. After receiving Bob’s order, Trudy could 
take Bob’s money and run. Or Trudy could carry out an identity theft by collect-
ing Bob’s name, address, and credit card number.
SSL addresses these issues by enhancing TCP with confidentiality, data integrity, 
server authentication, and client authentication.
SSL is often used to provide security to transactions that take place over HTTP. 
However, because SSL secures TCP, it can be employed by any application that runs 
over TCP. SSL provides a simple Application Programmer Interface (API) with sock-
ets, which is similar and analogous to TCP’s API. When an application wants to employ 
SSL, the application includes SSL classes/libraries. As shown in Figure 8.24, although 
SSL technically resides in the application layer, from the developer’s perspective it 
is a transport protocol that provides TCP’s services enhanced with security services.
8.6.1	The Big Picture
We begin by describing a simplified version of SSL, one that will allow us to get a 
big-picture understanding of the why and how of SSL. We will refer to this simplified 
Figure 8.24  ♦  
Although SSL technically resides in the application layer, 
from the developer’s perspective it is a transport-layer 
­
protocol
TCP
SSL sublayer
IP
Application
Application
layer
TCP enhanced with SSL
SSL socket
TCP socket
TCP
IP
Application
TCP API
TCP socket
8.6    •    Securing TCP Connections: SSL         661
version of SSL as “almost-SSL.” After describing almost-SSL, in the next subsec-
tion we’ll then describe the real SSL, filling in the details. Almost-SSL (and SSL) 
has three phases: handshake, key derivation, and data transfer. We now describe 
these three phases for a communication session between a client (Bob) and a server 
(Alice), with Alice having a private/public key pair and a certificate that binds her 
identity to her public key.
Handshake
During the handshake phase, Bob needs to (a) establish a TCP connection with Alice, 
(b) verify that Alice is really Alice, and (c) send Alice a master secret key, which 
will be used by both Alice and Bob to generate all the symmetric keys they need for 
the SSL session. These three steps are shown in Figure 8.25. Note that once the TCP 
connection is established, Bob sends Alice a hello message. Alice then responds with 
her certificate, which contains her public key. As discussed in Section 8.3, because 
the certificate has been certified by a CA, Bob knows for sure that the public key in 
the certificate belongs to Alice. Bob then generates a Master Secret (MS) (which will 
only be used for this SSL session), encrypts the MS with Alice’s public key to create 
the Encrypted Master Secret (EMS), and sends the EMS to Alice. Alice decrypts the 
EMS with her private key to get the MS. After this phase, both Bob and Alice (and 
no one else) know the master secret for this SSL session.
Figure 8.25  ♦  
The almost-SSL handshake, beginning with a TCP 
­
connection
TCP SYN
TCP/SYNACK
Decrypts EMS with
KA
– to get MS
EMS = KA
+(MS)
TCP ACK
SSL hello
certiﬁcate
(b)
(a)
(c) 
Create Master
Secret (MS)
662         Chapter 8    •    Security in Computer Networks
Key Derivation
In principle, the MS, now shared by Bob and Alice, could be used as the symmetric 
session key for all subsequent encryption and data integrity checking. It is, however, 
generally considered safer for Alice and Bob to each use different cryptographic 
keys, and also to use different keys for encryption and integrity checking. Thus, both 
Alice and Bob use the MS to generate four keys:
•	 EB = session encryption key for data sent from Bob to Alice
•	 MB = session MAC key for data sent from Bob to Alice
•	 EA = session encryption key for data sent from Alice to Bob
•	 MA = session MAC key for data sent from Alice to Bob
Alice and Bob each generate the four keys from the MS. This could be done by sim-
ply slicing the MS into four keys. (But in real SSL it is a little more complicated, as 
we’ll see.) At the end of the key derivation phase, both Alice and Bob have all four 
keys. The two encryption keys will be used to encrypt data; the two MAC keys will 
be used to verify the integrity of the data.
Data Transfer
Now that Alice and Bob share the same four session keys (EB, MB, EA, and MA), they 
can start to send secured data to each other over the TCP connection. Since TCP is a byte-
stream protocol, a natural approach would be for SSL to encrypt application data on the fly 
and then pass the encrypted data on the fly to TCP. But if we were to do this, where would 
we put the MAC for the integrity check? We certainly do not want to wait until the end 
of the TCP session to verify the integrity of all of Bob’s data that was sent over the entire 
session! To address this issue, SSL breaks the data stream into records, appends a MAC 
to each record for integrity checking, and then encrypts the record+MAC. To create the 
MAC, Bob inputs the record data along with the key MB into a hash function, as discussed 
in Section 8.3. To encrypt the package record+MAC, Bob uses his session encryption key 
EB. This encrypted package is then passed to TCP for transport over the Internet.
Although this approach goes a long way, it still isn’t bullet-proof when it comes 
to providing data integrity for the entire message stream. In particular, suppose 
Trudy is a woman-in-the-middle and has the ability to insert, delete, and replace 
segments in the stream of TCP segments sent between Alice and Bob. Trudy, for 
example, could capture two segments sent by Bob, reverse the order of the segments, 
adjust the TCP sequence numbers (which are not encrypted), and then send the two 
reverse-ordered segments to Alice. Assuming that each TCP segment encapsulates 
exactly one record, let’s now take a look at how Alice would process these segments.
	1.	 TCP running in Alice would think everything is fine and pass the two records 
to the SSL sublayer.
	2.	 SSL in Alice would decrypt the two records.
8.6    •    Securing TCP Connections: SSL         663
	3.	 SSL in Alice would use the MAC in each record to verify the data integrity of 
the two records.
	4.	 SSL would then pass the decrypted byte streams of the two records to the 
application layer; but the complete byte stream received by Alice would not be 
in the correct order due to reversal of the records!
You are encouraged to walk through similar scenarios for when Trudy removes seg-
ments or when Trudy replays segments.
The solution to this problem, as you probably guessed, is to use sequence num-
bers. SSL does this as follows. Bob maintains a sequence number counter, which 
begins at zero and is incremented for each SSL record he sends. Bob doesn’t actually 
include a sequence number in the record itself, but when he calculates the MAC, he 
includes the sequence number in the MAC calculation. Thus, the MAC is now a hash 
of the data plus the MAC key MB plus the current sequence number. Alice tracks 
Bob’s sequence numbers, allowing her to verify the data integrity of a record by 
including the appropriate sequence number in the MAC calculation. This use of SSL 
sequence numbers prevents Trudy from carrying out a woman-in-the-middle attack, 
such as reordering or replaying segments. (Why?)
SSL Record
The SSL record (as well as the almost-SSL record) is shown in Figure 8.26. The 
record consists of a type field, version field, length field, data field, and MAC field. 
Note that the first three fields are not encrypted. The type field indicates whether the 
record is a handshake message or a message that contains application data. It is also 
used to close the SSL connection, as discussed below. SSL at the receiving end uses 
the length field to extract the SSL records out of the incoming TCP byte stream. The 
version field is self-explanatory.
8.6.2	A More Complete Picture
The previous subsection covered the almost-SSL protocol; it served to give us a basic 
understanding of the why and how of SSL. Now that we have a basic understanding 
of SSL, we can dig a little deeper and examine the essentials of the actual SSL proto-
col. In parallel to reading this description of the SSL protocol, you are encouraged to 
complete the Wireshark SSL lab, available at the textbook’s Web site.
Figure 8.26  ♦  Record format for SSL
Version
Length
Type
Data
MAC
Encrypted with EB
664         Chapter 8    •    Security in Computer Networks
SSL Handshake
SSL does not mandate that Alice and Bob use a specific symmetric key algorithm, 
a specific public-key algorithm, or a specific MAC. Instead, SSL allows Alice and 
Bob to agree on the cryptographic algorithms at the beginning of the SSL session, 
during the handshake phase. Additionally, during the handshake phase, Alice and 
Bob send nonces to each other, which are used in the creation of the session keys 
(EB, MB, EA, and MA). The steps of the real SSL handshake are as follows:
	1.	 The client sends a list of cryptographic algorithms it supports, along with a 
­
client nonce.
	2.	 From the list, the server chooses a symmetric algorithm (for example, AES), 
a public key algorithm (for example, RSA with a specific key length), and a 
MAC algorithm. It sends back to the client its choices, as well as a certificate 
and a server nonce.
	3.	 The client verifies the certificate, extracts the server’s public key, generates a 
Pre-Master Secret (PMS), encrypts the PMS with the server’s public key, and 
sends the encrypted PMS to the server.
	4.	 Using the same key derivation function (as specified by the SSL standard), 
the client and server independently compute the Master Secret (MS) from the 
PMS and nonces. The MS is then sliced up to generate the two encryption and 
two MAC keys. Furthermore, when the chosen symmetric cipher employs 
CBC (such as 3DES or AES), then two Initialization Vectors (IVs)—one for 
each side of the connection—are also obtained from the MS. Henceforth, all 
­
messages sent between client and server are encrypted and authenticated (with 
the MAC).
	5.	 The client sends a MAC of all the handshake messages.
	6.	 The server sends a MAC of all the handshake messages.
The last two steps protect the handshake from tampering. To see this, observe 
that in step 1, the client typically offers a list of algorithms—some strong, some 
weak. This list of algorithms is sent in cleartext, since the encryption algorithms and 
keys have not yet been agreed upon. Trudy, as a woman-in-the-middle, could delete 
the stronger algorithms from the list, forcing the client to select a weak algorithm. 
To prevent such a tampering attack, in step 5 the client sends a MAC of the concat-
enation of all the handshake messages it sent and received. The server can compare 
this MAC with the MAC of the handshake messages it received and sent. If there 
is an inconsistency, the server can terminate the connection. Similarly, the server 
sends a MAC of the handshake messages it has seen, allowing the client to check for 
inconsistencies.
You may be wondering why there are nonces in steps 1 and 2. Don’t sequence 
numbers suffice for preventing the segment replay attack? The answer is yes, but they 
don’t alone prevent the “connection replay attack.” Consider the following connection 
8.7    •    Network-Layer Security: IPsec and Virtual Private Networks         665
replay attack. Suppose Trudy sniffs all messages between Alice and Bob. The next 
day, Trudy masquerades as Bob and sends to Alice exactly the same sequence of 
messages that Bob sent to Alice on the previous day. If Alice doesn’t use nonces, 
she will respond with exactly the same sequence of messages she sent the previous 
day. Alice will not suspect any funny business, as each message she receives will 
pass the integrity check. If Alice is an e-commerce server, she will think that Bob is 
placing a second order (for exactly the same thing). On the other hand, by including a 
nonce in the protocol, Alice will send different nonces for each TCP session, causing 
the encryption keys to be different on the two days. Therefore, when Alice receives 
played-back SSL records from Trudy, the records will fail the integrity checks, and 
the bogus e-commerce transaction will not succeed. In summary, in SSL, nonces are 
used to defend against the “connection replay attack” and sequence numbers are used 
to defend against replaying individual packets during an ongoing session.
Connection Closure
At some point, either Bob or Alice will want to end the SSL session. One approach 
would be to let Bob end the SSL session by simply terminating the underlying TCP 
connection—that is, by having Bob send a TCP FIN segment to Alice. But such a 
naive design sets the stage for the truncation attack whereby Trudy once again gets 
in the middle of an ongoing SSL session and ends the session early with a TCP 
FIN. If Trudy were to do this, Alice would think she received all of Bob’s data 
when ­
actuality she only received a portion of it. The solution to this problem is to 
indicate in the type field whether the record serves to terminate the SSL session. 
(Although the SSL type is sent in the clear, it is authenticated at the receiver using the 
record’s MAC.) By including such a field, if Alice were to receive a TCP FIN before 
­
receiving a closure SSL record, she would know that something funny was going on.
This completes our introduction to SSL. We’ve seen that it uses many of the 
cryptography principles discussed in Sections 8.2 and 8.3. Readers who want to 
explore SSL on yet a deeper level can read Rescorla’s highly readable book on SSL 
[Rescorla 2001].
8.7	 Network-Layer Security: IPsec and Virtual 
Private Networks
The IP security protocol, more commonly known as IPsec, provides security at the 
network layer. IPsec secures IP datagrams between any two network-layer entities, 
including hosts and routers. As we will soon describe, many institutions (corpora-
tions, government branches, non-profit organizations, and so on) use IPsec to create 
virtual private networks (VPNs) that run over the public Internet.
666         Chapter 8    •    Security in Computer Networks
Before getting into the specifics of IPsec, let’s step back and consider what 
it means to provide confidentiality at the network layer. With network-layer con-
fidentiality between a pair of network entities (for example, between two routers, 
between two hosts, or between a router and a host), the sending entity encrypts the 
payloads of all the datagrams it sends to the receiving entity. The encrypted payload 
could be a TCP segment, a UDP segment, an ICMP message, and so on. If such 
a network-layer service were in place, all data sent from one entity to the other—
including e-mail, Web pages, TCP handshake messages, and management mes-
sages (such as ICMP and SNMP)—would be hidden from any third party that might 
be sniffing the network. For this reason, network-layer security is said to provide 
 
“blanket coverage.”
In addition to confidentiality, a network-layer security protocol could potentially 
provide other security services. For example, it could provide source authentication, 
so that the receiving entity can verify the source of the secured datagram. A network-
layer security protocol could provide data integrity, so that the receiving entity can 
check for any tampering of the datagram that may have occurred while the datagram 
was in transit. A network-layer security service could also provide replay-attack pre-
vention, meaning that Bob could detect any duplicate datagrams that an attacker 
might insert. We will soon see that IPsec indeed provides mechanisms for all these 
security services, that is, for confidentiality, source authentication, data ­
integrity, and 
replay-attack prevention.
8.7.1	IPsec and Virtual Private Networks (VPNs)
An institution that extends over multiple geographical regions often desires its own 
IP network, so that its hosts and servers can send data to each other in a secure and 
confidential manner. To achieve this goal, the institution could actually deploy a 
stand-alone physical network—including routers, links, and a DNS ­
infrastructure—
that is completely separate from the public Internet. Such a disjoint network, dedi-
cated to a particular institution, is called a private network. Not surprisingly, a 
private network can be very costly, as the institution needs to purchase, install, and 
maintain its own physical network infrastructure.
Instead of deploying and maintaining a private network, many institutions 
today create VPNs over the existing public Internet. With a VPN, the institu-
tion’s inter-office traffic is sent over the public Internet rather than over a physi-
cally independent network. But to provide confidentiality, the inter-office traffic 
is encrypted before it enters the public Internet. A simple example of a VPN is 
shown in Figure  8.27. Here the institution consists of a headquarters, a branch 
office, and traveling salespersons that typically access the Internet from their hotel 
rooms. (There is only one salesperson shown in the figure.) In this VPN, whenever 
two hosts within headquarters send IP datagrams to each other or whenever two 
hosts within the branch office want to communicate, they use good-old vanilla 
IPv4 (that is, without IPsec services). However, when two of the institution’s hosts 
8.7    •    Network-Layer Security: IPsec and Virtual Private Networks         667
communicate over a path that traverses the public Internet, the traffic is encrypted 
before it enters the Internet.
To get a feel for how a VPN works, let’s walk through a simple example in the 
context of Figure 8.27. When a host in headquarters sends an IP datagram to a sales-
person in a hotel, the gateway router in headquarters converts the vanilla IPv4 data-
gram into an IPsec datagram and then forwards this IPsec datagram into the Internet. 
This IPsec datagram actually has a traditional IPv4 header, so that the routers in the 
public Internet process the datagram as if it were an ordinary IPv4 datagram—to 
them, the datagram is a perfectly ordinary datagram. But, as shown Figure 8.27, 
the payload of the IPsec datagram includes an IPsec header, which is used for IPsec 
processing; furthermore, the payload of the IPsec datagram is encrypted. When the 
IPsec datagram arrives at the salesperson’s laptop, the OS in the laptop decrypts the 
payload (and provides other security services, such as verifying data integrity) and 
passes the unencrypted payload to the upper-layer protocol (for example, to TCP 
 
or UDP).
We have just given a high-level overview of how an institution can employ 
IPsec to create a VPN. To see the forest through the trees, we have brushed aside 
many important details. Let’s now take a closer look.
Figure 8.27  ♦  Virtual private network (VPN)
IP
header
IPsec
header
Secure
payload
IP
header
IPsec
header
Secure
payload
IP
header
IPsec
header
Secure
payload
IP
header
Payload
IP
header
Payload
Laptop w/IPsec
Router
w/IPv4 and
IPsec
Router
w/IPv4 and
IPsec
Branch Ofﬁce
Headquarters
Salesperson
in Hotel
Public
Internet
668         Chapter 8    •    Security in Computer Networks
8.7.2	The AH and ESP Protocols
IPsec is a rather complex animal—it is defined in more than a dozen RFCs. Two 
important RFCs are RFC 4301, which describes the overall IP security architecture, 
and RFC 6071, which provides an overview of the IPsec protocol suite. Our goal in 
this textbook, as usual, is not simply to re-hash the dry and arcane RFCs, but instead 
take a more operational and pedagogic approach to describing the protocols.
In the IPsec protocol suite, there are two principal protocols: the Authentication 
Header (AH) protocol and the Encapsulation Security Payload (ESP) protocol. 
When a source IPsec entity (typically a host or a router) sends secure datagrams to a 
destination entity (also a host or a router), it does so with either the AH protocol or 
the ESP protocol. The AH protocol provides source authentication and data integrity 
but does not provide confidentiality. The ESP protocol provides source authentica-
tion, data integrity, and confidentiality. Because confidentiality is often critical for 
VPNs and other IPsec applications, the ESP protocol is much more widely used than 
the AH protocol. In order to de-mystify IPsec and avoid much of its complication, we 
will henceforth focus exclusively on the ESP protocol. Readers wanting to learn also 
about the AH protocol are encouraged to explore the RFCs and other online resources.
8.7.3	Security Associations
IPsec datagrams are sent between pairs of network entities, such as between two 
hosts, between two routers, or between a host and router. Before sending IPsec data-
grams from source entity to destination entity, the source and destination entities cre-
ate a network-layer logical connection. This logical connection is called a security 
association (SA). An SA is a simplex logical connection; that is, it is unidirectional 
from source to destination. If both entities want to send secure datagrams to each 
other, then two SAs (that is, two logical connections) need to be established, one in 
each direction.
For example, consider once again the institutional VPN in Figure 8.27. This 
institution consists of a headquarters office, a branch office and, say, n traveling 
salespersons. For the sake of example, let’s suppose that there is bi-directional IPsec 
traffic between headquarters and the branch office and bi-directional IPsec traffic 
between headquarters and the salespersons. In this VPN, how many SAs are there? 
To answer this question, note that there are two SAs between the headquarters gate-
way router and the branch-office gateway router (one in each direction); for each 
salesperson’s laptop, there are two SAs between the headquarters gateway router and 
the laptop (again, one in each direction). So, in total, there are (2 + 2n) SAs. Keep 
in mind, however, that not all traffic sent into the Internet by the gateway routers or 
by the laptops will be IPsec secured. For example, a host in headquarters may want 
to access a Web server (such as Amazon or Google) in the public Internet. Thus, 
the gateway router (and the laptops) will emit into the Internet both vanilla IPv4 
­
datagrams and secured IPsec datagrams.
8.7    •    Network-Layer Security: IPsec and Virtual Private Networks         669
Let’s now take a look “inside” an SA. To make the discussion tangible and 
­
concrete, let’s do this in the context of an SA from router R1 to router R2 in Fig-
ure 8.28. (You can think of Router R1 as the headquarters gateway router and Router 
R2 as the branch office gateway router from Figure 8.27.) Router R1 will maintain 
state information about this SA, which will include:
•	 A 32-bit identifier for the SA, called the Security Parameter Index (SPI)
•	 The origin interface of the SA (in this case 200.168.1.100) and the destination 
interface of the SA (in this case 193.68.2.23)
•	 The type of encryption to be used (for example, 3DES with CBC)
•	 The encryption key
•	 The type of integrity check (for example, HMAC with MD5)
•	 The authentication key
Whenever router R1 needs to construct an IPsec datagram for forwarding over 
this SA, it accesses this state information to determine how it should authenticate 
and encrypt the datagram. Similarly, router R2 will maintain the same state informa-
tion for this SA and will use this information to authenticate and decrypt any IPsec 
datagram that arrives from the SA.
An IPsec entity (router or host) often maintains state information for many SAs. 
For example, in the VPN example in Figure 8.27 with n salespersons, the headquar-
ters gateway router maintains state information for (2 + 2n) SAs. An IPsec entity 
stores the state information for all of its SAs in its Security Association Database 
(SAD), which is a data structure in the entity’s OS kernel.
8.7.4	The IPsec Datagram
Having now described SAs, we can now describe the actual IPsec datagram. IPsec 
has two different packet forms, one for the so-called tunnel mode and the other for 
the so-called transport mode. The tunnel mode, being more appropriate for VPNs, 
Figure 8.28  ♦  Security association (SA) from R1 to R2
Internet
SA
R1
172.16.1/24
Headquarters
Branch Ofﬁce
200.168.1.100
193.68.2.23
172.16.2/24
R2
670         Chapter 8    •    Security in Computer Networks
is more widely deployed than the transport mode. In order to further de-mystify 
IPsec and avoid much of its complication, we henceforth focus exclusively on the 
tunnel mode. Once you have a solid grip on the tunnel mode, you should be able to 
easily learn about the transport mode on your own.
The packet format of the IPsec datagram is shown in Figure 8.29. You might 
think that packet formats are boring and insipid, but we will soon see that the IPsec 
datagram actually looks and tastes like a popular Tex-Mex delicacy! Let’s examine 
the IPsec fields in the context of Figure 8.28. Suppose router R1 receives an ordinary 
IPv4 datagram from host 172.16.1.17 (in the headquarters network) which is destined 
to host 172.16.2.48 (in the branch-office network). Router R1 uses the ­
following 
recipe to convert this “original IPv4 datagram” into an IPsec datagram:
•	 Appends to the back of the original IPv4 datagram (which includes the original 
header fields!) an “ESP trailer” field
•	 Encrypts the result using the algorithm and key specified by the SA
•	 Appends to the front of this encrypted quantity a field called “ESP header”; the 
resulting package is called the “enchilada”
•	 Creates an authentication MAC over the whole enchilada using the algorithm and 
key specified in the SA
•	 Appends the MAC to the back of the enchilada forming the payload
•	 Finally, creates a brand new IP header with all the classic IPv4 header fields 
(together normally 20 bytes long), which it appends before the payload
Note that the resulting IPsec datagram is a bona fide IPv4 datagram, with the 
traditional IPv4 header fields followed by a payload. But in this case, the payload 
contains an ESP header, the original IP datagram, an ESP trailer, and an ESP authen-
tication field (with the original datagram and ESP trailer encrypted). The origi-
nal IP datagram has 172.16.1.17 for the source IP address and 172.16.2.48 for the 
Figure 8.29  ♦  IPsec datagram format
New IP
header
ESP
header
ESP
trailer
ESP
MAC
Original
IP header
Original IP
datagram payload
Encrypted
“Enchilada” authenticated
Pad
length
Padding
Next
header
SPI
Seq #
8.7    •    Network-Layer Security: IPsec and Virtual Private Networks         671
destination IP address. Because the IPsec datagram includes the original IP data-
gram, these addresses are included (and encrypted) as part of the payload of the 
IPsec packet. But what about the source and destination IP addresses that are in the 
new IP header, that is, in the left-most header of the IPsec datagram? As you might 
expect, they are set to the source and destination router interfaces at the two ends of 
the tunnels, namely, 200.168.1.100 and 193.68.2.23. Also, the protocol number in 
this new IPv4 header field is not set to that of TCP, UDP, or SMTP, but instead to 50, 
designating that this is an IPsec datagram using the ESP protocol.
After R1 sends the IPsec datagram into the public Internet, it will pass through 
many routers before reaching R2. Each of these routers will process the datagram as if it 
were an ordinary datagram—they are completely oblivious to the fact that the datagram 
is carrying IPsec-encrypted data. For these public Internet routers, because the destina-
tion IP address in the outer header is R2, the ultimate destination of the datagram is R2.
Having walked through an example of how an IPsec datagram is constructed, 
let’s now take a closer look at the ingredients in the enchilada. We see in Figure 8.29 
that the ESP trailer consists of three fields: padding; pad length; and next header. 
Recall that block ciphers require the message to be encrypted to be an integer mul-
tiple of the block length. Padding (consisting of meaningless bytes) is used so that 
when added to the original datagram (along with the pad length and next header 
fields), the resulting “message” is an integer number of blocks. The pad-length field 
indicates to the receiving entity how much padding was inserted (and thus needs to 
be removed). The next header identifies the type (e.g., UDP) of data contained in the 
payload-data field. The payload data (typically the original IP datagram) and the ESP 
trailer are concatenated and then encrypted.
Appended to the front of this encrypted unit is the ESP header, which is sent in 
the clear and consists of two fields: the SPI and the sequence number field. The SPI 
indicates to the receiving entity the SA to which the datagram belongs; the receiving 
entity can then index its SAD with the SPI to determine the appropriate authentica-
tion/decryption algorithms and keys. The sequence number field is used to defend 
against replay attacks.
The sending entity also appends an authentication MAC. As stated earlier, the 
sending entity calculates a MAC over the whole enchilada (consisting of the ESP 
header, the original IP datagram, and the ESP trailer—with the datagram and trailer 
being encrypted). Recall that to calculate a MAC, the sender appends a secret MAC 
key to the enchilada and then calculates a fixed-length hash of the result.
When R2 receives the IPsec datagram, R2 observes that the destination IP 
address of the datagram is R2 itself. R2 therefore processes the datagram. Because 
the protocol field (in the left-most IP header) is 50, R2 sees that it should apply 
IPsec ESP processing to the datagram. First, peering into the enchilada, R2 uses the 
SPI to determine to which SA the datagram belongs. Second, it calculates the MAC 
of the enchilada and verifies that the MAC is consistent with the value in the ESP 
MAC field. If it is, it knows that the enchilada comes from R1 and has not been tam-
pered with. Third, it checks the sequence-number field to verify that the datagram is 
672         Chapter 8    •    Security in Computer Networks
fresh (and not a replayed datagram). Fourth, it decrypts the encrypted unit using the 
decryption algorithm and key associated with the SA. Fifth, it removes padding and 
extracts the original, vanilla IP datagram. And finally, sixth, it forwards the original 
datagram into the branch office network toward its ultimate destination. Whew, what 
a complicated recipe, huh? Well no one ever said that preparing and unraveling an 
enchilada was easy!
There is actually another important subtlety that needs to be addressed. It centers 
on the following question: When R1 receives an (unsecured) datagram from a host 
in the headquarters network, and that datagram is destined to some destination IP 
address outside of headquarters, how does R1 know whether it should be converted to 
an IPsec datagram? And if it is to be processed by IPsec, how does R1 know which SA 
(of many SAs in its SAD) should be used to construct the IPsec datagram? The prob-
lem is solved as follows. Along with a SAD, the IPsec entity also maintains another 
data structure called the Security Policy Database (SPD). The SPD indicates what 
types of datagrams (as a function of source IP address, destination IP address, and 
protocol type) are to be IPsec processed; and for those that are to be IPsec processed, 
which SA should be used. In a sense, the information in a SPD indicates “what” to 
do with an arriving datagram; the information in the SAD indicates “how” to do it.
Summary of IPsec Services
So what services does IPsec provide, exactly? Let us examine these services from 
the perspective of an attacker, say Trudy, who is a woman-in-the-middle, sitting 
somewhere on the path between R1 and R2 in Figure 8.28. Assume throughout this 
­
discussion that Trudy does not know the authentication and encryption keys used by 
the SA. What can and cannot Trudy do? First, Trudy cannot see the original data-
gram. If fact, not only is the data in the original datagram hidden from Trudy, but 
so is the protocol number, the source IP address, and the destination IP address. For 
datagrams sent over the SA, Trudy only knows that the datagram originated from 
some host in 172.16.1.0/24 and is destined to some host in 172.16.2.0/24. She does 
not know if it is carrying TCP, UDP, or ICMP data; she does not know if it is carrying 
HTTP, SMTP, or some other type of application data. This confidentiality thus goes 
a lot farther than SSL. Second, suppose Trudy tries to tamper with a datagram in the 
SA by flipping some of its bits. When this tampered datagram arrives at R2, it will 
fail the integrity check (using the MAC), thwarting Trudy’s vicious attempts once 
again. Third, suppose Trudy tries to masquerade as R1, creating a IPsec datagram 
with source 200.168.1.100 and destination 193.68.2.23. Trudy’s attack will be futile, 
as this datagram will again fail the integrity check at R2. Finally, because IPsec 
includes sequence numbers, Trudy will not be able create a successful replay attack. 
In summary, as claimed at the beginning of this section, IPsec provides—between 
any pair of devices that process packets through the network layer—confidentiality, 
source authentication, data integrity, and replay-attack prevention.
8.7    •    Network-Layer Security: IPsec and Virtual Private Networks         673
8.7.5	IKE: Key Management in IPsec
When a VPN has a small number of end points (for example, just two routers as 
in Figure 8.28), the network administrator can manually enter the SA information 
(encryption/authentication algorithms and keys, and the SPIs) into the SADs of the 
endpoints. Such “manual keying” is clearly impractical for a large VPN, which 
may consist of hundreds or even thousands of IPsec routers and hosts. Large, geo-
graphically distributed deployments require an automated mechanism for creating 
the SAs. IPsec does this with the Internet Key Exchange (IKE) protocol, specified 
in RFC 5996.
IKE has some similarities with the handshake in SSL (see Section 8.6). Each 
IPsec entity has a certificate, which includes the entity’s public key. As with SSL, 
the IKE protocol has the two entities exchange certificates, negotiate authentication 
and encryption algorithms, and securely exchange key material for creating session 
keys in the IPsec SAs. Unlike SSL, IKE employs two phases to carry out these tasks.
Let’s investigate these two phases in the context of two routers, R1 and R2, 
 
in Figure 8.28. The first phase consists of two exchanges of message pairs between 
R1 and R2:
•	 During the first exchange of messages, the two sides use Diffie-Hellman (see 
Homework Problems) to create a bi-directional IKE SA between the routers. To 
keep us all confused, this bi-directional IKE SA is entirely different from the 
IPsec SAs discussed in Sections 8.6.3 and 8.6.4. The IKE SA provides an authen-
ticated and encrypted channel between the two routers. During this first message-
pair exchange, keys are established for encryption and authentication for the IKE 
SA. Also established is a master secret that will be used to compute IPSec SA 
keys later in phase 2. Observe that during this first step, RSA public and private 
keys are not used. In particular, neither R1 nor R2 reveals its identity by signing 
a message with its private key.
•	 During the second exchange of messages, both sides reveal their identity to each 
other by signing their messages. However, the identities are not revealed to a pas-
sive sniffer, since the messages are sent over the secured IKE SA channel. Also 
during this phase, the two sides negotiate the IPsec encryption and authentication 
algorithms to be employed by the IPsec SAs.
In phase 2 of IKE, the two sides create an SA in each direction. At the end of 
phase 2, the encryption and authentication session keys are established on both sides 
for the two SAs. The two sides can then use the SAs to send secured datagrams, as 
described in Sections 8.7.3 and 8.7.4. The primary motivation for having two phases 
in IKE is computational cost—since the second phase doesn’t involve any public-
key cryptography, IKE can generate a large number of SAs between the two IPsec 
entities with relatively little computational cost.
674         Chapter 8    •    Security in Computer Networks
8.8	 Securing Wireless LANs
Security is a particularly important concern in wireless networks, where radio waves 
carrying frames can propagate far beyond the building containing the wireless base 
station and hosts. In this section we present a brief introduction to wireless security. 
For a more in-depth treatment, see the highly readable book by Edney and Arbaugh 
[Edney 2003].
The issue of security in 802.11 has attracted considerable attention in both 
technical circles and in the media. While there has been considerable discussion, 
there has been little debate—there seems to be universal agreement that the origi-
nal 802.11 specification contains a number of serious security flaws. Indeed, public 
domain software can now be downloaded that exploits these holes, making those 
who use the vanilla 802.11 security mechanisms as open to security attacks as users 
who use no security features at all.
In the following section, we discuss the security mechanisms initially standard-
ized in the 802.11 specification, known collectively as Wired Equivalent Privacy 
(WEP). As the name suggests, WEP is meant to provide a level of security similar to 
that found in wired networks. We’ll then discuss a few of the security holes in WEP 
and discuss the 802.11i standard, a fundamentally more secure version of 802.11 
adopted in 2004.
8.8.1	Wired Equivalent Privacy (WEP)
The IEEE 802.11 WEP protocol was designed in 1999 to provide authentication and 
data encryption between a host and a wireless access point (that is, base station) using 
a symmetric shared key approach. WEP does not specify a key management algo-
rithm, so it is assumed that the host and wireless access point have somehow agreed 
on the key via an out-of-band method. Authentication is carried out as ­
follows:
	1.	 A wireless host requests authentication by an access point.
	2.	 The access point responds to the authentication request with a 128-byte nonce 
value.
	3.	 The wireless host encrypts the nonce using the symmetric key that it shares 
with the access point.
	4.	 The access point decrypts the host-encrypted nonce.
If the decrypted nonce matches the nonce value originally sent to the host, then the 
host is authenticated by the access point.
The WEP data encryption algorithm is illustrated in Figure 8.30. A secret 
40-bit symmetric key, KS, is assumed to be known by both a host and the access 
point. In addition, a 24-bit Initialization Vector (IV) is appended to the 40-bit 
key to create a 64-bit key that will be used to encrypt a single frame. The IV will 
8.8    •    Securing Wireless LANs         675
change from one frame to another, and hence each frame will be encrypted with 
a different 64-bit key. Encryption is performed as follows. First a 4-byte CRC 
value (see Section 6.2) is computed for the data payload. The payload and the four 
CRC bytes are then encrypted using the RC4 stream cipher. We will not cover 
the details of RC4 here (see [Schneier 1995] and [Edney 2003] for details). For 
our purposes, it is enough to know that when presented with a key value (in this 
case, the 64-bit (KS, IV) key), the RC4 algorithm produces a stream of key values, 
 
k1
IV, k2
IV, k3
IV, . . . that are used to encrypt the data and CRC value in a frame. For 
practical purposes, we can think of these operations being performed a byte at a 
time. Encryption is performed by XOR-ing the ith byte of data, di, with the ith key, 
ki
 IV, in the stream of key values generated by the (KS, IV) pair to produce the ith 
byte of ciphertext, ci:
ci = di ⊕ki
IV
The IV value changes from one frame to the next and is included in plaintext 
in the header of each WEP-encrypted 802.11 frame, as shown in Figure 8.30. The 
receiver takes the secret 40-bit symmetric key that it shares with the sender, appends 
the IV, and uses the resulting 64-bit key (which is identical to the key used by the 
sender to perform encryption) to decrypt the frame:
di = ci ⊕ki
IV
Proper use of the RC4 algorithm requires that the same 64-bit key value 
never be used more than once. Recall that the WEP key changes on a frame-
by-frame basis. For a given KS (which changes rarely, if ever), this means that 
there are only 224 unique keys. If these keys are chosen randomly, we can show 
[Edney 2003] that the probability of having chosen the same IV value (and hence 
used the same 64-bit key) is more than 99 percent after only 12,000 frames. With 
 
1 Kbyte frame sizes and a data transmission rate of 11 Mbps, only a few seconds are 
Figure 8.30  ♦  802.11 WEP protocol
Key sequence generator
(for given Ks, IV)
k1
IV
d1
c1
k2
IV k3
IV
kN
IV
IV
kN+1
IV
kN+4
Ks: 40-bit secret symmetric
Plaintext frame data plus CRC
IV (per frame)
802.11
header
IV
WEP-encrypted data
plus CRC
d2
c2
d3
c3
dN
cN
CRC1
cN+1
cN+4
CRC4
676         Chapter 8    •    Security in Computer Networks
needed before 12,000 frames are transmitted. Furthermore, since the IV is transmit-
ted in plaintext in the frame, an eavesdropper will know whenever a duplicate IV 
value is used.
To see one of the several problems that occur when a duplicate key is used, 
consider the following chosen-plaintext attack taken by Trudy against Alice. Sup-
pose that Trudy (possibly using IP spoofing) sends a request (for example, an HTTP 
 
or FTP request) to Alice to transmit a file with known content, d1, d2, d3, d4,. . . . 
Trudy also observes the encrypted data c1, c2, c3, c4. . . . Since di = ci ⊕ki
IV, if we 
XOR ci with each side of this equality we have
di ⊕ci = ki
IV
With this relationship, Trudy can use the known values of di and ci to compute ki
IV. 
The next time Trudy sees the same value of IV being used, she will know the key 
sequence k1
IV, k2
IV, k3
IV, . . . and will thus be able to decrypt the encrypted message.
There are several additional security concerns with WEP as well. [Fluhrer 
2001] described an attack exploiting a known weakness in RC4 when certain weak 
keys are chosen. [Stubblefield 2002] discusses efficient ways to implement and 
exploit this attack. Another concern with WEP involves the CRC bits shown in 
Figure 8.30 and transmitted in the 802.11 frame to detect altered bits in the pay-
load. However, an attacker who changes the encrypted content (e.g., substituting 
gibberish for the original encrypted data), computes a CRC over the substituted 
gibberish, and places the CRC into a WEP frame can produce an 802.11 frame 
that will be accepted by the receiver. What is needed here are message integrity 
techniques such as those we studied in Section 8.3 to detect content tampering or 
substitution. For more details of WEP security, see [Edney 2003; Wright 2015] and 
the ­
references therein.
8.8.2	IEEE 802.11i
Soon after the 1999 release of IEEE 802.11, work began on developing a new and 
improved version of 802.11 with stronger security mechanisms. The new standard, 
known as 802.11i, underwent final ratification in 2004. As we’ll see, while WEP 
provided relatively weak encryption, only a single way to perform authentication, 
and no key distribution mechanisms, IEEE 802.11i provides for much stronger forms 
of encryption, an extensible set of authentication mechanisms, and a key distribu-
tion mechanism. In the following, we present an overview of 802.11i; an excellent 
(streaming audio) technical overview of 802.11i is [TechOnline 2012].
Figure 8.31 overviews the 802.11i framework. In addition to the wireless cli-
ent and access point, 802.11i defines an authentication server with which the AP 
can communicate. Separating the authentication server from the AP allows one 
authentication server to serve many APs, centralizing the (often sensitive) decisions 
8.8    •    Securing Wireless LANs         677
regarding authentication and access within the single server, and keeping AP costs 
and complexity low. 802.11i operates in four phases:
	1.	 Discovery. In the discovery phase, the AP advertises its presence and the forms 
of authentication and encryption that can be provided to the wireless client 
node. The client then requests the specific forms of authentication and encryp-
tion that it desires. Although the client and AP are already exchanging mes-
sages, the client has not yet been authenticated nor does it have an encryption 
key, and so several more steps will be required before the client can communi-
cate with an arbitrary remote host over the wireless channel.
	2.	 Mutual authentication and Master Key (MK) generation. Authentication takes 
place between the wireless client and the authentication server. In this phase, the 
access point acts essentially as a relay, forwarding messages between the client  
and the authentication server. The Extensible Authentication Protocol (EAP) 
[RFC 3748] defines the end-to-end message formats used in a simple request/
response mode of interaction between the client and authentication server. As 
shown in Figure 8.32, EAP messages are encapsulated using EAPoL (EAP over 
LAN, [IEEE 802.1X]) and sent over the 802.11 wireless link. These EAP mes-
sages are then decapsulated at the access point, and then re-encapsulated using the 
RADIUS protocol for transmission over UDP/IP to the authentication server. While 
Figure 8.31  ♦  802.11i: Four phases of operation
STA:
client station
AP:
access point
Wired
network
AS:
authentication
server
1
Discovery of
security capabilities
4
STA, AP use PMK to derive
Temporal Key (TK) used for
message encryption, integrity
AS derives same PMK,
sends to AP
STA derives Pairwise 
Master Key (PMK)
2
3
3
STA and AS mutually authenticate, together generate
Master Key (MK). AP serves as “pass through”
678         Chapter 8    •    Security in Computer Networks
the RADIUS server and protocol [RFC 2865] are not required by the 802.11i pro-
tocol, they are de facto standard components for 802.11i. The recently standardized 
DIAMETER protocol [RFC 3588] is likely to replace RADIUS in the near future.
With EAP, the authentication server can choose one of a number of ways 
to perform authentication. While 802.11i does not mandate a particular authen-
tication method, the EAP-TLS authentication scheme [RFC 5216] is often 
used. EAP-TLS uses public key techniques (including nonce encryption and 
message digests) similar to those we studied in Section 8.3 to allow the client 
and the authentication server to mutually authenticate each other, and to derive 
a Master Key (MK) that is known to both parties.
	3.	 Pairwise Master Key (PMK) generation. The MK is a shared secret known 
only to the client and the authentication server, which they each use to generate 
a second key, the Pairwise Master Key (PMK). The authentication server then 
sends the PMK to the AP. This is where we wanted to be! The client and AP 
now have a shared key (recall that in WEP, the problem of key distribution was 
not addressed at all) and have mutually authenticated each other. They’re just 
about ready to get down to business.
	4.	 Temporal Key (TK) generation. With the PMK, the wireless client and AP can 
now generate additional keys that will be used for communication. Of ­
particular 
interest is the Temporal Key (TK), which will be used to perform the link-level 
encryption of data sent over the wireless link and to an arbitrary remote host.
802.11i provides several forms of encryption, including an AES-based encryption 
scheme and a strengthened version of WEP encryption.
Figure 8.32  ♦  
EAP is an end-to-end protocol. EAP messages are encap-
sulated using EAPoL over the wireless link between the 
­
client and the access point, and using RADIUS over UDP/IP 
between the access point and the authentication server
STA:
client station
AP:
access point
Wired
network
AS:
authentication
server
EAP TLS
EAP
EAP over LAN (EAPoL)
RADIUS
IEEE 802.11
UDP/IP
8.9    •    Operational Security: Firewalls and Intrusion Detection Systems         679
8.9	 Operational Security: Firewalls and Intrusion 
Detection Systems
We’ve seen throughout this chapter that the Internet is not a very safe place—bad 
guys are out there, wreaking all sorts of havoc. Given the hostile nature of the 
Internet, let’s now consider an organization’s network and the network administra-
tor who administers it. From a network administrator’s point of view, the world 
divides quite neatly into two camps—the good guys (who belong to the organiza-
tion’s network, and who should be able to access resources inside the organiza-
tion’s network in a relatively unconstrained manner) and the bad guys (everyone 
else, whose access to network resources must be carefully scrutinized). In many 
organizations, ranging from medieval castles to modern corporate office buildings, 
there is a single point of entry/exit where both good guys and bad guys entering and 
leaving the organization are security-checked. In a castle, this was done at a gate 
at one end of the drawbridge; in a corporate building, this is done at the security 
desk. In a computer network, when traffic entering/leaving a network is security-
checked, logged, dropped, or forwarded, it is done by operational devices known 
as firewalls, intrusion detection systems (IDSs), and intrusion prevention systems 
(IPSs).
8.9.1	Firewalls
A firewall is a combination of hardware and software that isolates an organization’s 
internal network from the Internet at large, allowing some packets to pass and block-
ing others. A firewall allows a network administrator to control access between the 
outside world and resources within the administered network by managing the traffic 
flow to and from these resources. A firewall has three goals:
•	 All traffic from outside to inside, and vice versa, passes through the firewall. 
Figure 8.33 shows a firewall, sitting squarely at the boundary between the admin-
istered network and the rest of the Internet. While large organizations may use 
multiple levels of firewalls or distributed firewalls [Skoudis 2006], locating a 
firewall at a single access point to the network, as shown in Figure 8.33, makes it 
easier to manage and enforce a security-access policy.
•	 Only authorized traffic, as defined by the local security policy, will be allowed 
to pass. With all traffic entering and leaving the institutional network passing 
through the firewall, the firewall can restrict access to authorized traffic.
•	 The firewall itself is immune to penetration. The firewall itself is a device con-
nected to the network. If not designed or installed properly, it can be compro-
mised, in which case it provides only a false sense of security (which is worse 
than no firewall at all!).
680         Chapter 8    •    Security in Computer Networks
Cisco and Check Point are two of the leading firewall vendors today. You can also easily 
create a firewall (packet filter) from a Linux box using iptables (public-domain software 
that is normally shipped with Linux). Furthermore, as discussed in Chapters 4 and 5, fire-
walls are now frequently implemented in routers and controlled remotely using SDNs.
Firewalls can be classified in three categories: traditional packet filters, state-
ful filters, and application gateways. We’ll cover each of these in turn in the fol-
lowing subsections.
Traditional Packet Filters
As shown in Figure 8.33, an organization typically has a gateway router connecting 
its internal network to its ISP (and hence to the larger public Internet). All traffic leav-
ing and entering the internal network passes through this router, and it is at this router 
where packet filtering occurs. A packet filter examines each datagram in isolation, 
determining whether the datagram should be allowed to pass or should be dropped 
based on administrator-specific rules. Filtering decisions are typically based on:
•	 IP source or destination address
•	 Protocol type in IP datagram field: TCP, UDP, ICMP, OSPF, and so on
•	 TCP or UDP source and destination port
Figure 8.33  ♦  
Firewall placement between the administered network and 
the outside world
Administered
network
Firewall
Public
Internet
8.9    •    Operational Security: Firewalls and Intrusion Detection Systems         681
•	 TCP flag bits: SYN, ACK, and so on
•	 ICMP message type
•	 Different rules for datagrams leaving and entering the network
•	 Different rules for the different router interfaces
A network administrator configures the firewall based on the policy of the organ-
ization. The policy may take user productivity and bandwidth usage into account as 
well as the security concerns of an organization. Table 8.5 lists a number of possible 
polices an organization may have, and how they would be addressed with a packet 
filter. For example, if the organization doesn’t want any incoming TCP connections 
except those for its public Web server, it can block all incoming TCP SYN segments 
except TCP SYN segments with destination port 80 and the destination IP address 
corresponding to the Web server. If the organization doesn’t want its users to monop-
olize access bandwidth with Internet radio applications, it can block all not-critical 
UDP traffic (since Internet radio is often sent over UDP). If the organization doesn’t 
want its internal network to be mapped (tracerouted) by an outsider, it can block all 
ICMP TTL expired messages leaving the organization’s network.
A filtering policy can be based on a combination of addresses and port numbers. 
For example, a filtering router could forward all Telnet datagrams (those with a port 
number of 23) except those going to and coming from a list of specific IP addresses. 
This policy permits Telnet connections to and from hosts on the allowed list. Unfor-
tunately, basing the policy on external addresses provides no protection against data-
grams that have had their source addresses spoofed.
Filtering can also be based on whether or not the TCP ACK bit is set. This trick 
is quite useful if an organization wants to let its internal clients connect to external 
servers but wants to prevent external clients from connecting to internal servers. 
Table 8.5  ♦  
Policies and corresponding filtering rules for an organization’s 
network 130.207/16 with Web server at 130.207.244.203
Policy
Firewall Setting
No outside Web access.
Drop all outgoing packets to any IP address, port 80.
No incoming TCP connections, except those  
for organization’s public Web server only.
Drop all incoming TCP SYN packets to any IP except 
130.207.244.203, port 80.
Prevent Web-radios from eating up the  
available bandwidth.
Drop all incoming UDP packets—except DNS packets.
Prevent your network from being used for a  
smurf DoS attack.
Drop all ICMP ping packets going to a “broadcast” 
address (eg 130.207.255.255).
Prevent your network from being tracerouted.
Drop all outgoing ICMP TTL expired traffic.
682         Chapter 8    •    Security in Computer Networks
Recall from Section 3.5 that the first segment in every TCP connection has the ACK 
bit set to 0, whereas all the other segments in the connection have the ACK bit set to 1. 
 
Thus, if an organization wants to prevent external clients from initiating connections 
to internal servers, it simply filters all incoming segments with the ACK bit set to 0. 
This policy kills all TCP connections originating from the outside, but permits con-
nections originating internally.
Firewall rules are implemented in routers with access control lists, with each 
router interface having its own list. An example of an access control list for an organ-
ization 222.22/16 is shown in Table 8.6. This access control list is for an interface 
that connects the router to the organization’s external ISPs. Rules are applied to each 
datagram that passes through the interface from top to bottom. The first two rules 
together allow internal users to surf the Web: The first rule allows any TCP packet 
with destination port 80 to leave the organization’s network; the second rule allows 
any TCP packet with source port 80 and the ACK bit set to enter the organization’s 
network. Note that if an external source attempts to establish a TCP connection with 
an internal host, the connection will be blocked, even if the source or destination 
port is 80. The second two rules together allow DNS packets to enter and leave the 
organization’s network. In summary, this rather restrictive access control list blocks 
all traffic except Web traffic initiated from within the organization and DNS traffic. 
[CERT Filtering 2012] provides a list of recommended port/protocol packet filterings 
to avoid a number of well-known security holes in existing network applications.
Stateful Packet Filters
In a traditional packet filter, filtering decisions are made on each packet in isola-
tion. Stateful filters actually track TCP connections, and use this knowledge to make 
­
filtering decisions.
Table 8.6  ♦  An access control list for a router interface
action
source address
dest address
protocol
source port
dest port
flag bit
allow
222.22/16
outside of 
222.22/16
TCP
> 1023
80
any
allow
outside of 
222.22/16
222.22/16
TCP
80
> 1023
ACK
allow
222.22/16
outside of 
222.22/16
UDP
> 1023
53
—
allow
outside of 
222.22/16
222.22/16
UDP
53
> 1023
—
deny
all
all
all
all
all
all
8.9    •    Operational Security: Firewalls and Intrusion Detection Systems         683
To understand stateful filters, let’s reexamine the access control list in 
Table 8.6. Although rather restrictive, the access control list in Table 8.6 neverthe-
less allows any packet arriving from the outside with ACK = 1 and source port 80 
to get through the filter. Such packets could be used by attackers in attempts to 
crash internal systems with malformed packets, carry out denial-of-service attacks, 
or map the internal network. The naive solution is to block TCP ACK packets as 
well, but such an approach would prevent the organization’s internal users from 
surfing the Web.
Stateful filters solve this problem by tracking all ongoing TCP connections in 
a connection table. This is possible because the firewall can observe the beginning 
of a new connection by observing a three-way handshake (SYN, SYNACK, and 
ACK); and it can observe the end of a connection when it sees a FIN packet for 
the connection. The firewall can also (conservatively) assume that the connection 
is over when it hasn’t seen any activity over the connection for, say, 60 seconds. 
An example connection table for a firewall is shown in Table 8.7. This connec-
tion table indicates that there are currently three ongoing TCP connections, all of 
which have been initiated from within the organization. Additionally, the stateful 
filter includes a new column, “check connection,” in its access control list, as 
shown in Table 8.8. Note that Table 8.8 is identical to the access control list in 
Table 8.6, except now it indicates that the connection should be checked for two 
of the rules.
Let’s walk through some examples to see how the connection table and the 
extended access control list work hand-in-hand. Suppose an attacker attempts 
to send a malformed packet into the organization’s network by sending a data-
gram with TCP source port 80 and with the ACK flag set. Further suppose that 
this packet has source port number 12543 and source IP address 150.23.23.155. 
When this packet reaches the firewall, the firewall checks the access control list in 
Table 8.7, which indicates that the connection table must also be checked before 
permitting this packet to enter the organization’s network. The firewall duly 
checks the connection table, sees that this packet is not part of an ongoing TCP 
connection, and rejects the packet. As a second example, suppose that an internal 
user wants to surf an external Web site. Because this user first sends a TCP SYN 
segment, the user’s TCP connection gets recorded in the connection table. When 
Table 8.7  ♦  Connection table for stateful filter
source address
dest address
source port
dest port
222.22.1.7
37.96.87.123
12699
80
222.22.93.2
199.1.205.23
37654
80
222.22.65.143
203.77.240.43
48712
80
684         Chapter 8    •    Security in Computer Networks
the Web server sends back packets (with the ACK bit necessarily set), the fire-
wall checks the table and sees that a corresponding connection is in progress. The 
firewall will thus let these packets pass, thereby not interfering with the internal 
user’s Web surfing activity.
Application Gateway
In the examples above, we have seen that packet-level filtering allows an organiza-
tion to perform coarse-grain filtering on the basis of the contents of IP and TCP/UDP 
headers, including IP addresses, port numbers, and acknowledgment bits. But what if 
an organization wants to provide a Telnet service to a restricted set of internal users 
(as opposed to IP addresses)? And what if the organization wants such privileged 
users to authenticate themselves first before being allowed to create Telnet sessions 
to the outside world? Such tasks are beyond the capabilities of traditional and stateful 
filters. Indeed, information about the identity of the internal users is application-layer 
data and is not included in the IP/TCP/UDP headers.
To have finer-level security, firewalls must combine packet filters with appli-
cation gateways. Application gateways look beyond the IP/TCP/UDP headers and 
make policy decisions based on application data. An application gateway is an 
application-specific server through which all application data (inbound and out-
bound) must pass. Multiple application gateways can run on the same host, but each 
gateway is a separate server with its own processes.
To get some insight into application gateways, let’s design a firewall that allows 
only a restricted set of internal users to Telnet outside and prevents all external cli-
ents from Telneting inside. Such a policy can be accomplished by implementing 
Table 8.8  ♦  Access control list for stateful filter
action
source  
address
dest  
address
protocol
source port
dest port
flag bit
check 
conxion
allow
222.22/16
outside of 
222.22/16
TCP
> 1023
80
any
allow
outside of 
222.22/16
222.22/16
TCP
80
> 1023
ACK
X
allow
222.22/16
outside of 
222.22/16
UDP
> 1023
53
—
allow
outside of 
222.22/16
222.22/16
UDP
53
> 1023
—
X
deny
all
all
all
all
all
all
8.9    •    Operational Security: Firewalls and Intrusion Detection Systems         685
a combination of a packet filter (in a router) and a Telnet application gateway, as 
shown in Figure 8.34. The router’s filter is configured to block all Telnet connec-
tions except those that originate from the IP address of the application gateway. 
Such a filter configuration forces all outbound Telnet connections to pass through 
the application gateway. Consider now an internal user who wants to Telnet to 
the outside world. The user must first set up a Telnet session with the application 
gateway. An application running in the gateway, which listens for incoming Telnet 
sessions, prompts the user for a user ID and password. When the user supplies this 
information, the application gateway checks to see if the user has permission to Tel-
net to the outside world. If not, the Telnet connection from the internal user to the 
gateway is terminated by the gateway. If the user has permission, then the gateway 
(1) prompts the user for the host name of the external host to which the user wants 
to connect, (2) sets up a Telnet session between the gateway and the external host, 
and (3) relays to the external host all data arriving from the user, and relays to the 
user all data arriving from the external host. Thus, the Telnet application gateway 
not only performs user authorization but also acts as a Telnet server and a Telnet 
client, relaying information between the user and the remote Telnet server. Note 
that the filter will permit step 2 because the gateway initiates the Telnet connection 
to the outside world.
Figure 8.34  ♦  Firewall consisting of an application gateway and a filter
Application
gateway
Host-to-gateway
Telnet session
Gateway-to-remote
host Telnet session
Router 
and ﬁlter
686         Chapter 8    •    Security in Computer Networks
ANONYMITY AND PRIVACY
Suppose you want to visit a controversial Web site (for example, a political activist 
site) and you (1) don’t want to reveal your IP address to the Web site, (2) don’t want 
your local ISP (which may be your home or office ISP) to know that you are visiting 
the site, and (3) don’t want your local ISP to see the data you are exchanging with 
the site. If you use the traditional approach of connecting directly to the Web site 
without any encryption, you fail on all three counts. Even if you use SSL, you fail 
on the first two counts: Your source IP address is presented to the Web site in every 
datagram you send; and the destination address of every packet you send can easily 
be sniffed by your local ISP.
To obtain privacy and anonymity, you can instead use a combination of a trusted 
proxy server and SSL, as shown in Figure 8.35. With this approach, you first make 
an SSL connection to the trusted proxy. You then send, into this SSL connection, 
an HTTP request for a page at the desired site. When the proxy receives the SSL-
encrypted HTTP request, it decrypts the request and forwards the cleartext HTTP 
request to the Web site. The Web site then responds to the proxy, which in turn for-
wards the response to you over SSL. Because the Web site only sees the IP address 
of the proxy, and not of your client’s address, you are indeed obtaining anony-
mous access to the Web site. And because all traffic between you and the proxy is 
encrypted, your local ISP cannot invade your privacy by logging the site you visited 
or recording the data you are exchanging. Many companies today (such as proxify 
.com) make available such proxy services.
Of course, in this solution, your proxy knows everything: It knows your IP address 
and the IP address of the site you’re surfing; and it can see all the traffic in ­
cleartext 
exchanged between you and the Web site. Such a solution, therefore, is only as 
good as the trustworthiness of the proxy. A more robust approach, taken by the 
TOR anonymizing and privacy service, is to route your traffic through a series of 
non-­
colluding proxy servers [TOR 2016]. In particular, TOR allows independent 
­
individuals to contribute proxies to its proxy pool. When a user connects to a server 
using TOR, TOR randomly chooses (from its proxy pool) a chain of three proxies and 
routes all traffic between client and server over the chain. In this manner, assuming 
the proxies do not collude, no one knows that communication took place between 
your IP address and the target Web site. Furthermore, although cleartext is sent 
between the last proxy and the server, the last proxy doesn’t know what IP address  
is sending and receiving the cleartext.
CASE HISTORY
8.9    •    Operational Security: Firewalls and Intrusion Detection Systems         687
Internal networks often have multiple application gateways, for example, gate-
ways for Telnet, HTTP, FTP, and e-mail. In fact, an organization’s mail server 
 
(see Section 2.3) and Web cache are application gateways.
Application gateways do not come without their disadvantages. First, a different 
application gateway is needed for each application. Second, there is a performance 
penalty to be paid, since all data will be relayed via the gateway. This becomes a 
concern particularly when multiple users or applications are using the same gateway 
machine. Finally, the client software must know how to contact the gateway when 
the user makes a request, and must know how to tell the application gateway what 
external server to connect to.
8.9.2	Intrusion Detection Systems
We’ve just seen that a packet filter (traditional and stateful) inspects IP, TCP, UDP, 
and ICMP header fields when deciding which packets to let pass through the firewall. 
However, to detect many attack types, we need to perform deep packet inspection, 
that is, look beyond the header fields and into the actual application data that the 
packets carry. As we saw in Section 8.9.1, application gateways often do deep packet 
inspection. But an application gateway only does this for a specific application.
Clearly, there is a niche for yet another device—a device that not only exam-
ines the headers of all packets passing through it (like a packet filter), but also per-
forms deep packet inspection (unlike a packet filter). When such a device observes 
a suspicious packet, or a suspicious series of packets, it could prevent those packets 
from entering the organizational network. Or, because the activity is only deemed 
as suspicious, the device could let the packets pass, but send alerts to a network 
administrator, who can then take a closer look at the traffic and take appropriate 
actions. A device that generates alerts when it observes potentially malicious traf-
fic is called an intrusion detection system (IDS). A device that filters out suspi-
cious traffic is called an intrusion prevention system (IPS). In this section we study 
Figure 8.35  ♦  Providing anonymity and privacy with a proxy
Alice
Anonymizing
Proxy
SSL
Cleartext
688         Chapter 8    •    Security in Computer Networks
both systems—IDS and IPS—together, since the most interesting technical aspect 
of these systems is how they detect suspicious traffic (and not whether they send 
alerts or drop packets). We will henceforth collectively refer to IDS systems and IPS 
systems as IDS systems.
An IDS can be used to detect a wide range of attacks, including network map-
ping (emanating, for example, from nmap), port scans, TCP stack scans, DoS band-
width-flooding attacks, worms and viruses, OS vulnerability attacks, and application 
vulnerability attacks. (See Section 1.6 for a survey of network attacks.) Today, 
thousands of organizations employ IDS systems. Many of these deployed systems 
are proprietary, marketed by Cisco, Check Point, and other security equipment ven-
dors. But many of the deployed IDS systems are public-domain systems, such as the 
immensely popular Snort IDS system (which we’ll discuss shortly).
An organization may deploy one or more IDS sensors in its organizational net-
work. Figure 8.36 shows an organization that has three IDS sensors. When multi-
ple sensors are deployed, they typically work in concert, sending information about 
Figure 8.36  ♦  
An organization deploying a filter, an application gateway, 
and IDS sensors
Internet
Web
server
FTP
server
DNS
server
Internal
network
Application
gateway
Demilitarized zone
Filter
Key:
= IDS sensors
8.9    •    Operational Security: Firewalls and Intrusion Detection Systems         689
suspicious traffic activity to a central IDS processor, which collects and integrates 
the information and sends alarms to network administrators when deemed appropri-
ate. In Figure 8.36, the organization has partitioned its network into two regions: a 
high-security region, protected by a packet filter and an application gateway and 
monitored by IDS sensors; and a lower-security region—referred to as the demilita-
rized zone (DMZ)—which is protected only by the packet filter, but also monitored 
by IDS sensors. Note that the DMZ includes the organization’s servers that need to 
communicate with the outside world, such as its public Web server and its authorita-
tive DNS server.
You may be wondering at this stage, why multiple IDS sensors? Why not just 
place one IDS sensor just behind the packet filter (or even integrated with the packet 
filter) in Figure 8.36? We will soon see that an IDS not only needs to do deep packet 
inspection, but must also compare each passing packet with tens of thousands of 
“signatures”; this can be a significant amount of processing, particularly if the organ-
ization receives gigabits/sec of traffic from the Internet. By placing the IDS sensors 
further downstream, each sensor sees only a fraction of the organization’s traffic, 
and can more easily keep up. Nevertheless, high-performance IDS and IPS systems 
are available today, and many organizations can actually get by with just one sensor 
located near its access router.
IDS 
systems 
are 
broadly 
classified 
as 
either 
signature-based 
systems 
or 
­
anomaly- 
based systems. A signature-based IDS maintains an extensive database of attack 
signatures. Each signature is a set of rules pertaining to an intrusion activity. A sig-
nature may simply be a list of characteristics about a single packet (e.g., source and 
destination port numbers, protocol type, and a specific string of bits in the packet 
payload), or may relate to a series of packets. The signatures are normally created by 
skilled network security engineers who research known attacks. An organization’s 
network administrator can customize the signatures or add its own to the database.
Operationally, a signature-based IDS sniffs every packet passing by it, compar-
ing each sniffed packet with the signatures in its database. If a packet (or series of 
packets) matches a signature in the database, the IDS generates an alert. The alert 
could be sent to the network administrator in an e-mail message, could be sent to the 
network management system, or could simply be logged for future inspection.
Signature-based IDS systems, although widely deployed, have a number of limi-
tations. Most importantly, they require previous knowledge of the attack to generate 
an accurate signature. In other words, a signature-based IDS is completely blind to 
new attacks that have yet to be recorded. Another disadvantage is that even if a sig-
nature is matched, it may not be the result of an attack, so that a false alarm is gener-
ated. Finally, because every packet must be compared with an extensive collection 
of signatures, the IDS can become overwhelmed with processing and actually fail to 
detect many malicious packets.
An anomaly-based IDS creates a traffic profile as it observes traffic in normal 
operation. It then looks for packet streams that are statistically unusual, for exam-
ple, an inordinate percentage of ICMP packets or a sudden exponential growth in 
690         Chapter 8    •    Security in Computer Networks
port scans and ping sweeps. The great thing about anomaly-based IDS systems is 
that they don’t rely on previous knowledge about existing attacks—that is, they can 
potentially detect new, undocumented attacks. On the other hand, it is an extremely 
challenging problem to distinguish between normal traffic and statistically unusual 
traffic. To date, most IDS deployments are primarily signature-based, although some 
include some anomaly-based features.
Snort
Snort is a public-domain, open source IDS with hundreds of thousands of existing 
deployments [Snort 2012; Koziol 2003]. It can run on Linux, UNIX, and Windows 
platforms. It uses the generic sniffing interface libpcap, which is also used by Wire-
shark and many other packet sniffers. It can easily handle 100 Mbps of traffic; for 
installations with gibabit/sec traffic rates, multiple Snort sensors may be needed.
To gain some insight into Snort, let’s take a look at an example of a Snort 
signature:
alert icmp $EXTERNAL_NET any -> $HOME_NET any 
(msg:”ICMP PING NMAP”; dsize: 0; itype: 8;)
This signature is matched by any ICMP packet that enters the organization’s network 
($HOME_NET) from the outside ($EXTERNAL_NET), is of type 8 (ICMP ping), and 
has an empty payload (dsize = 0). Since nmap (see Section 1.6) generates ping pack-
ets with these specific characteristics, this signature is designed to detect nmap ping 
sweeps. When a packet matches this signature, Snort generates an alert that includes 
the message “ICMP PING NMAP”.
Perhaps what is most impressive about Snort is the vast community of users and 
security experts that maintain its signature database. Typically within a few hours 
of a new attack, the Snort community writes and releases an attack signature, which 
is then downloaded by the hundreds of thousands of Snort deployments distributed 
around the world. Moreover, using the Snort signature syntax, network administra-
tors can tailor the signatures to their own organization’s needs by either modifying 
existing signatures or creating entirely new ones.
8.10	 Summary
In this chapter, we’ve examined the various mechanisms that our secret lovers, Bob 
and Alice, can use to communicate securely. We’ve seen that Bob and Alice are 
interested in confidentiality (so they alone are able to understand the contents of a 
transmitted message), end-point authentication (so they are sure that they are talking 
8.10    •    Summary         691
with each other), and message integrity (so they are sure that their messages are not 
altered in transit). Of course, the need for secure communication is not confined to 
secret lovers. Indeed, we saw in Sections 8.5 through 8.8 that security can be used in 
various layers in a network architecture to protect against bad guys who have a large 
arsenal of possible attacks at hand.
The first part of this chapter presented various principles underlying secure 
communication. In Section 8.2, we covered cryptographic techniques for encrypting 
and decrypting data, including symmetric key cryptography and public key cryp-
tography. DES and RSA were examined as specific case studies of these two major 
classes of cryptographic techniques in use in today’s networks.
In Section 8.3, we examined two approaches for providing message integrity: 
message authentication codes (MACs) and digital signatures. The two approaches 
have a number of parallels. Both use cryptographic hash functions and both tech-
niques enable us to verify the source of the message as well as the integrity of the 
message itself. One important difference is that MACs do not rely on encryption 
whereas digital signatures require a public key infrastructure. Both techniques are 
extensively used in practice, as we saw in Sections 8.5 through 8.8. Furthermore, 
digital signatures are used to create digital certificates, which are important for veri-
fying the validity of public keys. In Section 8.4, we examined endpoint authentica-
tion and introduced nonces to defend against the replay attack.
In Sections 8.5 through 8.8 we examined several security networking protocols 
that enjoy extensive use in practice. We saw that symmetric key cryptography is at 
the core of PGP, SSL, IPsec, and wireless security. We saw that public key cryptog-
raphy is crucial for both PGP and SSL. We saw that PGP uses digital signatures for 
message integrity, whereas SSL and IPsec use MACs. Having now an understand-
ing of the basic principles of cryptography, and having studied how these princi-
ples are actually used, you are now in position to design your own secure network 
protocols!
Armed with the techniques covered in Sections 8.2 through 8.8, Bob and Alice 
can communicate securely. (One can only hope that they are networking students 
who have learned this material and can thus avoid having their tryst uncovered by 
Trudy!) But confidentiality is only a small part of the network security picture. As 
we learned in Section 8.9, increasingly, the focus in network security has been on 
securing the network infrastructure against a potential onslaught by the bad guys. 
In the latter part of this chapter, we thus covered firewalls and IDS systems which 
inspect packets entering and leaving an organization’s network.
This chapter has covered a lot of ground, while focusing on the most important 
topics in modern network security. Readers who desire to dig deeper are encour-
aged to investigate the references cited in this chapter. In particular, we recommend 
[Skoudis 2006] for attacks and operational security, [Kaufman 1995] for cryptog-
raphy and how it applies to network security, [Rescorla 2001] for an in-depth but 
readable treatment of SSL, and [Edney 2003] for a thorough discussion of 802.11 
security, including an insightful investigation into WEP and its flaws.
692         Chapter 8    •    Security in Computer Networks
Homework Problems and Questions
Chapter 8 Review Problems
SECTION 8.1
	R1.	 Operational devices such as firewalls and intrusion detection systems are 
used to counter attacks against an organization’s network. What is the basic 
difference between a firewall and an intrusion detection system?
	R2.	 Internet entities (routers, switches, DNS servers, Web servers, user end 
systems, and so on) often need to communicate securely. Give three specific 
example pairs of Internet entities that may want secure communication.
SECTION 8.2
	R3.	 The encryption technique itself is known—published, standardized, and 
available to everyone, even a potential intruder. Then where does the security 
of an encryption technique come from?
	R4.	 What is the difference between known plaintext attack and chosen plaintext 
attack?
	R5.	 Consider a 16-block cipher. How many possible input blocks does this cipher 
have? How many possible mappings are there? If we view each mapping as a 
key, then how many possible keys does this cipher have?
	R6.	 Suppose N people want to communicate with each of N - 1 other peo-
ple using symmetric key encryption. All communication between any two 
people, i and j, is visible to all other people in this group of N, and no other 
person in this group should be able to decode their communication. How 
many keys are required in the system as a whole? Now suppose that public 
key encryption is used. How many keys are required in this case?
	R7.	 Suppose n = 1,000, a = 1,017, and b = 1,006. Use an identity of modular 
arithmetic to calculate in your head (a # b) mod n.
	R8.	 Suppose you want to encrypt the message 10010111 by encrypting the deci-
mal number that corresponds to the message. What is the decimal number?
SECTIONS 8.3–8.4 
	R9.	 In what way does a hash provide a better message integrity check than a 
checksum (such as the Internet checksum)?
	
R10.	 Can you “decrypt” a hash of a message to get the original message? Explain 
your answer.
	
R11.	 Consider a variation of the MAC algorithm (Figure 8.9) where the sender 
sends (m, H(m) + s), where H(m) + s is the concatenation of H(m) and s. Is 
this variation flawed? Why or why not?
Homework Problems and Questions         693
	
R12.	 What does it mean for a signed document to be verifiable and nonforgeable?
	
R13.	 In the link-state routing algorithm, we would somehow need to distribute the 
secret authentication key to each of the routers in the autonomous system. How 
do we distribute the shared authentication key to the communicating entities?
	
R14.	 Name two popular secure networking protocols in which public key certifica-
tion is used.
	
R15.	 Suppose Alice has a message that she is ready to send to anyone who asks. 
Thousands of people want to obtain Alice’s message, but each wants to be 
sure of the integrity of the message. In this context, do you think a MAC-
based or a digital-signature-based integrity scheme is more suitable? Why?
	
R16.	 What is the purpose of a nonce in an end-point authentication protocol?
	
R17.	 What does it mean to say that a nonce is a once-in-a-lifetime value? In whose 
lifetime?
	
R18.	 Is the message integrity scheme based on HMAC susceptible to playback 
attacks? If so, how can a nonce be incorporated into the scheme to remove 
this susceptibility?
SECTIONS 8.5–8.8
	
R19.	 What is the de facto e-mail encryption scheme? What does it use for authenti-
cation and message integrity?
	
R20.	 In the SSL record, there is a field for SSL sequence numbers. True or false?
	
R21.	 What is the purpose of the random nonces in the SSL handshake?
	
R22.	 Suppose an SSL session employs a block cipher with CBC. True or false: The 
server sends to the client the IV in the clear.
	
R23.	 Suppose Bob initiates a TCP connection to Trudy who is pretending to be Alice. 
During the handshake, Trudy sends Bob Alice’s certificate. In what step of the SSL 
handshake algorithm will Bob discover that he is not communicating with Alice?
	
R24.	 Consider sending a stream of packets from Host A to Host B using IPsec. 
Typically, a new SA will be established for each packet sent in the stream. 
True or false?
	
R25.	 Suppose that TCP is being run over IPsec between headquarters and the 
branch office in Figure 8.28. If TCP retransmits the same packet, then the 
two corresponding packets sent by R1 packets will have the same sequence 
number in the ESP header. True or false?
	
R26.	 Is there a fixed encryption algorithm in SSL?
	
R27.	 Consider WEP for 802.11. Suppose that the data is 10001101 and the  
keystream is 01101010. What is the resulting ciphertext?
	
R28.	 Is the Initialization Vector (IV) appended to the secret 40-bit symmetric key 
in WEP protocol sent encrypted?
694         Chapter 8    •    Security in Computer Networks
SECTION 8.9
	
R29.	 Stateful packet filters maintain two data structures. Name them and briefly 
describe what they do.
	
R30.	 Consider a traditional (stateless) packet filter. This packet filter may filter 
packets based on TCP flag bits as well as other header fields. True or false?
	
R31.	 In a traditional packet filter, each interface can have its own access control 
list. True or false?
	
R32.	 Why must an application gateway work in conjunction with a router filter to 
be effective?
	
R33.	 Signature-based IDSs and IPSs inspect into the payloads of TCP and UDP 
segments. True or false?
Problems
	 P1.	 Using the monoalphabetic cipher in Figure 8.3, encode the message “This is 
a secret message.” Decode the message “fsgg ash.”
	 P2.	 Show that Trudy’s known-plaintext attack, in which she knows the (cipher-
text, plaintext) translation pairs for seven letters, reduces the number of 
possible substitutions to be checked in the example in Section 8.2.1 by 
approximately 109.
	 P3.	 Consider the polyalphabetic system shown in Figure 8.4. Will a chosen-
plaintext attack that is able to get the plaintext encoding of the message “The 
quick brown fox jumps over the lazy dog.” be sufficient to decode all mes-
sages? Why or why not?
	 P4.	 Consider the block cipher in Figure 8.5. Suppose that each block cipher 
Ti simply reverses the order of the eight input bits (so that, for example, 
11110000 becomes 00001111). Further suppose that the 64-bit scrambler 
does not modify any bits (so that the output value of the mth bit is equal to 
the input value of the mth bit). (a) With n = 3 and the original 64-bit input 
equal to 10100000 repeated eight times, what is the value of the output?  
(b) Repeat part (a) but now change the last bit of the original 64-bit input 
from a 0 to a 1. (c) Repeat parts (a) and (b) but now suppose that the 64-bit 
scrambler inverses the order of the 64 bits.
	 P5.	 Consider the block cipher in Figure 8.5. Suppose, for a given “key,” Alice and 
Bob would need to keep 16 tables, each 16 bits by 8 bits. For Alice (or Bob) to 
store all 16 tables, how many bits of storage are necessary? How does this number 
compare with the number of bits required for a full-table 128-bit block cipher?
	 P6.	 Consider the 3-bit block cipher in Table 8.1. Suppose the plaintext is 
100100100. (a) Initially assume that CBC is not used. What is the resulting 
ciphertext? (b) Suppose Trudy sniffs the ciphertext. Assuming she knows that 
Problems         695
a 3-bit block cipher without CBC is being employed (but doesn’t know the 
specific cipher), what can she surmise? (c) Now suppose that CBC is used 
with IV = 111. What is the resulting ciphertext?
	 P7.	 a.	 
Using RSA, choose p = 5 and q = 7, and encode the numbers 12, 19, and 
27 separately. Apply the decryption algorithm to the encrypted version to 
recover the original plaintext message.
b.	 Choose p and q of your own and encrypt 1834 as one message m.
	 P8.	 Consider RSA with p = 7 and q = 13.
a.	 What are n and z?
b.	 Let e be 17. Why is this an acceptable choice for e?
c.	 Find d such that de = 1 (mod z).
d.	 Encrypt the message m = 9 using the key (n, e). Let c denote the correspond-
ing ciphertext. Show all work.
	 P9.	 In this problem, we explore the Diffie-Hellman (DH) public-key encryption 
algorithm, which allows two entities to agree on a shared key. The DH algo-
rithm makes use of a large prime number p and another large number g less 
than p. Both p and g are made public (so that an attacker would know them). 
In DH, Alice and Bob each independently choose secret keys, SA and SB, 
respectively. Alice then computes her public key, TA, by raising g to SA and 
then taking mod p. Bob similarly computes his own public key TB by raising 
g to SB and then taking mod p. Alice and Bob then exchange their public keys 
over the Internet. Alice then calculates the shared secret key S by raising TB 
to SA and then taking mod p. Similarly, Bob calculates the shared key S′ by 
raising TA to SB and then taking mod p.
a.	 Prove that, in general, Alice and Bob obtain the same symmetric key, that 
is, prove S = S′.
b.	 With p = 11 and g = 2, suppose Alice and Bob choose private keys 
SA = 5 and SB = 12, respectively. Calculate Alice’s and Bob’s public 
keys, TA and TB. Show all work.
c.	 Following up on part (b), now calculate S as the shared symmetric key. 
Show all work.
d.	 Provide a timing diagram that shows how Diffie-Hellman can be 
attacked by a man-in-the-middle. The timing diagram should have 
three vertical lines, one for Alice, one for Bob, and one for the attacker 
Trudy.
	
P10.	 Suppose Alice wants to communicate with Bob using symmetric key cryp-
tography using a session key KS. In Section 8.2, we learned how public-key 
cryptography can be used to distribute the session key from Alice to Bob.  
In this problem, we explore how the session key can be distributed—without 
696         Chapter 8    •    Security in Computer Networks
public key cryptography—using a key distribution center (KDC). The KDC 
is a server that shares a unique secret symmetric key with each registered 
user. For Alice and Bob, denote these keys by KA@KDC and KB@KDC. Design a 
scheme that uses the KDC to distribute KS to Alice and Bob. Your scheme 
should use three messages to distribute the session key: a message from Alice 
to the KDC; a message from the KDC to Alice; and finally a message from 
Alice to Bob. The first message is KA@KDC (A, B). Using the notation, KA@KDC, 
KB@KDC, S, A, and B answer the following questions.
a.	 What is the second message?
b.	 What is the third message?
	
P11.	 Compute a third message, different from the two messages in Figure 8.8, that 
has the same checksum as the messages in Figure 8.8.
	
P12.	 The sender can mix some randomness into the ciphertext so that identical 
plaintext blocks produce different ciphertext blocks. But for each cipher bit, 
the sender must now also send a random bit, doubling the required bandwidth. 
Is there any way around this?
	
P13.	 In the BitTorrent P2P file distribution protocol (see Chapter 2), the seed 
breaks the file into blocks, and the peers redistribute the blocks to each other. 
Without any protection, an attacker can easily wreak havoc in a torrent by 
masquerading as a benevolent peer and sending bogus blocks to a small 
subset of peers in the torrent. These unsuspecting peers then redistribute the 
bogus blocks to other peers, which in turn redistribute the bogus blocks to 
even more peers. Thus, it is critical for BitTorrent to have a mechanism that 
allows a peer to verify the integrity of a block, so that it doesn’t redistrib-
ute bogus blocks. Assume that when a peer joins a torrent, it initially gets a 
.torrent file from a fully trusted source. Describe a simple scheme that 
allows peers to verify the integrity of blocks.
	
P14.	 Solving factorization in polynomial time implies breaking the RSA cryptosystem. 
Is the converse true?
	
P15.	 Consider our authentication protocol in Figure 8.18 in which Alice authen-
ticates herself to Bob, which we saw works well (i.e., we found no flaws in 
it). Now suppose that while Alice is authenticating herself to Bob, Bob must 
authenticate himself to Alice. Give a scenario by which Trudy, pretending to 
be Alice, can now authenticate herself to Bob as Alice. (Hint: Consider that 
the sequence of operations of the protocol, one with Trudy initiating and one 
with Bob initiating, can be arbitrarily interleaved. Pay particular attention to 
the fact that both Bob and Alice will use a nonce, and that if care is not taken, 
the same nonce can be used maliciously.)
	
P16.	 A natural question is whether we can use a nonce and public key cryptography to 
solve the end-point authentication problem in Section 8.4. Consider the following 
natural protocol: (1) Alice sends the message “I am Alice” to Bob. (2) Bob 
chooses a nonce, R, and sends it to Alice. (3) Alice uses her private key to encrypt 
Problems         697
the nonce and sends the resulting value to Bob. (4) Bob applies Alice’s public key 
to the received message. Thus, Bob computes R and authenticates Alice.
a.	 Diagram this protocol, using the notation for public and private keys 
employed in the textbook.
b.	 Suppose that certificates are not used. Describe how Trudy can become  
a “woman-in-the-middle” by intercepting Alice’s messages and then 
­
pretending to be Alice to Bob.
	
P17.	 Figure 8.19 shows the operations that Alice must perform with PGP to pro-
vide confidentiality, authentication, and integrity. Diagram the corresponding 
operations that Bob must perform on the package received from Alice.
	
P18.	 Suppose Alice wants to send an e-mail to Bob. Bob has a public-private key  
pair (K  B
+, K  B
-), and Alice has Bob’s certificate. But Alice does not have a 
public, private key pair. Alice and Bob (and the entire world) share the same 
hash function H(#).
a.	 In this situation, is it possible to design a scheme so that Bob can verify 
that Alice created the message? If so, show how with a block diagram for 
Alice and Bob.
b.	 Is it possible to design a scheme that provides confidentiality for sending 
the message from Alice to Bob? If so, show how with a block diagram for 
Alice and Bob.
	
P19.	 Consider the Wireshark output below for a portion of an SSL session.
a.	 Is Wireshark packet 112 sent by the client or server?
b.	 What is the server’s IP address and port number?
c.	 Assuming no loss and no retransmissions, what will be the sequence num-
ber of the next TCP segment sent by the client?
d.	 How many SSL records does Wireshark packet 112 contain?
e.	 Does packet 112 contain a Master Secret or an Encrypted Master Secret or 
neither?
f.	 Assuming that the handshake type field is 1 byte and each length field is 
3 bytes, what are the values of the first and last bytes of the Master Secret 
(or Encrypted Master Secret)?
g.	 The client encrypted handshake message takes into account how many 
SSL records?
h.	 The server encrypted handshake message takes into account how many 
SSL records?
	
P20.	 In Section 8.6.1, it is shown that without sequence numbers, Trudy (a 
woman-in-the middle) can wreak havoc in an SSL session by interchanging 
TCP segments. Can Trudy do something similar by deleting a TCP seg-
ment? What does she need to do to succeed at the deletion attack? What 
effect will it have?
698         Chapter 8    •    Security in Computer Networks
	
P21.	 A router’s link-state message includes a list of its directly connected neighbors 
and the direct costs to these neighbors. Once a router receives link-state messages 
from all of the other routers, it can create a complete map of the network, run its 
least-cost routing algorithm, and configure its forwarding table. One relatively 
easy attack on the routing algorithm is for the attacker to distribute bogus link-
state messages with incorrect link-state information. How can this be prevented? 
	
P22.	 The following true/false questions pertain to Figure 8.28.
a.	 When a host in 172.16.1/24 sends a datagram to an Amazon.com server, 
the router R1 will encrypt the datagram using IPsec.
b.	 When a host in 172.16.1/24 sends a datagram to a host in 172.16.2/24, the 
router R1 will change the source and destination address of the IP datagram.
c.	 Suppose a host in 172.16.1/24 initiates a TCP connection to a Web server 
in 172.16.2/24. As part of this connection, all datagrams sent by R1 will 
have protocol number 50 in the left-most IPv4 header field.
(Wireshark screenshot reprinted by permission of the Wireshark Foundation.)
Problems         699
d.	 Consider sending a TCP segment from a host in 172.16.1/24 to a host in 
172.16.2/24. Suppose the acknowledgment for this segment gets lost, so 
that TCP resends the segment. Because IPsec uses sequence numbers, R1 
will not resend the TCP segment.
	
P23.	 When Bob signs a message, Bob must put something on the message that 
is unique to him. Bob could consider attaching a MAC for the signature, 
where the MAC is created by appending his key (unique to him) to the 
message, and then taking the hash. Will it cause any problem when Alice 
would try verification?
	
P24.	 Consider the following pseudo-WEP protocol. The key is 4 bits and the IV 
is 2 bits. The IV is appended to the end of the key when generating the key-
stream. Suppose that the shared secret key is 1010. The keystreams for the 
four possible inputs are as follows:
	
	 101000: 0010101101010101001011010100100 . . .
	
	 101001: 1010011011001010110100100101101 . . .
	
	 101010: 0001101000111100010100101001111 . . .
	
	 101011: 1111101010000000101010100010111 . . .
	
	 Suppose all messages are 8 bits long. Suppose the ICV (integrity check) is  
4 bits long, and is calculated by XOR-ing the first 4 bits of data with the last 
4 bits of data. Suppose the pseudo-WEP packet consists of three fields: first 
the IV field, then the message field, and last the ICV field, with some of these 
fields encrypted.
a.	 We want to send the message m = 10100000 using the IV = 11 and using 
WEP. What will be the values in the three WEP fields?
b.	 Show that when the receiver decrypts the WEP packet, it recovers the 
message and the ICV.
c.	 Suppose Trudy intercepts a WEP packet (not necessarily with the IV = 11) 
and wants to modify it before forwarding it to the receiver. Suppose Trudy 
flips the first ICV bit. Assuming that Trudy does not know the keystreams 
for any of the IVs, what other bit(s) must Trudy also flip so that the 
received packet passes the ICV check?
d.	 Justify your answer by modifying the bits in the WEP packet in 
part (a), decrypting the resulting packet, and verifying the integrity 
check.
	
P25.	 Provide a filter table and a connection table for a stateful firewall that is as 
restrictive as possible but accomplishes the following:
a.	 Allows all internal users to establish Telnet sessions with external hosts.
b.	 Allows external users to surf the company Web site at 222.22.0.12.
c.	 But otherwise blocks all inbound and outbound traffic.
700         Chapter 8    •    Security in Computer Networks
	
	 The internal network is 222.22/16. In your solution, suppose that the connec-
tion table is currently caching three connections, all from inside to outside. 
You’ll need to invent appropriate IP addresses and port numbers.
	
P26.	 Suppose Alice wants to visit the Web site activist.com using a TOR-like 
­
service. This service uses two non-colluding proxy servers, Proxy1 and 
Proxy2. Alice first obtains the certificates (each containing a public key)  
for Proxy1 and Proxy2 from some central server. Denote K1
+( ), K2
+( ), K1
-( ), 
and K2
-( ) for the encryption/decryption with public and private RSA keys.
a.	 Using a timing diagram, provide a protocol (as simple as possible) that 
enables Alice to establish a shared session key S1 with Proxy1. Denote 
S1(m) for encryption/decryption of data m with the shared key S1.
b.	 Using a timing diagram, provide a protocol (as simple as possible) that 
allows Alice to establish a shared session key S2 with Proxy2 without 
revealing her IP address to Proxy2.
c.	 Assume now that shared keys S1 and S2 are now established. Using a 
timing diagram, provide a protocol (as simple as possible and not using 
public-key cryptography) that allows Alice to request an html page from 
activist.com without revealing her IP address to Proxy2 and without 
revealing to Proxy1 which site she is visiting. Your diagram should end 
with an HTTP request arriving at activist.com.
Wireshark Lab
In this lab (available from the book Web site), we investigate the Secure Sockets 
Layer (SSL) protocol. Recall from Section 8.6 that SSL is used for securing a TCP 
connection, and that it is extensively used in practice for secure Internet transactions. 
In this lab, we will focus on the SSL records sent over the TCP connection. We will 
attempt to delineate and classify each of the records, with a goal of understanding the 
why and how for each record. We investigate the various SSL record types as well 
as the fields in the SSL messages. We do so by analyzing a trace of the SSL records 
sent between your host and an e-commerce server.
IPsec Lab
In this lab (available from the book Web site), we will explore how to create IPsec 
SAs between linux boxes. You can do the first part of the lab with two ordinary linux 
boxes, each with one Ethernet adapter. But for the second part of the lab, you will 
need four linux boxes, two of which having two Ethernet adapters. In the second half 
of the lab, you will create IPsec SAs using the ESP protocol in the tunnel mode. You 
will do this by first manually creating the SAs, and then by having IKE create the SAs.
701
What led you to specialize in the networking security area?
This is going to sound odd, but the answer is simple: It was fun. My background was in 
­
systems programming and systems administration, which leads fairly naturally to security. 
And I’ve always been interested in communications, ranging back to part-time systems 
­
programming jobs when I was in college.
My work on security continues to be motivated by two things—a desire to keep com-
puters useful, which means that their function can’t be corrupted by attackers, and a desire 
to protect privacy.
What was your vision for Usenet at the time that you were developing it? And now?
We originally viewed it as a way to talk about computer science and computer program-
ming around the country, with a lot of local use for administrative matters, for-sale ads, and 
so on. In fact, my original prediction was one to two messages per day, from 50–100 sites at 
the most—ever. But the real growth was in people-related topics, including—but not limited 
to—human interactions with computers. My favorite newsgroups, over the years, have been 
things like rec.woodworking, as well as sci.crypt.
To some extent, netnews has been displaced by the Web. Were I to start designing it 
today, it would look very different. But it still excels as a way to reach a very broad audi-
ence that is interested in the topic, without having to rely on particular Web sites.
Has anyone inspired you professionally? In what ways?
Professor Fred Brooks—the founder and original chair of the computer science department 
at the University of North Carolina at Chapel Hill, the manager of the team that developed 
the IBM S/360 and OS/360, and the author of The Mythical Man-Month—was a tremendous 
AN INTERVIEW WITH…
Steven M. Bellovin
Steven M. Bellovin joined the faculty at Columbia University after 
many years at the Network Services Research Lab at AT&T Labs 
Research in Florham Park, New Jersey. His focus is on networks, 
security, and why the two are incompatible. In 1995, he was 
awarded the Usenix Lifetime Achievement Award for his work in the 
creation of Usenet, the first newsgroup exchange network that linked 
two or more computers and allowed users to share information and 
join in discussions. Steve is also an elected member of the National 
Academy of Engineering. He received his BA from Columbia 
University and his PhD from the University of North Carolina at 
Chapel Hill.
702
influence on my career. More than anything else, he taught outlook and trade-offs—how to 
look at problems in the context of the real world (and how much messier the real world is 
than a theorist would like), and how to balance competing interests in designing a solution. 
Most computer work is engineering—the art of making the right trade-offs to satisfy many 
contradictory objectives.
What is your vision for the future of networking and security?
Thus far, much of the security we have has come from isolation. A firewall, for example, 
works by cutting off access to certain machines and services. But we’re in an era of increas-
ing connectivity—it’s gotten harder to isolate things. Worse yet, our production systems 
require far more separate pieces, interconnected by networks. Securing all that is one of our 
biggest challenges.
What would you say have been the greatest advances in security? How much further do 
we have to go?
At least scientifically, we know how to do cryptography. That’s been a big help. But most 
security problems are due to buggy code, and that’s a much harder problem. In fact, it’s 
the oldest unsolved problem in computer science, and I think it will remain that way. The 
challenge is figuring out how to secure systems when we have to build them out of insecure 
components. We can already do that for reliability in the face of hardware failures; can we 
do the same for security?
Do you have any advice for students about the Internet and networking security?
Learning the mechanisms is the easy part. Learning how to “think paranoid” is harder. You 
have to remember that probability distributions don’t apply—the attackers can and will find 
improbable conditions. And the details matter—a lot.
703
While lounging in bed or riding buses and subways, people in all corners of the world 
are currently using the Internet to watch movies and television shows on demand. 
Internet movie and television distribution companies such as Netflix and Amazon 
in North America and Youku and Kankan in China have practically become house-
hold names. But people are not only watching Internet videos, they are using sites 
like YouTube to upload and distribute their own user-generated content, becoming 
Internet video producers as well as consumers. Moreover, network applications such 
as Skype, Google Talk, and WeChat (enormously popular in China) allow people 
to not only make “telephone calls” over the Internet, but to also enhance those calls 
with video and multi-person conferencing. In fact, we predict that by the end of the 
current decade most of the video consumption and voice conversations will take 
place end-to-end over the Internet, more typically to wireless devices connected to 
 
the Internet via cellular and WiFi access networks. Traditional telephony and broad-
cast television are quickly becoming obsolete.
We begin this chapter with a taxonomy of multimedia applications in Sec­
tion 9.1. We’ll see that a multimedia application can be classified as either stream-
ing stored audio/video, conversational voice/video-over-IP, or streaming live audio/
video. We’ll see that each of these classes of applications has its own unique service 
requirements that differ significantly from those of traditional elastic applications 
such as e-mail, Web browsing, and remote login. In Section 9.2, we’ll examine video 
streaming in some detail. We’ll explore many of the underlying principles behind 
9
CHAPTER
Multimedia 
Networking
704         Chapter 9    •    Multimedia Networking
video streaming, including client buffering, prefetching, and adapting video qual-
ity to available bandwidth. In Section 9.3, we investigate conversational voice and 
video, which, unlike elastic applications, are highly sensitive to end-to-end delay 
but can tolerate occasional loss of data. Here we’ll examine how techniques such 
as adaptive playout, forward error correction, and error concealment can mitigate 
against network-induced packet loss and delay. We’ll also examine Skype as a case 
study. In Section 9.4, we’ll study RTP and SIP, two popular protocols for real-time 
conversational voice and video applications. In Section 9.5, we’ll investigate mecha-
nisms within the network that can be used to distinguish one class of traffic (e.g., 
delay-sensitive applications such as conversational voice) from another (e.g., elastic 
applications such as browsing Web pages), and provide differentiated service among 
multiple classes of traffic.
9.1	 Multimedia Networking Applications
We define a multimedia network application as any network application that employs 
audio or video. In this section, we provide a taxonomy of multimedia applications. 
We’ll see that each class of applications in the taxonomy has its own unique set of 
service requirements and design issues. But before diving into an in-depth discussion 
of Internet multimedia applications, it is useful to consider the intrinsic characteris-
tics of the audio and video media themselves.
9.1.1 Properties of Video
Perhaps the most salient characteristic of video is its high bit rate. Video distributed 
over the Internet typically ranges from 100 kbps for low-quality video conferencing 
to over 3 Mbps for streaming high-definition movies. To get a sense of how video 
bandwidth demands compare with those of other Internet applications, let’s briefly 
consider three different users, each using a different Internet application. Our first 
user, Frank, is going quickly through photos posted on his friends’ Facebook pages. 
Let’s assume that Frank is looking at a new photo every 10 seconds, and that photos 
are on average 200 Kbytes in size. (As usual, throughout this discussion we make 
the simplifying assumption that 1 Kbyte = 8,000 bits.) Our second user, Martha, 
is streaming music from the Internet (“the cloud”) to her smartphone. Let’s assume 
Martha is using a service such as Spotify to listen to many MP3 songs, one after the 
other, each encoded at a rate of 128 kbps. Our third user, Victor, is watching a video 
that has been encoded at 2 Mbps. Finally, let’s suppose that the session length for all 
three users is 4,000 seconds (approximately 67 minutes). Table 9.1 compares the bit 
rates and the total bytes transferred for these three users. We see that video streaming 
consumes by far the most bandwidth, having a bit rate of more than ten times greater 
than that of the Facebook and music-streaming applications. Therefore, when design-
9.1    •    Multimedia Networking Applications         705
ing networked video applications, the first thing we must keep in mind is the high 
bit-rate requirements of video. Given the popularity of video and its high bit rate, it 
is perhaps not surprising that Cisco predicts [Cisco 2015] that streaming and stored 
video will be approximately 80 percent of global consumer Internet traffic by 2019.
Another important characteristic of video is that it can be compressed, thereby 
trading off video quality with bit rate. A video is a sequence of images, typically 
being displayed at a constant rate, for example, at 24 or 30 images per second. An 
uncompressed, digitally encoded image consists of an array of pixels, with each 
pixel encoded into a number of bits to represent luminance and color. There are two 
types of redundancy in video, both of which can be exploited by video compression. 
 
Spatial redundancy is the redundancy within a given image. Intuitively, an image that 
consists of mostly white space has a high degree of redundancy and can be efficiently 
compressed without significantly sacrificing image quality. Temporal redundancy 
reflects repetition from image to subsequent image. If, for example, an image and the 
subsequent image are exactly the same, there is no reason to re-encode the subsequent 
image; it is instead more efficient simply to indicate during encoding that the subse-
quent image is exactly the same. Today’s off-the-shelf compression algorithms can 
compress a video to essentially any bit rate desired. Of course, the higher the bit rate, 
the better the image quality and the better the overall user viewing experience.
We can also use compression to create multiple versions of the same video, 
each at a different quality level. For example, we can use compression to create, 
say, three versions of the same video, at rates of 300 kbps, 1 Mbps, and 3 Mbps. 
Users can then decide which version they want to watch as a function of their current 
available bandwidth. Users with high-speed Internet connections might choose the 
3 Mbps version; users watching the video over 3G with a smartphone might choose 
the 300 kbps version. Similarly, the video in a video conference application can 
 
be compressed “on-the-fly” to provide the best video quality given the available 
 
end-to-end bandwidth between conversing users.
9.1.2 Properties of Audio
Digital audio (including digitized speech and music) has significantly lower band-
width requirements than video. Digital audio, however, has its own unique prop-
erties that must be considered when designing multimedia network applications. 
 
Table 9.1  ♦  Comparison of bit-rate requirements of three Internet applications
Bit rate
Bytes transferred in 67 min
Facebook Frank
160 kbps
80 Mbytes
Martha Music
128 kbps
64 Mbytes
Victor Video
2 Mbps
1 Gbyte
706         Chapter 9    •    Multimedia Networking
To understand these properties, let’s first consider how analog audio (which humans 
and musical instruments generate) is converted to a digital signal:
•	 The analog audio signal is sampled at some fixed rate, for example, at 8,000 
 
samples per second. The value of each sample will be some real number.
•	 Each of the samples is then rounded to one of a finite number of values. This 
operation is referred to as quantization. The number of such finite values—called 
quantization values—is typically a power of two, for example, 256 quantization 
values.
•	 Each of the quantization values is represented by a fixed number of bits. For 
example, if there are 256 quantization values, then each value—and hence each 
audio sample—is represented by one byte. The bit representations of all the sam-
ples are then concatenated together to form the digital representation of the signal. 
As an example, if an analog audio signal is sampled at 8,000 samples per second 
and each sample is quantized and represented by 8 bits, then the resulting digital 
signal will have a rate of 64,000 bits per second. For playback through audio 
speakers, the digital signal can then be converted back—that is, decoded—to an 
analog signal. However, the decoded analog signal is only an approximation of 
the original signal, and the sound quality may be noticeably degraded (for exam-
ple, high-frequency sounds may be missing in the decoded signal). By increasing 
the sampling rate and the number of quantization values, the decoded signal can 
better approximate the original analog signal. Thus (as with video), there is a 
trade-off between the quality of the decoded signal and the bit-rate and storage 
requirements of the digital signal.
The basic encoding technique that we just described is called pulse code modulation 
(PCM). Speech encoding often uses PCM, with a sampling rate of 8,000 samples per 
second and 8 bits per sample, resulting in a rate of 64 kbps. The audio compact disk 
(CD) also uses PCM, with a sampling rate of 44,100 samples per second with 16 
bits per sample; this gives a rate of 705.6 kbps for mono and 1.411 Mbps for stereo.
PCM-encoded speech and music, however, are rarely used in the Internet. 
Instead, as with video, compression techniques are used to reduce the bit rates of 
the stream. Human speech can be compressed to less than 10 kbps and still be intel-
ligible. A popular compression technique for near CD-quality stereo music is MPEG 
1 layer 3, more commonly known as MP3. MP3 encoders can compress to many 
different rates; 128 kbps is the most common encoding rate and produces very little 
sound degradation. A related standard is Advanced Audio Coding (AAC), which 
has been popularized by Apple. As with video, multiple versions of a prerecorded 
audio stream can be created, each at a different bit rate.
Although audio bit rates are generally much less than those of video, users are 
generally much more sensitive to audio glitches than video glitches. Consider, for 
example, a video conference taking place over the Internet. If, from time to time, 
the video signal is lost for a few seconds, the video conference can likely proceed 
9.1    •    Multimedia Networking Applications         707
without too much user frustration. If, however, the audio signal is frequently lost, the 
users may have to terminate the session.
9.1.3 Types of Multimedia Network Applications
The Internet supports a large variety of useful and entertaining multimedia applica-
tions. In this subsection, we classify multimedia applications into three broad cat-
egories: (i) streaming stored audio/video, (ii) conversational voice/video-over-IP, 
and (iii) streaming live audio/video. As we will soon see, each of these application 
categories has its own set of service requirements and design issues.
Streaming Stored Audio and Video
To keep the discussion concrete, we focus here on streaming stored video, which typ-
ically combines video and audio components. Streaming stored audio (such as Spo-
tify’s streaming music service) is very similar to streaming stored video, although the 
bit rates are typically much lower.
In this class of applications, the underlying medium is prerecorded video, such 
as a movie, a television show, a prerecorded sporting event, or a prerecorded user-
generated video (such as those commonly seen on YouTube). These prerecorded 
videos are placed on servers, and users send requests to the servers to view the vid-
eos on demand. Many Internet companies today provide streaming video, including 
YouTube (Google), Netflix, Amazon, and Hulu. Streaming stored video has three 
key distinguishing features.
•	 Streaming. In a streaming stored video application, the client typically begins 
video playout within a few seconds after it begins receiving the video from the 
server. This means that the client will be playing out from one location in the 
video while at the same time receiving later parts of the video from the server. 
This technique, known as streaming, avoids having to download the entire video 
file (and incurring a potentially long delay) before playout begins.
•	 Interactivity. Because the media is prerecorded, the user may pause, reposition 
forward, reposition backward, fast-forward, and so on through the video content. 
The time from when the user makes such a request until the action manifests itself 
at the client should be less than a few seconds for acceptable responsiveness.
•	 Continuous playout. Once playout of the video begins, it should proceed accord-
ing to the original timing of the recording. Therefore, data must be received from 
the server in time for its playout at the client; otherwise, users experience video 
frame freezing (when the client waits for the delayed frames) or frame skipping 
(when the client skips over delayed frames).
By far, the most important performance measure for streaming video is average 
throughput. In order to provide continuous playout, the network must provide an 
708         Chapter 9    •    Multimedia Networking
average throughput to the streaming application that is at least as large the bit rate of 
the video itself. As we will see in Section 9.2, by using buffering and prefetching, 
it is possible to provide continuous playout even when the throughput fluctuates, 
as long as the average throughput (averaged over 5–10 seconds) remains above the 
video rate [Wang 2008].
For many streaming video applications, prerecorded video is stored on, and 
streamed from, a CDN rather than from a single data center. There are also many 
P2P video streaming applications for which the video is stored on users’ hosts 
(peers), with different chunks of video arriving from different peers that may 
spread around the globe. Given the prominence of Internet video streaming, we 
will explore video streaming in some depth in Section 9.2, paying particular atten-
tion to client buffering, prefetching, adapting quality to bandwidth availability, and 
CDN distribution.
Conversational Voice- and Video-over-IP
Real-time conversational voice over the Internet is often referred to as Internet 
telephony, since, from the user’s perspective, it is similar to the traditional circuit-
switched telephone service. It is also commonly called Voice-over-IP (VoIP). Con-
versational video is similar, except that it includes the video of the participants as 
well as their voices. Most of today’s voice and video conversational systems allow 
users to create conferences with three or more participants. Conversational voice and 
video are widely used in the Internet today, with the Internet companies Skype, QQ, 
and Google Talk boasting hundreds of millions of daily users.
In our discussion of application service requirements in Chapter 2 (Figure 2.4), 
we identified a number of axes along which application requirements can be clas-
sified. Two of these axes—timing considerations and tolerance of data loss—are 
particularly important for conversational voice and video applications. Timing con-
siderations are important because audio and video conversational applications are 
highly delay-sensitive. For a conversation with two or more interacting speakers, the 
delay from when a user speaks or moves until the action is manifested at the other 
end should be less than a few hundred milliseconds. For voice, delays smaller than 
150 milliseconds are not perceived by a human listener, delays between 150 and 400 
milliseconds can be acceptable, and delays exceeding 400 milliseconds can result in 
frustrating, if not completely unintelligible, voice conversations.
On the other hand, conversational multimedia applications are loss-tolerant—
occasional loss only causes occasional glitches in audio/video playback, and these 
losses can often be partially or fully concealed. These delay-sensitive but loss-tolerant 
characteristics are clearly different from those of elastic data applications such as 
Web browsing, e-mail, social networks, and remote login. For elastic applications, 
long delays are annoying but not particularly harmful; the completeness and integrity 
of the transferred data, however, are of paramount importance. We will explore con-
versational voice and video in more depth in Section 9.3, paying particular attention 
9.2    •    Streaming Stored Video         709
to how adaptive playout, forward error correction, and error concealment can miti-
gate against network-induced packet loss and delay.
Streaming Live Audio and Video
This third class of applications is similar to traditional broadcast radio and television, 
except that transmission takes place over the Internet. These applications allow a 
user to receive a live radio or television transmission—such as a live sporting event 
or an ongoing news event—transmitted from any corner of the world. Today, thou-
sands of radio and television stations around the world are broadcasting content over 
the Internet.
Live, broadcast-like applications often have many users who receive the same 
audio/video program at the same time. In the Internet today, this is typically done 
with CDNs (Section 2.6). As with streaming stored multimedia, the network must 
provide each live multimedia flow with an average throughput that is larger than 
the video consumption rate. Because the event is live, delay can also be an issue, 
although the timing constraints are much less stringent than those for conversational 
voice. Delays of up to ten seconds or so from when the user chooses to view a live 
transmission to when playout begins can be tolerated. We will not cover stream-
ing live media in this book because many of the techniques used for streaming live 
media—initial buffering delay, adaptive bandwidth use, and CDN distribution—are 
similar to those for streaming stored media.
9.2	 Streaming Stored Video
For streaming video applications, prerecorded videos are placed on servers, and 
users send requests to these servers to view the videos on demand. The user may 
watch the video from beginning to end without interruption, may stop watching the 
video well before it ends, or interact with the video by pausing or repositioning to a 
future or past scene. Streaming video systems can be classified into three categories: 
UDP streaming, HTTP streaming, and adaptive HTTP streaming (see Section 
2.6). Although all three types of systems are used in practice, the majority of today’s 
systems employ HTTP streaming and adaptive HTTP streaming.
A common characteristic of all three forms of video streaming is the extensive 
use of client-side application buffering to mitigate the effects of varying end-to-end 
delays and varying amounts of available bandwidth between server and client. For 
streaming video (both stored and live), users generally can tolerate a small several-
second initial delay between when the client requests a video and when video playout 
begins at the client. Consequently, when the video starts to arrive at the client, the cli-
ent need not immediately begin playout, but can instead build up a reserve of video 
in an application buffer. Once the client has built up a reserve of several seconds of 
710         Chapter 9    •    Multimedia Networking
buffered-but-not-yet-played video, the client can then begin video playout. There 
are two important advantages provided by such client buffering. First, client-side 
buffering can absorb variations in server-to-client delay. If a particular piece of video 
data is delayed, as long as it arrives before the reserve of received-but-not-yet-played 
video is exhausted, this long delay will not be noticed. Second, if the server-to-client 
bandwidth briefly drops below the video consumption rate, a user can continue to 
enjoy continuous playback, again as long as the client application buffer does not 
become completely drained.
Figure 9.1 illustrates client-side buffering. In this simple example, suppose that 
video is encoded at a fixed bit rate, and thus each video block contains video frames 
that are to be played out over the same fixed amount of time, △. The server transmits 
the first video block at t0, the second block at t0 + △, the third block at t0 + 2△, 
and so on. Once the client begins playout, each block should be played out △ 
time units after the previous block in order to reproduce the timing of the original 
recorded video. Because of the variable end-to-end network delays, different video 
blocks experience different delays. The first video block arrives at the client at t1 and 
the second block arrives at t2. The network delay for the ith block is the horizontal 
distance between the time the block was transmitted by the server and the time it is 
received at the client; note that the network delay varies from one video block to 
another. In this example, if the client were to begin playout as soon as the first block 
arrived at t1, then the second block would not have arrived in time to be played out 
at out at t1 + △. In this case, video playout would either have to stall (waiting for 
block 2 to arrive) or block 2 could be skipped—both resulting in undesirable playout 
impairments. Instead, if the client were to delay the start of playout until t3, when 
blocks 1 through 6 have all arrived, periodic playout can proceed with all blocks hav-
ing been received before their playout time.
Variable
network
delay
Client
playout
delay
Constant bit
rate video
transmission
by server
1
2
3
4
5
6
7
8
9
10
11
12
Constant bit
rate video
playout
by client
Time
Video block number
t0
t1
t2
t3
t0+2D
t0+D
t1+D
t3+D
Video
reception
at client
Figure 9.1  ♦  Client playout delay in video streaming
9.2    •    Streaming Stored Video         711
9.2.1 UDP Streaming
We only briefly discuss UDP streaming here, referring the reader to more in-depth 
discussions of the protocols behind these systems where appropriate. With UDP 
streaming, the server transmits video at a rate that matches the client’s video con-
sumption rate by clocking out the video chunks over UDP at a steady rate. For exam-
ple, if the video consumption rate is 2 Mbps and each UDP packet carries 8,000 
bits of video, then the server would transmit one UDP packet into its socket every 
(8000 bits)/(2 Mbps) = 4 msec. As we learned in Chapter 3, because UDP does 
not employ a congestion-control mechanism, the server can push packets into the 
network at the consumption rate of the video without the rate-control restrictions of 
TCP. UDP streaming typically uses a small client-side buffer, big enough to hold less 
than a second of video.
Before passing the video chunks to UDP, the server will encapsulate the 
video chunks within transport packets specially designed for transporting audio 
and video, using the Real-Time Transport Protocol (RTP) [RFC 3550] or a simi-
lar (possibly proprietary) scheme. We delay our coverage of RTP until Section 
9.3, where we discuss RTP in the context of conversational voice and video 
systems.
Another distinguishing property of UDP streaming is that in addition to the 
server-to-client video stream, the client and server also maintain, in parallel, 
a separate control connection over which the client sends commands regard-
ing session state changes (such as pause, resume, reposition, and so on). The 
Real-Time Streaming Protocol (RTSP) [RFC 2326], explained in some detail 
in the Web site for this textbook, is a popular open protocol for such a control 
connection.
Although UDP streaming has been employed in many open-source systems and 
proprietary products, it suffers from three significant drawbacks. First, due to the 
unpredictable and varying amount of available bandwidth between server and client, 
constant-rate UDP streaming can fail to provide continuous playout. For example, 
consider the scenario where the video consumption rate is 1 Mbps and the server-to-
client available bandwidth is usually more than 1 Mbps, but every few minutes the 
available bandwidth drops below 1 Mbps for several seconds. In such a scenario, a 
UDP streaming system that transmits video at a constant rate of 1 Mbps over RTP/
UDP would likely provide a poor user experience, with freezing or skipped frames 
soon after the available bandwidth falls below 1 Mbps. The second drawback of 
UDP streaming is that it requires a media control server, such as an RTSP server, to 
process client-to-server interactivity requests and to track client state (e.g., the cli-
ent’s playout point in the video, whether the video is being paused or played, and so 
on) for each ongoing client session. This increases the overall cost and complexity of 
deploying a large-scale video-on-demand system. The third drawback is that many 
firewalls are configured to block UDP traffic, preventing the users behind these fire-
walls from receiving UDP video.
712         Chapter 9    •    Multimedia Networking
9.2.2 HTTP Streaming
In HTTP streaming, the video is simply stored in an HTTP server as an ordinary 
file with a specific URL. When a user wants to see the video, the client establishes 
a TCP connection with the server and issues an HTTP GET request for that URL. 
The server then sends the video file, within an HTTP response message, as quickly 
as possible, that is, as quickly as TCP congestion control and flow control will allow. 
On the client side, the bytes are collected in a client application buffer. Once the 
number of bytes in this buffer exceeds a predetermined threshold, the client applica-
tion begins playback—specifically, it periodically grabs video frames from the client 
application buffer, decompresses the frames, and displays them on the user’s screen.
We learned in Chapter 3 that when transferring a file over TCP, the server-
to-client transmission rate can vary significantly due to TCP’s congestion control 
mechanism. In particular, it is not uncommon for the transmission rate to vary in a 
“saw-tooth” manner associated with TCP congestion control. Furthermore, packets 
can also be significantly delayed due to TCP’s retransmission mechanism. Because 
of these characteristics of TCP, the conventional wisdom in the 1990s was that 
video streaming would never work well over TCP. Over time, however, designers 
of streaming video systems learned that TCP’s congestion control and reliable-data 
transfer mechanisms do not necessarily preclude continuous playout when client 
buffering and prefetching (discussed in the next section) are used.
The use of HTTP over TCP also allows the video to traverse firewalls and NATs 
more easily (which are often configured to block most UDP traffic but to allow 
most HTTP traffic). Streaming over HTTP also obviates the need for a media con-
trol server, such as an RTSP server, reducing the cost of a large-scale deployment 
over the Internet. Due to all of these advantages, most video streaming applications 
today—including YouTube and Netflix—use HTTP streaming (over TCP) as its 
underlying streaming protocol.
Prefetching Video
As we just learned, client-side buffering can be used to mitigate the effects of vary-
ing end-to-end delays and varying available bandwidth. In our earlier example in 
Figure 9.1, the server transmits video at the rate at which the video is to be played 
out. However, for streaming stored video, the client can attempt to download the 
video at a rate higher than the consumption rate, thereby prefetching video frames 
that are to be consumed in the future. This prefetched video is naturally stored in 
the client application buffer. Such prefetching occurs naturally with TCP streaming, 
since TCP’s congestion avoidance mechanism will attempt to use all of the available 
bandwidth between server and client.
To gain some insight into prefetching, let’s take a look at a simple example. Sup-
pose the video consumption rate is 1 Mbps but the network is capable of delivering 
the video from server to client at a constant rate of 1.5 Mbps. Then the client will 
9.2    •    Streaming Stored Video         713
not only be able to play out the video with a very small playout delay, but will also 
be able to increase the amount of buffered video data by 500 Kbits every second. 
In this manner, if in the future the client receives data at a rate of less than 1 Mbps 
for a brief period of time, the client will be able to continue to provide continuous 
playback due to the reserve in its buffer. [Wang 2008] shows that when the average 
TCP throughput is roughly twice the media bit rate, streaming over TCP results in 
minimal starvation and low buffering delays.
Client Application Buffer and TCP Buffers
Figure 9.2 illustrates the interaction between client and server for HTTP streaming. 
At the server side, the portion of the video file in white has already been sent into the 
server’s socket, while the darkened portion is what remains to be sent. After “pass-
ing through the socket door,” the bytes are placed in the TCP send buffer before 
being transmitted into the Internet, as described in Chapter 3. In Figure 9.2, because 
the TCP send buffer at the server side is shown to be full, the server is momentarily 
prevented from sending more bytes from the video file into the socket. On the client 
side, the client application (media player) reads bytes from the TCP receive buffer 
(through its client socket) and places the bytes into the client application buffer. At 
the same time, the client application periodically grabs video frames from the client 
application buffer, decompresses the frames, and displays them on the user’s screen. 
Note that if the client application buffer is larger than the video file, then the whole 
process of moving bytes from the server’s storage to the client’s application buffer 
is equivalent to an ordinary file download over HTTP—the client simply pulls the 
video off the server as fast as TCP will allow!
Video ﬁle
Web server
Client
TCP send
buffer
TCP receive
buffer
TCP application
buffer
Frames read
out periodically
from buffer,
decompressed,
and displayed
on screen
Figure 9.2  ♦  Streaming stored video over HTTP/TCP
714         Chapter 9    •    Multimedia Networking
Consider now what happens when the user pauses the video during the stream-
ing process. During the pause period, bits are not removed from the client application 
buffer, even though bits continue to enter the buffer from the server. If the client 
application buffer is finite, it may eventually become full, which will cause “back 
pressure” all the way back to the server. Specifically, once the client application 
buffer becomes full, bytes can no longer be removed from the client TCP receive 
buffer, so it too becomes full. Once the client receive TCP buffer becomes full, bytes 
can no longer be removed from the server TCP send buffer, so it also becomes full. 
Once the TCP becomes full, the server cannot send any more bytes into the socket. 
Thus, if the user pauses the video, the server may be forced to stop transmitting, in 
which case the server will be blocked until the user resumes the video.
In fact, even during regular playback (that is, without pausing), if the client 
application buffer becomes full, back pressure will cause the TCP buffers to become 
full, which will force the server to reduce its rate. To determine the resulting rate, 
note that when the client application removes f bits, it creates room for f bits in the 
client application buffer, which in turn allows the server to send f additional bits. 
Thus, the server send rate can be no higher than the video consumption rate at the 
client. Therefore, a full client application buffer indirectly imposes a limit on the rate 
that video can be sent from server to client when streaming over HTTP.
Analysis of Video Streaming
Some simple modeling will provide more insight into initial playout delay and freez-
ing due to application buffer depletion. As shown in Figure 9.3, let B denote the size 
Fill rate = x
Depletion rate = r
Video
server
Internet
Q
B
Client application buffer
Figure 9.3  ♦  Analysis of client-side buffering for video streaming
9.2    •    Streaming Stored Video         715
(in bits) of the client’s application buffer, and let Q denote the number of bits that 
must be buffered before the client application begins playout. (Of course, Q 6 B.) 
Let r denote the video consumption rate—the rate at which the client draws bits out 
of the client application buffer during playback. So, for example, if the video’s frame 
rate is 30 frames/sec, and each (compressed) frame is 100,000 bits, then r = 3 Mbps. 
To see the forest through the trees, we’ll ignore TCP’s send and receive buffers.
Let’s assume that the server sends bits at a constant rate x whenever the client 
buffer is not full. (This is a gross simplification, since TCP’s send rate varies due 
to congestion control; we’ll examine more realistic time-dependent rates x (t) in the 
problems at the end of this chapter.) Suppose at time t = 0, the application buffer is 
empty and video begins arriving to the client application buffer. We now ask at what 
time t = tp does playout begin? And while we are at it, at what time t = tf does the 
client application buffer become full?
First, let’s determine tp, the time when Q bits have entered the application buffer 
and playout begins. Recall that bits arrive to the client application buffer at rate x and 
no bits are removed from this buffer before playout begins. Thus, the amount of time 
required to build up Q bits (the initial buffering delay) is tp = Q/x.
Now let’s determine tf, the point in time when the client application buffer 
becomes full. We first observe that if x 6 r (that is, if the server send rate is less than 
the video consumption rate), then the client buffer will never become full! Indeed, 
starting at time tp, the buffer will be depleted at rate r and will only be filled at rate 
x 6 r. Eventually the client buffer will empty out entirely, at which time the video 
will freeze on the screen while the client buffer waits another tp seconds to build up 
Q bits of video. Thus, when the available rate in the network is less than the video 
rate, playout will alternate between periods of continuous playout and periods of 
freezing. In a homework problem, you will be asked to determine the length of each 
continuous playout and freezing period as a function of Q, r, and x. Now let’s deter-
mine tf for when x 7 r. In this case, starting at time tp, the buffer increases from Q 
to B at rate x - r since bits are being depleted at rate r but are arriving at rate x, as 
shown in Figure 9.3. Given these hints, you will be asked in a homework problem 
to determine tf, the time the client buffer becomes full. Note that when the available 
rate in the network is more than the video rate, after the initial buffering delay, the 
user will enjoy continuous playout until the video ends.
Early Termination and Repositioning the Video
HTTP streaming systems often make use of the HTTP byte-range header in the 
HTTP GET request message, which specifies the specific range of bytes the client 
currently wants to retrieve from the desired video. This is particularly useful when the 
user wants to reposition (that is, jump) to a future point in time in the video. When the 
user repositions to a new position, the client sends a new HTTP request, indicating with 
the byte-range header from which byte in the file should the server send data. When 
the server receives the new HTTP request, it can forget about any earlier request and 
instead send bytes beginning with the byte indicated in the byte-range request.
716         Chapter 9    •    Multimedia Networking
While we are on the subject of repositioning, we briefly mention that when a 
user repositions to a future point in the video or terminates the video early, some 
prefetched-but-not-yet-viewed data transmitted by the server will go unwatched—
a waste of network bandwidth and server resources. For example, suppose that 
the client buffer is full with B bits at some time t0 into the video, and at this time 
the user repositions to some instant t 7 t0 + B/r into the video, and then watches 
 
the video to completion from that point on. In this case, all B bits in the buffer will be 
unwatched and the bandwidth and server resources that were used to transmit those 
B bits have been completely wasted. There is significant wasted bandwidth in the 
Internet due to early termination, which can be quite costly, particularly for wireless 
links [Ihm 2011]. For this reason, many streaming systems use only a moderate-size 
client application buffer, or will limit the amount of prefetched video using the byte-
range header in HTTP requests [Rao 2011].
Repositioning and early termination are analogous to cooking a large meal, eat-
ing only a portion of it, and throwing the rest away, thereby wasting food. So the next 
time your parents criticize you for wasting food by not eating all your dinner, you can 
quickly retort by saying they are wasting bandwidth and server resources when they 
reposition while watching movies over the Internet! But, of course, two wrongs do 
not make a right—both food and bandwidth are not to be wasted!
In Sections 9.2.1 and 9.2.2, we covered UDP streaming and HTTP streaming, 
respectively. A third type of streaming is Dynamic Adaptive Streaming over HTTP 
(DASH), which uses multiple versions of the video, each compressed at a different 
rate. DASH is discussed in detail in Section 2.6.2. CDNs are often used to distribute 
stored and live video. CDNs are discussed in detail in Section 2.6.3.
9.3	 Voice-over-IP
Real-time conversational voice over the Internet is often referred to as Internet 
telephony, since, from the user’s perspective, it is similar to the traditional circuit-
switched telephone service. It is also commonly called Voice-over-IP (VoIP). In 
this section we describe the principles and protocols underlying VoIP. Conversa-
tional video is similar in many respects to VoIP, except that it includes the video 
of the participants as well as their voices. To keep the discussion focused and 
concrete, we focus here only on voice in this section rather than combined voice 
and video.
9.3.1 Limitations of the Best-Effort IP Service
The Internet’s network-layer protocol, IP, provides best-effort service. That is to say 
the service makes its best effort to move each datagram from source to destination 
as quickly as possible but makes no promises whatsoever about getting the packet 
9.3    •    Voice-over-IP         717
to the destination within some delay bound or about a limit on the percentage of 
packets lost. The lack of such guarantees poses significant challenges to the design 
of real-time conversational applications, which are acutely sensitive to packet delay, 
jitter, and loss.
In this section, we’ll cover several ways in which the performance of VoIP over 
a best-effort network can be enhanced. Our focus will be on application-layer tech-
niques, that is, approaches that do not require any changes in the network core or 
even in the transport layer at the end hosts. To keep the discussion concrete, we’ll 
discuss the limitations of best-effort IP service in the context of a specific VoIP 
example. The sender generates bytes at a rate of 8,000 bytes per second; every 
 
20 msecs the sender gathers these bytes into a chunk. A chunk and a special header 
(discussed below) are encapsulated in a UDP segment, via a call to the socket interface. 
Thus, the number of bytes in a chunk is (20 msecs) # (8,000 bytes/sec) = 160 bytes, 
and a UDP segment is sent every 20 msecs.
If each packet makes it to the receiver with a constant end-to-end delay, then 
packets arrive at the receiver periodically every 20 msecs. In these ideal conditions, 
the receiver can simply play back each chunk as soon as it arrives. But unfortunately, 
some packets can be lost and most packets will not have the same end-to-end delay, 
even in a lightly congested Internet. For this reason, the receiver must take more care 
in determining (1) when to play back a chunk, and (2) what to do with a missing chunk.
Packet Loss
Consider one of the UDP segments generated by our VoIP application. The UDP 
segment is encapsulated in an IP datagram. As the datagram wanders through the 
network, it passes through router buffers (that is, queues) while waiting for transmis-
sion on outbound links. It is possible that one or more of the buffers in the path from 
sender to receiver is full, in which case the arriving IP datagram may be discarded, 
never to arrive at the receiving application.
Loss could be eliminated by sending the packets over TCP (which provides for 
reliable data transfer) rather than over UDP. However, retransmission mechanisms 
are often considered unacceptable for conversational real-time audio applications 
such as VoIP, because they increase end-to-end delay [Bolot 1996]. Furthermore, 
due to TCP congestion control, packet loss may result in a reduction of the TCP 
sender’s transmission rate to a rate that is lower than the receiver’s drain rate, possi-
bly leading to buffer starvation. This can have a severe impact on voice intelligibility 
at the receiver. For these reasons, most existing VoIP applications run over UDP by 
default. [Baset 2006] reports that UDP is used by Skype unless a user is behind a 
NAT or firewall that blocks UDP segments (in which case TCP is used).
But losing packets is not necessarily as disastrous as one might think. Indeed, 
packet loss rates between 1 and 20 percent can be tolerated, depending on how voice 
is encoded and transmitted, and on how the loss is concealed at the receiver. For 
example, forward error correction (FEC) can help conceal packet loss. We’ll see 
718         Chapter 9    •    Multimedia Networking
below that with FEC, redundant information is transmitted along with the original 
information so that some of the lost original data can be recovered from the redundant 
information. Nevertheless, if one or more of the links between sender and receiver is 
severely congested, and packet loss exceeds 10 to 20 percent (for example, on a wire-
less link), then there is really nothing that can be done to achieve acceptable audio 
quality. Clearly, best-effort service has its limitations.
End-to-End Delay
End-to-end delay is the accumulation of transmission, processing, and queuing 
delays in routers; propagation delays in links; and end-system processing delays. 
For real-time conversational applications, such as VoIP, end-to-end delays smaller 
than 150 msecs are not perceived by a human listener; delays between 150 and 400 
msecs can be acceptable but are not ideal; and delays exceeding 400 msecs can seri-
ously hinder the interactivity in voice conversations. The receiving side of a VoIP 
application will typically disregard any packets that are delayed more than a certain 
threshold, for example, more than 400 msecs. Thus, packets that are delayed by more 
than the threshold are effectively lost.
Packet Jitter
A crucial component of end-to-end delay is the varying queuing delays that a packet 
experiences in the network’s routers. Because of these varying delays, the time from 
when a packet is generated at the source until it is received at the receiver can fluc-
tuate from packet to packet, as shown in Figure 9.1. This phenomenon is called 
jitter. As an example, consider two consecutive packets in our VoIP application. 
The sender sends the second packet 20 msecs after sending the first packet. But at 
the receiver, the spacing between these packets can become greater than 20 msecs. 
To see this, suppose the first packet arrives at a nearly empty queue at a router, but 
just before the second packet arrives at the queue a large number of packets from 
other sources arrive at the same queue. Because the first packet experiences a small 
queuing delay and the second packet suffers a large queuing delay at this router, 
the first and second packets become spaced by more than 20 msecs. The spacing 
between consecutive packets can also become less than 20 msecs. To see this, again 
consider two consecutive packets. Suppose the first packet joins the end of a queue 
with a large number of packets, and the second packet arrives at the queue before 
this first packet is transmitted and before any packets from other sources arrive at 
the queue. In this case, our two packets find themselves one right after the other in 
the queue. If the time it takes to transmit a packet on the router’s outbound link is 
less than 20 msecs, then the spacing between first and second packets becomes less 
than 20 msecs.
The situation is analogous to driving cars on roads. Suppose you and your friend 
are each driving in your own cars from San Diego to Phoenix. Suppose you and your 
9.3    •    Voice-over-IP         719
friend have similar driving styles, and that you both drive at 100 km/hour, traffic 
permitting. If your friend starts out one hour before you, depending on intervening 
traffic, you may arrive at Phoenix more or less than one hour after your friend.
If the receiver ignores the presence of jitter and plays out chunks as soon as 
they arrive, then the resulting audio quality can easily become unintelligible at the 
receiver. Fortunately, jitter can often be removed by using sequence numbers, 
timestamps, and a playout delay, as discussed below.
9.3.2 Removing Jitter at the Receiver for Audio
For our VoIP application, where packets are being generated periodically, the 
receiver should attempt to provide periodic playout of voice chunks in the presence 
of random network jitter. This is typically done by combining the following two 
mechanisms:
•	 Prepending each chunk with a timestamp. The sender stamps each chunk with the 
time at which the chunk was generated.
•	 Delaying playout of chunks at the receiver. As we saw in our earlier discussion of 
Figure 9.1, the playout delay of the received audio chunks must be long enough 
so that most of the packets are received before their scheduled playout times. This 
playout delay can either be fixed throughout the duration of the audio session or 
vary adaptively during the audio session lifetime.
We now discuss how these three mechanisms, when combined, can alleviate or even 
eliminate the effects of jitter. We examine two playback strategies: fixed playout 
delay and adaptive playout delay.
Fixed Playout Delay
With the fixed-delay strategy, the receiver attempts to play out each chunk exactly q 
msecs after the chunk is generated. So if a chunk is timestamped at the sender at time 
t, the receiver plays out the chunk at time t + q, assuming the chunk has arrived by 
that time. Packets that arrive after their scheduled playout times are discarded and 
considered lost.
What is a good choice for q? VoIP can support delays up to about 400 msecs, 
although a more satisfying conversational experience is achieved with smaller val-
ues of q. On the other hand, if q is made much smaller than 400 msecs, then many 
packets may miss their scheduled playback times due to the network-induced packet 
jitter. Roughly speaking, if large variations in end-to-end delay are typical, it is pref-
erable to use a large q; on the other hand, if delay is small and variations in delay are 
also small, it is preferable to use a small q, perhaps less than 150 msecs.
The trade-off between the playback delay and packet loss is illustrated in 
 
Figure 9.4. The figure shows the times at which packets are generated and played 
720         Chapter 9    •    Multimedia Networking
out for a single talk spurt. Two distinct initial playout delays are considered. As 
shown by the leftmost staircase, the sender generates packets at regular intervals—
say, every 20 msecs. The first packet in this talk spurt is received at time r. As shown 
in the figure, the arrivals of subsequent packets are not evenly spaced due to the 
network jitter.
For the first playout schedule, the fixed initial playout delay is set to p - r. 
With this schedule, the fourth packet does not arrive by its scheduled playout time, 
and the receiver considers it lost. For the second playout schedule, the fixed initial 
playout delay is set to p′ - r. For this schedule, all packets arrive before their sched-
uled playout times, and there is therefore no loss.
Adaptive Playout Delay
The previous example demonstrates an important delay-loss trade-off that arises 
when designing a playout strategy with fixed playout delays. By making the initial 
playout delay large, most packets will make their deadlines and there will therefore 
be negligible loss; however, for conversational services such as VoIP, long delays 
can become bothersome if not intolerable. Ideally, we would like the playout delay to 
be minimized subject to the constraint that the loss be below a few percent.
The natural way to deal with this trade-off is to estimate the network delay and 
the variance of the network delay, and to adjust the playout delay accordingly at the 
beginning of each talk spurt. This adaptive adjustment of playout delays at the begin-
ning of the talk spurts will cause the sender’s silent periods to be compressed and 
elongated; however, compression and elongation of silence by a small amount is not 
noticeable in speech.
Packets
generated
Time
Packets
r p
p'
Playout
schedule
p–r
Playout
schedule
p'–r
Packets
received
Missed
playout
Figure 9.4  ♦  Packet loss for different fixed playout delays
9.3    •    Voice-over-IP         721
Following [Ramjee 1994], we now describe a generic algorithm that the receiver 
can use to adaptively adjust its playout delays. To this end, let
ti = 
the timestamp of the ith packet = the time the packet was generated by 
the sender
ri = the time packet i is received by receiver
pi = the time packet i is played at receiver
The end-to-end network delay of the ith packet is ri - ti. Due to network jitter, 
this delay will vary from packet to packet. Let di denote an estimate of the average 
network delay upon reception of the ith packet. This estimate is constructed from the 
timestamps as follows:
di = (1 - u) di-1 + u (ri - ti)
where u is a fixed constant (for example, u = 0.01). Thus di is a smoothed average 
of the observed network delays r1 - t1, . . . , ri - ti. The estimate places more weight 
on the recently observed network delays than on the observed network delays of the 
distant past. This form of estimate should not be completely unfamiliar; a similar 
idea is used to estimate round-trip times in TCP, as discussed in Chapter 3. Let vi 
denote an estimate of the average deviation of the delay from the estimated average 
delay. This estimate is also constructed from the timestamps:
vi = (1 - u) vi-1 + u ∙ri - ti - di∙
The estimates di and vi are calculated for every packet received, although they are 
used only to determine the playout point for the first packet in any talk spurt.
Once having calculated these estimates, the receiver employs the following 
algorithm for the playout of packets. If packet i is the first packet of a talk spurt, its 
playout time, pi, is computed as:
pi = ti + di + Kvi
where K is a positive constant (for example, K = 4). The purpose of the Kvi term 
is to set the playout time far enough into the future so that only a small frac-
tion of the arriving packets in the talk spurt will be lost due to late arrivals. The 
playout point for any subsequent packet in a talk spurt is computed as an offset 
from the point in time when the first packet in the talk spurt was played out. In 
particular, let
qi = pi - ti
722         Chapter 9    •    Multimedia Networking
be the length of time from when the first packet in the talk spurt is generated until it 
is played out. If packet j also belongs to this talk spurt, it is played out at time
pj = tj + qi
The algorithm just described makes perfect sense assuming that the receiver can 
tell whether a packet is the first packet in the talk spurt. This can be done by examin-
ing the signal energy in each received packet.
9.3.3 Recovering from Packet Loss
We have discussed in some detail how a VoIP application can deal with packet 
jitter. We now briefly describe several schemes that attempt to preserve accept-
able audio quality in the presence of packet loss. Such schemes are called loss 
recovery schemes. Here we define packet loss in a broad sense: A packet is lost 
either if it never arrives at the receiver or if it arrives after its scheduled playout 
time. Our VoIP example will again serve as a context for describing loss recov-
ery schemes.
As mentioned at the beginning of this section, retransmitting lost packets may 
not be feasible in a real-time conversational application such as VoIP. Indeed, 
retransmitting a packet that has missed its playout deadline serves absolutely no 
purpose. And retransmitting a packet that overflowed a router queue cannot normally 
be accomplished quickly enough. Because of these considerations, VoIP applica-
tions often use some type of loss anticipation scheme. Two types of loss anticipation 
schemes are forward error correction (FEC) and interleaving.
Forward Error Correction (FEC)
The basic idea of FEC is to add redundant information to the original packet 
stream. For the cost of marginally increasing the transmission rate, the redundant 
information can be used to reconstruct approximations or exact versions of some of 
the lost packets. Following [Bolot 1996] and [Perkins 1998], we now outline two 
simple FEC mechanisms. The first mechanism sends a redundant encoded chunk 
after every n chunks. The redundant chunk is obtained by exclusive OR-ing the n 
original chunks [Shacham 1990]. In this manner if any one packet of the group of 
n + 1 packets is lost, the receiver can fully reconstruct the lost packet. But if two 
or more packets in a group are lost, the receiver cannot reconstruct the lost packets. 
By keeping n + 1, the group size, small, a large fraction of the lost packets can 
be recovered when loss is not excessive. However, the smaller the group size, the 
greater the relative increase of the transmission rate. In particular, the transmis-
sion rate increases by a factor of 1/n, so that, if n = 3, then the transmission rate 
increases by 33 percent. Furthermore, this simple scheme increases the playout 
delay, as the receiver must wait to receive the entire group of packets before it can 
9.3    •    Voice-over-IP         723
begin playout. For more practical details about how FEC works for multimedia 
transport see [RFC 5109].
The second FEC mechanism is to send a lower-resolution audio stream as the 
redundant information. For example, the sender might create a nominal audio stream 
and a corresponding low-resolution, low-bit rate audio stream. (The nominal stream 
could be a PCM encoding at 64 kbps, and the lower-quality stream could be a GSM 
encoding at 13 kbps.) The low-bit rate stream is referred to as the redundant stream. 
As shown in Figure 9.5, the sender constructs the nth packet by taking the nth chunk 
from the nominal stream and appending to it the (n - 1)st chunk from the redundant 
stream. In this manner, whenever there is nonconsecutive packet loss, the receiver 
can conceal the loss by playing out the low-bit rate encoded chunk that arrives with 
the subsequent packet. Of course, low-bit rate chunks give lower quality than the 
nominal chunks. However, a stream of mostly high-quality chunks, occasional low-
quality chunks, and no missing chunks gives good overall audio quality. Note that in 
this scheme, the receiver only has to receive two packets before playback, so that the 
increased playout delay is small. Furthermore, if the low-bit rate encoding is much 
less than the nominal encoding, then the marginal increase in the transmission rate 
will be small.
In order to cope with consecutive loss, we can use a simple variation. Instead of 
appending just the (n - 1)st low-bit rate chunk to the nth nominal chunk, the sender 
can append the (n - 1)st and (n - 2)nd low-bit rate chunk, or append the (n - 1)st 
and (n - 3)rd low-bit rate chunk, and so on. By appending more low-bit rate chunks 
to each nominal chunk, the audio quality at the receiver becomes acceptable for a 
wider variety of harsh best-effort environments. On the other hand, the additional 
chunks increase the transmission bandwidth and the playout delay.
1
1
1
1
1
2
2
2
2
3
3
loss
3
4
3
4
1
2
3
4
4
Redundancy
Received
stream
Original
stream
Reconstructed
stream
Figure 9.5  ♦  Piggybacking lower-quality redundant information
724         Chapter 9    •    Multimedia Networking
Interleaving
As an alternative to redundant transmission, a VoIP application can send interleaved 
audio. As shown in Figure 9.6, the sender resequences units of audio data before 
transmission, so that originally adjacent units are separated by a certain distance in 
the transmitted stream. Interleaving can mitigate the effect of packet losses. If, for 
example, units are 5 msecs in length and chunks are 20 msecs (that is, four units per 
chunk), then the first chunk could contain units 1, 5, 9, and 13; the second chunk could 
contain units 2, 6, 10, and 14; and so on. Figure 9.6 shows that the loss of a single 
packet from an interleaved stream results in multiple small gaps in the reconstructed 
stream, as opposed to the single large gap that would occur in a noninterleaved stream.
Interleaving can significantly improve the perceived quality of an audio stream 
[Perkins 1998]. It also has low overhead. The obvious disadvantage of interleaving 
is that it increases latency. This limits its use for conversational applications such as 
VoIP, although it can perform well for streaming stored audio. A major advantage 
of interleaving is that it does not increase the bandwidth requirements of a stream.
Error Concealment
Error concealment schemes attempt to produce a replacement for a lost packet that 
is similar to the original. As discussed in [Perkins 1998], this is possible since audio 
Original
stream
Interleaved
stream
Received
stream
Reconstructed
stream
5
9
13
1
2
4
1
2
3
4
1
5
9
13
1
2
10 14
6
5
8
6
5
7
8
6
2
10 14
loss
6
7
11 15
3
10
12
9
10 11 12
9
4
12
16
8
13
16
14
13
15
16
14
4
12
16
8
Figure 9.6  ♦  Sending interleaved audio
9.3    •    Voice-over-IP         725
signals, and in particular speech, exhibit large amounts of short-term self-similarity. 
As such, these techniques work for relatively small loss rates (less than 15 percent), 
and for small packets (4–40 msecs). When the loss length approaches the length of 
a phoneme (5–100 msecs) these techniques break down, since whole phonemes may 
be missed by the listener.
Perhaps the simplest form of receiver-based recovery is packet repetition. Packet 
repetition replaces lost packets with copies of the packets that arrived immediately 
before the loss. It has low computational complexity and performs reasonably well. 
Another form of receiver-based recovery is interpolation, which uses audio before 
and after the loss to interpolate a suitable packet to cover the loss. Interpolation per-
forms somewhat better than packet repetition but is significantly more computation-
ally intensive [Perkins 1998].
9.3.4 Case Study: VoIP with Skype
Skype is an immensely popular VoIP application with over 50 million accounts 
active on a daily basis. In addition to providing host-to-host VoIP service, Skype 
offers host-to-phone services, phone-to-host services, and multi-party host-to-host 
video conferencing services. (Here, a host is again any Internet connected IP device, 
including PCs, tablets, and smartphones.) Skype was acquired by Microsoft in 2011.
Because the Skype protocol is proprietary, and because all Skype’s control and 
media packets are encrypted, it is difficult to precisely determine how Skype operates. 
Nevertheless, from the Skype Web site and several measurement studies, researchers 
have learned how Skype generally works [Baset 2006; Guha 2006; Chen 2006; Suh 
2006; Ren 2006; Zhang X 2012]. For both voice and video, the Skype clients have 
at their disposal many different codecs, which are capable of encoding the media at 
a wide range of rates and qualities. For example, video rates for Skype have been 
measured to be as low as 30 kbps for a low-quality session up to almost 1 Mbps for a 
high quality session [Zhang X 2012]. Typically, Skype’s audio quality is better than 
the “POTS” (Plain Old Telephone Service) quality provided by the wire-line phone 
system. (Skype codecs typically sample voice at 16,000 samples/sec or higher, which 
provides richer tones than POTS, which samples at 8,000/sec.) By default, Skype 
sends audio and video packets over UDP. However, control packets are sent over 
TCP, and media packets are also sent over TCP when firewalls block UDP streams. 
Skype uses FEC for loss recovery for both voice and video streams sent over UDP. 
The Skype client also adapts the audio and video streams it sends to current network 
conditions, by changing video quality and FEC overhead [Zhang X 2012].
Skype uses P2P techniques in a number of innovative ways, nicely illustrating 
how P2P can be used in applications that go beyond content distribution and file 
sharing. As with instant messaging, host-to-host Internet telephony is inherently P2P 
since, at the heart of the application, pairs of users (that is, peers) communicate with 
each other in real time. But Skype also employs P2P techniques for two other impor-
tant functions, namely, for user location and for NAT traversal.
726         Chapter 9    •    Multimedia Networking
As shown in Figure 9.7, the peers (hosts) in Skype are organized into a hierar-
chical overlay network, with each peer classified as a super peer or an ordinary peer. 
Skype maintains an index that maps Skype usernames to current IP addresses (and 
port numbers). This index is distributed over the super peers. When Alice wants to 
call Bob, her Skype client searches the distributed index to determine Bob’s current 
IP address. Because the Skype protocol is proprietary, it is currently not known how 
the index mappings are organized across the super peers, although some form of 
DHT organization is very possible.
P2P techniques are also used in Skype relays, which are useful for establishing 
calls between hosts in home networks. Many home network configurations provide 
access to the Internet through NATs, as discussed in Chapter 4. Recall that a NAT 
prevents a host from outside the home network from initiating a connection to a 
host within the home network. If both Skype callers have NATs, then there is a 
problem—neither can accept a call initiated by the other, making a call seemingly 
impossible. The clever use of super peers and relays nicely solves this problem. 
Suppose that when Alice signs in, she is assigned to a non-NATed super peer and 
initiates a session to that super peer. (Since Alice is initiating the session, her NAT 
permits this session.) This session allows Alice and her super peer to exchange 
Callee
peer
Caller
peer
Relay
peer
Super
peer
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Figure 9.7  ♦  Skype peers
9.3    •    Voice-over-IP         727
control messages. The same happens for Bob when he signs in. Now, when Alice 
wants to call Bob, she informs her super peer, who in turn informs Bob’s super 
peer, who in turn informs Bob of Alice’s incoming call. If Bob accepts the call, the 
two super peers select a third non-NATed super peer—the relay peer—whose job 
will be to relay data between Alice and Bob. Alice’s and Bob’s super peers then 
instruct Alice and Bob respectively to initiate a session with the relay. As shown in 
 
Figure 9.7, Alice then sends voice packets to the relay over the Alice-to-relay con-
nection (which was initiated by Alice), and the relay then forwards these packets 
over the relay-to-Bob connection (which was initiated by Bob); packets from Bob 
to Alice flow over these same two relay connections in reverse. And voila!—Bob 
and Alice have an end-to-end connection even though neither can accept a session 
originating from outside.
Up to now, our discussion on Skype has focused on calls involving two persons. 
Now let’s examine multi-party audio conference calls. With N 7 2 participants, if 
each user were to send a copy of its audio stream to each of the N - 1 other users, 
then a total of N(N - 1) audio streams would need to be sent into the network to 
support the audio conference. To reduce this bandwidth usage, Skype employs a 
clever distribution technique. Specifically, each user sends its audio stream to the 
conference initiator. The conference initiator combines the audio streams into one 
stream (basically by adding all the audio signals together) and then sends a copy 
of each combined stream to each of the other N - 1 participants. In this manner, 
the number of streams is reduced to 2(N - 1). For ordinary two-person video con-
versations, Skype routes the call peer-to-peer, unless NAT traversal is required, 
in which case the call is relayed through a non-NATed peer, as described earlier. 
For a video conference call involving N 7 2 participants, due to the nature of the 
video medium, Skype does not combine the call into one stream at one location and 
then redistribute the stream to all the participants, as it does for voice calls. Instead, 
each participant’s video stream is routed to a server cluster (located in Estonia as of 
2011), which in turn relays to each participant the N - 1 streams of the N - 1 other 
participants [Zhang X 2012]. You may be wondering why each participant sends a 
copy to a server rather than directly sending a copy of its video stream to each of 
the other N - 1 participants? Indeed, for both approaches, N(N - 1) video streams 
are being collectively received by the N participants in the conference. The reason 
is, because upstream link bandwidths are significantly lower than downstream link 
bandwidths in most access links, the upstream links may not be able to support the 
N - 1 streams with the P2P approach.
VoIP systems such as Skype, WeChat, and Google Talk introduce new privacy 
concerns. Specifically, when Alice and Bob communicate over VoIP, Alice can sniff 
Bob’s IP address and then use geo-location services [MaxMind 2016; Quova 2016] 
to determine Bob’s current location and ISP (for example, his work or home ISP). In 
fact, with Skype it is possible for Alice to block the transmission of certain packets 
during call establishment so that she obtains Bob’s current IP address, say every 
hour, without Bob knowing that he is being tracked and without being on Bob’s 
728         Chapter 9    •    Multimedia Networking
contact list. Furthermore, the IP address discovered from Skype can be correlated 
with IP addresses found in BitTorrent, so that Alice can determine the files that Bob 
is downloading [LeBlond 2011]. Moreover, it is possible to partially decrypt a Skype 
call by doing a traffic analysis of the packet sizes in a stream [White 2011].
9.4	 Protocols for Real-Time Conversational 
Applications
Real-time conversational applications, including VoIP and video conferencing, are 
compelling and very popular. It is therefore not surprising that standards bodies, such 
as the IETF and ITU, have been busy for many years (and continue to be busy!) at 
hammering out standards for this class of applications. With the appropriate stand-
ards in place for real-time conversational applications, independent companies are 
creating new products that interoperate with each other. In this section we examine 
RTP and SIP for real-time conversational applications. Both standards are enjoying 
widespread implementation in industry products.
9.4.1 RTP
In the previous section, we learned that the sender side of a VoIP application appends 
header fields to the audio chunks before passing them to the transport layer. These 
header fields include sequence numbers and timestamps. Since most multimedia net-
working applications can make use of sequence numbers and timestamps, it is con-
venient to have a standardized packet structure that includes fields for audio/video 
data, sequence number, and timestamp, as well as other potentially useful fields. 
RTP, defined in RFC 3550, is such a standard. RTP can be used for transporting 
common formats such as PCM, ACC, and MP3 for sound and MPEG and H.263 
for video. It can also be used for transporting proprietary sound and video formats. 
Today, RTP enjoys widespread implementation in many products and research pro-
totypes. It is also complementary to other important real-time interactive protocols, 
such as SIP.
In this section, we provide an introduction to RTP. We also encourage you to 
visit Henning Schulzrinne’s RTP site [Schulzrinne-RTP 2012], which provides a 
wealth of information on the subject. Also, you may want to visit the RAT site [RAT 
2012], which documents VoIP application that uses RTP.
RTP Basics
RTP typically runs on top of UDP. The sending side encapsulates a media chunk 
within an RTP packet, then encapsulates the packet in a UDP segment, and then 
9.4    •    Protocols for Real-Time Conversational Applications         729
hands the segment to IP. The receiving side extracts the RTP packet from the UDP 
segment, then extracts the media chunk from the RTP packet, and then passes the 
chunk to the media player for decoding and rendering.
As an example, consider the use of RTP to transport voice. Suppose the voice 
source is PCM-encoded (that is, sampled, quantized, and digitized) at 64 kbps. Fur-
ther suppose that the application collects the encoded data in 20-msec chunks, that 
is, 160 bytes in a chunk. The sending side precedes each chunk of the audio data 
with an RTP header that includes the type of audio encoding, a sequence number, 
and a timestamp. The RTP header is normally 12 bytes. The audio chunk along with 
the RTP header form the RTP packet. The RTP packet is then sent into the UDP 
socket interface. At the receiver side, the application receives the RTP packet from 
its socket interface. The application extracts the audio chunk from the RTP packet 
and uses the header fields of the RTP packet to properly decode and play back the 
audio chunk.
If an application incorporates RTP—instead of a proprietary scheme to provide 
payload type, sequence numbers, or timestamps—then the application will more eas-
ily interoperate with other networked multimedia applications. For example, if two 
different companies develop VoIP software and they both incorporate RTP into their 
product, there may be some hope that a user using one of the VoIP products will 
be able to communicate with a user using the other VoIP product. In Section 9.4.2, 
we’ll see that RTP is often used in conjunction with SIP, an important standard for 
Internet telephony.
It should be emphasized that RTP does not provide any mechanism to ensure 
timely delivery of data or provide other quality-of-service (QoS) guarantees; it 
does not even guarantee delivery of packets or prevent out-of-order delivery of 
packets. Indeed, RTP encapsulation is seen only at the end systems. Routers do 
not distinguish between IP datagrams that carry RTP packets and IP datagrams 
that don’t.
RTP allows each source (for example, a camera or a microphone) to be assigned 
its own independent RTP stream of packets. For example, for a video conference 
between two participants, four RTP streams could be opened—two streams for 
transmitting the audio (one in each direction) and two streams for transmitting the 
video (again, one in each direction). However, many popular encoding techniques—
including MPEG 1 and MPEG 2—bundle the audio and video into a single stream 
during the encoding process. When the audio and video are bundled by the encoder, 
then only one RTP stream is generated in each direction.
RTP packets are not limited to unicast applications. They can also be sent over 
one-to-many and many-to-many multicast trees. For a many-to-many multicast ses-
sion, all of the session’s senders and sources typically use the same multicast group 
for sending their RTP streams. RTP multicast streams belonging together, such as 
audio and video streams emanating from multiple senders in a video conference 
application, belong to an RTP session.
730         Chapter 9    •    Multimedia Networking
RTP Packet Header Fields
As shown in Figure 9.8, the four main RTP packet header fields are the payload type, 
sequence number, timestamp, and source identifier fields.
The payload type field in the RTP packet is 7 bits long. For an audio stream, the 
payload type field is used to indicate the type of audio encoding (for example, PCM, 
adaptive delta modulation, linear predictive encoding) that is being used. If a sender 
decides to change the encoding in the middle of a session, the sender can inform the 
receiver of the change through this payload type field. The sender may want to change 
the encoding in order to increase the audio quality or to decrease the RTP stream bit 
rate. Table 9.2 lists some of the audio payload types currently supported by RTP.
For a video stream, the payload type is used to indicate the type of video encoding 
(for example, motion JPEG, MPEG 1, MPEG 2, H.261). Again, the sender can change 
video encoding on the fly during a session. Table 9.3 lists some of the video payload 
types currently supported by RTP. The other important fields are the following:
•	 Sequence number field. The sequence number field is 16 bits long. The sequence 
number increments by one for each RTP packet sent, and may be used by the 
receiver to detect packet loss and to restore packet sequence. For example, if 
the receiver side of the application receives a stream of RTP packets with a gap 
between sequence numbers 86 and 89, then the receiver knows that packets 87 
and 88 are missing. The receiver can then attempt to conceal the lost data.
•	 Timestamp field. The timestamp field is 32 bits long. It reflects the sampling 
instant of the first byte in the RTP data packet. As we saw in the preceding 
 
section, the receiver can use timestamps to remove packet jitter introduced in 
the network and to provide synchronous playout at the receiver. The timestamp 
is derived from a sampling clock at the sender. As an example, for audio the 
 
timestamp clock increments by one for each sampling period (for example, each 
125 μsec for an 8 kHz sampling clock); if the audio application generates chunks 
consisting of 160 encoded samples, then the timestamp increases by 160 for each 
RTP packet when the source is active. The timestamp clock continues to increase 
at a constant rate even if the source is inactive.
•	 Synchronization source identifier (SSRC). The SSRC field is 32 bits long. It iden-
tifies the source of the RTP stream. Typically, each stream in an RTP session 
has a distinct SSRC. The SSRC is not the IP address of the sender, but instead is 
a number that the source assigns randomly when the new stream is started. The 
probability that two streams get assigned the same SSRC is very small. Should 
this happen, the two sources pick a new SSRC value.
Payload
type
Sequence
number
Synchronization
source identiﬁer
Miscellaneous
ﬁelds
Timestamp
Figure 9.8  ♦  RTP header fields
9.4    •    Protocols for Real-Time Conversational Applications         731
9.4.2 SIP
The Session Initiation Protocol (SIP), defined in [RFC 3261; RFC 5411], is an open 
and lightweight protocol that does the following:
•	 It provides mechanisms for establishing calls between a caller and a callee over 
an IP network. It allows the caller to notify the callee that it wants to start a call. 
It allows the participants to agree on media encodings. It also allows participants 
to end calls.
•	 It provides mechanisms for the caller to determine the current IP address of the 
callee. Users do not have a single, fixed IP address because they may be assigned 
addresses dynamically (using DHCP) and because they may have multiple IP 
devices, each with a different IP address.
•	 It provides mechanisms for call management, such as adding new media streams 
during the call, changing the encoding during the call, inviting new participants 
during the call, call transfer, and call holding.
Table 9.2  ♦  Audio payload types supported by RTP
Payload-Type Number
Audio Format
Sampling Rate
Rate
0
PCM μ-law
8 kHz
64 kbps
1
1016
8 kHz
4.8 kbps
3
GSM
8 kHz
13 kbps
7
LPC
8 kHz
2.4 kbps
9
G.722
16 kHz
48–64 kbps
14
MPEG Audio
90 kHz
—
15
G.728
8 kHz
16 kbps
Table 9.3  ♦  Some video payload types supported by RTP
Payload-Type Number
Video Format
26
Motion JPEG
31
H.261
32
MPEG 1 video
33
MPEG 2 video
732         Chapter 9    •    Multimedia Networking
Setting Up a Call to a Known IP Address
To understand the essence of SIP, it is best to take a look at a concrete example. In 
this example, Alice is at her PC and she wants to call Bob, who is also working at 
his PC. Alice’s and Bob’s PCs are both equipped with SIP-based software for mak-
ing and receiving phone calls. In this initial example, we’ll assume that Alice knows 
the IP address of Bob’s PC. Figure 9.9 illustrates the SIP call-establishment process.
In Figure 9.9, we see that an SIP session begins when Alice sends Bob an INVITE 
message, which resembles an HTTP request message. This INVITE message is sent 
over UDP to the well-known port 5060 for SIP. (SIP messages can also be sent over 
TCP.) The INVITE message includes an identifier for Bob (bob@193.64.210.89), 
an indication of Alice’s current IP address, an indication that Alice desires to 
receive audio, which is to be encoded in format AVP 0 (PCM encoded μ-law) and 
Time
Time
167.180.112.24
INVITE bob@193.64.210.89
c=IN IP4 167.180.112.24
m=audio 38060 RTP/AVP 0
200 OK
c=In IP4 193.64.210.89
m=audio 48753 RTP/AVP 3
Bob’s
terminal rings
193.64.210.89
m Law audio
port 5060
port 5060
port 38060
Alice
Bob
port 5060
port 48753
ACK
GSM
Figure 9.9  ♦  SIP call establishment when Alice knows Bob’s IP address
9.4    •    Protocols for Real-Time Conversational Applications         733
encapsulated in RTP, and an indication that she wants to receive the RTP packets 
on port 38060. After receiving Alice’s INVITE message, Bob sends an SIP response 
message, which resembles an HTTP response message. This response SIP message 
is also sent to the SIP port 5060. Bob’s response includes a 200 OK as well as an 
indication of his IP address, his desired encoding and packetization for reception, and 
his port number to which the audio packets should be sent. Note that in this example 
Alice and Bob are going to use different audio-encoding mechanisms: Alice is asked 
to encode her audio with GSM whereas Bob is asked to encode his audio with PCM 
μ-law. After receiving Bob’s response, Alice sends Bob an SIP acknowledgment 
message. After this SIP transaction, Bob and Alice can talk. (For visual convenience, 
Figure 9.9 shows Alice talking after Bob, but in truth they would normally talk at the 
same time.) Bob will encode and packetize the audio as requested and send the audio 
packets to port number 38060 at IP address 167.180.112.24. Alice will also encode 
and packetize the audio as requested and send the audio packets to port number 
48753 at IP address 193.64.210.89.
From this simple example, we have learned a number of key characteristics of 
SIP. First, SIP is an out-of-band protocol: The SIP messages are sent and received in 
sockets that are different from those used for sending and receiving the media data. 
Second, the SIP messages themselves are ASCII-readable and resemble HTTP mes-
sages. Third, SIP requires all messages to be acknowledged, so it can run over UDP 
or TCP.
In this example, let’s consider what would happen if Bob does not have a PCM 
μ-law codec for encoding audio. In this case, instead of responding with 200 OK, 
Bob would likely respond with a 606 Not Acceptable and list in the message all the 
codecs he can use. Alice would then choose one of the listed codecs and send another 
INVITE message, this time advertising the chosen codec. Bob could also simply 
reject the call by sending one of many possible rejection reply codes. (There are 
many such codes, including “busy,” “gone,” “payment required,” and “forbidden.”)
SIP Addresses
In the previous example, Bob’s SIP address is sip:bob@193.64.210.89. However, we 
expect many—if not most—SIP addresses to resemble e-mail addresses. For exam-
ple, Bob’s address might be sip:bob@domain.com. When Alice’s SIP device sends 
an INVITE message, the message would include this e-mail-like address; the SIP 
infrastructure would then route the message to the IP device that Bob is currently 
using (as we’ll discuss below). Other possible forms for the SIP address could be 
Bob’s legacy phone number or simply Bob’s first/middle/last name (assuming it is 
unique).
An interesting feature of SIP addresses is that they can be included in Web 
pages, just as people’s e-mail addresses are included in Web pages with the mailto 
URL. For example, suppose Bob has a personal homepage, and he wants to provide 
a means for visitors to the homepage to call him. He could then simply include the 
734         Chapter 9    •    Multimedia Networking
URL sip:bob@domain.com. When the visitor clicks on the URL, the SIP application 
in the visitor’s device is launched and an INVITE message is sent to Bob.
SIP Messages
In this short introduction to SIP, we’ll not cover all SIP message types and headers. 
Instead, we’ll take a brief look at the SIP INVITE message, along with a few com-
mon header lines. Let us again suppose that Alice wants to initiate a VoIP call to 
Bob, and this time Alice knows only Bob’s SIP address, bob@domain.com, and does 
not know the IP address of the device that Bob is currently using. Then her message 
might look something like this:
INVITE sip:bob@domain.com SIP/2.0
Via: SIP/2.0/UDP 167.180.112.24
From: sip:alice@hereway.com
To: sip:bob@domain.com
Call-ID: a2e3a@pigeon.hereway.com
Content-Type: application/sdp
Content-Length: 885
c=IN IP4 167.180.112.24
m=audio 38060 RTP/AVP 0
The INVITE line includes the SIP version, as does an HTTP request message. 
Whenever an SIP message passes through an SIP device (including the device that origi-
nates the message), it attaches a Via header, which indicates the IP address of the device. 
(We’ll see soon that the typical INVITE message passes through many SIP devices 
before reaching the callee’s SIP application.) Similar to an e-mail message, the SIP mes-
sage includes a From header line and a To header line. The message includes a Call-ID, 
which uniquely identifies the call (similar to the message-ID in e-mail). It includes a 
Content-Type header line, which defines the format used to describe the content con-
tained in the SIP message. It also includes a Content-Length header line, which provides 
the length in bytes of the content in the message. Finally, after a carriage return and line 
feed, the message contains the content. In this case, the content provides information 
about Alice’s IP address and how Alice wants to receive the audio.
Name Translation and User Location
In the example in Figure 9.9, we assumed that Alice’s SIP device knew the IP address 
where Bob could be contacted. But this assumption is quite unrealistic, not only 
because IP addresses are often dynamically assigned with DHCP, but also because 
Bob may have multiple IP devices (for example, different devices for his home, 
work, and car). So now let us suppose that Alice knows only Bob’s e-mail address, 
9.4    •    Protocols for Real-Time Conversational Applications         735
bob@domain.com, and that this same address is used for SIP-based calls. In this 
case, Alice needs to obtain the IP address of the device that the user bob@domain.
com is currently using. To find this out, Alice creates an INVITE message that begins 
with INVITE bob@domain.com SIP/2.0 and sends this message to an SIP proxy. 
The proxy will respond with an SIP reply that might include the IP address of the 
 
device that bob@domain.com is currently using. Alternatively, the reply might 
include the IP address of Bob’s voicemail box, or it might include a URL of a Web 
page (that says “Bob is sleeping. Leave me alone!”). Also, the result returned by the 
proxy might depend on the caller: If the call is from Bob’s wife, he might accept 
the call and supply his IP address; if the call is from Bob’s mother-in-law, he might 
respond with the URL that points to the I-am-sleeping Web page!
Now, you are probably wondering, how can the proxy server determine the cur-
rent IP address for bob@domain.com? To answer this question, we need to say a few 
words about another SIP device, the SIP registrar. Every SIP user has an associated 
registrar. Whenever a user launches an SIP application on a device, the application 
sends an SIP register message to the registrar, informing the registrar of its current 
IP address. For example, when Bob launches his SIP application on his PDA, the 
application would send a message along the lines of:
REGISTER sip:domain.com SIP/2.0
Via: SIP/2.0/UDP 193.64.210.89
From: sip:bob@domain.com
To: sip:bob@domain.com
Expires: 3600
Bob’s registrar keeps track of Bob’s current IP address. Whenever Bob switches 
to a new SIP device, the new device sends a new register message, indicating the 
new IP address. Also, if Bob remains at the same device for an extended period of 
time, the device will send refresh register messages, indicating that the most recently 
sent IP address is still valid. (In the example above, refresh messages need to be sent 
every 3600 seconds to maintain the address at the registrar server.) It is worth noting 
that the registrar is analogous to a DNS authoritative name server: The DNS server 
translates fixed host names to fixed IP addresses; the SIP registrar translates fixed 
human identifiers (for example, bob@domain.com) to dynamic IP addresses. Often 
SIP registrars and SIP proxies are run on the same host.
Now let’s examine how Alice’s SIP proxy server obtains Bob’s current IP 
address. From the preceding discussion we see that the proxy server simply needs 
to forward Alice’s INVITE message to Bob’s registrar/proxy. The registrar/proxy 
could then forward the message to Bob’s current SIP device. Finally, Bob, having 
now received Alice’s INVITE message, could send an SIP response to Alice.
As an example, consider Figure 9.10, in which jim@umass.edu, currently 
working on 217.123.56.89, wants to initiate a Voice-over-IP (VoIP) session with 
 
keith@upenn.edu, currently working on 197.87.54.21. The following steps are taken: 
736         Chapter 9    •    Multimedia Networking
(1) Jim sends an INVITE message to the umass SIP proxy. (2) The proxy does a DNS 
lookup on the SIP registrar upenn.edu (not shown in diagram) and then forwards the 
message to the registrar server. (3) Because keith@upenn.edu is no longer registered 
at the upenn registrar, the upenn registrar sends a redirect response, indicating that 
it should try keith@nyu.edu. (4) The umass proxy sends an INVITE message to the 
NYU SIP registrar. (5) The NYU registrar knows the IP address of keith@upenn.
edu and forwards the INVITE message to the host 197.87.54.21, which is running 
Keith’s SIP client. (6–8) An SIP response is sent back through registrars/proxies to 
the SIP client on 217.123.56.89. (9) Media is sent directly between the two clients. 
(There is also an SIP acknowledgment message, which is not shown.)
Our discussion of SIP has focused on call initiation for voice calls. SIP, being 
a signaling protocol for initiating and ending calls in general, can be used for video 
conference calls as well as for text-based sessions. In fact, SIP has become a fun-
damental component in many instant messaging applications. Readers desiring to 
learn more about SIP are encouraged to visit Henning Schulzrinne’s SIP Web site 
[Schulzrinne-SIP 2016]. In particular, on this site you will find open source software 
for SIP clients and servers [SIP Software 2016].
9
5
6
4
7
2
3
1
8
SIP registrar
upenn.edu
SIP proxy
umass.edu
SIP client
217.123.56.89
SIP client
197.87.54.21
SIP registrar
nyu.edu
Figure 9.10  ♦  Session initiation, involving SIP proxies and registrars
9.5    •    Network Support for Multimedia         737
9.5	 Network Support for Multimedia
In Sections 9.2 through 9.4, we learned how application-level mechanisms such as 
client buffering, prefetching, adapting media quality to available bandwidth, adap-
tive playout, and loss mitigation techniques can be used by multimedia applications 
to improve a multimedia application’s performance. We also learned how content 
distribution networks and P2P overlay networks can be used to provide a system-
level approach for delivering multimedia content. These techniques and approaches 
are all designed to be used in today’s best-effort Internet. Indeed, they are in use 
today precisely because the Internet provides only a single, best-effort class of ser-
vice. But as designers of computer networks, we can’t help but ask whether the 
network (rather than the applications or application-level infrastructure alone) might 
provide mechanisms to support multimedia content delivery. As we’ll see shortly, 
the answer is, of course, “yes”! But we’ll also see that a number of these new 
 
network-level mechanisms have yet to be widely deployed. This may be due to their 
complexity and to the fact that application-level techniques together with best-effort 
service and properly dimensioned network resources (for example, bandwidth) can 
indeed provide a “good-enough” (even if not-always-perfect) end-to-end multimedia 
delivery service.
Table 9.4 summarizes three broad approaches towards providing network-level 
support for multimedia applications.
•	 Making the best of best-effort service. The application-level mechanisms and 
infrastructure that we studied in Sections 9.2 through 9.4 can be successfully used 
in a well-dimensioned network where packet loss and excessive end-to-end delay 
rarely occur. When demand increases are forecasted, the ISPs deploy additional 
bandwidth and switching capacity to continue to ensure satisfactory delay and 
packet-loss performance [Huang 2005]. We’ll discuss such network dimension-
ing further in Section 9.5.1.
•	 Differentiated service. Since the early days of the Internet, it’s been envisioned 
that different types of traffic (for example, as indicated in the Type-of-Service 
field in the IP4v packet header) could be provided with different classes of ser-
vice, rather than a single one-size-fits-all best-effort service. With differentiated 
service, one type of traffic might be given strict priority over another class of traf-
fic when both types of traffic are queued at a router. For example, packets belong-
ing to a real-time conversational application might be given priority over other 
packets due to their stringent delay constraints. Introducing differentiated service 
into the network will require new mechanisms for packet marking (indicating a 
packet’s class of service), packet scheduling, and more. We’ll cover differenti-
ated service, and new network mechanisms needed to implement this service, in 
Sections 9.5.2 and 9.5.3.
738         Chapter 9    •    Multimedia Networking
•	 Per-connection Quality-of-Service (QoS) Guarantees. With per-connection 
QoS guarantees, each instance of an application explicitly reserves end-to-end 
 
bandwidth and thus has a guaranteed end-to-end performance. A hard guarantee 
means the application will receive its requested quality of service (QoS) with cer-
tainty. A soft guarantee means the application will receive its requested quality 
of service with high probability. For example, if a user wants to make a VoIP call 
from Host A to Host B, the user’s VoIP application reserves bandwidth explicitly 
in each link along a route between the two hosts. But permitting applications to 
make reservations and requiring the network to honor the reservations requires 
some big changes. First, we need a protocol that, on behalf of the applications, 
reserves link bandwidth on the paths from the senders to their receivers. Second, 
we’ll need new scheduling policies in the router queues so that per-connection 
bandwidth reservations can be honored. Finally, in order to make a reservation, 
the applications must give the network a description of the traffic that they intend 
to send into the network and the network will need to police each application’s 
traffic to make sure that it abides by that description. These mechanisms, when 
combined, require new and complex software in hosts and routers. Because 
 
per-connection QoS guaranteed service has not seen significant deployment, 
we’ll cover these mechanisms only briefly in Section 9.5.4.
Table 9.4  ♦  
Three network-level approaches to supporting multimedia  
applications
Approach
Granularity
Guarantee
Mechanisms
Complexity
Deployment to date
Making the 
best of best-
effort service
all traffic 
treated 
equally
none, or  
soft
application-layer 
support, CDNs, 
overlays, network-
level resource 
provisioning
minimal
everywhere
Differentiated 
service
different 
classes 
of traffic 
treated 
differently
none,  
or soft
packet marking, 
policing, 
scheduling
medium
some
Per-connection 
Quality-of-
Service (QoS) 
Guarantees
each 
source-
destination 
flows 
treated 
differently
soft or hard,  
once flow  
is admitted
packet marking, 
policing, 
scheduling; call 
admission and 
signaling
light
little
9.5    •    Network Support for Multimedia         739
9.5.1 Dimensioning Best-Effort Networks
Fundamentally, the difficulty in supporting multimedia applications arises from 
their stringent performance requirements—low end-to-end packet delay, delay 
jitter, and loss—and the fact that packet delay, delay jitter, and loss occur when-
ever the network becomes congested. A first approach to improving the quality of 
multimedia applications—an approach that can often be used to solve just about 
any problem where resources are constrained—is simply to “throw money at the 
problem” and thus simply avoid resource contention. In the case of networked 
multimedia, this means providing enough link capacity throughout the network 
so that network congestion, and its consequent packet delay and loss, never (or 
only very rarely) occurs. With enough link capacity, packets could zip through 
today’s Internet without queuing delay or loss. From many perspectives this is an 
ideal situation—multimedia applications would perform perfectly, users would 
be happy, and this could all be achieved with no changes to Internet’s best-effort 
architecture.
The question, of course, is how much capacity is “enough” to achieve this nirvana, 
and whether the costs of providing “enough” bandwidth are practical from a business 
standpoint to the ISPs. The question of how much capacity to provide at network 
links in a given topology to achieve a given level of performance is often known as 
bandwidth provisioning. The even more complicated problem of how to design a 
network topology (where to place routers, how to interconnect routers with links, and 
what capacity to assign to links) to achieve a given level of end-to-end performance 
is a network design problem often referred to as network dimensioning. Both band-
width provisioning and network dimensioning are complex topics, well beyond the 
scope of this textbook. We note here, however, that the following issues must be 
addressed in order to predict application-level performance between two network 
end points, and thus provision enough capacity to meet an application’s performance 
requirements.
•	 Models of traffic demand between network end points. Models may need to be 
specified at both the call level (for example, users “arriving” to the network and 
starting up end-to-end applications) and at the packet level (for example, packets 
being generated by ongoing applications). Note that workload may change over 
time.
•	 Well-defined performance requirements. For example, a performance require-
ment for supporting delay-sensitive traffic, such as a conversational multimedia 
application, might be that the probability that the end-to-end delay of the packet 
is greater than a maximum tolerable delay be less than some small value [Fraleigh 
2003].
•	 Models to predict end-to-end performance for a given workload model, and 
 
techniques to find a minimal cost bandwidth allocation that will result in all user 
740         Chapter 9    •    Multimedia Networking
requirements being met. Here, researchers are busy developing performance 
models that can quantify performance for a given workload, and optimization 
techniques to find minimal-cost bandwidth allocations meeting performance 
requirements.
Given that today’s best-effort Internet could (from a technology standpoint) sup-
port multimedia traffic at an appropriate performance level if it were dimensioned 
to do so, the natural question is why today’s Internet doesn’t do so. The answers 
are primarily economic and organizational. From an economic standpoint, would 
users be willing to pay their ISPs enough for the ISPs to install sufficient bandwidth 
to support multimedia applications over a best-effort Internet? The organizational 
issues are perhaps even more daunting. Note that an end-to-end path between two 
multimedia end points will pass through the networks of multiple ISPs. From an 
organizational standpoint, would these ISPs be willing to cooperate (perhaps with 
revenue sharing) to ensure that the end-to-end path is properly dimensioned to sup-
port multimedia applications? For a perspective on these economic and organiza-
tional issues, see [Davies 2005]. For a perspective on provisioning tier-1 backbone 
networks to support delay-sensitive traffic, see [Fraleigh 2003].
9.5.2 Providing Multiple Classes of Service
Perhaps the simplest enhancement to the one-size-fits-all best-effort service in 
today’s Internet is to divide traffic into classes, and provide different levels of ser-
vice to these different classes of traffic. For example, an ISP might well want to 
provide a higher class of service to delay-sensitive Voice-over-IP or teleconferenc-
ing traffic (and charge more for this service!) than to elastic traffic such as e-mail or 
HTTP. Alternatively, an ISP may simply want to provide a higher quality of service 
to customers willing to pay more for this improved service. A number of residential 
wired-access ISPs and cellular wireless-access ISPs have adopted such tiered lev-
els of service—with platinum-service subscribers receiving better performance than 
gold- or silver-service subscribers.
We’re all familiar with different classes of service from our everyday lives—
first-class airline passengers get better service than business-class passengers, who 
in turn get better service than those of us who fly economy class; VIPs are provided 
immediate entry to events while everyone else waits in line; elders are revered in 
some countries and provided seats of honor and the finest food at a table. It’s impor-
tant to note that such differential service is provided among aggregates of traffic, 
that is, among classes of traffic, not among individual connections. For example, all 
first-class passengers are handled the same (with no first-class passenger receiving 
any better treatment than any other first-class passenger), just as all VoIP packets 
would receive the same treatment within the network, independent of the particular 
end-to-end connection to which they belong. As we will see, by dealing with a small 
number of traffic aggregates, rather than a large number of individual connections, 
9.5    •    Network Support for Multimedia         741
the new network mechanisms required to provide better-than-best service can be 
kept relatively simple.
The early Internet designers clearly had this notion of multiple classes of ser-
vice in mind. Recall the type-of-service (ToS) field in the IPv4 header discussed in 
Chapter 4. IEN123 [ISI 1979] describes the ToS field also present in an ancestor of 
the IPv4 datagram as follows: “The Type of Service [field] provides an indication 
of the abstract parameters of the quality of service desired. These parameters are to 
be used to guide the selection of the actual service parameters when transmitting a 
datagram through a particular network. Several networks offer service precedence, 
which somehow treats high precedence traffic as more important that other traffic.” 
More than four decades ago, the vision of providing different levels of service to dif-
ferent classes of traffic was clear! However, it’s taken us an equally long period of 
time to realize this vision.
Motivating Scenarios
Let’s begin our discussion of network mechanisms for providing multiple classes of 
service with a few motivating scenarios.
Figure 9.11 shows a simple network scenario in which two application packet 
flows originate on Hosts H1 and H2 on one LAN and are destined for Hosts H3 and 
H4 on another LAN. The routers on the two LANs are connected by a 1.5 Mbps link. 
Let’s assume the LAN speeds are significantly higher than 1.5 Mbps, and focus on 
the output queue of router R1; it is here that packet delay and packet loss will occur 
if the aggregate sending rate of H1 and H2 exceeds 1.5 Mbps. Let’s further suppose 
that a 1 Mbps audio application (for example, a CD-quality audio call) shares the 
R1
1.5 Mbps link
R2
H2
H1
H4
H3
Figure 9.11  ♦  Competing audio and HTTP applications
742         Chapter 9    •    Multimedia Networking
1.5 Mbps link between R1 and R2 with an HTTP Web-browsing application that is 
downloading a Web page from H2 to H4.
In the best-effort Internet, the audio and HTTP packets are mixed in the output 
queue at R1 and (typically) transmitted in a first-in-first-out (FIFO) order. In this 
scenario, a burst of packets from the Web server could potentially fill up the queue, 
causing IP audio packets to be excessively delayed or lost due to buffer overflow 
at R1. How should we solve this potential problem? Given that the HTTP Web-
browsing application does not have time constraints, our intuition might be to give 
strict priority to audio packets at R1. Under a strict priority scheduling discipline, an 
audio packet in the R1 output buffer would always be transmitted before any HTTP 
packet in the R1 output buffer. The link from R1 to R2 would look like a dedicated 
link of 1.5 Mbps to the audio traffic, with HTTP traffic using the R1-to-R2 link only 
when no audio traffic is queued. In order for R1 to distinguish between the audio and 
HTTP packets in its queue, each packet must be marked as belonging to one of these 
two classes of traffic. This was the original goal of the type-of-service (ToS) field in 
IPv4. As obvious as this might seem, this then is our first insight into mechanisms 
needed to provide multiple classes of traffic:
Insight 1: Packet marking allows a router to distinguish among packets 
belonging to different classes of traffic.
Note that although our example considers a competing multimedia and elastic 
flow, the same insight applies to the case that platinum, gold, and silver classes of 
service are implemented—a packet-marking mechanism is still needed to indicate 
that class of service to which a packet belongs.
Now suppose that the router is configured to give priority to packets marked 
as belonging to the 1 Mbps audio application. Since the outgoing link speed is 1.5 
Mbps, even though the HTTP packets receive lower priority, they can still, on aver-
age, receive 0.5 Mbps of transmission service. But what happens if the audio applica-
tion starts sending packets at a rate of 1.5 Mbps or higher (either maliciously or due 
to an error in the application)? In this case, the HTTP packets will starve, that is, they 
will not receive any service on the R1-to-R2 link. Similar problems would occur if 
multiple applications (for example, multiple audio calls), all with the same class of 
service as the audio application, were sharing the link’s bandwidth; they too could 
collectively starve the FTP session. Ideally, one wants a degree of isolation among 
classes of traffic so that one class of traffic can be protected from the other. This pro-
tection could be implemented at different places in the network—at each and every 
router, at first entry to the network, or at inter-domain network boundaries. This then 
is our second insight:
Insight 2: It is desirable to provide a degree of traffic isolation among 
classes so that one class is not adversely affected by another class of traffic 
that misbehaves.
9.5    •    Network Support for Multimedia         743
We’ll examine several specific mechanisms for providing such isolation among 
traffic classes. We note here that two broad approaches can be taken. First, it is pos-
sible to perform traffic policing, as shown in Figure 9.12. If a traffic class or flow 
must meet certain criteria (for example, that the audio flow not exceed a peak rate of 
1 Mbps), then a policing mechanism can be put into place to ensure that these criteria 
are indeed observed. If the policed application misbehaves, the policing mechanism 
will take some action (for example, drop or delay packets that are in violation of 
the criteria) so that the traffic actually entering the network conforms to the criteria. 
The leaky bucket mechanism that we’ll examine shortly is perhaps the most widely 
used policing mechanism. In Figure 9.12, the packet classification and marking 
mechanism (Insight 1) and the policing mechanism (Insight 2) are both implemented 
together at the network’s edge, either in the end system or at an edge router.
A complementary approach for providing isolation among traffic classes is for 
the link-level packet-scheduling mechanism to explicitly allocate a fixed amount of 
link bandwidth to each class. For example, the audio class could be allocated 1 Mbps 
at R1, and the HTTP class could be allocated 0.5 Mbps. In this case, the audio and 
R1
1.5 Mbps link
Packet marking
and policing
Metering and policing
Marks
R2
H2
H1
Key:
H4
H3
Figure 9.12  ♦  Policing (and marking) the audio and HTTP traffic classes
744         Chapter 9    •    Multimedia Networking
HTTP flows see a logical link with capacity 1.0 and 0.5 Mbps, respectively, as shown 
in Figure 9.13. With strict enforcement of the link-level allocation of bandwidth, a 
class can use only the amount of bandwidth that has been allocated; in particular, it 
cannot utilize bandwidth that is not currently being used by others. For example, if 
the audio flow goes silent (for example, if the speaker pauses and generates no audio 
packets), the HTTP flow would still not be able to transmit more than 0.5 Mbps over 
the R1-to-R2 link, even though the audio flow’s 1 Mbps bandwidth allocation is not 
being used at that moment. Since bandwidth is a “use-it-or-lose-it” resource, there is 
no reason to prevent HTTP traffic from using bandwidth not used by the audio traf-
fic. We’d like to use bandwidth as efficiently as possible, never wasting it when it 
could be otherwise used. This gives rise to our third insight:
Insight 3: While providing isolation among classes or flows, it is desirable 
to use resources (for example, link bandwidth and buffers) as efficiently as 
possible.
Recall from our discussion in Sections 1.3 and 4.2 that packets belonging to vari-
ous network flows are multiplexed and queued for transmission at the output buff-
ers associated with a link. The manner in which queued packets are selected for 
transmission on the link is known as the link-scheduling discipline, and was 
discussed in detail in Section 4.2. Recall that in Section 4.2 three link-scheduling 
disciplines were discussed, namely, FIFO, priority queuing, and Weighted Fair 
 
Queuing (WFQ). We’ll see soon see that WFQ will play a particularly important role 
for isolating the traffic classes.
R1
1.5 Mbps link
1.0 Mbps
logical link
0.5 Mbps
logical link
R2
H2
H1
H4
H3
Figure 9.13  ♦  Logical isolation of audio and HTTP traffic classes
9.5    •    Network Support for Multimedia         745
The Leaky Bucket
One of our earlier insights was that policing, the regulation of the rate at which a 
class or flow (we will assume the unit of policing is a flow in our discussion below) is 
allowed to inject packets into the network, is an important QoS mechanism. But what 
aspects of a flow’s packet rate should be policed? We can identify three important 
policing criteria, each differing from the other according to the time scale over which 
the packet flow is policed:
•	 Average rate. The network may wish to limit the long-term average rate (packets 
per time interval) at which a flow’s packets can be sent into the network. A crucial 
issue here is the interval of time over which the average rate will be policed. A 
flow whose average rate is limited to 100 packets per second is more constrained 
than a source that is limited to 6,000 packets per minute, even though both have 
the same average rate over a long enough interval of time. For example, the latter 
constraint would allow a flow to send 1,000 packets in a given second-long inter-
val of time, while the former constraint would disallow this sending behavior.
•	 Peak rate. While the average-rate constraint limits the amount of traffic that can 
be sent into the network over a relatively long period of time, a peak-rate con-
straint limits the maximum number of packets that can be sent over a shorter 
period of time. Using our example above, the network may police a flow at an 
average rate of 6,000 packets per minute, while limiting the flow’s peak rate to 
1,500 packets per second.
•	 Burst size. The network may also wish to limit the maximum number of packets 
(the “burst” of packets) that can be sent into the network over an extremely short 
interval of time. In the limit, as the interval length approaches zero, the burst size 
limits the number of packets that can be instantaneously sent into the network. 
Even though it is physically impossible to instantaneously send multiple packets 
into the network (after all, every link has a physical transmission rate that cannot 
be exceeded!), the abstraction of a maximum burst size is a useful one.
The leaky bucket mechanism is an abstraction that can be used to characterize 
these policing limits. As shown in Figure 9.14, a leaky bucket consists of a bucket 
that can hold up to b tokens. Tokens are added to this bucket as follows. New tokens, 
which may potentially be added to the bucket, are always being generated at a rate 
of r tokens per second. (We assume here for simplicity that the unit of time is a 
second.) If the bucket is filled with less than b tokens when a token is generated, the 
newly generated token is added to the bucket; otherwise the newly generated token 
is ignored, and the token bucket remains full with b tokens.
Let us now consider how the leaky bucket can be used to police a packet flow. 
Suppose that before a packet is transmitted into the network, it must first remove a 
token from the token bucket. If the token bucket is empty, the packet must wait for 
746         Chapter 9    •    Multimedia Networking
a token. (An alternative is for the packet to be dropped, although we will not con-
sider that option here.) Let us now consider how this behavior polices a traffic flow. 
Because there can be at most b tokens in the bucket, the maximum burst size for a 
leaky-bucket-policed flow is b packets. Furthermore, because the token generation 
rate is r, the maximum number of packets that can enter the network of any interval 
of time of length t is rt + b. Thus, the token-generation rate, r, serves to limit the 
long-term average rate at which packets can enter the network. It is also possible to 
use leaky buckets (specifically, two leaky buckets in series) to police a flow’s peak 
rate in addition to the long-term average rate; see the homework problems at the end 
of this chapter.
Leaky Bucket ∙Weighted Fair Queuing ∙Provable Maximum Delay 
in a Queue
Let’s close our discussion on policing by showing how the leaky bucket and WFQ 
can be combined to provide a bound on the delay through a router’s queue. (Readers 
 
who have forgotten about WFQ are encouraged to review WFQ, which is covered 
in Section 4.2.) Let’s consider a router’s output link that multiplexes n flows, each 
policed by a leaky bucket with parameters bi and ri, i = 1, . . . , n, using WFQ 
scheduling. We use the term flow here loosely to refer to the set of packets that are 
not distinguished from each other by the scheduler. In practice, a flow might be com-
prised of traffic from a single end-to-end connection or a collection of many such 
connections, see Figure 9.15.
Recall from our discussion of WFQ that each flow, i, is guaranteed to receive a 
share of the link bandwidth equal to at least R # wi>(gwj), where R is the transmission 
To network
Packets
Remove
token
Token
wait area
Bucket holds
up to
b tokens
r tokens/sec
Figure 9.14  ♦  The leaky bucket policer
9.5    •    Network Support for Multimedia         747
rate of the link in packets/sec. What then is the maximum delay that a packet will 
experience while waiting for service in the WFQ (that is, after passing through the 
leaky bucket)? Let us focus on flow 1. Suppose that flow 1’s token bucket is initially 
full. A burst of b1 packets then arrives to the leaky bucket policer for flow 1. These 
packets remove all of the tokens (without wait) from the leaky bucket and then join 
the WFQ waiting area for flow 1. Since these b1 packets are served at a rate of at least 
R # wi>(gwj) packet/sec, the last of these packets will then have a maximum delay, 
dmax, until its transmission is completed, where
dmax =
b1
R # w1> gwj
The rationale behind this formula is that if there are b1 packets in the queue and 
packets are being serviced (removed) from the queue at a rate of at least R # w1>(gwj) 
packets per second, then the amount of time until the last bit of the last packet is 
transmitted cannot be more than b1>(R # w1>(gwj)). A homework problem asks you 
to prove that as long as r1 6 R # w1>(gwj), then dmax is indeed the maximum delay 
that any packet in flow 1 will ever experience in the WFQ queue.
9.5.3 Diffserv
Having seen the motivation, insights, and specific mechanisms for providing 
 
multiple classes of service, let’s wrap up our study of approaches toward prov-
ing multiple classes of service with an example—the Internet Diffserv architecture 
[RFC 2475; Kilkki 1999]. Diffserv provides service differentiation—that is, the 
b1
r1
w1
wn
bn
rn
Figure 9.15  ♦   n multiplexed leaky bucket flows with WFQ scheduling
748         Chapter 9    •    Multimedia Networking
ability to handle different classes of traffic in different ways within the Internet 
in a scalable manner. The need for scalability arises from the fact that millions of 
simultaneous source-destination traffic flows may be present at a backbone router. 
We’ll see shortly that this need is met by placing only simple functionality within 
the network core, with more complex control operations being implemented at the 
network’s edge.
Let’s begin with the simple network shown in Figure 9.16. We’ll describe one 
possible use of Diffserv here; other variations are possible, as described in RFC 
2475. The Diffserv architecture consists of two sets of functional elements:
•	 Edge functions: Packet classification and traffic conditioning. At the incoming 
edge of the network (that is, at either a Diffserv-capable host that generates traf-
fic or at the first Diffserv-capable router that the traffic passes through), arriving 
packets are marked. More specifically, the differentiated service (DS) field in the 
IPv4 or IPv6 packet header is set to some value [RFC 3260]. The definition of 
the DS field is intended to supersede the earlier definitions of the IPv4 type-of-
service field and the IPv6 traffic class fields that we discussed in Chapter 4. For 
example, in Figure 9.16, packets being sent from H1 to H3 might be marked at 
R1, while packets being sent from H2 to H4 might be marked at R2. The mark 
that a packet receives identifies the class of traffic to which it belongs. Different 
classes of traffic will then receive different service within the core network.
R4
Leaf router
Key:
Core router
R2
R1
R6
R7
R3
R5
H1
H2
H4
H3
R2
R3
Figure 9.16  ♦  A simple Diffserv network example
9.5    •    Network Support for Multimedia         749
•	 Core function: Forwarding. When a DS-marked packet arrives at a Diffserv- 
capable router, the packet is forwarded onto its next hop according to the so-
called per-hop behavior (PHB) associated with that packet’s class. The per-hop 
behavior influences how a router’s buffers and link bandwidth are shared among 
the competing classes of traffic. A crucial tenet of the Diffserv architecture is that 
a router’s per-hop behavior will be based only on packet markings, that is, the 
class of traffic to which a packet belongs. Thus, if packets being sent from H1 to 
H3 in Figure 9.16 receive the same marking as packets being sent from H2 to H4, 
then the network routers treat these packets as an aggregate, without distinguishing 
whether the packets originated at H1 or H2. For example, R3 would not distin-
guish between packets from H1 and H2 when forwarding these packets on to R4. 
Thus, the Diffserv architecture obviates the need to keep router state for individual 
source-destination pairs—a critical consideration in making Diffserv scalable.
An analogy might prove useful here. At many large-scale social events (for example, 
a large public reception, a large dance club or discothèque, a concert, or a football 
game), people entering the event receive a pass of one type or another: VIP passes 
for Very Important People; over-21 passes for people who are 21 years old or older 
(for example, if alcoholic drinks are to be served); backstage passes at concerts; press 
passes for reporters; even an ordinary pass for the Ordinary Person. These passes 
are typically distributed upon entry to the event, that is, at the edge of the event. It 
is here at the edge where computationally intensive operations, such as paying for 
entry, checking for the appropriate type of invitation, and matching an invitation 
against a piece of identification, are performed. Furthermore, there may be a limit on 
the number of people of a given type that are allowed into an event. If there is such 
a limit, people may have to wait before entering the event. Once inside the event, 
one’s pass allows one to receive differentiated service at many locations around the 
event—a VIP is provided with free drinks, a better table, free food, entry to exclusive 
rooms, and fawning service. Conversely, an ordinary person is excluded from cer-
tain areas, pays for drinks, and receives only basic service. In both cases, the service 
received within the event depends solely on the type of one’s pass. Moreover, all 
people within a class are treated alike.
Figure 9.17 provides a logical view of the classification and marking functions 
within the edge router. Packets arriving to the edge router are first classified. The 
classifier selects packets based on the values of one or more packet header fields (for 
example, source address, destination address, source port, destination port, and pro-
tocol ID) and steers the packet to the appropriate marking function. As noted above, 
a packet’s marking is carried in the DS field in the packet header.
In some cases, an end user may have agreed to limit its packet-sending rate to 
conform to a declared traffic profile. The traffic profile might contain a limit on the 
peak rate, as well as the burstiness of the packet flow, as we saw previously with the 
leaky bucket mechanism. As long as the user sends packets into the network in a 
 
way that conforms to the negotiated traffic profile, the packets receive their priority 
750         Chapter 9    •    Multimedia Networking
marking and are forwarded along their route to the destination. On the other hand, 
if the traffic profile is violated, out-of-profile packets might be marked differently, 
might be shaped (for example, delayed so that a maximum rate constraint would be 
observed), or might be dropped at the network edge. The role of the metering function, 
 
shown in Figure 9.17, is to compare the incoming packet flow with the negotiated 
traffic profile and to determine whether a packet is within the negotiated traffic pro-
file. The actual decision about whether to immediately remark, forward, delay, or 
drop a packet is a policy issue determined by the network administrator and is not 
specified in the Diffserv architecture.
So far, we have focused on the marking and policing functions in the Diffserv 
architecture. The second key component of the Diffserv architecture involves the 
per-hop behavior (PHB) performed by Diffserv-capable routers. PHB is rather cryp-
tically, but carefully, defined as “a description of the externally observable forward-
ing behavior of a Diffserv node applied to a particular Diffserv behavior aggregate” 
[RFC 2475]. Digging a little deeper into this definition, we can see several important 
considerations embedded within:
•	 A PHB can result in different classes of traffic receiving different performance 
(that is, different externally observable forwarding behaviors).
•	 While a PHB defines differences in performance (behavior) among classes, it 
does not mandate any particular mechanism for achieving these behaviors. As 
long as the externally observable performance criteria are met, any implemen-
tation mechanism and any buffer/bandwidth allocation policy can be used. For 
example, a PHB would not require that a particular packet-queuing discipline (for 
example, a priority queue versus a WFQ queue versus a FCFS queue) be used to 
achieve a particular behavior. The PHB is the end, to which resource allocation 
and implementation mechanisms are the means.
•	 Differences in performance must be observable and hence measurable.
Packets
Forward
Classiﬁer
Marker
Drop
Shaper/
Dropper
Meter
Figure 9.17  ♦  A simple Diffserv network example
9.5    •    Network Support for Multimedia         751
Two PHBs have been defined: an expedited forwarding (EF) PHB [RFC 3246] and an 
assured forwarding (AF) PHB [RFC 2597]. The expedited forwarding PHB speci-
fies that the departure rate of a class of traffic from a router must equal or exceed 
a configured rate. The assured forwarding PHB divides traffic into four classes, 
where each AF class is guaranteed to be provided with some minimum amount of 
bandwidth and buffering.
Let’s close our discussion of Diffserv with a few observations regarding its ser-
vice model. First, we have implicitly assumed that Diffserv is deployed within a 
single administrative domain, but typically an end-to-end service must be fashioned 
from multiple ISPs sitting between communicating end systems. In order to provide 
end-to-end Diffserv service, all the ISPs between the end systems must not only pro-
vide this service, but most also cooperate and make settlements in order to offer end 
customers true end-to-end service. Without this kind of cooperation, ISPs directly 
selling Diffserv service to customers will find themselves repeatedly saying: “Yes, 
we know you paid extra, but we don’t have a service agreement with the ISP that 
dropped and delayed your traffic. I’m sorry that there were so many gaps in your 
VoIP call!” Second, if Diffserv were actually in place and the network ran at only 
moderate load, most of the time there would be no perceived difference between a 
best-effort service and a Diffserv service. Indeed, end-to-end delay is usually domi-
nated by access rates and router hops rather than by queuing delays in the routers. 
Imagine the unhappy Diffserv customer who has paid more for premium service but 
finds that the best-effort service being provided to others almost always has the same 
performance as premium service!
9.5.4 
Per-Connection Quality-of-Service (QoS) Guarantees: 
Resource Reservation and Call Admission
In the previous section, we have seen that packet marking and policing, traffic isola-
tion, and link-level scheduling can provide one class of service with better perfor-
mance than another. Under certain scheduling disciplines, such as priority scheduling, 
the lower classes of traffic are essentially “invisible” to the highest-priority class of 
traffic. With proper network dimensioning, the highest class of service can indeed 
achieve extremely low packet loss and delay—essentially circuit-like performance. 
But can the network guarantee that an ongoing flow in a high-priority traffic class 
will continue to receive such service throughout the flow’s duration using only the 
mechanisms that we have described so far? It cannot. In this section, we’ll see why 
yet additional network mechanisms and protocols are required when a hard service 
guarantee is provided to individual connections.
Let’s return to our scenario from Section 9.5.2 and consider two 1 Mbps 
audio applications transmitting their packets over the 1.5 Mbps link, as shown in 
 
Figure 9.18. The combined data rate of the two flows (2 Mbps) exceeds the link 
capacity. Even with classification and marking, isolation of flows, and sharing of 
unused bandwidth (of which there is none), this is clearly a losing proposition. There 
is simply not enough bandwidth to accommodate the needs of both applications at 
752         Chapter 9    •    Multimedia Networking
the same time. If the two applications equally share the bandwidth, each application 
would lose 25 percent of its transmitted packets. This is such an unacceptably low 
QoS that both audio applications are completely unusable; there’s no need even to 
transmit any audio packets in the first place.
Given that the two applications in Figure 9.18 cannot both be satisfied simul-
taneously, what should the network do? Allowing both to proceed with an unusable 
QoS wastes network resources on application flows that ultimately provide no utility 
to the end user. The answer is hopefully clear—one of the application flows should 
be blocked (that is, denied access to the network), while the other should be allowed 
to proceed on, using the full 1 Mbps needed by the application. The telephone net-
work is an example of a network that performs such call blocking—if the required 
resources (an end-to-end circuit in the case of the telephone network) cannot be allo-
cated to the call, the call is blocked (prevented from entering the network) and a busy 
signal is returned to the user. In our example, there is no gain in allowing a flow into 
the network if it will not receive a sufficient QoS to be considered usable. Indeed, 
there is a cost to admitting a flow that does not receive its needed QoS, as network 
resources are being used to support a flow that provides no utility to the end user.
By explicitly admitting or blocking flows based on their resource requirements, 
and the source requirements of already-admitted flows, the network can guarantee 
that admitted flows will be able to receive their requested QoS. Implicit in the need 
to provide a guaranteed QoS to a flow is the need for the flow to declare its QoS 
requirements. This process of having a flow declare its QoS requirement, and then 
having the network either accept the flow (at the required QoS) or block the flow is 
referred to as the call admission process. This then is our fourth insight (in addi-
tion to the three earlier insights from Section 9.5.2,) into the mechanisms needed to 
provide QoS.
R1
1.5 Mbps link
1 Mbps
audio
1 Mbps
audio
R2
H2
H1
H4
H3
Figure 9.18  ♦  
Two competing audio applications overloading the  
R1-to-R2 link
9.5    •    Network Support for Multimedia         753
Insight 4: If sufficient resources will not always be available, and QoS is 
to be guaranteed, a call admission process is needed in which flows declare 
their QoS requirements and are then either admitted to the network (at the 
required QoS) or blocked from the network (if the required QoS cannot be 
provided by the network).
Our motivating example in Figure 9.18 highlights the need for several new network 
mechanisms and protocols if a call (an end-to-end flow) is to be guaranteed a given 
quality of service once it begins:
•	 Resource reservation.  The only way to guarantee that a call will have the resources 
(link bandwidth, buffers) needed to meet its desired QoS is to explicitly allocate 
those resources to the call—a process known in networking parlance as resource 
reservation. Once resources are reserved, the call has on-demand access to these 
resources throughout its duration, regardless of the demands of all other calls. If 
a call reserves and receives a guarantee of x Mbps of link bandwidth, and never 
transmits at a rate greater than x, the call will see loss- and delay-free performance.
•	 Call admission. If resources are to be reserved, then the network must have a 
mechanism for calls to request and reserve resources. Since resources are not 
infinite, a call making a call admission request will be denied admission, that is, 
be blocked, if the requested resources are not available. Such a call admission 
is performed by the telephone network—we request resources when we dial a 
number. If the circuits (TDMA slots) needed to complete the call are available, 
the circuits are allocated and the call is completed. If the circuits are not avail-
able, then the call is blocked, and we receive a busy signal. A blocked call can try 
again to gain admission to the network, but it is not allowed to send traffic into the 
network until it has successfully completed the call admission process. Of course, 
a router that allocates link bandwidth should not allocate more than is available 
at that link. Typically, a call may reserve only a fraction of the link’s bandwidth, 
and so a router may allocate link bandwidth to more than one call. However, the 
sum of the allocated bandwidth to all calls should be less than the link capacity if 
hard quality of service guarantees are to be provided.
•	 Call setup signaling. The call admission process described above requires that a 
call be able to reserve sufficient resources at each and every network router on its 
source-to-destination path to ensure that its end-to-end QoS requirement is met. 
Each router must determine the local resources required by the session, consider 
the amounts of its resources that are already committed to other ongoing sessions, 
and determine whether it has sufficient resources to satisfy the per-hop QoS 
requirement of the session at this router without violating local QoS guarantees 
made to an already-admitted session. A signaling protocol is needed to coordinate 
these various activities—the per-hop allocation of local resources, as well as the 
overall end-to-end decision of whether or not the call has been able to reserve suf-
754         Chapter 9    •    Multimedia Networking
ficient resources at each and every router on the end-to-end path. This is the job 
of the call setup protocol, as shown in Figure 9.19. The RSVP protocol [Zhang 
1993, RFC 2210] was proposed for this purpose within an Internet architecture 
for providing quality-of-service guarantees. In ATM networks, the Q2931b pro-
tocol [Black 1995] carries this information among the ATM network’s switches 
and end point.
Despite a tremendous amount of research and development, and even products 
that provide for per-connection quality of service guarantees, there has been almost 
no extended deployment of such services. There are many possible reasons. First and 
foremost, it may well be the case that the simple application-level mechanisms that 
we studied in Sections 9.2 through 9.4, combined with proper network dimensioning 
(Section 9.5.1) provide “good enough” best-effort network service for multimedia 
applications. In addition, the added complexity and cost of deploying and managing 
a network that provides per-connection quality of service guarantees may be judged 
by ISPs to be simply too high given predicted customer revenues for that service.
9.6	 Summary
Multimedia networking is one of the most exciting developments in the Internet 
today. People throughout the world less and less time in front of their televisions, 
and are instead use their smartphones and devices to receive audio and video trans-
QoS call signaling setup
Request/reply
Figure 9.19  ♦  The call setup process
Homework Problems and Questions         755
missions, both live and prerecorded. Moreover, with sites like YouTube, users have 
become producers as well as consumers of multimedia Internet content. In addition 
to video distribution, the Internet is also being used to transport phone calls. In fact, 
over the next 10 years, the Internet, along with wireless Internet access, may make 
the traditional circuit-switched telephone system a thing of the past. VoIP not only 
provides phone service inexpensively, but also provides numerous value-added ser-
vices, such as video conferencing, online directory services, voice messaging, and 
integration into social networks such as Facebook and WeChat.
In Section 9.1, we described the intrinsic characteristics of video and voice, and 
then classified multimedia applications into three categories: (i) streaming stored 
audio/video, (ii) conversational voice/video-over-IP, and (iii) streaming live audio/
video.
In Section 9.2, we studied streaming stored video in some depth. For stream-
ing video applications, prerecorded videos are placed on servers, and users send 
requests to these servers to view the videos on demand. We saw that streaming 
video systems can be classified into two categories: UDP streaming and HTTP. 
We observed that the most important performance measure for streaming video is 
average throughput.
In Section 9.3, we examined how conversational multimedia applications, such 
as VoIP, can be designed to run over a best-effort network. For conversational mul-
timedia, timing considerations are important because conversational applications 
are highly delay-sensitive. On the other hand, conversational multimedia applica-
tions are loss—tolerant—occasional loss only causes occasional glitches in audio/
video playback, and these losses can often be partially or fully concealed. We saw 
how a combination of client buffers, packet sequence numbers, and timestamps can 
greatly alleviate the effects of network-induced jitter. We also surveyed the tech-
nology behind Skype, one of the leading voice- and video-over-IP companies. In 
 
Section 9.4, we examined two of the most important standardized protocols for 
VoIP, namely, RTP and SIP.
In Section 9.5, we introduced how several network mechanisms (link-level 
scheduling disciplines and traffic policing) can be used to provide differentiated ser-
vice among several classes of traffic.
Homework Problems and Questions
Chapter 9 Review Questions
SECTION 9.1
	R1.	 Reconstruct Table 9.1 for when Victor Video is watching a 5 Mbps video, 
Facebook Frank is looking at a new 150 Kbyte image every 25 seconds, and 
Martha Music is listening to 210 kbps audio stream. 
756         Chapter 9    •    Multimedia Networking
	R2.	 For 128 quantization levels, what is the size of each sample signal?
	R3.	 Suppose an analog audio signal is sampled 8,000 times per second, and each 
sample is quantized into one of 512 levels. What would be the resulting bit 
rate of the PCM digital audio signal?
	R4.	 Many Internet companies today provide streaming video, including YouTube 
(Google), Netflix, and Hulu. Streaming stored video has three key distinguishing 
features. List them.
SECTION 9.2
	R5.	 What are advantages of client buffering?
	R6.	 In video streaming applications, why is HTTP streaming more popular than 
UDP streaming? 
	R7.	 With HTTP streaming, are the TCP receive buffer and the client’s application 
buffer the same thing? If not, how do they interact?
	R8.	 Consider the simple model for HTTP streaming. Suppose the server sends 
bits at a constant rate of 2 Mbps and playback begins when 8 million bits 
have been received. What is the initial buffering delay tp?
SECTION 9.3
	R9.	 What mechanisms are used at the receiver side to eliminate packet jitter?
	
R10.	 What are the two types of loss anticipation schemes used in VoIP?
	
R11.	 Section 9.3 describes two FEC schemes. Briefly summarize them. Both 
schemes increase the transmission rate of the stream by adding overhead. 
Does interleaving also increase the transmission rate?
SECTION 9.4
	
R12.	 What are the four main RTP header fields?
	
R13.	 What is network dimensioning?
Problems
	 P1.	 Consider the figure below. Similar to our discussion of Figure 9.1, suppose 
that video is encoded at a fixed bit rate, and thus each video block contains 
video frames that are to be played out over the same fixed amount of time, △.  
The server transmits the first video block at t0, the second block at t0 + △,  
Problems         757
the third block at t0 + 2△, and so on. Once the client begins playout, each 
block should be played out △ time units after the previous block.
Constant bit
rate video
transmission
by server
1
2
3
4
5
6
7
8
9
Time
D D
D D D D D D D D D
Video block number
t0
t1
Video
reception
at client
a.	 Suppose that the client begins playout as soon as the first block arrives 
at t1. In the figure below, how many blocks of video (including the first 
block) will have arrived at the client in time for their playout? Explain 
how you arrived at your answer.
b.	 Suppose that the client begins playout now at t1 + △. How many blocks 
of video (including the first block) will have arrived at the client in time 
for their playout? Explain how you arrived at your answer.
c.	 In the same scenario at (b) above, what is the largest number of blocks 
that is ever stored in the client buffer, awaiting playout? Explain how you 
arrived at your answer.
d.	 What is the smallest playout delay at the client, such that every video block 
has arrived in time for its playout? Explain how you arrived at your answer.
	 P2.	 Recall the simple model for HTTP streaming shown in Figure 9.3. Recall that 
B denotes the size of the client’s application buffer, and Q denotes the num-
ber of bits that must be buffered before the client application begins playout. 
Also r denotes the video consumption rate. Assume that the server sends bits 
at a constant rate x whenever the client buffer is not full.
a.	 Suppose that x 6 r. As discussed in the text, in this case playout will 
alternate between periods of continuous playout and periods of freezing. 
Determine the length of each continuous playout and freezing period as a 
function of Q, r, and x.
b.	 Now suppose that x 7 r. At what time t = tf does the client application 
buffer become full?
758         Chapter 9    •    Multimedia Networking
	 P3.	 Recall the simple model for HTTP streaming shown in Figure 9.3. Suppose 
the buffer size is infinite but the server sends bits at variable rate x(t). Specifi-
cally, suppose x(t) has the following saw-tooth shape. The rate is initially 
zero at time t = 0 and linearly climbs to H at time t = T. It then repeats this 
pattern again and again, as shown in the figure below.
H
Time
T
2T
3T
4T
Bit rate x(t)
a.	 What is the server’s average send rate?
b.	 Suppose that Q = 0, so that the client starts playback as soon as it 
receives a video frame. What will happen?
c.	 Now suppose Q 7 0 and HT/2 Ú Q. Determine as a function of Q, H, 
and T the time at which playback first begins.
d.	 Suppose H 7 2r and Q = HT/2. Prove there will be no freezing after the 
initial playout delay.
e.	 Suppose H 7 2r. Find the smallest value of Q such that there will be no 
freezing after the initial playback delay.
f.	 Now suppose that the buffer size B is finite. Suppose H 7 2r. As a func-
tion of Q, B, T, and H, determine the time t = tf when the client applica-
tion buffer first becomes full.
	 P4.	 Consider the following in the context of prefetching. Suppose the video 
consumption rate is 2 Mbps but the network is capable of delivering the  
video from server to client at a constant rate of 2.5 Mbps. Then the client 
will not only be able to play out the video with a very small playout delay,  
but will also be able to increase the amount of buffered video data by 
500 Kbits every second. In this manner, if in the future, the client receives 
data at a rate of less than 2 Mbps for a brief period of time, the client will 
be able to continue to provide continuous playback due to the reserve in 
its buffer. At what throughput does streaming over TCP result in minimal 
starvation and low buffering delays?
Problems         759
	 P5.	 As an example of jitter, consider two consecutive packets in our VoIP appli-
cation. The sender sends the second packet 20 msecs after sending the first 
packet. But at the receiver, the spacing between these packets can become 
greater than 20 msecs. To see this, suppose the first packet arrives at a nearly 
empty queue at a router, but just before the second packet arrives at the queue a 
large number of packets from other sources arrive at the same queue. Because 
the first packet experiences a small queuing delay and the second packet suffers 
a large queuing delay at this router, the first and second packets become spaced 
by more than 20 msecs. Give an analogy with driving cars on roads. 
	 P6.	 In the VoIP example in Section 9.3, let h be the total number of header byte 
added to each chunk, including UDP and IP header.
a.	 Assuming an IP datagram is emitted every 40 msecs, find the transmission rate 
in bits per second for the datagrams generated by one side of this application.
b.	 What is a typical value of h when RTP is used? How much time is 
required to transmit the header?
	 P7.	 Consider the procedure described in Section 9.3 for estimating average delay 
di. Suppose that u = 0.1. Let r1 - t1 be the most recent sample delay, let 
r2 - t2 be the next most recent sample delay, and so on.
a.	 For a given audio application suppose four packets have arrived at the 
receiver with sample delays r4 - t4, r3 - t3, r2 - t2, and r1 - t1. Express 
the estimate of delay d in terms of the four samples.
b.	 Generalize your formula for n sample delays.
c.	 For the formula in part (b), let n approach infinity and give the resulting 
formula. Comment on why this averaging procedure is called an exponen-
tial moving average.
	 P8.	 Repeat parts (a) and (b) in Question P7 for the estimate of average delay deviation.
	 P9.	 For the VoIP example in Section 9.3, we introduced an online procedure 
(exponential moving average) for estimating delay. In this problem we will 
examine an alternative procedure. Let ti be the timestamp of the ith packet 
received; let ri be the time at which the ith packet is received. Let dn be our 
estimate of average delay after receiving the nth packet. After the first packet 
is received, we set the delay estimate equal to d1 = r1 - t1.
a.	 Suppose that we would like dn = (r1 - t1 + r2 - t2 + g + rn - tn)/n 
for all n. Give a recursive formula for dn in terms of dn-1, rn, and tn.
b.	 Describe why for Internet telephony, the delay estimate described in  
Section 9.3 is more appropriate than the delay estimate outlined in part (a).
	
P10.	 With the fixed-delay strategy, the receiver attempts to play out each chunk 
exactly q msecs after the chunk is generated. So if a chunk is timestamped at 
the sender at time t, the receiver plays out the chunk at time t 1 q, assuming 
the chunk has arrived by that time. Packets that arrive after their scheduled 
playout times are discarded and considered lost. What is a good choice for q? 
760         Chapter 9    •    Multimedia Networking
	
P11.	 Consider the figure below (which is similar to Figure 9.3). A sender begins 
sending packetized audio periodically at t = 1. The first packet arrives at the 
receiver at t = 8.
Packets
generated
Time
Packets
1
8
Packets
received
a.	 What are the delays (from sender to receiver, ignoring any playout delays) 
of packets 2 through 8? Note that each vertical and horizontal line segment 
in the figure has a length of 1, 2, or 3 time units.
b.	 If audio playout begins as soon as the first packet arrives at the receiver at 
t = 8, which of the first eight packets sent will not arrive in time for playout?
c.	 If audio playout begins at t = 9, which of the first eight packets sent will 
not arrive in time for playout?
d.	 What is the minimum playout delay at the receiver that results in all of the 
first eight packets arriving in time for their playout?
	
P12.	 Consider again the figure in P11, showing packet audio transmission and 
reception times.
a.	 Compute the estimated delay for packets 2 through 8, using the formula 
for di from Section 9.3.2. Use a value of u = 0.1.
b.	 Compute the estimated deviation of the delay from the estimated average 
for packets 2 through 8, using the formula for vi from Section 9.3.2. Use a 
value of u = 0.1.
	
P13.	 A is at her PC and she wants to call B, who is also working at his PC. A’s and 
B’s PCs are both equipped with SIP-based software for making and receiving 
phone calls. Assume that A knows the IP address of B’s PC. Illustrate the SIP 
call-establishment process. 
Problems         761
	
P14.	 a.	 
Consider an audio conference call in Skype with N 7 2 participants.  
Suppose each participant generates a constant stream of rate r bps. How 
many bits per second will the call initiator need to send? How many bits 
per second will each of the other N - 1 participants need to send? What is 
the total send rate, aggregated over all participants?
b.	 Repeat part (a) for a Skype video conference call using a central server.
c.	 Repeat part (b), but now for when each peer sends a copy of its video 
stream to each of the N - 1 other peers.
	
P15.	 a.	 
Suppose we send into the Internet two IP datagrams, each carrying a 
different UDP segment. The first datagram has source IP address A1, 
destination IP address B, source port P1, and destination port T. The 
second datagram has source IP address A2, destination IP address B, 
source port P2, and destination port T. Suppose that A1 is different from 
A2 and that P1 is different from P2. Assuming that both datagrams reach 
their final destination, will the two UDP datagrams be received by the 
same socket? Why or why not?
b.	 Suppose Alice, Bob, and Claire want to have an audio conference call 
using SIP and RTP. For Alice to send and receive RTP packets to and 
from Bob and Claire, is only one UDP socket sufficient (in addition to 
the socket needed for the SIP messages)? If yes, then how does Alice’s 
SIP client distinguish between the RTP packets received from Bob and 
Claire?
	
P16.	 True or false:
a.	 If stored video is streamed directly from a Web server to a media player, 
then the application is using TCP as the underlying transport protocol.
b.	 When using RTP, it is possible for a sender to change encoding in the 
middle of a session.
c.	 All applications that use RTP must use port 87.
d.	 If an RTP session has a separate audio and video stream for each sender, 
then the audio and video streams use the same SSRC.
e.	 In differentiated services, while per-hop behavior defines differences in 
performance among classes, it does not mandate any particular mecha-
nism for achieving these performances.
762         Chapter 9    •    Multimedia Networking
f.	 Suppose Alice wants to establish an SIP session with Bob. In her INVITE 
message she includes the line: m=audio 48753 RTP/AVP 3 (AVP 3 denotes 
GSM audio). Alice has therefore indicated in this message that she wishes 
to send GSM audio.
g.	 Referring to the preceding statement, Alice has indicated in her INVITE 
message that she will send audio to port 48753.
h.	 SIP messages are typically sent between SIP entities using a default SIP 
port number.
i.	 In order to maintain registration, SIP clients must periodically send  
REGISTER messages.
j.	 SIP mandates that all SIP clients support G.711 audio encoding.
	
P17.	 Consider the figure below, which shows a leaky bucket policer being fed by 
a stream of packets. The token buffer can hold at most two tokens, and is 
initially full at t = 0. New tokens arrive at a rate of one token per slot. The 
output link speed is such that if two packets obtain tokens at the beginning 
of a time slot, they can both go to the output link in the same slot. The tim-
ing details of the system are as follows:
1.	 Packets (if any) arrive at the beginning of the slot. Thus in the figure, 
packets 1, 2, and 3 arrive in slot 0. If there are already packets in the 
queue, then the arriving packets join the end of the queue. Packets pro-
ceed towards the front of the queue in a FIFO manner.
2.	 After the arrivals have been added to the queue, if there are any queued 
packets, one or two of those packets (depending on the number of avail-
able tokens) will each remove a token from the token buffer and go to the 
output link during that slot. Thus, packets 1 and 2 each remove a token 
from the buffer (since there are initially two tokens) and go to the output 
link during slot 0.
Arrivals
Packet queue
(wait for tokens)
9
10
7
6
4
8
5
1
3
2
t = 8
t = 6
t = 4
t = 2
t = 0
t = 4
t = 2
t = 0
r = 1 token/slot
b = 2 tokens
Programming Assignment         763
3.	 A new token is added to the token buffer if it is not full, since the token 
generation rate is r = 1 token/slot.
4.	 Time then advances to the next time slot, and these steps repeat.
Answer the following questions:
a.	 For each time slot, identify the packets that are in the queue and the number 
of tokens in the bucket, immediately after the arrivals have been processed 
(step 1 above) but before any of the packets have passed through the queue 
and removed a token. Thus, for the t = 0 time slot in the example above, 
packets 1, 2, and 3 are in the queue, and there are two tokens in the buffer.
b.	 For each time slot indicate which packets appear on the output after the 
token(s) have been removed from the queue. Thus, for the t = 0 time slot 
in the example above, packets 1 and 2 appear on the output link from the 
leaky buffer during slot 0.
	
P18.	 Repeat P17 but assume that r = 2. Assume again that the bucket is initially full.
	
P19.	 Consider P18 and suppose now that r = 3 and that b = 2 as before. Will 
your answer to the question above change?
	
P20.	 Consider the leaky bucket policer that polices the average rate and burst size 
of a packet flow. We now want to police the peak rate, p, as well. Show how 
the output of this leaky bucket policer can be fed into a second leaky bucket 
policer so that the two leaky buckets in series police the average rate, peak 
rate, and burst size. Be sure to give the bucket size and token generation rate 
for the second policer.
	
P21.	 A packet flow is said to conform to a leaky bucket specification (r, b) with 
burst size b and average rate r if the number of packets that arrive to the 
leaky bucket is less than rt + b packets in every interval of time of length t 
for all t. Will a packet flow that conforms to a leaky bucket specification  
(r, b) ever have to wait at a leaky bucket policer with parameters r and b? 
Justify your answer.
	
P22.	  Show that as long as r1 6 Rw1>(gwj), then dmax is indeed the maximum 
delay that any packet in flow 1 will ever experience in the WFQ queue.
Programming Assignment
In this lab, you will implement a streaming video server and client. The client will 
use the real-time streaming protocol (RTSP) to control the actions of the server. The 
server will use the real-time protocol (RTP) to packetize the video for transport over 
UDP. You will be given Python code that partially implements RTSP and RTP at 
the client and server. Your job will be to complete both the client and server code. 
764         Chapter 9    •    Multimedia Networking
When you are finished, you will have created a client-server application that does 
the following:
•	 The client sends SETUP, PLAY, PAUSE, and TEARDOWN RTSP commands, 
and the server responds to the commands.
•	 When the server is in the playing state, it periodically grabs a stored JPEG frame, 
packetizes the frame with RTP, and sends the RTP packet into a UDP socket.
•	 The client receives the RTP packets, removes the JPEG frames, decompresses the 
frames, and renders the frames on the client’s monitor.
The code you will be given implements the RTSP protocol in the server and 
the RTP depacketization in the client. The code also takes care of displaying the 
transmitted video. You will need to implement RTSP in the client and RTP server. 
This programming assignment will significantly enhance the student’s understand-
ing of RTP, RTSP, and streaming video. It is highly recommended. The assignment 
also suggests a number of optional exercises, including implementing the RTSP 
DESCRIBE command at both client and server. You can find full details of the 
assignment, as well as an overview of the RTSP protocol, at the Web site www 
.pearsonglobaleditions.com/kurose.
765
What made you decide to specialize in multimedia networking?
This happened almost by accident. As a PhD student, I got involved with DARTnet, an 
experimental network spanning the United States with T1 lines. DARTnet was used as a 
proving ground for multicast and Internet real-time tools. That led me to write my first 
audio tool, NeVoT. Through some of the DARTnet participants, I became involved in the 
IETF, in the then-nascent Audio Video Transport working group. This group later ended up 
standardizing RTP.
What was your first job in the computer industry? What did it entail?
My first job in the computer industry was soldering together an Altair computer kit when I 
was a high school student in Livermore, California. Back in Germany, I started a little con-
sulting company that devised an address management program for a travel agency—storing 
data on cassette tapes for our TRS-80 and using an IBM Selectric typewriter with a home-
brew hardware interface as a printer.
My first real job was with AT&T Bell Laboratories, developing a network emulator for 
constructing experimental networks in a lab environment.
What are the goals of the Internet Real-Time Lab?
Our goal is to provide components and building blocks for the Internet as the single future 
communications infrastructure. This includes developing new protocols, such as GIST 
(for network-layer signaling) and LoST (for finding resources by location), or enhancing 
protocols that we have worked on earlier, such as SIP, through work on rich presence, peer-
to-peer systems, next-generation emergency calling, and service creation tools. Recently, 
we have also looked extensively at wireless systems for VoIP, as 802.11b and 802.11n net-
works and maybe WiMax networks are likely to become important last-mile technologies 
for telephony. We are also trying to greatly improve the ability of users to diagnose faults 
in the complicated tangle of providers and equipment, using a peer-to-peer fault diagnosis 
system called DYSWIS (Do You See What I See).
Henning Schulzrinne is a professor, chair of the Department of 
Computer Science, and head of the Internet Real-Time Laboratory 
at Columbia University. He is the co-author of RTP, RTSP, SIP, and 
GIST—key protocols for audio and video communications over 
the Internet. Henning received his BS in electrical and industrial 
engineering at TU Darmstadt in Germany, his MS in electrical and 
computer engineering at the University of Cincinnati, and his PhD in 
electrical engineering at the University of Massachusetts, Amherst.
Henning Schulzrinne
AN INTERVIEW WITH . . .
766
We try to do practically relevant work, by building prototypes and open source sys-
tems, by measuring performance of real systems, and by contributing to IETF standards.
What is your vision for the future of multimedia networking?
We are now in a transition phase; just a few years shy of when IP will be the universal plat-
form for multimedia services, from IPTV to VoIP. We expect radio, telephone, and TV to 
be available even during snowstorms and earthquakes, so when the Internet takes over the 
role of these dedicated networks, users will expect the same level of reliability.
We will have to learn to design network technologies for an ecosystem of compet-
ing carriers, service and content providers, serving lots of technically untrained users 
and defending them against a small, but destructive, set of malicious and criminal users. 
Changing protocols is becoming increasingly hard. They are also becoming more complex, 
as they need to take into account competing business interests, security, privacy, and the 
lack of transparency of networks caused by firewalls and network address translators.
Since multimedia networking is becoming the foundation for almost all of consumer 
entertainment, there will be an emphasis on managing very large networks, at low cost. 
Users will expect ease of use, such as finding the same content on all of their devices.
Why does SIP have a promising future?
As the current wireless network upgrade to 3G networks proceeds, there is the hope of 
a single multimedia signaling mechanism spanning all types of networks, from cable 
modems, to corporate telephone networks and public wireless networks. Together with 
software radios, this will make it possible in the future that a single device can be used 
on a home network, as a cordless BlueTooth phone, in a corporate network via 802.11 
and in the wide area via 3G networks. Even before we have such a single universal wire-
less device, the personal mobility mechanisms make it possible to hide the differences 
between networks. One identifier becomes the universal means of reaching a person, 
rather than remembering or passing around half a dozen technology- or location-specific 
telephone numbers.
SIP also breaks apart the provision of voice (bit) transport from voice services. It now 
becomes technically possible to break apart the local telephone monopoly, where one com-
pany provides neutral bit transport, while others provide IP “dial tone” and the classical 
telephone services, such as gateways, call forwarding, and caller ID.
Beyond multimedia signaling, SIP offers a new service that has been missing in the 
Internet: event notification. We have approximated such services with HTTP kludges and 
e-mail, but this was never very satisfactory. Since events are a common abstraction for dis-
tributed systems, this may simplify the construction of new services.
767
Do you have any advice for students entering the networking field?
Networking bridges disciplines. It draws from electrical engineering, all aspects of computer 
science, operations research, statistics, economics, and other disciplines. Thus, networking 
researchers have to be familiar with subjects well beyond protocols and routing algorithms.
Given that networks are becoming such an important part of everyday life, students wanting 
to make a difference in the field should think of the new resource constraints in networks: 
human time and effort, rather than just bandwidth or storage.
Work in networking research can be immensely satisfying since it is about allowing 
people to communicate and exchange ideas, one of the essentials of being human. The 
Internet has become the third major global infrastructure, next to the transportation system 
and energy distribution. Almost no part of the economy can work without high-performance 
networks, so there should be plenty of opportunities for the foreseeable future.
This page intentionally left blank
769
References
A note on URLs. In the references below, we have provided URLs for Web pages, 
Web-only documents, and other material that has not been published in a confer-
ence or journal (when we have been able to locate a URL for such material). We 
have not provided URLs for conference and journal publications, as these docu-
ments can usually be located via a search engine, from the conference Web site 
(e.g., papers in all ACM SIGCOMM conferences and workshops can be located via 
http://www.acm.org/sigcomm), or via a digital library subscription. While all URLs 
provided below were valid (and tested) in Jan. 2016, URLs can become out of date. 
Please consult the online version of this book (www.pearsonglobaleditions.com/
kurose) for an up-to-date bibliography.
A note on Internet Request for Comments (RFCs): Copies of Internet RFCs are 
available at many sites. The RFC Editor of the Internet Society (the body that over-
sees the RFCs) maintains the site, http://www.rfc-editor.org. This site allows you to 
search for a specific RFC by title, number, or authors, and will show updates to any 
RFCs listed. Internet RFCs can be updated or obsoleted by later RFCs. Our favorite 
site for getting RFCs is the original source—http://www.rfc-editor.org.
[3GPP 2016] Third Generation Partnership Project homepage, http://www.3gpp.org/
[Abramson 1970] N. Abramson, “The Aloha System—Another Alternative for 
Computer Communications,” Proc. 1970 Fall Joint Computer Conference, AFIPS 
Conference, p. 37, 1970.
[Abramson 1985] N. Abramson, “Development of the Alohanet,” IEEE Transac-
tions on Information Theory, Vol. IT-31, No. 3 (Mar. 1985), pp. 119–123.
[Abramson 2009] N. Abramson, “The Alohanet—Surfing for Wireless Data,” 
IEEE Communications Magazine, Vol. 47, No. 12, pp. 21–25.
[Adhikari 2011a] V. K. Adhikari, S. Jain, Y. Chen, Z. L. Zhang, “Vivisecting 
YouTube: An Active Measurement Study,” Technical Report, University of  
Minnesota, 2011.
[Adhikari 2012] V. K. Adhikari, Y. Gao, F. Hao, M. Varvello, V. Hilt, M. Steiner, 
Z. L. Zhang, “Unreeling Netflix: Understanding and Improving Multi-CDN Movie 
Delivery,” Technical Report, University of Minnesota, 2012.
[Afanasyev 2010] A. Afanasyev, N. Tilley, P. Reiher, L. Kleinrock, “Host-to-Host  
Congestion Control for TCP,” IEEE Communications Surveys & Tutorials, Vol. 12, 
No. 3, pp. 304–342.
770         References
[Agarwal 2009] S. Agarwal, J. Lorch, “Matchmaking for Online Games and Other 
Latency-sensitive P2P Systems,” Proc. 2009 ACM SIGCOMM.
[Ager 2012] B. Ager, N. Chatzis, A. Feldmann, N. Sarrar, S. Uhlig, W. Willinger, 
“Anatomy of a Large European ISP,” Sigcomm, 2012.
[Ahn 1995] J. S. Ahn, P. B. Danzig, Z. Liu, and Y. Yan, “Experience with TCP 
Vegas: Emulation and Experiment,” Proc. 1995 ACM SIGCOMM (Boston, MA, 
Aug. 1995), pp. 185–195.
[Akamai 2016] Akamai homepage, http://www.akamai.com
[Akella 2003] A. Akella, S. Seshan, A. Shaikh, “An Empirical Evaluation of Wide-
Area Internet Bottlenecks,” Proc. 2003 ACM Internet Measurement Conference 
(Miami, FL, Nov. 2003).
[Akhshabi 2011] S. Akhshabi, A. C. Begen, C. Dovrolis, “An Experimental Evalu-
ation of Rate-Adaptation Algorithms in Adaptive Streaming over HTTP,” Proc. 2011 
ACM Multimedia Systems Conf.
[Akyildiz 2010] I. Akyildiz, D. Gutierrex-Estevez, E. Reyes, “The Evolution to 4G 
Cellular Systems, LTE Advanced,” Physical Communication, Elsevier, 3 (2010), 
217–244.
[Albitz 1993] P. Albitz and C. Liu, DNS and BIND, O’Reilly & Associates, Petaluma, 
CA, 1993.
[Al-Fares 2008] M. Al-Fares, A. Loukissas, A. Vahdat, “A Scalable, Commodity 
Data Center Network Architecture,” Proc. 2008 ACM SIGCOMM.
[Amazon 2014] J. Hamilton, “AWS: Innovation at Scale, YouTube video, https://
www.youtube.com/watch?v=JIQETrFC_SQ
[Anderson 1995] J. B. Andersen, T. S. Rappaport, S. Yoshida, “Propagation Mea-
surements and Models for Wireless Communications Channels,” IEEE Communi-
cations Magazine, (Jan. 1995), pp. 42–49.
[Alizadeh 2010] M. Alizadeh, A. Greenberg, D. Maltz, J. Padhye, P. Patel,  
B. Prabhakar, S. Sengupta, M. Sridharan. “Data center TCP (DCTCP),” ACM  
SIGCOMM 2010 Conference, ACM, New York, NY, USA, pp. 63–74.
[Allman 2011] E. Allman, “The Robustness Principle Reconsidered: Seeking a 
Middle Ground,” Communications of the ACM, Vol. 54, No. 8 (Aug. 2011), pp. 
40–45.
[Appenzeller 2004] G. Appenzeller, I. Keslassy, N. McKeown, “Sizing Router 
Buffers,” Proc. 2004 ACM SIGCOMM (Portland, OR, Aug. 2004).
[ASO-ICANN 2016] The Address Supporting Organization homepage,  
http://www.aso.icann.org
References         771
[AT&T 2013] “AT&T Vision Alignment Challenge Technology Survey,” AT&T 
Domain 2.0 Vision White Paper, November 13, 2013.
[Atheros 2016] Atheros Communications Inc., “Atheros AR5006 WLAN Chipset 
Product Bulletins,” http://www.atheros.com/pt/AR5006Bulletins.htm
[Ayanoglu 1995] E. Ayanoglu, S. Paul, T. F. La Porta, K. K. Sabnani, R. D. Gitlin, 
“AIRMAIL: A Link-Layer Protocol for Wireless Networks,” ACM ACM/Baltzer 
Wireless Networks Journal, 1: 47–60, Feb. 1995.
[Bakre 1995] A. Bakre, B. R. Badrinath, “I-TCP: Indirect TCP for Mobile Hosts,” 
Proc. 1995 Int. Conf. on Distributed Computing Systems (ICDCS) (May 1995),  
pp. 136–143.
[Balakrishnan 1997] H. Balakrishnan, V. Padmanabhan, S. Seshan, R. Katz, 
“A Comparison of Mechanisms for Improving TCP Performance Over Wireless 
Links,” IEEE/ACM Transactions on Networking Vol. 5, No. 6 (Dec. 1997).
[Balakrishnan 2003] H. Balakrishnan, F. Kaashoek, D. Karger, R. Morris, I. 
Stoica, “Looking Up Data in P2P Systems,” Communications of the ACM, Vol. 46, 
No. 2 (Feb. 2003), pp. 43–48.
[Baldauf 2007] M. Baldauf, S. Dustdar, F. Rosenberg, “A Survey on Context-
Aware Systems,” Int. J. Ad Hoc and Ubiquitous Computing, Vol. 2, No. 4 (2007), 
pp. 263–277.
[Baran 1964] P. Baran, “On Distributed Communication Networks,” IEEE Trans-
actions on Communication Systems, Mar. 1964. Rand Corporation Technical report 
with the same title (Memorandum RM-3420-PR, 1964). http://www.rand.org/publi-
cations/RM/RM3420/
[Bardwell 2004] J. Bardwell, “You Believe You Understand What You Think I 
Said . . . The Truth About 802.11 Signal and Noise Metrics: A Discussion Clarify-
ing Often-Misused 802.11 WLAN Terminologies,” http://www.connect802.com/
download/techpubs/2004/you_believe_D100201.pdf
[Barford 2009] P. Barford, N. Duffield, A. Ron, J. Sommers, “Network Perfor-
mance Anomaly Detection and Localization,” Proc. 2009 IEEE INFOCOM  
(Apr. 2009).
[Baronti 2007] P. Baronti, P. Pillai, V. Chook, S. Chessa, A. Gotta, Y. Hu,  
“Wireless Sensor Networks: A Survey on the State of the Art and the 802.15.4 
and ZigBee Standards,” Computer Communications, Vol. 30, No. 7 (2007), pp. 
1655–1695.
[Baset 2006] S. A. Basset and H. Schulzrinne, “An Analysis of the Skype Peer-to-
Peer Internet Telephony Protocol,” Proc. 2006 IEEE INFOCOM (Barcelona, Spain, 
Apr. 2006).
772         References
[BBC 2001] BBC news online “A Small Slice of Design,” Apr. 2001, http://news.
bbc.co.uk/2/hi/science/nature/1264205.stm
[Beheshti 2008] N. Beheshti, Y. Ganjali, M. Ghobadi, N. McKeown, G. Salmon,  
“Experimental Study of Router Buffer Sizing,” Proc. ACM Internet Measurement  
Conference (Oct. 2008, Vouliagmeni, Greece).
[Bender 2000] P. Bender, P. Black, M. Grob, R. Padovani, N. Sindhushayana, A. 
Viterbi, “CDMA/HDR: A Bandwidth-Efficient High-Speed Wireless Data Service 
for Nomadic Users,” IEEE Commun. Mag., Vol. 38, No. 7 (July 2000),  
pp. 70–77.
[Berners-Lee 1989] T. Berners-Lee, CERN, “Information Management: A  
Proposal,” Mar. 1989, May 1990. http://www.w3.org/History/1989/proposal 
.html
[Berners-Lee 1994] T. Berners-Lee, R. Cailliau, A. Luotonen, H. Frystyk Nielsen,  
A. Secret, “The World-Wide Web,” Communications of the ACM, Vol. 37, No. 8  
(Aug. 1994), pp. 76–82.
[Bertsekas 1991] D. Bertsekas, R. Gallagher, Data Networks, 2nd Ed., Prentice 
Hall, Englewood Cliffs, NJ, 1991.
[Biersack 1992] E. W. Biersack, “Performance Evaluation of Forward Error Cor-
rection in ATM Networks,” Proc. 1999 ACM SIGCOMM (Baltimore, MD, Aug. 
1992), pp. 248–257.
[BIND 2016] Internet Software Consortium page on BIND, http://www.isc.org/
bind.html
[Bisdikian 2001] C. Bisdikian, “An Overview of the Bluetooth Wireless Technol-
ogy,” IEEE Communications Magazine, No. 12 (Dec. 2001), pp. 86–94.
[Bishop 2003] M. Bishop, Computer Security: Art and Science, Boston: Addison 
Wesley, Boston MA, 2003.
[Black 1995] U. Black, ATM Volume I: Foundation for Broadband Networks, 
Prentice Hall, 1995.
[Black 1997] U. Black, ATM Volume II: Signaling in Broadband Networks, Prentice 
Hall, 1997.
[Blumenthal 2001] M. Blumenthal, D. Clark, “Rethinking the Design of the  
Internet: The End-to-end Arguments vs. the Brave New World,” ACM Transactions 
on Internet Technology, Vol. 1, No. 1 (Aug. 2001), pp. 70–109.
[Bochman 1984] G. V. Bochmann, C. A. Sunshine, “Formal Methods in Commu-
nication Protocol Design,” IEEE Transactions on Communications, Vol. 28, No. 4 
(Apr. 1980) pp. 624–631.
References         773
[Bolot 1996] J-C. Bolot, A. Vega-Garcia, “Control Mechanisms for Packet Audio 
in the Internet,” Proc. 1996 IEEE INFOCOM, pp. 232–239.
[Bosshart 2013] P. Bosshart, G. Gibb, H. Kim, G. Varghese, N. McKeown,  
M. Izzard, F. Mujica, M. Horowitz, “Forwarding Metamorphosis: Fast Program-
mable Match-Action Processing in Hardware for SDN,” ACM SIGCOMM Comput.  
Commun. Rev. 43, 4 (Aug. 2013), 99–110.
[Bosshart 2014] P. Bosshart, D. Daly, G. Gibb, M. Izzard, N. McKeown,  
J. Rexford, C. Schlesinger, D. Talayco, A. Vahdat, G. Varghese, D. Walker,  
“P4: Programming Protocol-Independent Packet Processors,” ACM SIGCOMM 
Comput. Commun. Rev. 44, 3 (July 2014), pp. 87–95.
[Brakmo 1995] L. Brakmo, L. Peterson, “TCP Vegas: End to End Congestion 
Avoidance on a Global Internet,” IEEE Journal of Selected Areas in Communica-
tions, Vol. 13, No. 8 (Oct. 1995), pp. 1465–1480.
[Bryant 1988] B. Bryant, “Designing an Authentication System: A Dialogue in 
Four Scenes,” http://web.mit.edu/kerberos/www/dialogue.html
[Bush 1945] V. Bush, “As We May Think,” The Atlantic Monthly, July 1945. 
http://www.theatlantic.com/unbound/flashbks/computer/bushf.htm
[Byers 1998] J. Byers, M. Luby, M. Mitzenmacher, A. Rege, “A Digital Fountain 
Approach to Reliable Distribution of Bulk Data,” Proc. 1998 ACM SIGCOMM 
(Vancouver, Canada, Aug. 1998), pp. 56–67.
[Caesar 2005a] M. Caesar, D. Caldwell, N. Feamster, J. Rexford, A. Shaikh, J. van 
der Merwe, “Design and implementation of a Routing Control Platform,” Proc. 
Networked Systems Design and Implementation (May 2005).
[Caesar 2005b] M. Caesar, J. Rexford, “BGP Routing Policies in ISP Networks,” 
IEEE Network Magazine, Vol. 19, No. 6 (Nov. 2005).
[Caldwell 2012] C. Caldwell, “The Prime Pages,” http://www.utm.edu/research/
primes/prove
[Cardwell 2000] N. Cardwell, S. Savage, T. Anderson, “Modeling TCP Latency,” 
Proc. 2000 IEEE INFOCOM (Tel-Aviv, Israel, Mar. 2000).
[Casado 2007] M. Casado, M. Freedman, J. Pettit, J. Luo, N. McKeown, S. Shen-
ker, “Ethane: Taking Control of the Enterprise,” Proc. ACM SIGCOMM ’07, New 
York, pp. 1–12. See also IEEE/ACM Trans. Networking, 17, 4 (Aug. 2007), pp. 
270–1283.
[Casado 2009] M. Casado, M. Freedman, J. Pettit, J. Luo, N. Gude, N. McKeown,  
S. Shenker, “Rethinking Enterprise Network Control,” IEEE/ACM Transactions on 
Networking (ToN), Vol. 17, No. 4 (Aug. 2009), pp. 1270–1283.
774         References
[Casado 2014] M. Casado, N. Foster, A. Guha, “Abstractions for Software- 
Defined Networks,” Communications of the ACM, Vol. 57 No. 10, (Oct. 2014),  
pp. 86–95.
[Cerf 1974] V. Cerf, R. Kahn, “A Protocol for Packet Network Interconnection,” 
IEEE Transactions on Communications Technology, Vol. COM-22, No. 5, pp. 
627–641.
[CERT 2001–09] CERT, “Advisory 2001–09: Statistical Weaknesses in TCP/IP 
Initial Sequence Numbers,” http://www.cert.org/advisories/CA-2001-09.html
[CERT 2003–04] CERT, “CERT Advisory CA-2003-04 MS-SQL Server Worm,” 
http://www.cert.org/advisories/CA-2003-04.html
[CERT 2016] CERT, http://www.cert.org
[CERT Filtering 2012] CERT, “Packet Filtering for Firewall Systems,” http://
www.cert.org/tech_tips/packet_filtering.html
[Cert SYN 1996] CERT, “Advisory CA-96.21: TCP SYN Flooding and IP Spoof-
ing Attacks,” http://www.cert.org/advisories/CA-1998-01.html
[Chandra 2007] T. Chandra, R. Greisemer, J. Redstone, “Paxos Made Live: an 
Engineering Perspective,” Proc. of 2007 ACM Symposium on Principles of Distrib-
uted Computing (PODC), pp. 398–407.
[Chao 2001] H. J. Chao, C. Lam, E. Oki, Broadband Packet Switching Technol-
ogies—A Practical Guide to ATM Switches and IP Routers, John Wiley & Sons, 
2001.
[Chao 2011] C. Zhang, P. Dunghel, D. Wu, K. W. Ross, “Unraveling the  
BitTorrent Ecosystem,” IEEE Transactions on Parallel and Distributed Systems, 
Vol. 22, No. 7 (July 2011).
[Chen 2000] G. Chen, D. Kotz, “A Survey of Context-Aware Mobile Computing 
Research,” Technical Report TR2000-381, Dept. of Computer Science, Dartmouth 
College, Nov. 2000. http://www.cs.dartmouth.edu/reports/TR2000-381.pdf
[Chen 2006] K.-T. Chen, C.-Y. Huang, P. Huang, C.-L. Lei, “Quantifying Skype 
User Satisfaction,” Proc. 2006 ACM SIGCOMM (Pisa, Italy, Sept. 2006).
[Chen 2011] Y. Chen, S. Jain, V. K. Adhikari, Z. Zhang, “Characterizing Roles  
of Front-End Servers in End-to-End Performance of Dynamic Content Distribu-
tion,” Proc. 2011 ACM Internet Measurement Conference (Berlin, Germany,  
Nov. 2011).
[Cheswick 2000] B. Cheswick, H. Burch, S. Branigan, “Mapping and Visualizing 
the Internet,” Proc. 2000 Usenix Conference (San Diego, CA, June 2000).
References         775
[Chiu 1989] D. Chiu, R. Jain, “Analysis of the Increase and Decrease Algorithms 
for Congestion Avoidance in Computer Networks,” Computer Networks and ISDN 
Systems, Vol. 17, No. 1, pp. 1–14. http://www.cs.wustl.edu/~jain/papers/cong_
av.htm
[Christiansen 2001] M. Christiansen, K. Jeffay, D. Ott, F. D. Smith, “Tuning Red 
for Web Traffic,” IEEE/ACM Transactions on Networking, Vol. 9, No. 3 (June 
2001), pp. 249–264.
[Chuang 2005] S. Chuang, S. Iyer, N. McKeown, “Practical Algorithms for Perfor-
mance Guarantees in Buffered Crossbars,” Proc. 2005 IEEE INFOCOM.
[Cisco 802.11ac 2014] Cisco Systems, “802.11ac: The Fifth Generation of Wi-Fi,”  
Technical White Paper, Mar. 2014.
[Cisco 7600 2016] Cisco Systems, “Cisco 7600 Series Solution and Design Guide,” 
 
http://www.cisco.com/en/US/products/hw/routers/ps368/prod_technical_ 
reference09186a0080092246.html
[Cisco 8500 2012] Cisco Systems Inc., “Catalyst 8500 Campus Switch Router  
Architecture,” http://www.cisco.com/univercd/cc/td/doc/product/l3sw/8540/
rel_12_0/w5_6f/softcnfg/1cfg8500.pdf
[Cisco 12000 2016] Cisco Systems Inc., “Cisco XR 12000 Series and Cisco 12000 
Series Routers,” http://www.cisco.com/en/US/products/ps6342/index.html
[Cisco 2012] Cisco 2012, Data Centers, http://www.cisco.com/go/dce
[Cisco 2015] Cisco Visual Networking Index: Forecast and Methodology, 2014–
2019, White Paper, 2015.
[Cisco 6500 2016] Cisco Systems, “Cisco Catalyst 6500 Architecture White  
Paper,” http://www.cisco.com/c/en/us/products/collateral/switches/ 
catalyst-6500-series-switches/prod_white_paper0900aecd80673385.html
[Cisco NAT 2016] Cisco Systems Inc., “How NAT Works,” http://www.cisco.
com/en/US/tech/tk648/tk361/technologies_tech_note09186a0080094831.shtml
[Cisco QoS 2016] Cisco Systems Inc., “Advanced QoS Services for the Intelligent 
Internet,” http://www.cisco.com/warp/public/cc/pd/iosw/ioft/ioqo/tech/qos_wp.htm
[Cisco Queue 2016] Cisco Systems Inc., “Congestion Management Overview,” 
http://www.cisco.com/en/US/docs/ios/12_2/qos/configuration/guide/qcfconmg.
html
[Cisco SYN 2016] Cisco Systems Inc., “Defining Strategies to Protect Against 
TCP SYN Denial of Service Attacks,” http://www.cisco.com/en/US/tech/tk828/
technologies_tech_note09186a00800f67d5.shtml
776         References
[Cisco TCAM 2014] Cisco Systems Inc., “CAT 6500 and 7600 Series Routers and 
Switches TCAM Allocation Adjustment Procedures,” http://www.cisco.com/c/en/
us/support/docs/switches/catalyst-6500-series-switches/117712-problemsolution-
cat6500-00.html
[Cisco VNI 2015] Cisco Systems Inc., “Visual Networking Index,” http://www.
cisco.com/web/solutions/sp/vni/vni_forecast_highlights/index.html
[Clark 1988] D. Clark, “The Design Philosophy of the DARPA Internet Proto-
cols,” Proc. 1988 ACM SIGCOMM (Stanford, CA, Aug. 1988).
[Cohen 1977] D. Cohen, “Issues in Transnet Packetized Voice Communication,” 
Proc. Fifth Data Communications Symposium (Snowbird, UT, Sept. 1977),  
pp. 6–13.
[Cookie Central 2016] Cookie Central homepage, http://www.cookiecentral.com/ 
n_cookie_faq.htm
[Cormen 2001] T. H. Cormen, Introduction to Algorithms, 2nd Ed., MIT Press, 
Cambridge, MA, 2001.
[Crow 1997] B. Crow, I. Widjaja, J. Kim, P. Sakai, “IEEE 802.11 Wireless  
Local Area Networks,” IEEE Communications Magazine (Sept. 1997),  
pp. 116–126.
[Cusumano 1998] M. A. Cusumano, D. B. Yoffie, Competing on Internet Time: 
Lessons from Netscape and Its Battle with Microsoft, Free Press, New York, NY, 
1998.
[Czyz 2014] J. Czyz, M. Allman, J. Zhang, S. Iekel-Johnson, E. Osterweil, M. Bai-
ley, “Measuring IPv6 Adoption,” Proc. ACM SIGCOMM 2014, ACM, New York, 
NY, USA, pp. 87–98.
[Dahlman 1998] E. Dahlman, B. Gudmundson, M. Nilsson, J. Sköld, “UMTS/
IMT-2000 Based on Wideband CDMA,” IEEE Communications Magazine (Sept. 
1998), pp. 70–80.
[Daigle 1991] J. N. Daigle, Queuing Theory for Telecommunications, Addison-
Wesley, Reading, MA, 1991.
[DAM 2016] Digital Attack Map, http://www.digitalattackmap.com
[Davie 2000] B. Davie and Y. Rekhter, MPLS: Technology and Applications,  
Morgan Kaufmann Series in Networking, 2000.
[Davies 2005] G. Davies, F. Kelly, “Network Dimensioning, Service Costing, and  
Pricing in a Packet-Switched Environment,” Telecommunications Policy, Vol. 28, 
No. 4, pp. 391–412.
References         777
[DEC 1990] Digital Equipment Corporation, “In Memoriam: J. C. R. Licklider 
1915–1990,” SRC Research Report 61, Aug. 1990. http://www.memex.org/ 
licklider.pdf
[DeClercq 2002] J. DeClercq, O. Paridaens, “Scalability Implications of Virtual 
Private Networks,” IEEE Communications Magazine, Vol. 40, No. 5 (May 2002), 
pp. 151–157.
[Demers 1990] A. Demers, S. Keshav, S. Shenker, “Analysis and Simulation of a 
Fair Queuing Algorithm,” Internetworking: Research and Experience, Vol. 1, No. 1 
(1990), pp. 3–26.
[dhc 2016] IETF Dynamic Host Configuration working group homepage, http://
www.ietf.org/html.charters/dhc-charter.html
[Dhungel 2012] P. Dhungel, K. W. Ross, M. Steiner., Y. Tian, X. Hei, “Xunlei: 
Peer-Assisted Download Acceleration on a Massive Scale,” Passive and Active 
Measurement Conference (PAM) 2012, Vienna, 2012.
[Diffie 1976] W. Diffie, M. E. Hellman, “New Directions in Cryptography,” IEEE 
Transactions on Information Theory, Vol IT-22 (1976), pp. 644–654.
[Diggavi 2004] S. N. Diggavi, N. Al-Dhahir, A. Stamoulis, R. Calderbank, “Great 
Expectations: The Value of Spatial Diversity in Wireless Networks,” Proceedings 
of the IEEE, Vol. 92, No. 2 (Feb. 2004).
[Dilley 2002] J. Dilley, B. Maggs, J. Parikh, H. Prokop, R. Sitaraman, B. Weihl, 
“Globally Distributed Content Delivert,” IEEE Internet Computing (Sept.–Oct. 
2002).
[Diot 2000] C. Diot, B. N. Levine, B. Lyles, H. Kassem, D. Balensiefen, “Deploy-
ment Issues for the IP Multicast Service and Architecture,” IEEE Network, Vol. 14, 
No. 1 (Jan./Feb. 2000) pp. 78–88.
[Dischinger 2007] M. Dischinger, A. Haeberlen, K. Gummadi, S. Saroiu, “Charac-
terizing residential broadband networks,” Proc. 2007 ACM Internet Measurement 
Conference, pp. 24–26.
[Dmitiropoulos 2007] X. Dmitiropoulos, D. Krioukov, M. Fomenkov, B. Huffaker,  
Y. Hyun, K. C. Claffy, G. Riley, “AS Relationships: Inference and Validation,” 
ACM Computer Communication Review (Jan. 2007).
[DOCSIS 2011] Data-Over-Cable Service Interface Specifications, DOCSIS 3.0: 
MAC and Upper Layer Protocols Interface Specification, CM-SP-MULPIv3.0-
I16-110623, 2011.
[Dodge 2016] M. Dodge, “An Atlas of Cyberspaces,” http://www.cybergeography.
org/atlas/isp_maps.html
778         References
[Donahoo 2001] M. Donahoo, K. Calvert, TCP/IP Sockets in C: Practical Guide 
for Programmers, Morgan Kaufman, 2001.
[DSL 2016] DSL Forum homepage, http://www.dslforum.org/
[Dhunghel 2008] P. Dhungel, D. Wu, B. Schonhorst, K.W. Ross, “A Measurement 
Study of Attacks on BitTorrent Leechers,” 7th International Workshop on Peer-to-
Peer Systems (IPTPS 2008) (Tampa Bay, FL, Feb. 2008).
[Droms 2002] R. Droms, T. Lemon, The DHCP Handbook (2nd Edition), SAMS 
Publishing, 2002.
[Edney 2003] J. Edney and W. A. Arbaugh, Real 802.11 Security: Wi-Fi Protected 
Access and 802.11i, Addison-Wesley Professional, 2003.
[Edwards 2011] W. K. Edwards, R. Grinter, R. Mahajan, D. Wetherall, “Advancing 
the State of Home Networking,” Communications of the ACM, Vol. 54, No. 6 (June 
2011), pp. 62–71.
[Ellis 1987] H. Ellis, “The Story of Non-Secret Encryption,” http://jya.com/ellis-
doc.htm
[Erickson 2013] D. Erickson, “ The Beacon Openflow Controller,” 2nd ACM SIG-
COMM Workshop on Hot Topics in Software Defined Networking (HotSDN ’13). 
ACM, New York, NY, USA, pp. 13–18.
[Ericsson 2012] Ericsson, “The Evolution of Edge,” http://www.ericsson.com/
technology/whitepapers/broadband/evolution_of_EDGE.shtml
[Facebook 2014] A. Andreyev, “Introducing Data Center Fabric, the Next-
Generation Facebook Data Center Network,” https://code.facebook.com/
posts/360346274145943/introducing-data-center-fabric-the-next-generation-face-
book-data-center-network
[Faloutsos 1999] C. Faloutsos, M. Faloutsos, P. Faloutsos, “What Does the Internet 
Look Like? Empirical Laws of the Internet Topology,” Proc. 1999 ACM SIG-
COMM (Boston, MA, Aug. 1999).
[Farrington 2010] N. Farrington, G. Porter, S. Radhakrishnan, H. Bazzaz, V. Sub-
ramanya, Y. Fainman, G. Papen, A. Vahdat, “Helios: A Hybrid Electrical/Optical 
Switch Architecture for Modular Data Centers,” Proc. 2010 ACM SIGCOMM.
[Feamster 2004] N. Feamster, H. Balakrishnan, J. Rexford, A. Shaikh, K. van der 
Merwe, “The Case for Separating Routing from Routers,” ACM SIGCOMM Work-
shop on Future Directions in Network Architecture, Sept. 2004.
[Feamster 2004] N. Feamster, J. Winick, J. Rexford, “A Model for BGP Routing 
for Network Engineering,” Proc. 2004 ACM SIGMETRICS (New York, NY, June 
2004).
References         779
[Feamster 2005] N. Feamster, H. Balakrishnan, “Detecting BGP Configuration 
Faults with Static Analysis,” NSDI (May 2005).
[Feamster 2013] N. Feamster, J. Rexford, E. Zegura, “The Road to SDN,” ACM 
Queue, Volume 11, Issue 12, (Dec. 2013).
[Feldmeier 1995] D. Feldmeier, “Fast Software Implementation of Error Detection 
Codes,” IEEE/ACM Transactions on Networking, Vol. 3, No. 6 (Dec. 1995), pp. 
640–652.
[Ferguson 2013] A. Ferguson, A. Guha, C. Liang, R. Fonseca, S. Krishnamurthi, 
“Participatory Networking: An API for Application Control of SDNs,” Proceedings 
ACM SIGCOMM 2013, pp. 327–338.
[Fielding 2000] R. Fielding, “Architectural Styles and the Design of Network-
based Software Architectures,” 2000. PhD Thesis, UC Irvine, 2000.
[FIPS 1995] Federal Information Processing Standard, “Secure Hash Standard,” 
FIPS Publication 180-1. http://www.itl.nist.gov/fipspubs/fip180-1.htm
[Floyd 1999] S. Floyd, K. Fall, “Promoting the Use of End-to-End Congestion 
Control in the Internet,” IEEE/ACM Transactions on Networking, Vol. 6, No. 5 
(Oct. 1998), pp. 458–472.
[Floyd 2000] S. Floyd, M. Handley, J. Padhye, J. Widmer, “Equation-Based  
Congestion Control for Unicast Applications,” Proc. 2000 ACM SIGCOMM 
(Stockholm, Sweden, Aug. 2000).
[Floyd 2001] S. Floyd, “A Report on Some Recent Developments in TCP Conges-
tion Control,” IEEE Communications Magazine (Apr. 2001).
[Floyd 2016] S. Floyd, “References on RED (Random Early Detection) Queue 
Management,” http://www.icir.org/floyd/red.html
[Floyd Synchronization 1994] S. Floyd, V. Jacobson, “Synchronization of Peri-
odic Routing Messages,” IEEE/ACM Transactions on Networking, Vol. 2, No. 2 
(Apr. 1997) pp. 122–136.
[Floyd TCP 1994] S. Floyd, “TCP and Explicit Congestion Notification,” ACM 
SIGCOMM Computer Communications Review, Vol. 24, No. 5 (Oct. 1994), pp. 
10–23.
[Fluhrer 2001] S. Fluhrer, I. Mantin, A. Shamir, “Weaknesses in the Key Schedul-
ing Algorithm of RC4,” Eighth Annual Workshop on Selected Areas in Cryptogra-
phy (Toronto, Canada, Aug. 2002).
[Fortz 2000] B. Fortz, M. Thorup, “Internet Traffic Engineering by Optimizing 
OSPF Weights,” Proc. 2000 IEEE INFOCOM (Tel Aviv, Israel, Apr. 2000).
780         References
[Fortz 2002] B. Fortz, J. Rexford, M. Thorup, “Traffic Engineering with  
Traditional IP Routing Protocols,” IEEE Communication Magazine  
(Oct. 2002).
[Fraleigh 2003] C. Fraleigh, F. Tobagi, C. Diot, “Provisioning IP Backbone Net-
works to Support Latency Sensitive Traffic,” Proc. 2003 IEEE INFOCOM (San 
Francisco, CA, Mar. 2003).
[Frost 1994] J. Frost, “BSD Sockets: A Quick and Dirty Primer,” http://world.std 
.com/~jimf/papers/sockets/sockets.html
[FTC 2015] Internet of Things: Privacy and Security in a Connected World, Fed-
eral Trade Commission, 2015, https://www.ftc.gov/system/files/documents/reports/
federal-trade-commission-staff-report-november-2013-workshop-entitled-internet-
things-privacy/150127iotrpt.pdf
[FTTH 2016] Fiber to the Home Council, http://www.ftthcouncil.org/
[Gao 2001] L. Gao, J. Rexford, “Stable Internet Routing Without Global Coordi-
nation,” IEEE/ACM Transactions on Networking, Vol. 9, No. 6 (Dec. 2001), pp. 
681–692.
[Gartner 2014] Gartner report on Internet of Things, http://www.gartner.com/ 
technology/research/internet-of-things
[Gauthier 1999] L. Gauthier, C. Diot, and J. Kurose, “End-to-End Transmission 
Control Mechanisms for Multiparty Interactive Applications on the Internet,” Proc. 
1999 IEEE INFOCOM (New York, NY, Apr. 1999).
[Gember-Jacobson 2014] A. Gember-Jacobson, R. Viswanathan, C. Prakash,  
R. Grandl, J. Khalid, S. Das, A. Akella, “OpenNF: Enabling Innovation in Network 
Function Control,” Proc. ACM SIGCOMM 2014, pp. 163–174.
[Goodman 1997] David J. Goodman, Wireless Personal Communications Systems, 
Prentice-Hall, 1997.
[Google IPv6 2015] Google Inc. “IPv6 Statistics,” https://www.google.com/intl/en/
ipv6/statistics.html
[Google Locations 2016] Google data centers. http://www.google.com/corporate/
datacenter/locations.html
[Goralski 1999] W. Goralski, Frame Relay for High-Speed Networks, John Wiley, 
New York, 1999.
[Greenberg 2009a] A. Greenberg, J. Hamilton, D. Maltz, P. Patel, “The Cost of a 
Cloud: Research Problems in Data Center Networks,” ACM Computer Communica-
tions Review (Jan. 2009).
References         781
[Greenberg 2009b] A. Greenberg, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. 
Maltz, P. Patel, S. Sengupta, “VL2: A Scalable and Flexible Data Center Network,” 
Proc. 2009 ACM SIGCOMM.
[Greenberg 2011] A. Greenberg, J. Hamilton, N. Jain, S. Kandula, C. Kim,  
P. Lahiri, D. Maltz, P. Patel, S. Sengupta, “VL2: A Scalable and Flexible Data  
Center Network,” Communications of the ACM, Vol. 54, No. 3 (Mar. 2011),  
pp. 95–104.
[Greenberg 2015] A. Greenberg, “SDN for the Cloud,” Sigcomm 2015 Keynote 
Address, http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/keynote.pdf
[Griffin 2012] T. Griffin, “Interdomain Routing Links,” http://www.cl.cam.
ac.uk/~tgg22/interdomain/
[Gude 2008] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, N. McKeown, 
and S. Shenker, “NOX: Towards an Operating System for Networks,” ACM SIG-
COMM Computer Communication Review, July 2008.
[Guha 2006] S. Guha, N. Daswani, R. Jain, “An Experimental Study of the Skype 
Peer-to-Peer VoIP System,” Proc. Fifth Int. Workshop on P2P Systems (Santa 
Barbara, CA, 2006).
[Guo 2005] L. Guo, S. Chen, Z. Xiao, E. Tan, X. Ding, X. Zhang, “Measurement, 
Analysis, and Modeling of BitTorrent-Like Systems,” Proc. 2005 ACM Internet 
Measurement Conference.
[Guo 2009] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang,  
S. Lu, “BCube: A High Performance, Server-centric Network Architecture for 
Modular Data Centers,” Proc. 2009 ACM SIGCOMM.
[Gupta 2001] P. Gupta, N. McKeown, “Algorithms for Packet Classification,” 
IEEE Network Magazine, Vol. 15, No. 2 (Mar./Apr. 2001), pp. 24–32.
[Gupta 2014] A. Gupta, L. Vanbever, M. Shahbaz, S. Donovan, B. Schlinker,  
N. Feamster, J. Rexford, S. Shenker, R. Clark, E. Katz-Bassett, “SDX: A Software 
Defined Internet Exchange, “ Proc. ACM SIGCOMM 2014 (Aug. 2014),  
pp. 551–562.
[Ha 2008] S. Ha, I. Rhee, L. Xu, “CUBIC: A New TCP-Friendly High-Speed TCP  
Variant,” ACM SIGOPS Operating System Review, 2008.
[Halabi 2000] S. Halabi, Internet Routing Architectures, 2nd Ed., Cisco Press, 
2000.
[Hanabali 2005] A. A. Hanbali, E. Altman, P. Nain, “A Survey of TCP over Ad 
Hoc Networks,” IEEE Commun. Surveys and Tutorials, Vol. 7, No. 3 (2005),  
pp. 22–36.
782         References
[Hei 2007] X. Hei, C. Liang, J. Liang, Y. Liu, K. W. Ross, “A Measurement Study 
of a Large-scale P2P IPTV System,” IEEE Trans. on Multimedia (Dec. 2007).
[Heidemann 1997] J. Heidemann, K. Obraczka, J. Touch, “Modeling the Perfor-
mance of HTTP over Several Transport Protocols,” IEEE/ACM Transactions on 
Networking, Vol. 5, No. 5 (Oct. 1997), pp. 616–630.
[Held 2001] G. Held, Data Over Wireless Networks: Bluetooth, WAP, and Wireless 
LANs, McGraw-Hill, 2001.
[Holland 2001] G. Holland, N. Vaidya, V. Bahl, “A Rate-Adaptive MAC Protocol 
for Multi-Hop Wireless Networks,” Proc. 2001 ACM Int. Conference of Mobile 
Computing and Networking (Mobicom01) (Rome, Italy, July 2001).
[Hollot 2002] C.V. Hollot, V. Misra, D. Towsley, W. Gong, “Analysis and Design 
of Controllers for AQM Routers Supporting TCP Flows,” IEEE Transactions on 
Automatic Control, Vol. 47, No. 6 (June 2002), pp. 945–959.
[Hong 2013] C. Hong, S, Kandula, R. Mahajan, M.Zhang, V. Gill, M. Nanduri, 
R. Wattenhofer, “Achieving High Utilization with Software-driven WAN,” ACM 
SIGCOMM Conference (Aug. 2013), pp.15–26.
[Huang 2002] C. Haung, V. Sharma, K. Owens, V. Makam, “Building Reliable 
MPLS Networks Using a Path Protection Mechanism,” IEEE Communications 
Magazine, Vol. 40, No. 3 (Mar. 2002), pp. 156–162.
[Huang 2005] Y. Huang, R. Guerin, “Does Over-Provisioning Become More or 
Less Efficient as Networks Grow Larger?,” Proc. IEEE Int. Conf. Network Proto-
cols (ICNP) (Boston MA, Nov. 2005).
[Huang 2008] C. Huang, J. Li, A. Wang, K. W. Ross, “Understanding Hybrid CDN-
P2P: Why Limelight Needs Its Own Red Swoosh,” Proc. 2008 NOSSDAV, Braunsch-
weig, Germany.
[Huitema 1998] C. Huitema, IPv6: The New Internet Protocol, 2nd Ed., Prentice 
Hall, Englewood Cliffs, NJ, 1998.
[Huston 1999a] G. Huston, “Interconnection, Peering, and Settlements—Part I,” 
The Internet Protocol Journal, Vol. 2, No. 1 (Mar. 1999).
[Huston 2004] G. Huston, “NAT Anatomy: A Look Inside Network Address 
Translators,” The Internet Protocol Journal, Vol. 7, No. 3 (Sept. 2004).
[Huston 2008a] G. Huston, “Confronting IPv4 Address Exhaustion,” http://www.
potaroo.net/ispcol/2008-10/v4depletion.html
[Huston 2008b] G. Huston, G. Michaelson, “IPv6 Deployment: Just where are 
we?” http://www.potaroo.net/ispcol/2008-04/ipv6.html
References         783
[Huston 2011a] G. Huston, “A Rough Guide to Address Exhaustion,” The Internet 
Protocol Journal, Vol. 14, No. 1 (Mar. 2011).
[Huston 2011b] G. Huston, “Transitioning Protocols,” The Internet Protocol Jour-
nal, Vol. 14, No. 1 (Mar. 2011).
[IAB 2016] Internet Architecture Board homepage, http://www.iab.org/
[IANA Protocol Numbers 2016] Internet Assigned Numbers Authority, Protocol 
Numbers, http://www.iana.org/assignments/protocol-numbers/protocol-numbers.
xhtml
[IBM 1997] IBM Corp., IBM Inside APPN - The Essential Guide to the Next- 
Generation SNA, SG24-3669-03, June 1997.
[ICANN 2016] The Internet Corporation for Assigned Names and Numbers 
homepage, http://www.icann.org
[IEEE 802 2016] IEEE 802 LAN/MAN Standards Committee homepage, http://
www.ieee802.org/
[IEEE 802.11 1999] IEEE 802.11, “1999 Edition (ISO/IEC 8802-11: 1999) IEEE 
Standards for Information Technology—Telecommunications and Information 
Exchange Between Systems—Local and Metropolitan Area Network—Specific 
Requirements—Part 11: Wireless LAN Medium Access Control (MAC) and 
Physical Layer (PHY) Specification,” http://standards.ieee.org/getieee802/down-
load/802.11-1999.pdf
[IEEE 802.11ac 2013] IEEE, “802.11ac-2013—IEEE Standard for Information 
technology—Telecommunications and Information Exchange Between Systems—
Local and Metropolitan Area Networks—Specific Requirements—Part 11: Wire-
less LAN Medium Access Control (MAC) and Physical Layer (PHY) Specifica-
tions—Amendment 4: Enhancements for Very High Throughput for Operation in 
Bands Below 6 GHz.”
[IEEE 802.11n 2012] IEEE, “IEEE P802.11—Task Group N—Meeting Update: 
Status of 802.11n,” http://grouper.ieee.org/groups/802/11/Reports/tgn_update 
.htm
[IEEE 802.15 2012] IEEE 802.15 Working Group for WPAN homepage, http://
grouper.ieee.org/groups/802/15/.
[IEEE 802.15.4 2012] IEEE 802.15 WPAN Task Group 4, http://www.ieee802.
org/15/pub/TG4.html
[IEEE 802.16d 2004] IEEE, “IEEE Standard for Local and Metropolitan Area 
Networks, Part 16: Air Interface for Fixed Broadband Wireless Access Systems,” 
http://standards.ieee.org/getieee802/download/802.16-2004.pdf
784         References
[IEEE 802.16e 2005] IEEE, “IEEE Standard for Local and Metropolitan Area 
Networks, Part 16: Air Interface for Fixed and Mobile Broadband Wireless Access 
Systems, Amendment 2: Physical and Medium Access Control Layers for Com-
bined Fixed and Mobile Operation in Licensed Bands and Corrigendum 1,” http://
standards.ieee.org/getieee802/download/802.16e-2005.pdf
[IEEE 802.1q 2005] IEEE, “IEEE Standard for Local and Metropolitan Area  
Networks: Virtual Bridged Local Area Networks,” http://standards.ieee.org/ 
getieee802/download/802.1Q-2005.pdf
[IEEE 802.1X] IEEE Std 802.1X-2001 Port-Based Network Access Control,  
http://standards.ieee.org/reading/ieee/std_public/description/lanman/ 
802.1x-2001_desc.html
[IEEE 802.3 2012] IEEE, “IEEE 802.3 CSMA/CD (Ethernet),” http://grouper.ieee.
org/groups/802/3/
[IEEE 802.5 2012] IEEE, IEEE 802.5 homepage, http://www.ieee802.org/5/ 
www8025org/
[IETF 2016] Internet Engineering Task Force homepage, http://www.ietf.org
[Ihm 2011] S. Ihm, V. S. Pai, “Towards Understanding Modern Web Traffic,” 
Proc. 2011 ACM Internet Measurement Conference (Berlin).
[IMAP 2012] The IMAP Connection, http://www.imap.org/
[Intel 2016] Intel Corp., “Intel 710 Ethernet Adapter,” http://www.intel.com/ 
content/www/us/en/ethernet-products/converged-network-adapters/ethernet-xl710 
.html
[Internet2 Multicast 2012] Internet2 Multicast Working Group homepage, http://
www.internet2.edu/multicast/
[ISC 2016] Internet Systems Consortium homepage, http://www.isc.org
[ISI 1979] Information Sciences Institute, “DoD Standard Internet Protocol,”  
Internet Engineering Note 123 (Dec. 1979), http://www.isi.edu/in-notes/ien/ 
ien123.txt
[ISO 2016] International Organization for Standardization homepage, International 
Organization for Standardization, http://www.iso.org/
[ISO X.680 2002] International Organization for Standardization, “X.680: ITU-T 
Recommendation X.680 (2002) Information Technology—Abstract Syntax Nota-
tion One (ASN.1): Specification of Basic Notation,” http://www.itu.int/ITU-T/
studygroups/com17/languages/X.680-0207.pdf
References         785
[ITU 1999] Asymmetric Digital Subscriber Line (ADSL) Transceivers. ITU-T 
G.992.1, 1999.
[ITU 2003] Asymmetric Digital Subscriber Line (ADSL) Transceivers—Extended 
Bandwidth ADSL2 (ADSL2Plus). ITU-T G.992.5, 2003.
[ITU 2005a] International Telecommunication Union, “ITU-T X.509, The Direc-
tory: Public-key and attribute certificate frameworks” (Aug. 2005).
[ITU 2006] ITU, “G.993.1: Very High Speed Digital Subscriber Line Transceivers 
(VDSL),” https://www.itu.int/rec/T-REC-G.993.1-200406-I/en, 2006.
[ITU 2015] “Measuring the Information Society Report,” 2015, http://www.itu.int/
en/ITU-D/Statistics/Pages/publications/mis2015.aspx
[ITU 2012] The ITU homepage, http://www.itu.int/
[ITU-T Q.2931 1995] International Telecommunication Union, “Recommendation 
Q.2931 (02/95)—Broadband Integrated Services Digital Network (B-ISDN)— 
Digital Subscriber Signalling System No. 2 (DSS 2)—User-Network Interface 
(UNI)—Layer 3 Specification for Basic Call/Connection Control.”
[IXP List 2016] List of IXPs, Wikipedia, https://en.wikipedia.org/wiki/List_of_ 
Internet_exchange_points
[Iyengar 2015] J. Iyengar, I. Swett, “QUIC: A UDP-Based Secure and Reliable 
Transport for HTTP/2,” Internet Draft draft-tsvwg-quic-protocol-00, June 2015.
[Iyer 2008] S. Iyer, R. R. Kompella, N. McKeown, “Designing Packet Buffers for 
Router Line Cards,” IEEE Transactions on Networking, Vol. 16, No. 3 (June 2008), 
pp. 705–717.
[Jacobson 1988] V. Jacobson, “Congestion Avoidance and Control,” Proc. 1988 
ACM SIGCOMM (Stanford, CA, Aug. 1988), pp. 314–329.
[Jain 1986] R. Jain, “A Timeout-Based Congestion Control Scheme for Window 
Flow-Controlled Networks,” IEEE Journal on Selected Areas in Communications 
SAC-4, 7 (Oct. 1986).
[Jain 1989] R. Jain, “A Delay-Based Approach for Congestion Avoidance in 
Interconnected Heterogeneous Computer Networks,” ACM SIGCOMM Computer 
Communications Review, Vol. 19, No. 5 (1989), pp. 56–71.
[Jain 1994] R. Jain, FDDI Handbook: High-Speed Networking Using Fiber and 
Other Media, Addison-Wesley, Reading, MA, 1994.
[Jain 1996] R. Jain. S. Kalyanaraman, S. Fahmy, R. Goyal, S. Kim, “Tutorial 
Paper on ABR Source Behavior,” ATM Forum/96-1270, Oct. 1996. http://www.cse.
wustl.edu/~jain/atmf/ftp/atm96-1270.pdf
786         References
[Jain 2013] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski, A. Singh, 
S.Venkata, J. Wanderer, J. Zhou, M. Zhu, J. Zolla, U. Hölzle, S. Stuart, A, Vahdat, 
“B4: Experience with a Globally Deployed Software Defined Wan,” ACM  
SIGCOMM 2013, pp. 3–14.
[Jaiswal 2003] S. Jaiswal, G. Iannaccone, C. Diot, J. Kurose, D. Towsley, “Mea-
surement and Classification of Out-of-Sequence Packets in a Tier-1 IP backbone,” 
Proc. 2003 IEEE INFOCOM.
[Ji 2003] P. Ji, Z. Ge, J. Kurose, D. Towsley, “A Comparison of Hard-State and 
Soft-State Signaling Protocols,” Proc. 2003 ACM SIGCOMM (Karlsruhe, Ger-
many, Aug. 2003).
[Jimenez 1997] D. Jimenez, “Outside Hackers Infiltrate MIT Network, Compro-
mise Security,” The Tech, Vol. 117, No 49 (Oct. 1997), p. 1, http://www-tech.mit.
edu/V117/N49/hackers.49n.html
[Jin 2004] C. Jin, D. X. We, S. Low, “FAST TCP: Motivation, Architecture,  
Algorithms, Performance,” Proc. 2004 IEEE INFOCOM (Hong Kong,  
Mar. 2004).
[Juniper Contrail 2016] Juniper Networks, “Contrail,” http://www.juniper.net/us/
en/products-services/sdn/contrail/
[Juniper MX2020 2015] Juniper Networks, “MX2020 and MX2010 3D Universal 
Edge Routers,” www.juniper.net/us/en/local/pdf/.../1000417-en.pdf
[Kaaranen 2001] H. Kaaranen, S. Naghian, L. Laitinen, A. Ahtiainen, V. Niemi, 
Networks: Architecture, Mobility and Services, New York: John Wiley & Sons, 
2001.
[Kahn 1967] D. Kahn, The Codebreakers: The Story of Secret Writing, The  
Macmillan Company, 1967.
[Kahn 1978] R. E. Kahn, S. Gronemeyer, J. Burchfiel, R. Kunzelman,  
“Advances in Packet Radio Technology,” Proc. 1978 IEEE INFOCOM, 66, 11 
(Nov. 1978).
[Kamerman 1997] A. Kamerman, L. Monteban, “WaveLAN-II: A High– 
Performance Wireless LAN for the Unlicensed Band,” Bell Labs Technical Journal 
(Summer 1997), pp. 118–133.
[Kar 2000] K. Kar, M. Kodialam, T. V. Lakshman, “Minimum Interference Rout-
ing of Bandwidth Guaranteed Tunnels with MPLS Traffic Engineering Applica-
tions,” IEEE J. Selected Areas in Communications (Dec. 2000).
[Karn 1987] P. Karn, C. Partridge, “Improving Round-Trip Time Estimates in 
Reliable Transport Protocols,” Proc. 1987 ACM SIGCOMM.
References         787
[Karol 1987] M. Karol, M. Hluchyj, A. Morgan, “Input Versus Output Queuing on 
a Space-Division Packet Switch,” IEEE Transactions on Communications, Vol. 35, 
No. 12 (Dec.1987), pp. 1347–1356.
[Kaufman 1995] C. Kaufman, R. Perlman, M. Speciner, Network Security, Private 
Communication in a Public World, Prentice Hall, Englewood Cliffs, NJ, 1995.
[Kelly 1998] F. P. Kelly, A. Maulloo, D. Tan, “Rate Control for Communication 
Networks: Shadow Prices, Proportional Fairness and Stability,” J. Operations Res. 
Soc., Vol. 49, No. 3 (Mar. 1998), pp. 237–252.
[Kelly 2003] T. Kelly, “Scalable TCP: Improving Performance in High Speed 
Wide Area  
Networks,” ACM SIGCOMM Computer Communications Review, Volume 33, No. 
2 (Apr. 2003), pp 83–91.
[Kilkki 1999] K. Kilkki, Differentiated Services for the Internet, Macmillan Tech-
nical Publishing, Indianapolis, IN, 1999.
[Kim 2005] H. Kim, S. Rixner, V. Pai, “Network Interface Data Caching,” IEEE 
Transactions on Computers, Vol. 54, No. 11 (Nov. 2005), pp. 1394–1408.
[Kim 2008] C. Kim, M. Caesar, J. Rexford, “Floodless in SEATTLE: A Scalable 
Ethernet Architecture for Large Enterprises,” Proc. 2008 ACM SIGCOMM (Se-
attle, WA, Aug. 2008).
[Kleinrock 1961] L. Kleinrock, “Information Flow in Large Communication Net-
works,” RLE Quarterly Progress Report, July 1961.
[Kleinrock 1964] L. Kleinrock, 1964 Communication Nets: Stochastic Message 
Flow and Delay, McGraw-Hill, New York, NY, 1964.
[Kleinrock 1975] L. Kleinrock, Queuing Systems, Vol. 1, John Wiley, New York, 
1975.
[Kleinrock 1975b] L. Kleinrock, F. A. Tobagi, “Packet Switching in Radio Chan-
nels: Part I—Carrier Sense Multiple-Access Modes and Their Throughput-Delay 
Characteristics,” IEEE Transactions on Communications, Vol. 23, No. 12 (Dec. 
1975), pp. 1400–1416.
[Kleinrock 1976] L. Kleinrock, Queuing Systems, Vol. 2, John Wiley, New York, 
1976.
[Kleinrock 2004] L. Kleinrock, “The Birth of the Internet,” http://www.lk.cs.ucla.
edu/LK/Inet/birth.html
[Kohler 2006] E. Kohler, M. Handley, S. Floyd, “DDCP: Designing DCCP: 
Congestion Control Without Reliability,” Proc. 2006 ACM SIGCOMM (Pisa, Italy, 
Sept. 2006).
788         References
[Kolding 2003] T. Kolding, K. Pedersen, J. Wigard, F. Frederiksen, P. Mogensen, 
“High Speed Downlink Packet Access: WCDMA Evolution,” IEEE Vehicular 
Technology Society News (Feb. 2003), pp. 4–10.
[Koponen 2010] T. Koponen, M. Casado, N. Gude, J. Stribling, L. Poutievski,  
M. Zhu, R. Ramanathan, Y. Iwata, H. Inoue, T. Hama, S. Shenker, “Onix: A 
Distributed Control Platform for Large-Scale Production Networks,” 9th USENIX 
conference on Operating systems design and implementation (OSDI’10), pp. 1–6.
[Koponen 2011] T. Koponen, S. Shenker, H. Balakrishnan, N. Feamster, I. 
Ganichev, A. Ghodsi, P. B. Godfrey, N. McKeown, G. Parulkar, B. Raghavan, J. 
Rexford, S. Arianfar, D. Kuptsov, “Architecting for Innovation,” ACM Computer 
Communications Review, 2011.
[Korhonen 2003] J. Korhonen, Introduction to 3G Mobile Communications, 2nd 
ed., Artech House, 2003.
[Koziol 2003] J. Koziol, Intrusion Detection with Snort, Sams Publishing, 2003.
[Kreutz 2015] D. Kreutz, F.M.V. Ramos, P. Esteves Verissimo, C. Rothenberg,  
S. Azodolmolky, S. Uhlig, “Software-Defined Networking: A Comprehensive  
Survey,” Proceedings of the IEEE, Vol. 103, No. 1 (Jan. 2015), pp. 14-76.  
This paper is also being updated at https://github.com/SDN-Survey/latex/wiki
[Krishnamurthy 2001] B. Krishnamurthy, J. Rexford, Web Protocols and Prac-
tice: HTTP/ 1.1, Networking Protocols, and Traffic Measurement, Addison-Wes-
ley, Boston, MA, 2001.
[Kulkarni 2005] S. Kulkarni, C. Rosenberg, “Opportunistic Scheduling: General-
izations to Include Multiple Constraints, Multiple Interfaces, and Short Term Fair-
ness,” Wireless Networks, 11 (2005), 557–569.
[Kumar 2006] R. Kumar, K.W. Ross, “Optimal Peer-Assisted File Distribution: 
Single and Multi-Class Problems,” IEEE Workshop on Hot Topics in Web Systems 
and Technologies (Boston, MA, 2006).
[Labovitz 1997] C. Labovitz, G. R. Malan, F. Jahanian, “Internet Routing Instabil-
ity,” Proc. 1997 ACM SIGCOMM (Cannes, France, Sept. 1997), pp. 115–126.
[Labovitz 2010] C. Labovitz, S. Iekel-Johnson, D. McPherson, J. Oberheide, F. 
Jahanian, “Internet Inter-Domain Traffic,” Proc. 2010 ACM SIGCOMM.
[Labrador 1999] M. Labrador, S. Banerjee, “Packet Dropping Policies for ATM 
and IP Networks,” IEEE Communications Surveys, Vol. 2, No. 3 (Third Quarter 
1999), pp. 2–14.
[Lacage 2004] M. Lacage, M.H. Manshaei, T. Turletti, “IEEE 802.11 Rate Adapta-
tion: A Practical Approach,” ACM Int. Symposium on Modeling, Analysis, and 
Simulation of Wireless and Mobile Systems (MSWiM) (Venice, Italy, Oct. 2004).
References         789
[Lakhina 2004] A. Lakhina, M. Crovella, C. Diot, “Diagnosing Network-Wide 
Traffic Anomalies,” Proc. 2004 ACM SIGCOMM.
[Lakhina 2005] A. Lakhina, M. Crovella, C. Diot, “Mining Anomalies Using Traf-
fic Feature Distributions,” Proc. 2005 ACM SIGCOMM.
[Lakshman 1997] T. V. Lakshman, U. Madhow, “The Performance of TCP/IP for 
Networks with High Bandwidth-Delay Products and Random Loss,” IEEE/ACM 
Transactions on Networking, Vol. 5, No. 3 (1997), pp. 336–350.
[Lakshman 2004] T. V. Lakshman, T. Nandagopal, R. Ramjee, K. Sabnani, T. 
Woo, “The SoftRouter Architecture,” Proc. 3nd ACM Workshop on Hot Topics in 
Networks (Hotnets-III), Nov. 2004.
[Lam 1980] S. Lam, “A Carrier Sense Multiple Access Protocol for Local Net-
works,” Computer Networks, Vol. 4 (1980), pp. 21–32.
[Lamport 1989] L. Lamport, “The Part-Time Parliament,” Technical Report 49, 
Systems Research Center, Digital Equipment Corp., Palo Alto, Sept. 1989.
[Lampson 1983] Lampson, Butler W. “Hints for computer system design,” ACM 
SIGOPS Operating Systems Review, Vol. 17, No. 5, 1983.
[Lampson 1996] B. Lampson, “How to Build a Highly Available System Using 
Consensus,” Proc. 10th International Workshop on Distributed Algorithms (WDAG 
’96), Özalp Babaoglu and Keith Marzullo (Eds.), Springer-Verlag, pp. 1–17.
[Lawton 2001] G. Lawton, “Is IPv6 Finally Gaining Ground?” IEEE Computer 
Magazine (Aug. 2001), pp. 11–15.
[LeBlond 2011] S. Le Blond, C. Zhang, A. Legout, K. Ross, W. Dabbous. 2011,  
“I know where you are and what you are sharing: exploiting P2P communications 
to invade users’ privacy.” 2011 ACM Internet Measurement Conference, ACM, 
New York, NY, USA, pp. 45–60.
[Leighton 2009] T. Leighton, “Improving Performance on the Internet,” Communi-
cations of the ACM, Vol. 52, No. 2 (Feb. 2009), pp. 44–51.
[Leiner 1998] B. Leiner, V. Cerf, D. Clark, R. Kahn, L. Kleinrock, D. Lynch, J. 
Postel, L. Roberts, S. Woolf, “A Brief History of the Internet,” http://www.isoc.
org/internet/history/brief.html
[Leung 2006] K. Leung, V. O.K. Li, “TCP in Wireless Networks: Issues,  
Approaches, and Challenges,” IEEE Commun. Surveys and Tutorials, Vol. 8, No. 4 
(2006), pp. 64–79.
[Levin 2012] D. Levin, A. Wundsam, B. Heller, N. Handigol, A. Feldmann, “Logi-
cally Centralized?: State Distribution Trade-offs in Software Defined Networks,” 
Proc. First Workshop on Hot Topics in Software Defined Networks (Aug. 2012), 
pp. 1–6.
790         References
[Li 2004] L. Li, D. Alderson, W. Willinger, J. Doyle, “A First-Principles Approach 
to Understanding the Internet’s Router-Level Topology,” Proc. 2004 ACM SIG-
COMM (Portland, OR, Aug. 2004).
[Li 2007] J. Li, M. Guidero, Z. Wu, E. Purpus, T. Ehrenkranz, “BGP Routing  
Dynamics Revisited.” ACM Computer Communication Review (Apr. 2007).
[Li 2015] S.Q. Li, “Building Softcom Ecosystem Foundation,” Open Networking 
Summit, 2015.
[Lin 2001] Y. Lin, I. Chlamtac, Wireless and Mobile Network Architectures, John 
Wiley and Sons, New York, NY, 2001.
[Liogkas 2006] N. Liogkas, R. Nelson, E. Kohler, L. Zhang, “Exploiting BitTor-
rent for Fun (but Not Profit),” 6th International Workshop on Peer-to-Peer Systems 
(IPTPS 2006).
[Liu 2003] J. Liu, I. Matta, M. Crovella, “End-to-End Inference of Loss Nature in 
a Hybrid Wired/Wireless Environment,” Proc. WiOpt’03: Modeling and Optimiza-
tion in Mobile, Ad Hoc and Wireless Networks.
[Locher 2006] T. Locher, P. Moor, S. Schmid, R. Wattenhofer, “Free Riding in 
BitTorrent is Cheap,” Proc. ACM HotNets 2006 (Irvine CA, Nov. 2006).
[Lui 2004] J. Lui, V. Misra, D. Rubenstein, “On the Robustness of Soft State Pro-
tocols,” Proc. IEEE Int. Conference on Network Protocols (ICNP ’04), pp. 50–60.
[Mahdavi 1997] J. Mahdavi, S. Floyd, “TCP-Friendly Unicast Rate-Based Flow 
Control,” unpublished note (Jan. 1997).
[MaxMind 2016] http://www.maxmind.com/app/ip-location
[Maymounkov 2002] P. Maymounkov, D. Mazières. “Kademlia: A Peer-to-Peer 
Information System Based on the XOR Metric.” Proceedings of the 1st Interna-
tional Workshop on Peerto-Peer Systems (IPTPS ‘02) (Mar. 2002), pp. 53–65.
[McKeown 1997a] N. McKeown, M. Izzard, A. Mekkittikul, W. Ellersick, M. 
Horowitz, “The Tiny Tera: A Packet Switch Core,” IEEE Micro Magazine  
(Jan.–Feb. 1997).
[McKeown 1997b] N. McKeown, “A Fast Switched Backplane for a Gigabit 
Switched Router,” Business Communications Review, Vol. 27, No. 12. http://tiny-
tera.stanford.edu/~nickm/papers/cisco_fasts_wp.pdf
[McKeown 2008] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,  
L. Peterson, J. Rexford, S. Shenker, J. Turner. 2008. OpenFlow: Enabling Innova-
tion in Campus Networks. SIGCOMM Comput. Commun. Rev. 38, 2 (Mar. 2008),  
pp. 69–74.
References         791
[McQuillan 1980] J. McQuillan, I. Richer, E. Rosen, “The New Routing Algo-
rithm for the Arpanet,” IEEE Transactions on Communications, Vol. 28, No. 5 
(May 1980), pp. 711–719.
[Metcalfe 1976] R. M. Metcalfe, D. R. Boggs. “Ethernet: Distributed Packet 
Switching for Local Computer Networks,” Communications of the Association for 
Computing Machinery, Vol. 19, No. 7 (July 1976), pp. 395–404.
[Meyers 2004] A. Myers, T. Ng, H. Zhang, “Rethinking the Service Model: Scal-
ing Ethernet to a Million Nodes,” ACM Hotnets Conference, 2004.
[MFA Forum 2016] IP/MPLS Forum homepage, http://www.ipmplsforum.org/
[Mockapetris 1988] P. V. Mockapetris, K. J. Dunlap, “Development of the Do-
main Name System,” Proc. 1988 ACM SIGCOMM (Stanford, CA, Aug. 1988).
[Mockapetris 2005] P. Mockapetris, Sigcomm Award Lecture, video available at 
http://www.postel.org/sigcomm
[Molinero-Fernandez 2002] P. Molinaro-Fernandez, N. McKeown, H. Zhang, 
“Is IP Going to Take Over the World (of Communications)?” Proc. 2002 ACM 
Hotnets.
[Molle 1987] M. L. Molle, K. Sohraby, A. N. Venetsanopoulos, “Space-Time 
Models of Asynchronous CSMA Protocols for Local Area Networks,” IEEE Jour-
nal on Selected Areas in Communications, Vol. 5, No. 6 (1987), pp. 956–968.
[Moore 2001] D. Moore, G. Voelker, S. Savage, “Inferring Internet Denial of Ser-
vice Activity,” Proc. 2001 USENIX Security Symposium (Washington, DC, Aug. 
2001).
[Motorola 2007] Motorola, “Long Term Evolution (LTE): A Technical Overview,” 
 http://www.motorola.com/staticfiles/Business/Solutions/Industry%20Solu-
tions/Service%20Providers/Wireless%20Operators/LTE/_Document/Static%20
Files/6834_MotDoc_New.pdf
[Mouly 1992] M. Mouly, M. Pautet, The GSM System for Mobile Communications, 
Cell and Sys, Palaiseau, France, 1992.
[Moy 1998] J. Moy, OSPF: Anatomy of An Internet Routing Protocol, Addison-
Wesley, Reading, MA, 1998.
[Mukherjee 1997] B. Mukherjee, Optical Communication Networks, McGraw-
Hill, 1997.
[Mukherjee 2006] B. Mukherjee, Optical WDM Networks, Springer, 2006.
[Mysore 2009] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri,  
S. Radhakrishnan, V. Subramanya, A. Vahdat, “PortLand: A Scalable Fault- 
Tolerant Layer 2 Data Center Network Fabric,” Proc. 2009 ACM SIGCOMM.
792         References
[Nahum 2002] E. Nahum, T. Barzilai, D. Kandlur, “Performance Issues in WWW 
Servers,” IEEE/ACM Transactions on Networking, Vol 10, No. 1 (Feb. 2002).
[Netflix Open Connect 2016] Netflix Open Connect CDN, 2016, https:// 
openconnect.netflix.com/
[Netflix Video 1] Designing Netflix’s Content Delivery System, D. Fulllager, 
2014, https://www.youtube.com/watch?v=LkLLpYdDINA
[Netflix Video 2] Scaling the Netflix Global CDN, D. Temkin, 2015, https://www 
.youtube.com/watch?v=tbqcsHg-Q_o
[Neumann 1997] R. Neumann, “Internet Routing Black Hole,” The Risks Digest: 
Forum on Risks to the Public in Computers and Related Systems, Vol. 19, No. 12 
(May 1997). http://catless.ncl.ac.uk/Risks/19.12.html#subj1.1
[Neville-Neil 2009] G. Neville-Neil, “Whither Sockets?” Communications of the 
ACM, Vol. 52, No. 6 (June 2009), pp. 51–55.
[Nicholson 2006] A Nicholson, Y. Chawathe, M. Chen, B. Noble, D. Wetherall, 
“Improved Access Point Selection,” Proc. 2006 ACM Mobisys Conference  
(Uppsala Sweden, 2006).
[Nielsen 1997] H. F. Nielsen, J. Gettys, A. Baird-Smith, E. Prud’hommeaux, H. W. 
Lie, C. Lilley, “Network Performance Effects of HTTP/1.1, CSS1, and PNG,” W3C 
Document, 1997 (also appears in Proc. 1997 ACM SIGCOM (Cannes, France, Sept 
1997), pp. 155–166.
[NIST 2001] National Institute of Standards and Technology, “Advanced Encryp-
tion Standard (AES),” Federal Information Processing Standards 197, Nov. 2001, 
http://csrc.nist.gov/publications/fips/fips197/fips-197.pdf
[NIST IPv6 2015] US National Institute of Standards and Technology, “Estimating 
IPv6 & DNSSEC Deployment SnapShots,” http://fedv6-deployment.antd.nist.gov/
snap-all.html
[Nmap 2012] Nmap homepage, http://www.insecure.com/nmap
[Nonnenmacher 1998] J. Nonnenmacher, E. Biersak, D. Towsley, “Parity-Based 
Loss Recovery for Reliable Multicast Transmission,” IEEE/ACM Transactions on 
Networking, Vol. 6, No. 4 (Aug. 1998), pp. 349–361.
[Nygren 2010] Erik Nygren, Ramesh K. Sitaraman, and Jennifer Sun, “The Aka-
mai Network: A Platform for High-performance Internet Applications,” SIGOPS 
Oper. Syst. Rev. 44, 3 (Aug. 2010), 2–19.
[ONF 2016] Open Networking Foundation, Technical Library, https://www.open-
networking.org/sdn-resources/technical-library
References         793
[ONOS 2016] Open Network Operating System (ONOS), “Architecture Guide,” 
https://wiki.onosproject.org/display/ONOS/Architecture+Guide, 2016.
[OpenFlow 2009] Open Network Foundation, “OpenFlow Switch Specification 
1.0.0, TS-001,” https://www.opennetworking.org/images/stories/downloads/sdn-
resources/onf-specifications/openflow/openflow-spec-v1.0.0.pdf
[OpenDaylight Lithium 2016] OpenDaylight, “Lithium,” https://www.openday-
light.org/lithium
[OSI 2012] International Organization for Standardization homepage, http://www.
iso.org/iso/en/ISOOnline.frontpage
[Osterweil 2012] E. Osterweil, D. McPherson, S. DiBenedetto, C. Papadopoulos, D. 
Massey, “Behavior of DNS Top Talkers,” Passive and Active Measurement Confer-
ence, 2012.
[Padhye 2000] J. Padhye, V. Firoiu, D. Towsley, J. Kurose, “Modeling TCP Reno 
Performance: A Simple Model and Its Empirical Validation,” IEEE/ACM Transac-
tions on Networking, Vol. 8 No. 2 (Apr. 2000), pp. 133–145.
[Padhye 2001] J. Padhye, S. Floyd, “On Inferring TCP Behavior,” Proc. 2001 
ACM SIGCOMM (San Diego, CA, Aug. 2001).
[Palat 2009] S. Palat, P. Godin, “The LTE Network Architecture: A Comprehensive 
Tutorial,” in LTE—The UMTS Long Term Evolution: From Theory to Practice. 
Also available as a standalone Alcatel white paper.
[Panda 2013] A. Panda, C. Scott, A. Ghodsi, T. Koponen, S. Shenker, “CAP for 
Networks,” Proc. ACM HotSDN ’13, pp. 91–96.
[Parekh 1993] A. Parekh, R. Gallagher, “A Generalized Processor Sharing Ap-
proach to Flow Control in Integrated Services Networks: The Single-Node Case,” 
IEEE/ACM Transactions on Networking, Vol. 1, No. 3 (June 1993), pp. 344–357.
[Partridge 1992] C. Partridge, S. Pink, “An Implementation of the Revised Internet 
Stream Protocol (ST-2),” Journal of Internetworking: Research and Experience, Vol. 3, 
No. 1 (Mar. 1992).
[Partridge 1998] C. Partridge, et al. “A Fifty Gigabit per second IP Router,” IEEE/
ACM Transactions on Networking, Vol. 6, No. 3 (Jun. 1998), pp. 237–248.
[Pathak 2010] A. Pathak, Y. A. Wang, C. Huang, A. Greenberg, Y. C. Hu, J. Li, 
K. W. Ross, “Measuring and Evaluating TCP Splitting for Cloud Services,” Pas-
sive and Active Measurement (PAM) Conference (Zurich, 2010).
[Perkins 1994] A. Perkins, “Networking with Bob Metcalfe,” The Red Herring 
Magazine (Nov. 1994).
794         References
[Perkins 1998] C. Perkins, O. Hodson, V. Hardman, “A Survey of Packet Loss 
Recovery Techniques for Streaming Audio,” IEEE Network Magazine (Sept./Oct. 
1998), pp. 40–47.
[Perkins 1998b] C. Perkins, Mobile IP: Design Principles and Practice, Addison-
Wesley, Reading, MA, 1998.
[Perkins 2000] C. Perkins, Ad Hoc Networking, Addison-Wesley, Reading, MA, 
2000.
[Perlman 1999] R. Perlman, Interconnections: Bridges, Routers, Switches, and In-
ternetworking Protocols, 2nd ed., Addison-Wesley Professional Computing Series, 
Reading, MA, 1999.
[PGPI 2016] The International PGP homepage, http://www.pgpi.org
[Phifer 2000] L. Phifer, “The Trouble with NAT,” The Internet Protocol Journal, 
Vol. 3, No. 4 (Dec. 2000), http://www.cisco.com/warp/public/759/ipj_3-4/ipj_ 
3-4_nat.html
[Piatek 2007] M. Piatek, T. Isdal, T. Anderson, A. Krishnamurthy, A. Venkataram-
ani, “Do Incentives Build Robustness in Bittorrent?,” Proc. NSDI (2007).
[Piatek 2008] M. Piatek, T. Isdal, A. Krishnamurthy, T. Anderson, “One Hop 
Reputations for Peer-to-peer File Sharing Workloads,” Proc. NSDI (2008).
[Pickholtz 1982] R. Pickholtz, D. Schilling, L. Milstein, “Theory of Spread Spec-
trum Communication—a Tutorial,” IEEE Transactions on Communications, Vol. 
30, No. 5 (May 1982), pp. 855–884.
[PingPlotter 2016] PingPlotter homepage, http://www.pingplotter.com
[Piscatello 1993] D. Piscatello, A. Lyman Chapin, Open Systems Networking, 
Addison-Wesley, Reading, MA, 1993.
[Pomeranz 2010] H. Pomeranz, “Practical, Visual, Three-Dimensional Pedagogy 
for Internet Protocol Packet Header Control Fields,” https://righteousit.wordpress.
com/2010/06/27/practical-visual-three-dimensional-pedagogy-for-internet-proto-
col-packet-header-control-fields/, June 2010.
[Potaroo 2016] “Growth of the BGP Table–1994 to Present,” http://bgp.potaroo.
net/
[PPLive 2012] PPLive homepage, http://www.pplive.com
[Qazi 2013] Z. Qazi, C. Tu, L. Chiang, R. Miao, V. Sekar, M. Yu, “SIMPLE-fying 
Middlebox Policy Enforcement Using SDN,” ACM SIGCOMM Conference  
(Aug. 2013), pp. 27–38.
[Quagga 2012] Quagga, “Quagga Routing Suite,” http://www.quagga.net/
References         795
[Quittner 1998] J. Quittner, M. Slatalla, Speeding the Net: The Inside Story of 
Netscape and How It Challenged Microsoft, Atlantic Monthly Press, 1998.
[Quova 2016] www.quova.com
[Ramakrishnan 1990] K. K. Ramakrishnan, R. Jain, “A Binary Feedback Scheme 
for Congestion Avoidance in Computer Networks,” ACM Transactions on Com-
puter Systems, Vol. 8, No. 2 (May 1990), pp. 158–181.
[Raman 1999] S. Raman, S. McCanne, “A Model, Analysis, and Protocol Frame-
work for Soft State-based Communication,” Proc. 1999 ACM SIGCOMM (Boston, 
MA, Aug. 1999).
[Raman 2007] B. Raman, K. Chebrolu, “Experiences in Using WiFi for Rural In-
ternet in India,” IEEE Communications Magazine, Special Issue on New Directions 
in Networking Technologies in Emerging Economies (Jan. 2007).
[Ramaswami 2010] R. Ramaswami, K. Sivarajan, G. Sasaki, Optical Networks: A 
Practical Perspective, Morgan Kaufman Publishers, 2010.
[Ramjee 1994] R. Ramjee, J. Kurose, D. Towsley, H. Schulzrinne, “Adaptive 
Playout Mechanisms for Packetized Audio Applications in Wide-Area Networks,” 
Proc. 1994 IEEE INFOCOM.
[Rao 2011] A. S. Rao, Y. S. Lim, C. Barakat, A. Legout, D. Towsley, W. Dabbous, 
 
“Network Characteristics of Video Streaming Traffic,” Proc. 2011 ACM CoNEXT 
(Tokyo).
[Ren 2006] S. Ren, L. Guo, X. Zhang, “ASAP: An AS-Aware Peer-Relay Protocol 
for High Quality VoIP,” Proc. 2006 IEEE ICDCS (Lisboa, Portugal, July 2006).
[Rescorla 2001] E. Rescorla, SSL and TLS: Designing and Building Secure Sys-
tems, Addison-Wesley, Boston, 2001.
[RFC 001] S. Crocker, “Host Software,” RFC 001 (the very first RFC!).
[RFC 768] J. Postel, “User Datagram Protocol,” RFC 768, Aug. 1980.
[RFC 791] J. Postel, “Internet Protocol: DARPA Internet Program Protocol Speci-
fication,” RFC 791, Sept. 1981.
[RFC 792] J. Postel, “Internet Control Message Protocol,” RFC 792, Sept. 1981.
[RFC 793] J. Postel, “Transmission Control Protocol,” RFC 793, Sept. 1981.
[RFC 801] J. Postel, “NCP/TCP Transition Plan,” RFC 801, Nov. 1981.
[RFC 826] D. C. Plummer, “An Ethernet Address Resolution Protocol—or— 
Converting Network Protocol Addresses to 48-bit Ethernet Address for Transmis-
sion on Ethernet Hardware,” RFC 826, Nov. 1982.
796         References
[RFC 829] V. Cerf, “Packet Satellite Technology Reference Sources,” RFC 829, 
Nov. 1982.
[RFC 854] J. Postel, J. Reynolds, “TELNET Protocol Specification,” RFC 854, 
May 1993.
[RFC 950] J. Mogul, J. Postel, “Internet Standard Subnetting Procedure,” RFC 
950, Aug. 1985.
[RFC 959] J. Postel and J. Reynolds, “File Transfer Protocol (FTP),” RFC 959, 
Oct. 1985.
[RFC 1034] P. V. Mockapetris, “Domain Names—Concepts and Facilities,” RFC 
1034, Nov. 1987.
[RFC 1035] P. Mockapetris, “Domain Names—Implementation and Specifica-
tion,” RFC 1035, Nov. 1987.
[RFC 1058] C. L. Hendrick, “Routing Information Protocol,” RFC 1058, June 
1988.
[RFC 1071] R. Braden, D. Borman, and C. Partridge, “Computing the Internet 
Checksum,” RFC 1071, Sept. 1988.
[RFC 1122] R. Braden, “Requirements for Internet Hosts—Communication  
Layers,” RFC 1122, Oct. 1989.
[RFC 1123] R. Braden, ed., “Requirements for Internet Hosts—Application and 
Support,” RFC-1123, Oct. 1989.
[RFC 1142] D. Oran, “OSI IS-IS Intra-Domain Routing Protocol,” RFC 1142,  
Feb. 1990.
[RFC 1190] C. Topolcic, “Experimental Internet Stream Protocol: Version 2  
(ST-II),” RFC 1190, Oct. 1990.
[RFC 1256] S. Deering, “ICMP Router Discovery Messages,” RFC 1256, Sept. 
1991.
[RFC 1320] R. Rivest, “The MD4 Message-Digest Algorithm,” RFC 1320, Apr. 
1992.
[RFC 1321] R. Rivest, “The MD5 Message-Digest Algorithm,” RFC 1321, Apr. 
1992.
[RFC 1323] V. Jacobson, S. Braden, D. Borman, “TCP Extensions for High Per-
formance,” RFC 1323, May 1992.
[RFC 1422] S. Kent, “Privacy Enhancement for Internet Electronic Mail: Part II: 
Certificate-Based Key Management,” RFC 1422.
References         797
[RFC 1546] C. Partridge, T. Mendez, W. Milliken, “Host Anycasting Service,” 
RFC 1546, 1993.
[RFC 1584] J. Moy, “Multicast Extensions to OSPF,” RFC 1584, Mar. 1994.
[RFC 1633] R. Braden, D. Clark, S. Shenker, “Integrated Services in the Internet 
Architecture: an Overview,” RFC 1633, June 1994.
[RFC 1636] R. Braden, D. Clark, S. Crocker, C. Huitema, “Report of IAB Work-
shop on Security in the Internet Architecture,” RFC 1636, Nov. 1994.
[RFC 1700] J. Reynolds, J. Postel, “Assigned Numbers,” RFC 1700, Oct. 1994.
[RFC 1752] S. Bradner, A. Mankin, “The Recommendations for the IP Next Gen-
eration Protocol,” RFC 1752, Jan. 1995.
[RFC 1918] Y. Rekhter, B. Moskowitz, D. Karrenberg, G. J. de Groot, E. Lear, 
“Address Allocation for Private Internets,” RFC 1918, Feb. 1996.
[RFC 1930] J. Hawkinson, T. Bates, “Guidelines for Creation, Selection, and Reg-
istration of an Autonomous System (AS),” RFC 1930, Mar. 1996.
[RFC 1939] J. Myers, M. Rose, “Post Office Protocol—Version 3,” RFC 1939, 
May 1996.
[RFC 1945] T. Berners-Lee, R. Fielding, H. Frystyk, “Hypertext Transfer Proto-
col—HTTP/1.0,” RFC 1945, May 1996.
[RFC 2003] C. Perkins, “IP Encapsulation Within IP,” RFC 2003, Oct. 1996.
[RFC 2004] C. Perkins, “Minimal Encapsulation Within IP,” RFC 2004, Oct. 
1996.
[RFC 2018] M. Mathis, J. Mahdavi, S. Floyd, A. Romanow, “TCP Selective  
Acknowledgment Options,” RFC 2018, Oct. 1996.
[RFC 2131] R. Droms, “Dynamic Host Configuration Protocol,” RFC 2131, Mar. 
1997.
[RFC 2136] P. Vixie, S. Thomson, Y. Rekhter, J. Bound, “Dynamic Updates in the 
Domain Name System,” RFC 2136, Apr. 1997.
[RFC 2205] R. Braden, Ed., L. Zhang, S. Berson, S. Herzog, S. Jamin, “Resource 
ReSerVation Protocol (RSVP)—Version 1 Functional Specification,” RFC 2205, 
Sept. 1997.
[RFC 2210] J. Wroclawski, “The Use of RSVP with IETF Integrated Services,” 
RFC 2210, Sept. 1997.
[RFC 2211] J. Wroclawski, “Specification of the Controlled-Load Network Ele-
ment Service,” RFC 2211, Sept. 1997.
798         References
[RFC 2215] S. Shenker, J. Wroclawski, “General Characterization Parameters for 
Integrated Service Network Elements,” RFC 2215, Sept. 1997.
[RFC 2326] H. Schulzrinne, A. Rao, R. Lanphier, “Real Time Streaming Protocol 
(RTSP),” RFC 2326, Apr. 1998.
[RFC 2328] J. Moy, “OSPF Version 2,” RFC 2328, Apr. 1998.
[RFC 2420] H. Kummert, “The PPP Triple-DES Encryption Protocol (3DESE),” 
RFC 2420, Sept. 1998.
[RFC 2453] G. Malkin, “RIP Version 2,” RFC 2453, Nov. 1998.
[RFC 2460] S. Deering, R. Hinden, “Internet Protocol, Version 6 (IPv6) Specifica-
tion,” RFC 2460, Dec. 1998.
[RFC 2475] S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, W. Weiss, “An 
Architecture for Differentiated Services,” RFC 2475, Dec. 1998.
[RFC 2578] K. McCloghrie, D. Perkins, J. Schoenwaelder, “Structure of Manage-
ment Information Version 2 (SMIv2),” RFC 2578, Apr. 1999.
[RFC 2579] K. McCloghrie, D. Perkins, J. Schoenwaelder, “Textual Conventions 
for SMIv2,” RFC 2579, Apr. 1999.
[RFC 2580] K. McCloghrie, D. Perkins, J. Schoenwaelder, “Conformance State-
ments for SMIv2,” RFC 2580, Apr. 1999.
[RFC 2597] J. Heinanen, F. Baker, W. Weiss, J. Wroclawski, “Assured Forward-
ing PHB Group,” RFC 2597, June 1999.
[RFC 2616] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P. Leach, T. 
Berners-Lee, R. Fielding, “Hypertext Transfer Protocol—HTTP/1.1,” RFC 2616, 
June 1999.
[RFC 2663] P. Srisuresh, M. Holdrege, “IP Network Address Translator (NAT) 
Terminology and Considerations,” RFC 2663.
[RFC 2702] D. Awduche, J. Malcolm, J. Agogbua, M. O’Dell, J. McManus, “Re-
quirements for Traffic Engineering Over MPLS,” RFC 2702, Sept. 1999.
[RFC 2827] P. Ferguson, D. Senie, “Network Ingress Filtering: Defeating Denial 
of Service Attacks which Employ IP Source Address Spoofing,” RFC 2827, May 
2000.
[RFC 2865] C. Rigney, S. Willens, A. Rubens, W. Simpson, “Remote Authentica-
tion Dial In User Service (RADIUS),” RFC 2865, June 2000.
[RFC 3007] B. Wellington, “Secure Domain Name System (DNS) Dynamic  
Update,” RFC 3007, Nov. 2000.
References         799
[RFC 3022] P. Srisuresh, K. Egevang, “Traditional IP Network Address Translator 
(Traditional NAT),” RFC 3022, Jan. 2001.
[RFC 3022] P. Srisuresh, K. Egevang, “Traditional IP Network Address Translator 
(Traditional NAT),” RFC 3022, Jan. 2001.
[RFC 3031] E. Rosen, A. Viswanathan, R. Callon, “Multiprotocol Label Switching 
Architecture,” RFC 3031, Jan. 2001.
[RFC 3032] E. Rosen, D. Tappan, G. Fedorkow, Y. Rekhter, D. Farinacci, T. Li, 
A. Conta, “MPLS Label Stack Encoding,” RFC 3032, Jan. 2001.
[RFC 3168] K. Ramakrishnan, S. Floyd, D. Black, “The Addition of Explicit Con-
gestion Notification (ECN) to IP,” RFC 3168, Sept. 2001.
[RFC 3209] D. Awduche, L. Berger, D. Gan, T. Li, V. Srinivasan, G. Swallow, 
“RSVP-TE: Extensions to RSVP for LSP Tunnels,” RFC 3209, Dec. 2001.
[RFC 3221] G. Huston, “Commentary on Inter-Domain Routing in the Internet,” 
RFC 3221, Dec. 2001.
[RFC 3232] J. Reynolds, “Assigned Numbers: RFC 1700 Is Replaced by an On-
line Database,” RFC 3232, Jan. 2002.
[RFC 3234] B. Carpenter, S. Brim, “Middleboxes: Taxonomy and Issues,” RFC 
3234, Feb. 2002.
[RFC 3246] B. Davie, A. Charny, J.C.R. Bennet, K. Benson, J.Y. Le Boudec, W. 
Courtney, S. Davari, V. Firoiu, D. Stiliadis, “An Expedited Forwarding PHB  
(Per-Hop Behavior),” RFC 3246, Mar. 2002.
[RFC 3260] D. Grossman, “New Terminology and Clarifications for Diffserv,” 
RFC 3260, Apr. 2002.
[RFC 3261] J. Rosenberg, H. Schulzrinne, G. Carmarillo, A. Johnston, J. Peterson, 
R. Sparks, M. Handley, E. Schooler, “SIP: Session Initiation Protocol,” RFC 3261, 
July 2002.
[RFC 3272] J. Boyle, V. Gill, A. Hannan, D. Cooper, D. Awduche, B. Christian, 
W. S. Lai, “Overview and Principles of Internet Traffic Engineering,” RFC 3272, 
May 2002.
[RFC 3286] L. Ong, J. Yoakum, “An Introduction to the Stream Control Transmis-
sion Protocol (SCTP),” RFC 3286, May 2002.
[RFC 3346] J. Boyle, V. Gill, A. Hannan, D. Cooper, D. Awduche, B. Christian, 
W. S. Lai, “Applicability Statement for Traffic Engineering with MPLS,” RFC 
3346, Aug. 2002.
800         References
[RFC 3390] M. Allman, S. Floyd, C. Partridge, “Increasing TCP’s Initial  
Window,” RFC 3390, Oct. 2002.
[RFC 3410] J. Case, R. Mundy, D. Partain, “Introduction and Applicability State-
ments for Internet Standard Management Framework,” RFC 3410, Dec. 2002.
[RFC 3414] U. Blumenthal and B. Wijnen, “User-based Security Model (USM) for 
Version 3 of the Simple Network Management Protocol (SNMPv3),” RFC 3414, 
Dec. 2002.
[RFC 3416] R. Presuhn, J. Case, K. McCloghrie, M. Rose, S. Waldbusser, “Ver-
sion 2 of the Protocol Operations for the Simple Network Management Protocol 
(SNMP),” Dec. 2002.
[RFC 3439] R. Bush, D. Meyer, “Some Internet Architectural Guidelines and Phi-
losophy,” RFC 3439, Dec. 2003.
[RFC 3447] J. Jonsson, B. Kaliski, “Public-Key Cryptography Standards (PKCS) 
#1: RSA Cryptography Specifications Version 2.1,” RFC 3447, Feb. 2003.
[RFC 3468] L. Andersson, G. Swallow, “The Multiprotocol Label Switching 
(MPLS) Working Group Decision on MPLS Signaling Protocols,” RFC 3468,  
Feb. 2003.
[RFC 3469] V. Sharma, Ed., F. Hellstrand, Ed, “Framework for Multi-Protocol  
Label Switching (MPLS)-based Recovery,” RFC 3469, Feb. 2003.  
ftp://ftp.rfc-editor.org/in-notes/rfc3469.txt
[RFC 3501] M. Crispin, “Internet Message Access Protocol—Version 4rev1,” RFC 
3501, Mar. 2003.
[RFC 3550] H. Schulzrinne, S. Casner, R. Frederick, V. Jacobson, “RTP: A Trans-
port Protocol for Real-Time Applications,” RFC 3550, July 2003.
[RFC 3588] P. Calhoun, J. Loughney, E. Guttman, G. Zorn, J. Arkko, “Diameter 
Base Protocol,” RFC 3588, Sept. 2003.
[RFC 3649] S. Floyd, “HighSpeed TCP for Large Congestion Windows,” RFC 
3649, Dec. 2003.
[RFC 3746] L. Yang, R. Dantu, T. Anderson, R. Gopal, “Forwarding and Control 
Element Separation (ForCES) Framework,” Internet, RFC 3746, Apr. 2004.
[RFC 3748] B. Aboba, L. Blunk, J. Vollbrecht, J. Carlson, H. Levkowetz, Ed., 
“Extensible Authentication Protocol (EAP),” RFC 3748, June 2004.
[RFC 3782] S. Floyd, T. Henderson, A. Gurtov, “The NewReno Modification to 
TCP’s Fast Recovery Algorithm,” RFC 3782, Apr. 2004.
References         801
[RFC 4213] E. Nordmark, R. Gilligan, “Basic Transition Mechanisms for IPv6 
Hosts and Routers,” RFC 4213, Oct. 2005.
[RFC 4271] Y. Rekhter, T. Li, S. Hares, Ed., “A Border Gateway Protocol 4 (BGP-
4),” RFC 4271, Jan. 2006.
[RFC 4272] S. Murphy, “BGP Security Vulnerabilities Analysis,” RFC 4274, Jan. 
2006.
[RFC 4291] R. Hinden, S. Deering, “IP Version 6 Addressing Architecture,” RFC 
4291, Feb. 2006.
[RFC 4340] E. Kohler, M. Handley, S. Floyd, “Datagram Congestion Control 
Protocol (DCCP),” RFC 4340, Mar. 2006.
[RFC 4443] A. Conta, S. Deering, M. Gupta, Ed., “Internet Control Message Pro-
tocol (ICMPv6) for the Internet Protocol Version 6 (IPv6) Specification,”  
RFC 4443, Mar. 2006.
[RFC 4346] T. Dierks, E. Rescorla, “The Transport Layer Security (TLS) Protocol 
Version 1.1,” RFC 4346, Apr. 2006.
[RFC 4514] K. Zeilenga, Ed., “Lightweight Directory Access Protocol (LDAP): 
String Representation of Distinguished Names,” RFC 4514, June 2006.
[RFC 4601] B. Fenner, M. Handley, H. Holbrook, I. Kouvelas, “Protocol  
Independent Multicast—Sparse Mode (PIM-SM): Protocol Specification  
(Revised),” RFC 4601, Aug. 2006.
[RFC 4632] V. Fuller, T. Li, “Classless Inter-domain Routing (CIDR): The Inter-
net Address Assignment and Aggregation Plan,” RFC 4632, Aug. 2006.
[RFC 4960] R. Stewart, ed., “Stream Control Transmission Protocol,” RFC 4960, 
Sept. 2007.
[RFC 4987] W. Eddy, “TCP SYN Flooding Attacks and Common Mitigations,” 
RFC 4987, Aug. 2007.
[RFC 5000] RFC editor, “Internet Official Protocol Standards,” RFC 5000, May 
2008.
[RFC 5109] A. Li (ed.), “RTP Payload Format for Generic Forward Error Correc-
tion,” RFC 5109, Dec. 2007.
[RFC 5216] D. Simon, B. Aboba, R. Hurst, “The EAP-TLS Authentication Proto-
col,” RFC 5216, Mar. 2008.
[RFC 5218] D. Thaler, B. Aboba, “What Makes for a Successful Protocol?,” RFC 
5218, July 2008.
802         References
[RFC 5321] J. Klensin, “Simple Mail Transfer Protocol,” RFC 5321, Oct. 2008.
[RFC 5322] P. Resnick, Ed., “Internet Message Format,” RFC 5322, Oct. 2008.
[RFC 5348] S. Floyd, M. Handley, J. Padhye, J. Widmer, “TCP Friendly Rate 
Control (TFRC): Protocol Specification,” RFC 5348, Sept. 2008.
[RFC 5389] J. Rosenberg, R. Mahy, P. Matthews, D. Wing, “Session Traversal 
Utilities for NAT (STUN),” RFC 5389, Oct. 2008.
[RFC 5411] J Rosenberg, “A Hitchhiker’s Guide to the Session Initiation Protocol 
(SIP),” RFC 5411, Feb. 2009.
[RFC 5681] M. Allman, V. Paxson, E. Blanton, “TCP Congestion Control,” RFC 
5681, Sept. 2009.
[RFC 5944] C. Perkins, Ed., “IP Mobility Support for IPv4, Revised,” RFC 5944, 
Nov. 2010.
[RFC 6265] A Barth, “HTTP State Management Mechanism,” RFC 6265, Apr. 
2011.
[RFC 6298] V. Paxson, M. Allman, J. Chu, M. Sargent, “Computing TCP’s Re-
transmission Timer,” RFC 6298, June 2011.
[RFC 7020] R. Housley, J. Curran, G. Huston, D. Conrad, “The Internet Numbers 
Registry System,” RFC 7020, Aug. 2013.
[RFC 7094] D. McPherson, D. Oran, D. Thaler, E. Osterweil, “Architectural Con-
siderations of IP Anycast,” RFC 7094, Jan. 2014.
[RFC 7323] D. Borman, R. Braden, V. Jacobson, R. Scheffenegger (ed.), “TCP 
Extensions for High Performance,” RFC 7323, Sept. 2014.
[RFC 7540] M. Belshe, R. Peon, M. Thomson (Eds), “Hypertext Transfer Protocol 
Version 2 (HTTP/2),” RFC 7540, May 2015.
[Richter 2015] P. Richter, M. Allman, R. Bush, V. Paxson, “A Primer on IPv4 
Scarcity,” ACM SIGCOMM Computer Communication Review, Vol. 45, No. 2 
(Apr. 2015), pp. 21–32.
[Roberts 1967] L. Roberts, T. Merril, “Toward a Cooperative Network of Time-
Shared Computers,” AFIPS Fall Conference (Oct. 1966).
[Rodriguez 2010] R. Rodrigues, P. Druschel, “Peer-to-Peer Systems,” Communi-
cations of the ACM, Vol. 53, No. 10 (Oct. 2010), pp. 72–82.
[Rohde 2008] Rohde, Schwarz, “UMTS Long Term Evolution (LTE) Technology 
Introduction,” Application Note 1MA111.
References         803
[Rom 1990] R. Rom, M. Sidi, Multiple Access Protocols: Performance and Analy-
sis, Springer-Verlag, New York, 1990.
[Root Servers 2016] Root Servers home page, http://www.root-servers.org/
[RSA 1978] R. Rivest, A. Shamir, L. Adelman, “A Method for Obtaining Digital 
Signatures and Public-key Cryptosystems,” Communications of the ACM, Vol. 21, 
No. 2 (Feb. 1978), pp. 120–126.
[RSA Fast 2012] RSA Laboratories, “How Fast Is RSA?” http://www.rsa.com/
rsalabs/node.asp?id=2215
[RSA Key 2012] RSA Laboratories, “How Large a Key Should Be Used in the 
RSA Crypto System?” http://www.rsa.com/rsalabs/node.asp?id=2218
[Rubenstein 1998] D. Rubenstein, J. Kurose, D. Towsley, “Real-Time Reliable 
Multicast Using Proactive Forward Error Correction,” Proceedings of NOSSDAV 
‘98 (Cambridge, UK, July 1998).
[Ruiz-Sanchez 2001] M. Ruiz-Sánchez, E. Biersack, W. Dabbous, “Survey and 
Taxonomy of IP Address Lookup Algorithms,” IEEE Network Magazine, Vol. 15, 
No. 2 (Mar./Apr. 2001), pp. 8–23.
[Saltzer 1984] J. Saltzer, D. Reed, D. Clark, “End-to-End Arguments in System 
Design,” ACM Transactions on Computer Systems (TOCS), Vol. 2, No. 4 (Nov. 
1984).
[Sandvine 2015] “Global Internet Phenomena Report, Spring 2011,” http://www.
sandvine.com/news/global broadband trends.asp, 2011.
[Sardar 2006] B. Sardar, D. Saha, “A Survey of TCP Enhancements for Last-Hop 
Wireless Networks,” IEEE Commun. Surveys and Tutorials, Vol. 8, No. 3 (2006), 
pp. 20–34.
[Saroiu 2002] S. Saroiu, P. K. Gummadi, S. D. Gribble, “A Measurement Study of 
Peer-to-Peer File Sharing Systems,” Proc. of Multimedia Computing and Network-
ing (MMCN) (2002).
[Sauter 2014] M. Sauter, From GSM to LTE-Advanced, John Wiley and Sons, 
2014.
[Savage 2015] D. Savage, J. Ng, S. Moore, D. Slice, P. Paluch, R. White,  
“Enhanced Interior Gateway Routing Protocol,” Internet Draft, draft- 
savage-eigrp-04.txt, Aug. 2015.
[Saydam 1996] T. Saydam, T. Magedanz, “From Networks and Network Man-
agement into Service and Service Management,” Journal of Networks and System 
Management, Vol. 4, No. 4 (Dec. 1996), pp. 345–348.
804         References
[Schiller 2003] J. Schiller, Mobile Communications 2nd edition, Addison Wesley, 
2003.
[Schneier 1995] B. Schneier, Applied Cryptography: Protocols, Algorithms, and 
Source Code in C, John Wiley and Sons, 1995.
[Schulzrinne-RTP 2012] Henning Schulzrinne’s RTP site, http://www.cs.columbia 
.edu/~hgs/rtp
[Schulzrinne-SIP 2016] Henning Schulzrinne’s SIP site, http://www.cs.columbia.
edu/~hgs/sip
[Schwartz 1977] M. Schwartz, Computer-Communication Network Design and 
Analysis, Prentice-Hall, Englewood Cliffs, NJ, 1997.
[Schwartz 1980] M. Schwartz, Information, Transmission, Modulation, and Noise, 
McGraw Hill, New York, NY 1980.
[Schwartz 1982] M. Schwartz, “Performance Analysis of the SNA Virtual Route  
Pacing Control,” IEEE Transactions on Communications, Vol. 30, No. 1 (Jan. 
1982), pp. 172–184.
[Scourias 2012] J. Scourias, “Overview of the Global System for Mobile Commu-
nications: GSM.” http://www.privateline.com/PCS/GSM0.html
[SDNHub 2016] SDNHub, “App Development Tutorials,” http://sdnhub.org/ 
tutorials/
[Segaller 1998] S. Segaller, Nerds 2.0.1, A Brief History of the Internet, TV Books, 
New York, 1998.
[Sekar 2011] V. Sekar, S. Ratnasamy, M. Reiter, N. Egi, G. Shi, “ The Middle-
box Manifesto: Enabling Innovation in Middlebox Deployment,” Proc. 10th ACM 
Workshop on Hot Topics in Networks (HotNets), Article 21, 6 pages.
[Serpanos 2011] D. Serpanos, T. Wolf, Architecture of Network Systems, Morgan 
Kaufmann Publishers, 2011.
[Shacham 1990] N. Shacham, P. McKenney, “Packet Recovery in High-Speed 
Networks Using Coding and Buffer Management,” Proc. 1990 IEEE INFOCOM 
(San Francisco, CA, Apr. 1990), pp. 124–131.
[Shaikh 2001] A. Shaikh, R. Tewari, M. Agrawal, “On the Effectiveness of DNS-
based Server Selection,” Proc. 2001 IEEE INFOCOM.
[Singh 1999] S. Singh, The Code Book: The Evolution of Secrecy from Mary, 
Queen of Scotsto Quantum Cryptography, Doubleday Press, 1999.
References         805
[Singh 2015] A. Singh, J. Ong,. Agarwal, G. Anderson, A. Armistead, R. Banno, S. 
Boving, G. Desai, B. Felderman, P. Germano, A. Kanagala, J. Provost, J. Simmons, 
E. Tanda, J. Wanderer, U. Hölzle, S. Stuart, A. Vahdat, “Jupiter Rising: A Decade 
of Clos Topologies and Centralized Control in Google’s Datacenter Network,” 
Sigcomm, 2015.
[SIP Software 2016] H. Schulzrinne Software Package site, http://www.
cs.columbia.edu/IRT/software
[Skoudis 2004] E. Skoudis, L. Zeltser, Malware: Fighting Malicious Code, Pren-
tice Hall, 2004.
[Skoudis 2006] E. Skoudis, T. Liston, Counter Hack Reloaded: A Step-by-Step 
Guide to Computer Attacks and Effective Defenses (2nd Edition), Prentice Hall, 
2006.
[Smith 2009] J. Smith, “Fighting Physics: A Tough Battle,” Communications of the 
ACM, Vol. 52, No. 7 (July 2009), pp. 60–65.
[Snort 2012] Sourcefire Inc., Snort homepage, http://http://www.snort.org/
[Solensky 1996] F. Solensky, “IPv4 Address Lifetime Expectations,” in IPng: 
Internet Protocol Next Generation (S. Bradner, A. Mankin, ed.), Addison-Wesley, 
Reading, MA, 1996.
[Spragins 1991] J. D. Spragins, Telecommunications Protocols and Design, 
Addison-Wesley, Reading, MA, 1991.
[Srikant 2004] R. Srikant, The Mathematics of Internet Congestion Control, 
Birkhauser, 2004
[Steinder 2002] M. Steinder, A. Sethi, “Increasing Robustness of Fault Localiza-
tion Through Analysis of Lost, Spurious, and Positive Symptoms,” Proc. 2002 
IEEE INFOCOM.
[Stevens 1990] W. R. Stevens, Unix Network Programming, Prentice-Hall, Engle-
wood Cliffs, NJ.
[Stevens 1994] W. R. Stevens, TCP/IP Illustrated, Vol. 1: The Protocols, Addison-
Wesley, Reading, MA, 1994.
[Stevens 1997] W.R. Stevens, Unix Network Programming, Volume 1: Networking 
APIs-Sockets and XTI, 2nd edition, Prentice-Hall, Englewood Cliffs, NJ, 1997.
[Stewart 1999] J. Stewart, BGP4: Interdomain Routing in the Internet, Addison-
Wesley, 1999.
806         References
[Stone 1998] J. Stone, M. Greenwald, C. Partridge, J. Hughes, “Performance of 
Checksums and CRC’s Over Real Data,” IEEE/ACM Transactions on Networking, 
Vol. 6, No. 5 (Oct. 1998), pp. 529–543.
[Stone 2000] J. Stone, C. Partridge, “When Reality and the Checksum Disagree,” 
Proc. 2000 ACM SIGCOMM (Stockholm, Sweden, Aug. 2000).
[Strayer 1992] W. T. Strayer, B. Dempsey, A. Weaver, XTP: The Xpress Transfer 
Protocol, Addison-Wesley, Reading, MA, 1992.
[Stubblefield 2002] A. Stubblefield, J. Ioannidis, A. Rubin, “Using the Fluhrer, 
Mantin, and Shamir Attack to Break WEP,” Proceedings of 2002 Network and 
Distributed Systems Security Symposium (2002), pp. 17–22.
[Subramanian 2000] M. Subramanian, Network Management: Principles and 
Practice, Addison-Wesley, Reading, MA, 2000.
[Subramanian 2002] L. Subramanian, S. Agarwal, J. Rexford, R. Katz, “Charac-
terizing the Internet Hierarchy from Multiple Vantage Points,” Proc. 2002 IEEE 
INFOCOM.
[Sundaresan 2006] K.Sundaresan, K. Papagiannaki, “The Need for Cross-layer 
Information in Access Point Selection,” Proc. 2006 ACM Internet Measurement 
Conference (Rio De Janeiro, Oct. 2006).
[Suh 2006] K. Suh, D. R. Figueiredo, J. Kurose and D. Towsley, “Characterizing 
and Detecting Relayed Traffic: A Case Study Using Skype,” Proc. 2006 IEEE 
INFOCOM (Barcelona, Spain, Apr. 2006).
[Sunshine 1978] C. Sunshine, Y. Dalal, “Connection Management in Transport 
Protocols,” Computer Networks, North-Holland, Amsterdam, 1978.
[Tariq 2008] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, M. Ammar, “An-
swering What-If Deployment and Configuration Questions with WISE,” Proc. 2008 
ACM SIGCOMM (Aug. 2008).
[TechnOnLine 2012] TechOnLine, “Protected Wireless Networks,” online  
webcast tutorial, http://www.techonline.com/community/tech_topic/internet/21752
[Teixeira 2006] R. Teixeira, J. Rexford, “Managing Routing Disruptions in Inter-
net Service Provider Networks,” IEEE Communications Magazine (Mar. 2006).
[Think 2012] Technical History of Network Protocols, “Cyclades,” http://www.
cs.utexas.edu/users/chris/think/Cyclades/index.shtml
[Tian 2012] Y. Tian, R. Dey, Y. Liu, K. W. Ross, “China’s Internet: Topology 
Mapping and Geolocating,” IEEE INFOCOM Mini-Conference 2012 (Orlando, FL, 
2012).
References         807
[TLD list 2016] TLD list maintained by Wikipedia, https://en.wikipedia.org/wiki/
List_of_Internet_top-level_domains
[Tobagi 1990] F. Tobagi, “Fast Packet Switch Architectures for Broadband Inte-
grated Networks,” Proc. 1990 IEEE INFOCOM, Vol. 78, No. 1 (Jan. 1990), pp. 
133–167.
[TOR 2016] Tor: Anonymity Online, http://www.torproject.org
[Torres 2011] R. Torres, A. Finamore, J. R. Kim, M. M. Munafo, S. Rao, “Dissect-
ing Video Server Selection Strategies in the YouTube CDN,” Proc. 2011 Int. Conf. 
on Distributed Computing Systems.
[Tourrilhes 2014] J. Tourrilhes, P. Sharma, S. Banerjee, J. Petit, “SDN and Open-
flow Evolution: A Standards Perspective,” IEEE Computer Magazine, Nov. 2014,  
pp. 22–29.
[Turner 1988] J. S. Turner, “Design of a Broadcast packet switching network,” 
IEEE Transactions on Communications, Vol. 36, No. 6 (June 1988), pp. 734–743.
[Turner 2012] B. Turner, “2G, 3G, 4G Wireless Tutorial,” http://blogs.nmscom-
munications.com/communications/2008/10/2g-3g-4g-wireless-tutorial.html
[UPnP Forum 2016] UPnP Forum homepage, http://www.upnp.org/
[van der Berg 2008] R. van der Berg, “How the ’Net Works: An Introduction to 
Peering and Transit,” http://arstechnica.com/guides/other/peering-and-transit.ars
[van der Merwe 1998] J. van der Merwe, S. Rooney, I. Leslie, S. Crosby, “The 
Tempest: A Practical Framework for Network Programmability,” IEEE Network, 
Vol. 12, No. 3 (May 1998), pp. 20–28.
[Varghese 1997] G. Varghese, A. Lauck, “Hashed and Hierarchical Timing 
Wheels: Efficient Data Structures for Implementing a Timer Facility,” IEEE/ACM 
Transactions on Networking, Vol. 5, No. 6 (Dec. 1997), pp. 824–834.
[Vasudevan 2012] S. Vasudevan, C. Diot, J. Kurose, D. Towsley, “Facilitating Ac-
cess Point Selection in IEEE 802.11 Wireless Networks,” Proc. 2005 ACM Internet 
Measurement Conference, (San Francisco CA, Oct. 2005).
[Villamizar 1994] C. Villamizar, C. Song. “High Performance tcp in ansnet,” 
ACM SIGCOMM Computer Communications Review, Vol. 24, No. 5 (1994),  
pp. 45–60.
[Viterbi 1995] A. Viterbi, CDMA: Principles of Spread Spectrum Communication, 
Addison-Wesley, Reading, MA, 1995.
[Vixie 2009] P. Vixie, “What DNS Is Not,” Communications of the ACM, Vol. 52, 
No. 12 (Dec. 2009), pp. 43–47.
808         References
[Wakeman 1992] I. Wakeman, J. Crowcroft, Z. Wang, D. Sirovica, “Layering 
Considered Harmful,” IEEE Network (Jan. 1992), pp. 20–24.
[Waldrop 2007] M. Waldrop, “Data Center in a Box,” Scientific American (July 
2007).
[Wang 2004] B. Wang, J. Kurose, P. Shenoy, D. Towsley, “Multimedia Streaming 
via TCP: An Analytic Performance Study,” Proc. 2004 ACM Multimedia Confer-
ence (New York, NY, Oct. 2004).
[Wang 2008] B. Wang, J. Kurose, P. Shenoy, D. Towsley, “Multimedia Streaming  
via TCP: An Analytic Performance Study,” ACM Transactions on Multimedia 
Computing Communications and Applications (TOMCCAP), Vol. 4, No. 2 (Apr. 
2008), p. 16. 1–22.
[Wang 2010] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. S. E. 
Ng, M. Kozuch, M. Ryan, “c-Through: Part-time Optics in Data Centers,” Proc. 
2010 ACM SIGCOMM.
[Wei 2006] W. Wei, C. Zhang, H. Zang, J. Kurose, D. Towsley, “Inference and 
Evaluation of Split-Connection Approaches in Cellular Data Networks,” Proc.  
Active and Passive Measurement Workshop (Adelaide, Australia, Mar. 2006).
[Wei 2007] D. X. Wei, C. Jin, S. H. Low, S. Hegde, “FAST TCP: Motivation, 
Architecture, Algorithms, Performance,” IEEE/ACM Transactions on Networking 
(2007).
[Weiser 1991] M. Weiser, “The Computer for the Twenty-First Century,”  
Scientific American (Sept. 1991): 94–10. http://www.ubiq.com/hypertext/weiser/ 
SciAmDraft3.html
[White 2011] A. White, K. Snow, A. Matthews, F. Monrose, “Hookt on fon-iks: 
Phonotactic Reconstruction of Encrypted VoIP Conversations,” IEEE Symposium 
on Security and Privacy, Oakland, CA, 2011.
[Wigle.net 2016] Wireless Geographic Logging Engine, http://www.wigle.net
[Wiki Satellite 2016] Satellite Internet access, https://en.wikipedia.org/wiki/Satel-
lite_Internet_access
[Wireshark 2016] Wireshark homepage, http://www.wireshark.org
[Wischik 2005] D. Wischik, N. McKeown, “Part I: Buffer Sizes for Core  
Routers,” ACM SIGCOMM Computer Communications Review, Vol. 35, No. 3 
(July 2005).
References         809
[Woo 1994] T. Woo, R. Bindignavle, S. Su, S. Lam, “SNP: an interface for secure 
network programming,” Proc. 1994 Summer USENIX (Boston, MA, June 1994), 
pp. 45–58.
[Wright 2015] J. Wright, J. Wireless Security Secrets & Solutions, 3e, “Hacking 
Exposed Wireless,” McGraw-Hill Education, 2015.
[Wu 2005] J. Wu, Z. M. Mao, J. Rexford, J. Wang, “Finding a Needle in a Hay-
stack: Pinpointing Significant BGP Routing Changes in an IP Network,” Proc. 
USENIX NSDI (2005).
[Xanadu 2012] Xanadu Project homepage, http://www.xanadu.com/
[Xiao 2000] X. Xiao, A. Hannan, B. Bailey, L. Ni, “Traffic Engineering with 
MPLS in the Internet,” IEEE Network (Mar./Apr. 2000).
[Xu 2004] L. Xu, K Harfoush, I. Rhee, “Binary Increase Congestion Control (BIC) 
for Fast Long-Distance Networks,” IEEE INFOCOM 2004, pp. 2514–2524.
[Yavatkar 1994] R. Yavatkar, N. Bhagwat, “Improving End-to-End Performance 
of TCP over Mobile Internetworks,” Proc. Mobile 94 Workshop on Mobile Com-
puting Systems and Applications (Dec. 1994).
[YouTube 2009] YouTube 2009, Google container data center tour, 2009.
[YouTube 2016] YouTube Statistics, 2016, https://www.youtube.com/yt/press/ 
statistics.html
[Yu 2004] Yu, Fang, H. Katz, Tirunellai V. Lakshman. “Gigabit Rate Packet 
Pattern-Matching Using TCAM,” Proc. 2004 Int. Conf. Network Protocols,  
pp. 174–183.
[Yu 2011] M. Yu, J. Rexford, X. Sun, S. Rao, N. Feamster, “A Survey of VLAN 
Usage in Campus Networks,” IEEE Communications Magazine, July 2011.
[Zegura 1997] E. Zegura, K. Calvert, M. Donahoo, “A Quantitative Comparison of 
Graph-based Models for Internet Topology,” IEEE/ACM Transactions on Network-
ing, Vol. 5, No. 6, (Dec. 1997). See also http://www.cc.gatech.edu/projects/gtim for 
a software package that generates networks with a transit-stub structure.
[Zhang 1993] L. Zhang, S. Deering, D. Estrin, S. Shenker, D. Zappala, “RSVP: 
A New Resource Reservation Protocol,” IEEE Network Magazine, Vol. 7, No. 9 
(Sept. 1993), pp. 8–18.
[Zhang 2007] L. Zhang, “A Retrospective View of NAT,” The IETF Journal, Vol. 
3, Issue 2 (Oct. 2007).
810         References
[Zhang 2015] G. Zhang, W. Liu, X. Hei, W. Cheng, “Unreeling Xunlei Kankan: 
Understanding Hybrid CDN-P2P Video-on-Demand Streaming,” IEEE Transac-
tions on Multimedia, Vol. 17, No. 2, Feb. 2015.
[Zhang X 2102] X. Zhang, Y. Xu, Y. Liu, Z. Guo, Y. Wang, “Profiling Skype 
Video Calls: Rate Control and Video Quality,” IEEE INFOCOM (Mar. 2012).
[Zink 2009] M. Zink, K. Suh, Y. Gu, J. Kurose, “Characteristics of YouTube Network 
Traffic at a Campus Network—Measurements, Models, and Implications,” Computer 
Networks, Vol. 53, No. 4, pp. 501–514, 2009.
811
Index
A
AAC. See Advanced Audio Coding
ABR. See ATM Available Bit Rate
Abramson, Norman, 90, 488, 506
access control lists, 682, 684
access networks, 40–46, 432
cable, 42–43, 92, 491, 493–495
dial-up access, 44, 519
DSL, 41–42, 92
enterprise, 44–45
Ethernet, 44–45
FTTH, 43–44, 92
HFC, 42
LTE, 46, 580, 581, 585–588
radio access networks, 584–588
satellite, 44, 467
3G, 46
WiFi, 44–45
access points (APs), 550, 551
in infrastructure LANs, 562
MAC addresses, 561, 563
mobility between, 588–589
power management and, 576
scanning for, 564
ACK (positive acknowledgments), 
238–242
corrupted, 240
DHCP, 372–373, 530
duplicate, 242, 277
in 802.11 RTC/CTS system, 
569–570
TCP generation recommendation, 
278
ACK bit, 264
TCP, 681–682
ack clocking, 331
ACK frames, 569–570
acknowledged segments, 299
acknowledgement frames, 567
acknowledgements
cumulative, 252, 266
link-layer, 565, 566, 567
negative, 238–242, 269, 476
piggybacked, 269
positive, 238–242, 277, 372–373, 
569–570
TCP, 265–267, 280
acknowledgment number, 265–267
piggybacked, 269
Telnet and, 267–269
acknowledgment number field, 264
ACK received events, 273, 274
active optical networks (AONs), 43
active queue management (AQM), 
352
active scanning, 564
adapters, 471, 472
ARP query and, 500
CSMA/CD operation and, 490–491
datagram transmission and, 499, 
501–502
802.11, 561, 565
error detection in, 476
Ethernet frames and, 504–506
jabbering, 512
LAN-on-motherboard configuration, 
471
layer independence and, 498
MAC addresses, 496–498
on separate cards, 471
812         Index
adaptive backoff, 544
adaptive congestion control,  
231
adaptive HTTP streaming, 709
adaptive playout delay, 720–722
additive-increase,  
multiplicative-decrease  
(AIMD), 305
fairness of, 307–310
address aggregation, 367
addresses. See also IP addresses; 
MAC addresses
anycast, 377
care-of, 592, 593–595, 601
foreign, 592
IP broadcast, 369, 371
LAN, 496
MAC broadcast, 498, 500
mobile node, 589
obtaining with DHCP,  
370–373
permanent, 592
physical, 496
realm with private, 373
SIP, 733–734
addressing, 117–118
classful, 366–367
IPv4, 362–373
link-layer, 496–502
mobility and, 590–592
mobility management and,  
590–592
processes, 117–118
SIP, 733–734
address lease time, 372
Address Resolution Protocol (ARP), 
496, 498–501, 531
data center network design and, 
525
Address Supporting Organization of 
ICANN, 369
ad hoc networks, 562
mobile, 552–553, 590
vehicular, 553
Adleman, Leonard, 633
administrative autonomy, 420
Advanced Audio Coding (AAC), 706, 
728
Advanced Research Projects Agency 
(ARPA), 88, 399, 544
AES (Advanced Encryption 
Standard), 630, 664
agent advertisement, 599, 602
agent discovery, 598, 599–601
agent solicitation, 600
aging time, 511
AH protocol. See Authentication 
Header protocol
AIMD. See additive-increase, 
­
multiplicative-decrease
Akamai, 178, 183
aliasing
host, 156, 164
mail server, 156
ALOHANet, 88, 90, 488
ALOHA protocols, 483, 506
backoff in, 544
pure, 486
slotted, 484–486, 544
alternating-bit protocol, 245, 246
Alto computers, 506
Amazon, 92, 528, 703
cloud services, 182
Netflix and, 182–184
video streaming, 175, 707
anchor foreign agent, 597
anchor MSC, 607
Andreessen, Marc, 91, 212–213
Andreessen Horowitz, 212
Android devices, 46
anomaly-based systems, 689
anonymity, 686
Index         813
anycast address, 377
AONs. See active optical networks
Apache Web server, 186, 227
API. See Application Programming 
Interface
application architecture, 114
application delay, 71
application gateways, 680, 684–685, 
687
application layer, 78
application-layer message, 82
application-layer protocols, 123
defining, 124–125
DNS, 78
FTP, 78
HTTP, 78, 124–125
Skype, 124
SMTP, 78, 125
application-level transport reliability, 
231–232
Application Programming Interface 
(API), 117
application protocols, well-known, 
223
applications. See also multimedia 
­
applications; network applica-
tions
bandwidth-sensitive, 120
delay-sensitive, 708
distributed, 33
elastic, 120
loss-tolerant, 119, 708
network-service, 444
peer-to-peer, 168–175
real-time conversational, 728–736
SDN control, 438–440
transport services available to, 
118–121
APs. See access points
AQM. See active queue management
A records, 163
ARP. See Address Resolution 
Protocol
ARPA. See Advanced Research 
Projects Agency
ARPAnet, 262
ALOHAnet connection to, 488
Cerf on, 399
development of, 88–91
Lam on, 544
Metcalfe and, 506
routing algorithms, 407, 414
ARPAnet Satellite System, 544
ARP packet, 500
ARP query, 531
ARP reply, 531
ARP table, 500
ARQ (Automatic Repeat reQuest) 
protocols, 238, 476
ASN. See autonomous system number
AS numbers. See autonomous system 
number
AS-PATH, 427, 429
ASs. See autonomous systems
associate, 563
association
in 802.11, 562–865
security, 668–669, 673
assured forwarding PHB, 750
Atheros AR5006, 471
ATM, 545
congestion control, 296
delay and bandwidth guarantees, 
340
Ethernet and, 503
IP device interconnection with, 520
MPLS headers for, 520
Q2931b and, 753
QoS information in, 753
SDN and, 443
ATM Available Bit Rate (ABR), 289
congestion control, 296
814         Index
AT&T, 33, 403, 464, 764
audio
AAC, 706, 728
jitter removal for, 719–722
MP3, 706, 728
properties of, 705–707
quantization, 706
RTP payloads, 731
Skype quality adaptation for,  
725
streaming, 707–708
authentication
802.11i, 677–678
end-point, 86–87, 623
MD5, 422
message, code for, 641–645,  
662–663, 670
in OSPF, 422
receiver, 655
sender, 655, 656
simple, 422
wireless LANs, 564–565
Authentication Header protocol  
(AH ­
protocol), 668
authentication key, 641
authentication protocol,  
649–653
authoritative DNS servers,  
160, 532
autonomous system number (ASN), 
420
autonomous systems (ASs), 420
in BGP route advertisement,  
424–426
hierarchy within, 422–423
iBGP connections within, 425
routing between, 419–423,  
433, 444
availability zones, 528
average throughput, 72
Azure, 93
B
B4, 403, 441, 444
backbone providers, 432
backoff
adaptive, 544
binary exponential, 491
random, 567
in slotted ALOHA, 544
back pressure, 714
bandwidth, 56–57
application sensitivity to, 120
ATM guarantees, 340
best-effort service and, 340
bottleneck, 139
congestion control and, 299
fairness and, 307–310
guaranteed minimal, 339–340
host-to-host, 526, 527
HTTP streaming and, 176–177
link-layer switching properties and, 
512
memory, 347
P2P applications and, 168, 170
QoS guarantees, 738, 753
Skype usage of, 727
TCP and high, 306–307
throughput and, 119–120
traffic class isolation and, 743–744
UDP streaming and, 709, 711
video early termination wasting, 
716
video prefetching and, 712
video properties and, 704–705
video streaming quality adaptation 
to, 708
Web caching and, 139–140
bandwidth flooding, 84, 85, 167
bandwidth probing, 299, 305
bandwidth provisioning, 739
bandwidth-sensitive applications, 120
Baran, Paul, 88
Index         815
base HTML file, 126
base station (BS), 550, 561
handoff and, 605–607
base station controller (BSC), 582
base station subsystem (BSS), 582
base transceiver station (BTS), 582
basic service set (BSS), 561, 572
mobility across, 574–575
BBN, 89
beacon frames, 563, 564, 576
beacon signals, 606
Bellman-Ford equation, 412–413
Bellovin, Steven, 701–702
BER. See bit error rate
Berkeley Internet Name Domain 
(BIND), 155
Berners-Lee, Tim, 91
best-effort delivery services, 220
best-effort services, 340
dimensioning, 737, 739–740
limitations of, 716–717
multiple classes of service for, 
740–747
BGP. See Border Gateway Protocol
BGP attributes, 426–427
BGP connection, 425
bidirectional data transfer, 236
binary exponential backoff, 491
BIND. See Berkeley Internet Name 
Domain
bind(), 223
bit error rate (BER), 554–556
rate adaptation and, 575
bit errors
data transfer over channel with, 
237–242
data transfer over lossy channel 
with, 242–245
undetected, 473
bit-level error detection and 
correction, 472
BITNET, 90
BitTorrent, 169, 172–175, 186,  
728
trackers, 172–174
trading algorithm, 174
blades, 523
block ciphers, 628–630
Bluetooth, 577–578
Boggs, David, 503, 506
Border Gateway Protocol (BGP), 402, 
407, 414, 423–435, 532
determining best routes, 426–430
in Google SDN, 441
hot potato routing, 428–429
internal BGP, 425–426
IP-anycast implementation with, 
430–431
outside-AS destinations, 428
role of, 423–424
route attributes, 427
route information advertisement, 
424–426
route-selection algorithm, 429–430
routing policy, 431–434
routing tables, 429–430
border routers, 422–423, 523
botnets, 84
bottleneck bandwidth, 139
bottleneck link, 73
TCP fairness and, 307–308
bounded delay, 339
bring home, 178
broadband Internet, 92
broadcast
in ALOHA, 90, 486, 487, 488
ARP messages, 499–501, 531
CSMA and, 488–489
CSMA/CD and, 490
CTS and RTS frames, 569
DHCP requests, 529–530
Ethernet links, 508
816         Index
broadcast. (continued)
forwarding to, 386
link-layer, 479
link-state, 407, 419
MAC address for, 498, 500
in OSPF, 421–422
packet sniffing and, 86
probe frames, 564
in switch poisoning, 513
wireless LANs, 479
broadcast address
IP, 369, 371
MAC, 498, 500
broadcast channels, 481
broadcast link, 479–481
broadcast media, 126, 331
live streaming as, 709
broadcast storms, 514–515
Brooks, Fred, 701
browsers, 709
brute-force attacks, 629
BS. See base station
BSC. See base station controller
BSS. See base station subsystem; 
basic service set
BTS. See base transceiver station
buffered distributors, 508
buffer overflows, congestion causing, 
294–295
buffers
client application, 713–714
client-side, 710
output, 52
receive, 281, 282
send, 263
sizing for routers, 353
in streaming, 713–714
TCP, 713–714
buffer starvation, 717
bus, switching via, 348
Bush, Vannevar, 91
C
CA. See Certification Authority
cable Internet access, 42–43, 92
binary exponential backoff in, 491
DOCSIS, 493–495
cable modem termination system 
(CMTS), 43, 493–494
caching, 331
DNS, 162–163
push, 183–184
Web, 138–144
Caesar cipher, 625
call admission, 752, 753
call routing, to mobile users, 604–605
call setup protocol, 753
call setup signaling, 753
canonical hostname, 156
care-of address (COA), 592
in agent discovery, 601
indirect routing and, 593–595
carrier sense multiple access (CSMA), 
483, 487–489
carrier sensing, 487
carrier-sensing random access, 565
CAST, 658
CBC. See Cipher Block Chaining
CD. See compact disk
CDMA. See code division multiple 
access
CDNs. See Content Distribution 
Networks
cells, 581
cell towers, 550
cellular data networks, 551
CDMA in, 556
cellular Internet access, 579
cellular network architecture,  
579–582
4G, 585–588
3G, 582–585
2G, 581–582
Index         817
cellular networks
CDMA use in, 483
4G, 548, 580, 581, 585–588
GSM, 579–582, 605–608
LTE, 46, 580, 581, 585–588
mobility management in, 602–610
3G, 46, 548, 551, 582–585, 705
2G, 581–582
cellular telephony, 46, 92, 483, 547
centralized designs, 157–158
centralized routing algorithm, 406–
407
in LS algorithm, 408
central office (CO), 41–42
Cerf, Vinton, 90, 262, 399–400, 544
CERT Coordination Center, 624
certificates, 648
Certification Authority (CA), 647–
648, 658
channel partitioning protocols, 481–
483, 556, 565
CDMA, 485
FDM, 485
TDM, 484–485
channel propagation delay, 489
channels
with bit errors, 238–244
broadcast, 481
802.11, 562–565
lossy, 242–245
multiple access, 479, 480
perfectly reliable, 236, 237
satellite radio, 49
terrestrial radio, 48–49
channel utilization, 247
Check Point, 680, 688
checksum field, 264
checksumming, 476
checksums
corrupted ACK and NAK packet 
detection, 240
error detection with, 476
hash functions and, 639–640
Internet, 476, 639–640
IPv4 headers, 359–360
UDP, 232–234
China Telecom, 403
China Unicom, 403
chipping rate, 556
choke packets, 296
chosen-plaintext attacks, 627
chunks
BitTorrent, 172–174
Netflix streaming platform, 183
CIDR. See Classless Interdomain 
Routing
Cipher Block Chaining (CBC), 630–
632, 669
in SSL, 664
ciphertext, 625
ciphertext-only attacks, 627
circuit, 55
circuit switching, 55–59
packet switching versus, 58–59
Cisco, 32, 92, 680, 688
Cisco 12000 series, switching fabric, 
348–349
Cisco Catalyst 6500 Series, 346
switching bus, 348
Cisco Catalyst 7600 Series, 346
switching fabric, 349
Cisco Catalyst 8500 Series, switching 
fabric, 348
Cisco CRS, switching strategy, 349
Clark, Jim, 91, 212
classful addressing, 366–367
Classless Interdomain Routing 
(CIDR), 365–366, 530
cleartext, 625
Clear to Send (CTS), 568–570
client application buffers, 713–714
client buffering, 710
818         Index
client process, 261
clients, 39, 114
processes, 116–117
client-server architecture, 114, 115
file distribution, 169–172
TCP socket programming, 193
UDP socket programming, 188
client-side buffers, 710
cloud computing, 93
Amazon, 182–184
cloud data centers, 524
hierarchical architecture in,  
525–526
cloud services, response time of, 303
cluster selection strategy, 181
CMTS. See cable modem termination 
system
CNAME records, 164
CO. See central office
COA. See care-of address
coaxial cable, 48
code division multiple access 
(CDMA), 483, 485, 548,  
556–560
collide, 481
collision avoidance, 565
RTS/CTS frames for, 568–570
collision detection, 487
CSMA/CD, 487, 489–492, 560, 
567
802.11 MAC protocol and,  
565–566
collisions
in broadcast channels, 481
in CSMA, 488–489
CSMA/CD and, 567
FDM avoiding, 483
link-layer switching eliminating, 
512
random access protocols and,  
483
in slotted ALOHA, 484
TDM eliminating, 482
communication layer, SDN, 438
communication links, 32
Communications-Electronics Security 
Group, 633
compact disk (CD), 706
computational complexity, of LS 
algorithm, 410
computer networks, 30
graph model of, 405–406
history of, 87–93
process interface to, 117
throughput in, 71–74
conditional GET, 142–144
confidentiality, 622, 655
congestion
alternative algorithms, 305
buffer overflows from, 294–295
causes and costs of, 289–295
delays from, 291
lost segments and, 299
multihop paths and, 293–295
retransmission and, 292–293
routers and, 290–295
throughput and, 290–295
congestion avoidance, 301–302
congestion control, 220, 281
ABR, 231
adaptive, 231
AIMD, 305
approaches to, 296
bandwidth and, 299
end-to-end, 296
network-assisted, 296, 297
principles of, 289–296
TCP, 297–311
congestion window, 298, 304
connection flooding, 85
connectionless demultiplexing, 
223–224
Index         819
connectionless multiplexing,  
223–224
connectionless transport, 228–234
connection management, TCP,  
283–287, 289
connection-oriented demultiplexing, 
224–227
connection-oriented multiplexing, 
224–227
connection-oriented transport,  
261–289
connection replay attack, 664
connection requests, 225
connection state, 230
connection tables, 683
Content Distribution Networks 
(CDNs), 142, 177–181
case studies, 181–185
cluster selection strategy,  
181
data centers, 178
DNS and, 179–180
Google, 184
IP-anycast and, 430
ISPs and, 178
IXPs and, 178
Kankan, 184–185
Netflix, 182–184
operation of, 178–181
private, 178, 182–184
third-party, 178
video streaming and,  
180–181
YouTube, 184
content ingestion, 182
content processing, 182
content provider networks,  
62
control frames, 568–570
control packets, 342
in Skype, 725
control plane, 333, 343, 401
4G, 586
SDN, 435–444
convergence, routing algorithm speed 
of, 419
conversational voice and video, 
708–709
cookies, 136–138
SYN, 288
correspondent, 590
in indirect routing, 594–595
correspondent agent, 596
countdown timer, 244
CRC. See cyclic redundancy check
crossbar switches, 347–349
cryptographic hash functions,  
639–641
cryptography, 702
attack types against, 627
components, 625
principles of, 624–638
public key encryption, 625,  
632–638, 658, 664
symmetric key, 626–632, 653, 655, 
658, 664
CSMA. See carrier sense multiple 
access
CSMA/CA. See CSMA with collision 
avoidance
CSMA/CD. See CSMA with collision 
detection
CSMA with collision avoidance 
(CSMA/CA), 565, 567
CSMA with collision detection 
(CSMA/CD), 487, 489–492, 
560, 567
CSNET, 90
CTS. See Clear to Send
cumulative acknowledgement, 252, 
266
customer, 60
820         Index
cwnd, 298, 300–304
Cyclades, 89
cyclic redundancy check (CRC), 
477–479, 554
in 802.11, 566, 571
in Ethernet frames, 505
D
DARPA. See Defense Advanced 
Research Projects Agency
DARTnet, 764
DASH. See Dynamic Adaptive 
Streaming over HTTP
data center network design, 523
data center networks, 523–528
hierarchical architecture,  
525–526
load balancing, 524–525
trends in, 526–528
data centers, 39, 114
CDNs, 178
Google, 179
hosts, 523
modular, 526–527
Data Center TCP (DCTCP),  
311, 313
DATA frames, 568–570
Datagram Congestion Control 
Protocol (DCCP), 313–314
datagrams, 79, 219
indirect routing of, 599
inspecting, 376
IP, 529
IPsec, 669–672
IPv4 format, 358–360
IPv4 fragmentation, 360–363
IPv6 format, 377–379
NAT and, 374–375
network-layer, 82
reassembly of, 361–362, 379
subnet dispatch of, 501–502
Data-Over-Cable Service Interface 
Specifications (DOCSIS), 
493–495
binary exponential backoff in, 491
data plane, 333, 389
4G, 586
generalized forwarding and SDN, 
382–389
IP, 357–382
routers, 341–357
SDN and, 436, 442–443
data received events, 273, 274
Davies, Donald, 88
DCCP. See Datagram Congestion 
Control Protocol
DCTCP. See Data Center TCP
DDoS. See distributed DoS
decentralized routing algorithm, 
406–407
DECnet, 498
decryption algorithm, 625
deep packet inspection (DPI), 376, 
382, 687
Deering, Steve, 617
Defense Advanced Research Projects 
Agency (DARPA), 89, 90, 399
delays
adaptive playout, 720–722
application, 71
bounded, 339
channel propagation, 489
in end systems, 71
end-to-end, 69–71, 717, 718
fixed playout, 719–720
network congestion and, 291
nodal, 64
nodal processing, 63
in packet-switched networks, 
63–74
playout, 719–720, 720–722
processing, 64
Index         821
propagation, 63, 65–67
queuing, 52–53, 63, 64, 67–69, 291
in shared medium, 71
total nodal, 63
transmission, 63, 64–67
types of, 63–67
delay-sensitive applications, 708
demilitarized zone (DMZ), 689
demultiplexing, 221–228, 530
connectionless, 223–224
connection-oriented, 224–227
transport-layer, 220
denial-of-service (DoS) attacks,  
84–85
distributed, 85, 86, 167
SYN floods for, 288
DES (Data Encryption Standard), 630, 
658
destination-based forwarding, 343, 
344–347
destination port number, 264
destination port number field, 222
DHCP. See Dynamic Host 
Configuration Protocol
DHCP ACK message, 372–373, 530
DHCP discover message, 371
DHCP offer message, 371–372
DHCP request message, 372, 529
dial-up Internet access, 44, 519
DIAMETER, 565, 678
DIC, 305
differentiated service, 737
Diffserv, 747–751
Diffie-Hellman Key Exchange, 632
Diffserv, 747–751
DIFS. See Distributed Inter-frame 
Space
Digital Attack Map, 84
digital signatures, 638, 642–648
digital subscriber line (DSL),  
41–42, 92
digital subscriber line access  
multiplexer (DSLAM), 41–42
Dijkstra's algorithm, 407, 414
in OSPF, 420
directional antennas, 570
direct routing, 596–597
Direct Sequence Wideband CDMA 
(DS-WCDMA), 584
distance-vector algorithm (DV algo-
rithm), 412–419
decentralization, 414
link-cost changes and link failure, 
416–418
LS compared with, 418–419
message complexity, 418–419
poisoned reverse, 418
robustness, 419
speed of convergence, 419
distributed applications, 33
distributed DoS (DDoS), 85
DNS servers targeted by, 167
Distributed Inter-frame Space (DIFS), 
567
distribution time, 169–172
DMZ. See demilitarized zone
DNS. See domain name system
DNS caching, 162–163
DNS query message, 531
DNS reply message, 532
DNS servers, 155
authoritative, 160, 532
BIND, 155
DDoS attacks targeting, 167
interactions of, 161
local, 160
root, 159, 162
TLD, 158, 159
DOCSIS. See Data-Over-Cable 
Service Interface Specifications
DOCSIS 2.0, 43
domain names, 434
822         Index
domain name system (DNS), 78, 
154–168, 531
CDNs and, 179–180
distributed, hierarchical database, 
158–162
inserting records, 166, 168
Internet presence and, 434–435
intra-domain routing to, 531–532
intruder interference with, 624
IP-anycast in, 430
iterative queries, 161
messages, 164–166, 531–532
operation of, 157–163
query chain, 161–162
records, 163–164, 166, 168
recursive queries, 161–162
resource records, 163–164, 532
services provided by, 155–157
UDP usage by, 229
vulnerabilities, 167
DoS. See denial-of-service attacks
dotted-decimal notation, 363, 498–499
DPI. See deep packet inspection
dropping
OpenFlow, 386
packets, strategies for, 352
drop-tail, 352
DSL. See digital subscriber line
DSLAM. See digital subscriber line 
access multiplexer
duplicate ACKs, 242, 277
duplicate data packets, 244
duplicate packets, 240
DV algorithm. See distance-vector 
algorithm
Dynamic Adaptive Streaming over 
HTTP (DASH), 176–177, 183, 
716
Dynamic Host Configuration Protocol 
(DHCP), 370–372
address obtainment with, 370–373
messages, 371–372, 529–530
mobile nodes and, 372
NAT and, 373
requests, 529–530
dynamic routing algorithms, 407
DYSWIS, 764
E
EAP. See Extensible Authentication 
Protocol
EAPoL, 677
Earthlink, 551
eavesdropping, 624
e-Bay, 92
eBGP. See external BGP
EC2, 93
ECE. See Explicit Congestion 
Notification Echo
echo request, 447
ECN. See Explicit Congestion 
Notification
edge routers, 342
Educause, 159
efficiency
of CSMA/CD, 492
of slotted ALOHA, 485–486
of slotted multiple access  
protocols, 485
802.11. See IEEE 802.11
EIGRP protocol, 420
elastic applications, 120
e-mail, 144–154
access protocols, 150–154
IMAP, 151, 153–154
message formats, 149–150
securing, 654–659
servers, 144–145, 156
SMTP, 78, 125, 146–151
web-based, 154
encapsulation, 81–83
in indirect routing, 594
Index         823
Encapsulation Security Payload 
(ESP), 668, 670–671
encrypted, 622
encryption
attack types against, 627
passwords, 651–652
polyalphabetic, 627–628
public key, 625, 632–638, 658, 664
security associations and, 669
standards for, 630
symmetric key, 626–632
encryption algorithm, 625
end-end principle, 233
end-point authentication, 86–87, 623
end systems, 30, 32, 37, 39
delay in, 71
end-to-end congestion control, 296
end-to-end connection, 55
end-to-end delay, 69–71, 717, 718
enhanced packet core (EPC), 586
eNodeB, 586
enter deep, 178, 179
entity body, 134
EPC. See enhanced packet core
error checking, UDP checksums and, 
232–234
error correction, 471
bit-level, 472
techniques for, 472–479
error detection, 238, 471
bit-level, 472
checksumming, 476
parity bits, 474–476
techniques for, 472–479
ESP. See Encapsulation Security 
Payload
EstimatedRTT, 270
Estrin, Deborah, 617–619
Ethane project, 443–444
Ethernet, 33, 44–45, 471, 488, 502–503
binary exponential backoff in, 491
broadcast, 479
CSMA used by, 483
development of, 90
802.1Q-tagged VLAN frames, 518
frames, 529
frame structure, 504–506, 518
Gigabit, 508
MAC protocol, 507
MTU, 263
packet sniffing, 86
repeater, 507
run lengths, 507–508
speeds, 507–508
technologies, 506–509
topologies, 503
even parity schemes, 474
event-based programming, 253
EWMA. See exponential weighted 
moving average
expedited forwarding PHB, 750
Explicit Congestion Notification 
(ECN), 310–311
Explicit Congestion Notification Echo 
(ECE), 311
exponential weighted moving average 
(EWMA), 270
extended FSM, 252
Extensible Authentication Protocol 
(EAP), 677
external BGP (eBGP), 425
F
Facebook, 704
fading, 556
failover paths, 522
fairness
of AIMD, 307–310
parallel TCP connections and, 310
TCP and, 307–310
UDP and, 309–310
fast recovery, 302–304
824         Index
fast retransmit, 277–279
FCFS. See first-come-first-served
FDDI. See fiber distributed data inter-
face
FDM. See frequency-division  
multiplexing
FDMA, 581
feature phones, 618
FEC. See forward error correction
Feynman, Richard, 332
FHSS. See frequency-hopping spread 
spectrum
fiber distributed data interface 
(FDDI), 493, 503
fiber optics, 92
in cable systems, 42–43
physical media, 48
fiber to the home (FTTH), 43–44, 92
FIFO. See first-in-first-out
file distribution
client-server, 169–172
P2P, 168–175
filtering, 509
FIN bit, 265
finite-state machine (FSM)
for data transfer over channel with 
bit errors, 238–244
for data transfer over lossy channel 
with bit errors, 244–245
for data transfer over perfectly reli-
able channel, 236, 237
extended, 252
for GBN protocol, 250–252
TCP congestion control, 301, 302
firewalls, 376, 382, 623, 679–687
application gateways, 684–685, 
687
stateful filters, 680, 682–684
traditional packet filters, 680–682
first-come-first-served (FCFS), 353
first-in-first-out (FIFO), 353–354
fixed playout delay, 719–720
flag days, 380
flag field, 264
flow, 377
flow-based forwarding, 435–436
flow control, TCP, 280–282
flow-control service, 280
flow table, 384
match-plus-action, 443
SDN, 438
wildcards in, 385
Floyd, Sally, 465–466
foreign address, 592
foreign agent, 590
anchor, 597
discovery, 598, 599–600
foreign network, 590–592
forward error correction (FEC), 476, 
717
packet loss recovery with, 722–723
in Skype, 725
forwarding, 334, 337, 340
to broadcast, 386
destination-based, 343, 344–347
flow-based, 435–436
generalized, 343, 382–389
link-layer switches, 509–510
longest prefix matching rule, 345, 
367
MPLS-enhanced, 521
OpenFlow, 386
packets, 336
SDN, 435–436
forwarding plane, 342–343
forwarding tables, 53–54, 336, 337
in input processing, 345–346
IP, 530
line cards, 345
in LS algorithm, 409–410
match-plus-action, 384
prefixes, 345
Index         825
routers, 336, 337
in SDN, 342, 344
4G, 548, 580
core network, 585–587
data plane, 586
network architecture, 585–588
QoS in, 586
radio access network, 587–588
tunneling in, 586
wireless LANs versus, 580
4G LTE, 580, 581
fragment, 361
fragmentation
IPv4 datagram, 360–363
IPv6 datagram, 379
frame control fields, 573
frame relay, 520
frames, 80
ACK, 569–570
acknowledgement, 567
beacon, 563, 564, 576
collisions between, 481
control, 568–570
CTS, 568–570
DATA, 568–570
802.11, 570–573
802.11 transmission, 566
Ethernet, 529
Ethernet structure for, 504–506, 
507
link-layer, 82, 468
MPLS, 520–521
probe, 564
RTS, 568–570
time, 482
VLAN, 518
framing, 470
frequency-division multiplexing 
(FDM), 56–57, 481, 582
channel partitioning, 485
collision avoidance, 483
in DOCSIS, 493
orthogonal, 587
frequency-hopping spread spectrum 
(FHSS), 577
FSM. See finite-state machine
FTP protocol, 78
FTTH. See fiber to the home
full-duplex service, 261
fully connected topology, 526
G
Gateway GPRS Support Nodes 
(GGSNs), 584
Gateway Mobile services Switching 
Center (GMSC), 603
gateway router, 424
gateways, 343
application, 680, 684–685, 687
P-GW, 586
S-GW, 586
GBN protocol. See Go-Back-N (GBN) 
protocol
GE Information Services, 89
generalized forwarding, 343,  
382–389
action, 386
match, 384–386
match-plus-action, 386–389
Generalized Packet Radio Service 
(GPRS), 584
generator, 477
geographically closest, 181
geostationary satellites, 49
GET requests, 132, 532
conditional, 142–144
DASH and, 176–177
HTTP streaming and, 176
GGSNs. See Gateway GPRS Support 
Nodes
Gigabit Ethernet, 508
GIST, 764
826         Index
Global System for Mobile 
Communications (GSM), 
579–580
handoffs in, 605–608
mobile IP commonalities to,  
608
2G standards, 581–582
GMSC. See Gateway Mobile services 
Switching Center
Go-Back-N (GBN) protocol,  
249–254
events, 252
TCP as, 280
Google, 39, 92, 313
CDNs, 184
data centers, 179
network infrastructure, 179
private network, 62, 93, 403
SDN use by, 403, 441, 444
web-based e-mail, 154
Google Application Engine, 93
Google Chrome browser, 186
QUIC protocol, 230, 231, 313
Google Chromium, 313
Google Talk, 703, 708, 727
GPRS. See Generalized Packet Radio 
Service
graph, 405
graph algorithms, 407
GSM. See Global System for Mobile 
Communications
guaranteed delivery, 339
guaranteed delivery with bounded 
delay, 339
guaranteed minimal bandwidth, 
339–340
guided media, 47
H
H.263, 728
Handley, Mark, 617
handoff
in 802.11 subnets, 574–575
in GSM, 605–608
in wireless networks, 552
handshaking, 121
SSL, 661, 664–665
TCP three-way, 130, 193, 262, 
284–285, 532
hard guarantees, 738
hash functions
checksums and, 639–640
cryptographic, 639–641
digital signatures using, 644
MD5, 640, 642
SHA-1, 641
HDLC. See high-level data link con-
trol
header length field, 264
header lines, 132, 134
headers
IPv4, 359–360
MIME, 658
MPLS, 520–521
RTP, 730
Web browsers and, 135–136
head-of-the-line blocking (HOL 
blocking), 350
HELLO message, 422
HFC. See hybrid fiber coax
hidden terminal problem, 556,  
568–570
hierarchical architectures
within ASs, 422–423
in data center networks, 525–526
DNS distributed database, 158–162
Skype peers, 726
high bit rate, 704
high-level data link control (HDLC), 
479
HLR. See home location register
HMAC, 642
Index         827
HOL blocking. See head-of-the-line 
blocking
home agent, 590
discovery, 598, 599–600
indirect routing and, 593–594
registration with, 600–601, 602
home location register (HLR), 603
call routing and, 604–605
home MSC, 603
home network, 590, 603
home public land mobile network 
(home PMLN), 603
Home Subscriber Service (HSS), 587
hop limit, 379
host addresses, obtaining with DHCP, 
370–373
host aliasing, 156, 164
hostname, 154–155
alias, 156, 164
in DNS queries, 160–161
in DNS resource records,  
163–165
DNS services and, 155–156
hosts, 30, 37, 39
data center, 523
wireless, 548
Hotmail, 154
hot potato routing, 428–429
HSPA (High Speed Packet Access), 
585
HSS. See Home Subscriber Service
HTML, development of, 91
HTML file, 126
HTTP. See HyperText Transfer 
Protocol
HTTP byte-range header, 715
HTTP GET message, 532
HTTP protocol, 78, 91
HTTP request, 530
HTTP response message, 533
HTTP streaming, 176–177, 709
adaptive, 709
client application buffer, 713–714
DASH, 176–177, 183, 716
early termination, 715–716
prefetching video, 712–713
repositioning video, 715–716
TCP buffer, 713–714
YouTube use of, 184
hub, 503
HughesNet, 44
Hulu, 707
hybrid fiber coax (HFC), 42–43, 467
hyperlinks, 130
HyperText Transfer Protocol (HTTP), 
124–125, 126–128
cookies, 136–138
ICMP and, 447
message format, 131–136
non-persistent connections, 128–
131
persistent connections, 128, 131
ports, 227–228
request message, 131–133
response message, 133–136
SMTP comparison with, 149
I
IANA, 377
iBGP. See internal BGP
IBM, 89
ICANN. See Internet Corporation for 
Assigned Names and Numbers
ICMP. See Internet Control Message 
Protocol
ICMP messages, 167
IDEA, 658
IDSs. See intrusion detection systems
IEEE 802.1Q, 517, 519
IEEE 802.3 (Ethernet), 507
IEEE 802.3z (Gigabit Ethernet), 507
IEEE 802.5, 493
828         Index
IEEE 802.11, 45, 548
access points, 561–562
adapters, 561, 565
ad hoc networks, 562
architecture, 561–565
authentication, 564–565
basic service set, 561, 572,  
574–575
channels and association,  
562–565
collision detection, 565–566
CRCs, 566, 571
frame transmission, 566
frequency ranges, 560
handoff in subnets, 574–575
hidden terminals and, 568–570
MAC addresses in, 497
MAC protocol, 565–570
mobility on same IP subnet, 
574–575
as point-to-point link, 570
power management, 576
public access, 92, 551
rate adaptation, 575–576
RTS/CTS control frames, 568–570
standards, 560
IEEE 802.11ac, 560
IEEE 802.11b, interference from other 
devices, 553
IEEE 802.11 frames, 570
address fields, 571–573
MAC addresses in, 571–573
payload and CRC fields, 571
sequence number, duration, and 
frame control fields, 573
IEEE 802.11g, 560
IEEE 802.11i, 676–678
IEEE 802.11n, 560
IEEE 802.15.1, 577–578
IEEE 802.15.4, 578–579
IEEE 802.16, 588
IEEE 802 LAN/MAN Standards 
Committee, 33
IETF. See Internet Engineering  
Task Force
IETF Mobile Ad Hoc Network  
working group, 590
IKE. See Internet Key Exchange
IKE SA, 673
IMAP. See Internet Mail Access 
Protocol
indirect routing, 593–596, 599
information propagation, 331
infrastructure mode, 550
infrastructure wireless LANs, 562
Initialization Vector (IV), 631, 664, 
676–677
in-order packet delivery, 339
input port, 342
input port processing, 344–347
forwarding tables in, 345–346
input queuing, 350
instantaneous throughput, 72
integrity checks, 669
Intel, 506
Intel 710 adapter, 471
intelligent software agents, 108
inter-area routing, 422–423
inter-autonomous system routing pro-
tocol, 423, 433
interconnection networks, 523
routing algorithms in, 527
switching via, 348–349
interface, 363
API, 117
NIC, 471
SDN controller, 438–439
socket, 34, 117
interference, 553
interleaving, 722, 724
internal BGP (iBGP), 425–426
internal router, 424
Index         829
International Organization for 
Standardization (ISO), 80
International Telecommunication 
Union (ITU), 648
Internet. See also access networks
best-effort service in, 340
broadband, 92
Cerf on, 399–400
commercialization of, 91
components of, 30–33
DNS and presence on, 434–435
enterprise access, 44–45
history of, 87–93
home access, 41–44
network core, 49, 50
network edges, 37–39
network layer, 340
obtaining presence on,  
434–435
registries, 369
router self-synchronization, 411
routing algorithms used in, 407
as service infrastructure, 33–34
services not provided by, 123
topology of, 514
transport layer, 219–221
transport services provided by, 
121–123
Internet applications, transport  
protocols used by, 231
Internet checksum, 476, 639–640
Internet Control Message Protocol 
(ICMP), 447–449
IPv6 and, 449
message types, 448
packet filtering and, 681
Internet Corporation for Assigned 
Names and Numbers (ICANN), 
166, 369, 420
Internet Engineering Task Force 
(IETF), 33, 376, 648, 764
Internet Exchange Points (IXPs), 
61–62
CDNs in, 178
Google infrastructure at, 179, 184
Netflix infrastructure at, 183
Internet Key Exchange (IKE), 673
Internet Mail Access Protocol 
(IMAP), 151, 153–154
Internet of Things (IoT), 39, 618
Internet Protocol (IP), 33, 79, 399, 
545. See also IPv4; IPv6
best-effort service, 716–717
ICMP and, 447
service model, 220
stack for, 78
total annual traffic using, 32
transition to, 90
Internet Real-Time Lab, 764
Internet registrars, 434
Internet Service Providers (ISPs), 32–33
access, 60
backbone, 432
CDNs and, 178
AS configurations, 420
global transit, 60
Google infrastructure at, 179, 184
multi-home, 61
multi-homed access, 432
Netflix infrastructure at, 183
peering agreements among,  
432–433
PoP, 61
routing among, 423–435
Internet standards, 33
Internet Systems Consortium, 373
Internet telephony, 123, 708, 718
Internet video, 175–176
internetworking, 88–90
intra-autonomous system routing, 
419–423, 433
SDN in, 444
830         Index
intra-domain routing, 531–532
intrusion detection systems (IDSs), 
376, 623, 687–690
intrusion prevention systems (IPSs), 
376, 687
Intserv, 340
IoT. See Internet of Things
IP. See Internet Protocol
IP addresses, 91, 118, 155
broadcast, 369, 371
classes of, 366–367
DHCP, 370–373
Internet presence and, 434
IPv4, 362–373
IPv6, 377
MAC addresses and, 497
NAT and, 373–375
obtaining blocks of, 369
SIP and, 732–734
socket programming, 187
temporary, 370
IP-anycast, 430–431
IP datagram, 529
IP forwarding table, 530
IP fragmentation
IPv4, 360–363
IPv6, 379
iPhones, 46
IPsec, 382, 645, 665, 666–667
AH and ESP, 668
datagram, 669–672
key management, 673
packet forms, 669–670
security associations, 668–669,  
673
IP spoofing, 86–87, 651
IPSs. See intrusion prevention systems
IP traffic, volume of, 32
IPv4
addressing, 362–373
datagram format, 358–360
datagram fragmentation,  
360–363
transitioning to IPv6 from,  
380–382
IPv6, 376
adoption of, 380–381
datagram format, 377–379
ICMP, 449
transitioning to, 380–382
tunneling, 380–381
IPX, 414, 498
IS-IS, 420, 441, 532
ISO. See International Organization 
for Standardization
ISO IDRP, 414
ISPs. See Internet Service Providers
iterative queries, 161
ITU. See International 
Telecommunication Union
IV. See Initialization Vector
IXPs. See Internet Exchange Points
J
jabbering adapters, 512
Jacobson, Van, 331–332, 617
Java, client-server programming with, 
187
Jet Propulsion Laboratory, 400
jitter
packet, 718–719
removing, for audio, 719–722
Juniper MX2020, 342
Juniper Networks Contrail, 444
K
Kahn, Robert, 399, 400, 544
ARPAnet development and,  
88–90
TCP/IP creation and, 262
Kankan, 184–185, 703
Karels, Mike, 331
Index         831
keys, 625
IPsec management of, 673
SSL, 662
Kleinrock, Leonard, 88, 107–109,  
399, 544
known-plaintext attacks, 627
L
label-switched router, 521
Lam, Simon, 544–546
Lampson, Butler, 386
LAN. See local area network
LAN address, 496
LAN-on-motherboard configuration, 
471
layer 4 switching, 343
layer 5 switching, 343
layered architectures, 75–81
encapsulation, 81–83
layers, 77
leaky bucket policer, 744–747
least-cost path, 406
Bellman-Ford equation for,  
412–413
in LS algorithm, 408–410
LEO satellites. See low-earth orbiting 
satellites
Level 3 Communications, 33
CDN, 178
Licklider, J. C. R., 88
Limelight, 178
line cards, 471
forwarding tables in, 345
input and output ports, 342
processing on, 348
line speeds, queuing and,  
349–350
link access, 470
link capacity
buffer sizing and, 353
network congestion and, 291
link failure
DV algorithm and, 416–418
precomputed failover paths for, 
522
link layer, 79–80, 468, 469
broadcast, 479
implementation locations, 471–472
IP datagram size and, 361
network as, 519–522
services provided by, 470–471
link-layer acknowledgement, 565, 566, 
567
link-layer addressing, 496–502
link-layer frame, 82, 468
link-layer switches, 32, 51, 341, 
509–515
destination address lookup in, 346
forwarding and filtering, 509–510
properties of, 512
self-learning, 511–512
links, 468
wireless, 549
link-scheduling discipline, 744
link-state algorithms (LS algorithms), 
406, 407–411, 414
centralized routing algorithm, 408
computational complexity of, 410
DV compared with, 418–419
forwarding tables, 409–410
message complexity, 418–419
oscillations in, 410–411
OSPF, 420
robustness, 419
speed of convergence, 419
steps of, 408–409
link-state broadcast, 407
erroneous, 419
link virtualization, 519–522
link weights, in OSPF, 421
live streaming, 709
load balancers, 382, 524–525
832         Index
load distribution, 156
load-insensitive algorithms, 407
load-sensitive algorithm, 407
local area network (LAN), 44–45. See 
also virtual local area networks; 
wireless LANs
sniffing, 513
switched, 495–519
local DNS server, 160
local preference, 429
logical communication, 216
logically centralized control, 402–403, 
404, 465
logically centralized routing controllers, 
338
longest prefix matching rule, 345, 367
Long-Term Evolution (LTE), 46, 580, 
581
network architecture, 585–588
time slots in, 587–588
lookup algorithms, 346
loss anticipation schemes, 722–725
loss recovery schemes
error concealment, 724–725
FEC, 722–723
interleaving, 724
loss-tolerant applications, 119, 708
lossy channels, 242–245
LoST, 764
lost segments, 299
low-earth orbiting (LEO) satellites, 49
LS algorithms. See link-state algo-
rithms
LTE. See Long-Term Evolution
LTE-Advanced, 588
M
MAC. See message authentication 
code
MAC addresses
ARP and, 499–500
in beacon frames, 563
broadcast address, 498, 500
802.11 access points, 561, 563
802.11 frames, 571–573
network adapters, 496–498
wireless LAN authentication by, 
564
MAC protocol. See medium access 
control protocol
mailbox, 144
mail servers, 144–145
aliasing, 156
malware, 83–84
self-replicating, 84
managed device, 450
managed objects, 450
Management Information Base (MIB), 
450, 452
managing server, 450
MANETs. See mobile ad hoc net-
works
manifest file, 177
MAP message, 493
Master Key, 677
Master Secret (MS), 664
match-plus-action, 346
forwarding table, 384
in generalized forwarding,  
382–383
OpenFlow, 386–389
match-plus-action flow tables,  
443
maximum segment size (MSS), 263, 
307
negotiating, 264
maximum transmission unit (MTU), 
263, 361
MD4, 641
MD5 authentication, 422
MD5 hash algorithm, 640, 642
MDCs. See modular data centers
Index         833
medium access control protocol 
(MAC protocol), 470
802.11, 565–570
Ethernet, 507
medium access protocol, 467
memory
access times, 346
bandwidth of, 347
switching via, 347–348
mesh networks, wireless, 552
message authentication code (MAC), 
641–642
digital signatures and, 644–645
IPsec datagrams, 670
in SSL, 662–663
message integrity, 622–623, 638, 655, 
656
message queue, 145
messages, 51, 78, 116
application-layer, 82
ARP, 499–501, 531
complexity in LS algorithms, 
418–419
DHCP, 371–372, 529–530
DNS, 164–166, 531–532
e-mail formats, 149–150
HELLO, 422
HTTP format, 131–136
ICMP ping, 167
intruder actions on, 624
OpenFlow, 443
port-status, 443
SIP, 734
source quench, 447–448
Metcalfe, Bob, 488, 503, 506
metering function, 750
MIB. See Management Information 
Base
Microsoft, 92
private network, 93
Microsoft Research, 403
middleboxes, 375, 382
MIME headers, 658
MIMO. See multiple input  
multiple-output
Minitel, 91
MME. See Mobility Management 
Entity
mobile ad hoc networks (MANETs), 
552–553, 590
mobile IP, 598–602
agent discovery, 599–600
GSM commonalities to, 608
registration with home agent, 
600–601, 602
mobile nodes
addressing, 590–592
DHCP and, 372
direct routing to, 596–597
in foreign networks, 591–592
indirect routing to, 593–596
routing to, 592–597
mobile station roaming number 
(MSRN), 604
mobile switching center (MSC), 582, 
584, 603
anchor, 607
handoffs and, 606–608
mobile-user location protocol, 596
mobility, 547–548, 618
addressing and, 590–592
cellular network management of, 
602–610
degrees of, 588–589
handoff and, 552
higher-layer protocols and, 608–
610
management, 588–597
node address and, 589
on same IP subnet, 574–575
within VLANs, 576
wired infrastructure and, 590
834         Index
Mobility Management Entity (MME), 
587
modify-field action, 386
modular data centers (MDCs),  
526–527
modulation techniques
dynamic selection of, 555–556
PCM, 706, 728
SNR and BER for, 554–556
monoalphabetic cipher, 625
Mosaic Communications, 91
MOSPF. See multicast OSPF
MP3. See MPEG 1 layer 3
MPEG, 728
MPEG 1 layer 3 (MP3), 706, 728
MPLS. See Multiprotocol Label 
Switching
MPLS-enhanced forwarding, 521
MS. See Master Secret
MSC. See mobile switching center
MSRN. See mobile station roaming 
number
MSS. See maximum segment size
MTU. See maximum transmission 
unit
multicast OSPF (MOSPF), 422
multicast routing, 617
in OSPF, 422
multi-home, 61
multi-homed access ISP, 432
multi-hop, infrastructure-based wire-
less networks, 552
multi-hop, infrastructure-less wireless 
networks, 552–553
multihop path, 293–295
multi-hop wireless networks, point-to-
point 802.11 links in, 570
multimedia applications, 765
audio properties, 705–707
conversational voice and video 
over IP, 708–709
network support for, 737–754
streaming live audio and video, 
709
streaming stored audio and video, 
707–708
TCP use by, 230
types of, 707–709
UDP use by, 230–231
video properties, 704–705
multipath propagation, 553
multiple access channels, 479, 480
multiple access problem, 479
multiple access protocols, 480–481, 
565
multiple input multiple-output (MIMO), 
560
multiple same-cost paths, in OSPF, 
422
multiple versions, 705
multiplexing, 221–228
connectionless, 223–224
connection-oriented, 224–227
transport-layer, 220
Multiprotocol Label Switching 
(MPLS), 519, 520–522
MX records, 156, 164
N
NAK (negative acknowledgments), 
238–242, 476
corrupted, 240
NAT. See network address translation; 
network address translator
National Physical Laboratory, 88
NAT translation table, 374
NAT traversal, 375
NCP. See network-control protocol
NCS. See network control server
negative acknowledgments, 238
neighbor, 405
neighboring peers, 173–174
Index         835
Nelson, Ted, 91
Netflix, 175, 703, 707
CDNs, 182–184
netnews, 701
Netscape Communications, 91–92, 212, 
659
network adapters, 471, 472
ARP query and, 500
CSMA/CD operation and, 490–491
datagram transmission and, 499, 
501–502
802.11, 561, 565
error detection in, 476
Ethernet frames and, 504–506
jabbering, 512
LAN-on-motherboard configuration, 
471
layer independence and, 498
MAC address assignment, 496–498
MAC addresses, 496–498
on separate cards, 471
network address translation (NAT), 
373–375, 382
Skype traversal of, 725–727
network address translator (NAT), 346
network applications, 111
architectures, 114–116
communication for, 113
principles of, 112–125
proprietary, 186
service requirements, 121
standards-based, 186
transport services available to, 
118–121
network-assisted congestion control, 
296, 297
network control functions, in SDN, 
436
network-control protocol (NCP), 88, 
90
network control server (NCS), 441
network core, 49–50
circuit switching, 55–59
4G networks, 585–587
network of networks, 59–62
packet switching, 50–54, 58–59
3G networks, 584
network dimensioning, 737, 739–740
network functions virtualization (NFV), 
444
Network Information Base (NIB), 441
network infrastructure, wireless net-
works and, 550–553
network interface card (NIC), 471
network layer, 51. See also control 
plane; data plane
best-effort service, 340
forwarding and routing, 334–339
security, 340, 665–673
services, 339–340
transport layer relationship to, 
216–219
network-layer datagram, 82
network management, 449–454
defining, 449
framework for, 450–451
intruder interference with, 624
link-layer switching and, 512
network management agent, 450–451
network management protocol, 451
network of networks, 59–62, 90
network operations center (NOC), 450
network protocols, 36–37
networks. See also access networks; 
cellular networks; Internet; local 
area network; wireless networks
ad hoc, 562
attacks against, 83–87
CDNs, 142, 177–185
cellular, 46, 551, 556, 579–588, 
602–610
content provider, 62
836         Index
networks. (continued)
data center, 523–528
edges, 37–39, 592
foreign, 590–592
home, 590, 603
home PMLN, 603
mobile ad hoc, 552–553, 590
multimedia application support, 
737–754
multimedia support, 737–754
packet-radio, 89
packet-satellite, 88
personal area, 576–579
PMLN, 603
private, 62, 93, 373, 403, 666
programmable, 436
proliferation of, 90–91
proprietary, 88–90
provider, 432
radio access, 584–585, 587–588
telephone, 519
throughput in, 71–74
topology of switched, 514
virtual-circuit, 520
visited, 590, 603
VLANs, 515–519, 525, 576
VPNs, 522, 576, 665–667
wireless mesh, 552
WPANs, 577–578
network security, 622–624
network-service applications, 444
network service model, 339–340
NeVoT, 764
NEXT-HOP, 427–428, 429
NFV. See network functions virtual-
ization
NIB. See Network Information Base
NIC. See network interface card
NIST, 380, 630
nmap, 226, 289
NOC. See network operations center
nodal delay, 64
nodal processing delay, 63
nodes, 468
nomadic computing, 108
non-blocking switches, 348
nonce, 653, 676
non-persistent connections, 128–131
non-preemptive priority queuing, 355
Novell IPX, 414
NOX controller, 440, 444
NSFNET, 90, 91
nslookup program, 166
NS records, 164
NTT, 33
O
object, 126
OC. See Optical Carrier standard
odd parity schemes, 474
OFA. See Open Flow Agent
OFC. See Open Flow Controller
OFDM. See orthogonal frequency 
division multiplexing
offered load, 292
OLT. See optical line terminator
one-bit parity, 474–475
ONIX SDN controller, 441
ONOS, 440, 444, 446–447
ONT. See optical network terminator
OpenDaylight, 440, 444–445
OpenDaylight Lithium, 444
OpenFlow, 438, 440–441, 442–443
action, 386
flow table, 384
match, 384–386
match-plus-action, 386–389
Open Flow Agent (OFA), 441
Open Flow Controller (OFC), 441
Open Shortest Path First (OSPF), 402, 
407, 420–423, 532
authentication in, 422
Index         837
broadcast in, 421–422
Dijkstra's algorithm, 420
link weights, 421
multicast, 422
security and, 422
subnets, 420
Open Systems Interconnection (OSI), 
80
operational security, 623, 679–690
firewalls, 679–687
IDSs, 376, 623, 687–690
opportunistic scheduling, 588
Optical Carrier standard (OC), 48
optical line terminator (OLT), 44
optical network terminator (ONT), 43
optimistically choked peers, 174
options field, 264
orthogonal frequency division 
multiplexing (OFDM), 587
OSI. See Open Systems 
Interconnection
OSI reference model, 78, 80–81
OSPF. See Open Shortest Path First
out-of-order packets, 253
output buffer, 52
output port, 342
forwarding to, 346
output port processing, 349
output queue, 52
output queueing, 351–352
outside-AS destinations, 428
OVSDB, 445
P
P2P architecture, 114–116, 545
file distribution, 168–175
scalability of, 169–172
Skype use of, 725–727
P2P live streaming, 175
P2P streaming, 185
P2P video streaming, 709
packet classification, 748
Packet Data Network Gateway (P-GW), 
586
packet-dropping strategies, 352
packet filtering, 680–682, 689
packet header overhead, 230
packet headers
MPLS, 520–521
routing and, 336, 337
packet jitter, 718–719
packet loss, 53, 69, 349
error concealment, 724–725
FEC for, 722–723
interleaving for, 724
recovery from, 722–725
VoIP and, 717–718
packet marking, 742
packet-marking strategies, 352
Packet Radio, 544
packet-radio networks, 89
packets, 32, 51
ARP, 500
choke, 296
control, 342, 725
deep inspection of, 376, 382, 687
duplicate, 240
duplicate data, 244
forwarding, 336
in-order delivery of, 339
IPsec forms, 669–670
out-of-order, 253
processing, 514
RTP, 729
Packet Satellite, 544
packet-satellite networks, 88
packet scheduler, 353
packet scheduling
FIFO, 353–354
priority queuing, 354–356
round robin, 356–357
WFQ, 356–357
838         Index
packet sniffer, 86, 105
packet-switched networks, delays in, 
63–74
packet switches, 32, 51, 341
packet switching, 51–54, 55, 107
circuit switching versus, 58–59
development of, 87–88
store-and-forward, 51–52
paging, 582
pairwise communication, 331
Pairwise Master Key (PMK), 678
parallel TCP connections, fairness 
and, 310
parity bit, 474
parity checks, 474–476
passive optical networks (PONs), 
43–44
passive scanning, 564
passwords, 651–652
path loss, 553
paths, 32
failover, 522
high-bandwidth, 306–307
least-cost, 406, 408–410, 412–413
multihop, 293–295
multiple same-cost, 422
shortest, 406
Paxos, 441
payload field, 82
in 802.11 frames, 571
PCM. See pulse code modulation
PDUs. See protocol data units
peering agreements, 432–433
peers, 61, 114–115
BitTorrent, 172–174
neighboring, 173–174
optimistically choked, 174
P2P streaming, 185
relay, 726–727
Skype, 726–727
unchoked, 174
peer-to-peer applications, 168–175
per-connection throughput, 290–291
per-hop behavior (PHB), 748, 750
permanent address, 592
per-router control, 402, 403
persistent connections, 128, 131
personal area networks, 576–579
PGP. See Pretty Good Privacy
P-GW. See Packet Data Network 
Gateway
PHB. See per-hop behavior
Photobell, 107
physical address, 496
physical layer, 80
physical media, 46–49
coaxial cable, 48
fiber optics, 48
satellite radio, 49
terrestrial radio, 48–49
twisted-pair copper wire, 47–48
piconet, 577
piggybacked acknowledgments, 269
ping, 447
ping messages, 167
pipelined reliable data transfer proto-
cols, 245, 247–249
pipelining, 249
TCP, 271
Plain Old Telephone Service (POTS), 
725
plaintext, 625, 627
playback attack, 652
playout delay
adaptive, 720–722
fixed, 719–720
plug-and-play, 370, 512
PMK. See Pairwise Master Key
PMLN. See public land mobile net-
work
PMS. See Pre-Master Secret
points of presence (PoPs), 61
Index         839
point-to-point connections, 261
point-to-point link, 479
802.11 as, 570
Point-to-Point Protocol (PPP), 468, 
479
MTU, 263
poisoned reverse, 418
polling protocol, 492
polls, 492
polyalphabetic encryption, 627–628
polynomial codes, 477
PONs. See passive optical networks
PoPs. See points of presence
port numbers, 118, 187
NAT and, 373–375
socket, 223–224
well-known, 222
port scanning, 226
port-status message, 443
positive acknowledgments, 238
Post Office Protocol—Version 3 
(POP3), 151–153
POTS. See Plain Old Telephone 
Service
Pouzin, Louis, 89
power management, 576
PPLive, 175
PPP. See Point-to-Point Protocol
ppstream, 175
prefetching video, 712–713
prefix, 345, 346, 366, 367
Pre-Master Secret (PMS), 664
Pretty Good Privacy (PGP), 654, 
658–659
Prim's algorithm, 407
priority queueing, 353, 354–356
non-preemptive, 355
privacy, 686
VoIP and, 727
private CDNs, 178
Netflix, 182–184
private key, 633
private networks, 62, 93, 373,  
403, 666
probe frames, 564
processes
addressing, 117–118
client, 116–117
communicating, 116–118
network interface, 117
server, 116–117, 261
transport layer protocols connecting, 
216
processing delay, 64
programmable network, 436
propagation delay, 63, 65–67
proprietary networks, 88–90
protocol data units (PDUs), 452, 453
protocol layering, 77–78
protocols, 5. See also specific protocols
defining, 35–37
network, 36–37
routing, 53–54
protocol stack, 78
provider, 60
provider networks, 432
proxy server, 138, 686
SIP, 735
PSH bit, 265
public key, 633
certifying, 658
public key certification, 645–648, 
658–659
public key encryption, 625, 632–638
in PGP, 658
in SSL, 664
public land mobile network (PMLN), 
603
public WiFi, 92, 551
pull protocol, 149
pulse code modulation (PCM),  
706, 728
840         Index
pure ALOHA protocol, 486
push caching, 183–184
push protocol, 149
Python, 186
port numbers, 223
TCP connections, 194–197
UDP connections, 189–192, 223
Q
Q2931b protocol, 753
QoS. See quality of service
QQ, 708
quality of service (QoS)
call admission, 752
in 4G, 586
per-connection guarantees, 738, 
751–754
resource reservation, 753
RTP and, 729
traffic policing and, 745
quantization, 706
query
ARP, 500, 531
DNS chain, 161–162
DNS message, 531
queueing delays, 52–53, 63, 64, 67–69
network congestion and, 291
queuing
FIFO, 353–354
input, 350
line speed and, 349–350
non-preemptive priority, 355
output, 351–352
priority, 353, 354–356
round-robin, 353, 356–357
in routers, 349–353
traffic load and, 350
transmission rate and, 349–350
WFQ, 356–357
work-conserving, 355, 356
QUIC protocol, 230, 231, 313
R
RA. See router agent
radio access network
4G, 587–588
3G, 584–585
Radio Network Controller (RNC), 584, 
586
RADIUS, 565, 677–678
Rand Institute, 88
random access protocols, 481,  
483–492, 506, 565
random backoff, 567
Random Early Detection (RED), 352
rarest first, 174
rate adaptation, 575–576
RC4 stream cipher, 675
RCP. See Routing Control Platform
realm with private addresses, 373
real-time conversational applications. 
See also Voice-over-IP
protocols for, 728–736
RTP, 728–731
SIP, 731–736, 765
real-time measurements, 181
Real-Time Streaming Protocol (RTSP), 
711
Real-Time Transport Protocol (RTP), 
711, 728–730
audio and video payload types, 731
packet header fields, 730
reassembly
IPv4 datagram, 361–362
IPv6 datagram, 379
receive buffer, 281, 282
receiver
in CRC operation, 477
in parity bit operation, 474–476
receiver authentication, 655
receiver feedback, 238
receive window, 264, 281, 282
recursive queries, 161
Index         841
RED. See Random Early Detection
reduced-function devices, 578
regional ISP, 60–61
registrar, 166, 434
SIP, 735
registration
with home agent, 600–602
in mobile IP, 602
registries, 369
relay peers, 726–727
relays, Skype, 726–727
reliable data transfer, 119, 220,  
259–260
over channel with bit errors, 
237–242
over lossy channel with bit errors, 
242–245
over perfectly reliable channel, 
236–237
principles of, 234–260
service implementation for, 235, 
236
service model for, 234, 235
TCP, 272–379
reliable data transfer protocol, 234
building, 236–245
pipelined, 245, 247–249
reliable data transfer service, 272
reliable delivery service, link-layer, 
470
repeater, 507
request line, 132
request messages, HTTP, 131–133
requests for comments (RFCs), 33
as protocol standards, 186
Request to Send (RTS), 568–570
resource records (RRs), 163–164, 532
resource reservation, 753
response messages, HTTP, 133–136
response time, cloud service perfor-
mance, 303
retransmission, 238
congestion and, 292–293
CSMA/CA and, 567
CSMA/CD and, 567
duplicate packets from, 240
fast, 277–279
in random access protocols, 483
sequence numbers for handling, 
240–241
in slotted ALOHA, 484
TCP timeout interval for,  
270–271
TCP timer management for, 
272–273
time-based, 244–245
Rexford, Jennifer, 464–466
RFC 2616, 186
RFCs. See requests for comments
RIP, 407, 414, 532
Rivest, Ron, 633, 640
RNC. See Radio Network Controller
roaming number, 604
Roberts, Lawrence, 88, 544
robustness, LS and DV algorithms, 
419
root DNS servers, 159, 162
round-robin queuing, 353,  
356–357
round-trip time (RTT), 130
buffer sizing and, 353
TCP estimation for, 269–271
TCP variable tracking, 297–298
route, 32
BGP, 427
BGP selection algorithm for, 
429–430
route aggregation, 367
route information, advertising in BGP, 
424–426
router agent (RA), 402
router discovery, 599
842         Index
routers, 32, 51, 382
architecture of, 341
border, 422–423, 523
buffer sizing, 353
components of, 341–344
congestion and, 290–295
data plane, 341–357
destination-based forwarding, 343, 
344–347
edge, 342
forwarding plane, 342–343
forwarding tables, 336, 337
gateway, 424
input port processing, 344–347
internal, 424
label-switched, 521
NAT-enabled, 373–375
output port processing, 349
per-router control, 402, 403
queuing in, 349–353
self-synchronization, 411
switches versus, 513–515
switching fabric, 347–349
route summarization, 367
routing, 336, 337
calls to mobile users, 604–605
direct, to mobile nodes,  
596–597
hot potato, 428–429
indirect, in mobile IP, 599
indirect, to mobile nodes,  
593–596
inter-area, 422–423
intra-ASs, 419–423, 433, 444
intra-domain, 531–532
intruder interference with, 624
among ISPs, 423–435
link weights in, 421
logically centralized, 338
to mobile nodes, 592–597
multicast, 422, 617
routing algorithms, 336, 337, 404–419
ARPAnet, 407, 414
centralized, 406–408
convergence speed, 419
decentralized, 406–407
distance-vector, 412–419
dynamic, 407
in interconnection networks, 527
link-state, 407–411
load sensitivity, 407
static, 407
routing controllers
logically centralized, 338
SDN and, 339
Routing Control Platform (RCP), 464
routing loop, 417
routing policy, BGP, 431–434
routing processor, 342
routing protocols, 53–54
routing tables, 414
BGP, 429–430
RRs. See resource records
RSA algorithm, 633–638, 658
RST bit, 264
RSVP protocol, 753
RTP. See Real-Time Transport 
Protocol
RTP header, 729
RTP packet, 729
RTP session, 729
RTS. See Request to Send
RTSP. See Real-Time Streaming  
Protocol
RTT. See round-trip time
rwnd, 297–298
S
SA. See security association
SAD. See Security Association 
Database
SAL. See Service Abstraction Layer
Index         843
SampleRTT, 270
satellite Internet access, 44, 467
satellite radio channels, 49
Scantlebury, Roger, 88
scheduling algorithms, 588
Schulzrinne, Henning, 728, 764–766
SCTP. See Stream Control 
Transmission Protocol
SDN. See software-defined network-
ing
SDN controller, 438–440, 465
secure communication, 622
secure e-mail, 655–658
Secure Hash Algorithm (SHA-1), 641, 
642
Secure Sockets Layer (SSL), 122, 
212, 544, 659–665, 686
connection closure, 665
data transfer, 662–663
handshake, 661, 664–665
key derivation, 662
security, 701–702
datagram inspection, 376
DNS vulnerabilities, 167
e-mail, 654–659
firewalls, 376, 382, 623, 679–687
IDSs, 376, 623, 687–690
network layer, 340, 665–673
operational, 412, 623, 679–690
OSPF and, 422
switch poisoning, 513
SYN flood attacks, 288
transport protocol, 120–121
wireless LANs, 674–678
security association (SA), 668–669,  
673
Security Association Database (SAD), 
 
669
Security Parameter Index (SPI), 669
Security Policy Database (SPD), 672
segment replay attack, 664
segments, 79, 216, 219
acknowledged, 299
lost, 299
maximum size, 263, 264, 307
TCP, 263
TCP structure, 264–269
TCP SYN, 532, 681
transport-layer, 82
UDP, 529
UDP structure, 232
selective acknowledgment, 280
selective repeat (SR), 249, 254–260
events and actions, 256
operation of, 257
TCP as, 280
window size, 258, 259
self-clocking, 298
self-learning, 511–512, 530
self-replicating malware, 84
self-scalability, 115
self-synchronization, 411
send buffer, 263
sender authentication, 655, 656
senders
in CRC operation, 477–478
in parity bit operation, 474
sending rate, 292
sequence number, 240
in 802.11 frames, 573
in GBN protocol, 249–250
jitter control with, 719
in pipelined protocols, 249
retransmission handling with, 
240–241
RTP, 730
in SR protocol, 255, 258
in SSL MAC calculation, 663
TCP, 265–267
for TCP segment, 266
Telnet and, 267–269
sequence number field, 264
844         Index
servers, 39, 114, 116
authoritative DNS, 160, 532
DNS, 155, 159–162, 160, 167
DNS root, 159, 162
enter-deep, 179
local DNS, 160
mail, 144–145, 156
managing, 450
network control, 441
processes, 116–117, 261
proxy, 686, 735, 138
TCP socket programming,  
196–198
UDP socket programming,  
191–192
user interaction with via cookies, 
136–138
web, 91, 127, 227–228
Service Abstraction Layer (SAL), 
444–445
service differentiation, 737, 747–751
Service Level Agreements (SLAs), 
450
service model, 77
IP, 220
network, 339–340
reliable data transfer, 234, 235
services
DNS, 155–157
flow-control, 280
full-duplex, 261
layering, 77
link layer, 470–471
network layer, 339–340
TCP, 220
transport layer, 118–123
UDP, 123
unreliable, 220
Service Set Identifier (SSID), 562
in beacon frames, 563
Serving Gateway (S-GW), 586
Serving GPRS Support Nodes 
(SGSNs), 584
Session Initiation Protocol (SIP), 
731–736, 765
addresses, 733–734
call to known IP address, 732–733
messages, 734
name translation and user location, 
734–736
session keys, 637, 655
SGSNs. See Serving GPRS Support 
Nodes
S-GW. See Serving Gateway
SHA-1. See Secure Hash Algorithm
Shamir, Adi, 633
shared medium, 48
delays in, 71
shipping containers, 526–527
shortest path, 406
Short Inter-frame Spacing (SIFS), 566
SIFS. See Short Inter-frame Spacing
signal strength, 553
fading, 556
signal-to-noise ratio (SNR), 554–556
rate adaptation and, 575
signature-based systems, 689, 690
silent periods, 57
simple authentication, 422
Simple Mail Transfer Protocol 
(SMTP), 78, 125, 144, 146–148
HTTP comparison with, 149
mail access protocols and, 150–151
Simple Network Management 
Protocol (SNMP), 445, 452–454
single-hop, infrastructure-based wire-
less networks, 552
single-hop, infrastructure-less wireless 
networks, 552
SIP. See Session Initiation Protocol
SIP addresses, 733–734
SIP proxy, 735
Index         845
SIP registrar, 735
Skype, 703, 708, 725–728
audio and video quality, 725
control packets in, 725
P2P techniques in, 725–727
peer hierarchy, 726
relay peers, 726–727
TCP use by, 725
UDP use by, 123, 725
Slammer worm, 226
SLAs. See Service Level  
Agreements
sliding-window protocol, 250
slotted ALOHA
backoff in, 544
collisions in, 484
efficiency of, 485–486
retransmission in, 484
slow start, 300–301
small office, home office (SOHO), 
subnets, 373
smart phones, 618
smart spaces, 108
SMI. See Structure of Management 
Information
SMTP. See Simple Mail Transfer 
Protocol
SNA, 89
sniffing, 86, 105, 513
SNMP. See Simple Network 
Management Protocol
Snort, 690
SNR. See signal-to-noise ratio
social networks, 93
socket interface, 34, 117
socket programming, 185–186
client-server architecture, 188
IP addresses, 187
port numbers, 187, 223–224
TCP, 192–198
UDP, 187–192
sockets, 221
port numbers, 223–224
simultaneous, 226
TCP, 530, 532
welcoming, 225
soft guarantees, 738
software agents, 108
software-defined networking (SDN), 
334, 339, 464, 465, 618
architecture of, 436
control applications, 438–440
control plane, 343, 435–444
data plane, 436, 442–443
forwarding tables in, 342, 344
generalized forwarding and, 
382–389
key characteristics of, 435–436
link state change in, 442–443
logically centralized control in, 
402–403
packet forwarding and, 340
routing processor responsibilities 
in, 342
SOHO. See small office, home  
office
source port number, 264
source port number field, 222
source quench message, 447–448
spanning trees, 514
spatial redundancy, 705
SPD. See Security Policy Database
spectrum access rights, 551
SPI. See Security Parameter Index
split-connection approaches, 610
Spotify, 704
Sprint, 33
SR. See selective repeat
SRI. See Stanford Research Institute
SSID. See Service Set Identifier
SSL. See Secure Sockets Layer
SSL record, 663
846         Index
SSRC. See synchronization source 
identifier
ssthresh, 301–304
Stanford Research Institute (SRI), 88, 
107
StarBand, 44
stateful filters, 680, 682–684
stateless protocols, 128
state-management layer, SDN, 438
static routing algorithms, 407
status line, 134
stop-and-wait protocols, 239, 247, 248
store-and-forward transmission,  
51–52
stream ciphers, 628, 675
Stream Control Transmission Protocol 
(SCTP), 313
streaming
adaptive HTTP, 709
CDNs and, 180–181
DASH, 176–177, 183, 716
HTTP, 176–177, 709, 713–716
live, 709
live video, 709
Netflix platform, 182–184
P2P, 185
P2P live, 175
P2P video, 709
processing for, 182
RTSP, 711
stored audio and video, 707–708
TCP buffers in, 713–714
UDP, 709, 711
video, 175–176, 180–184
streetlamp wireless hotspots, 551
Structure of Management Information 
(SMI), 450
subnet mask, 364
subnets, 363–367
datagram transmission to, 501–502
mobility on, 574–575
obtaining blocks of IP addresses, 
369
in OSPF, 420
SOHO, 373
successful slot, 485
super peers, 726
SWAN, 403
switch, 503
switched networks, topology of, 514
switches
crossbar, 347–349
forwarding and filtering by,  
509–510
layer 4, 343
layer 5, 343
link-layer, 32, 51, 341, 346,  
509–515
non-blocking, 348
plug-and-play, 512
properties of, 512
routers versus, 513–515
self-learning, 511–512
top of rack, 523
VLANs and, 516
switch filtering, 509–510
switch forwarding, 509–510
switching, 340
in destination-based forwarding, 
346
techniques for, 347–349
switching fabric, 342
bus, 348
crossbar, 347–349
interconnection network, 348–349
memory, 347–348
queuing and speed of, 349–350
switch poisoning, 513
switch table, 509
poisoning, 513
symmetric key cryptography, 626–632
block ciphers, 628–630
Index         847
cipher-block chaining,  
630–632
nonce use with, 653
in PGP, 658
polyalphabetic encryption,  
627–628
secure e-mail using, 655
in SSL handshake, 664
SYNACK segment, 283, 287
SYN bit, 265
synchronization source identifier 
(SSRC), 730
SYN cookies, 288
SYN flood attack, 288
T
Tag Protocol Identifier (TPID),  
517
taking-turns protocols, 481, 492–493, 
565
TCAMs. See Ternary Content 
Addressable Memories
TCP. See Transmission Control 
Protocol
TCP ACK bits, 681–682
TCP congestion-control algorithm, 
299–304
TCP connection, 121
TCP-Friendly Rate Control (TFRC), 
313–314
TCP/IP, 33, 262
TCP Reno, 304, 305
TCP segments, 263
TCP services, 121–123
TCP socket, 530, 532
TCP splitting, 303
TCP states, 285–287
TCP SYN segment, 532, 681
TCP Tahoe, 304
TCP Vegas, 305
TDM. See time-division multiplexing
telco. See telephone company
Telenet, 89
telephone company (telco), 41
telephone networks, 519
Telnet, 148, 267–269, 651,  
684–685
Temporal Key (TK), 678
temporal redundancy, 705
temporary IP addresses, 370
Ternary Content Addressable 
Memories (TCAMs), 346
terrestrial radio channels, 48–49
TFRC. See TCP-Friendly Rate 
Control
3rd Generation Partnership Project 
(3GPP), 583, 585
Third Generation Partnership 
Program, 381
third-party CDNs, 178
3Com, 506
3G, 46, 548, 551
core network, 584
network architecture, 582–585
radio access network, 584–585
video over, 705
3GPP. See 3rd Generation Partnership 
project
three-way handshake, 130, 193, 262, 
284–285, 532
throughput, 71–74, 119–120
average, 72
congestion and, 290–295
instantaneous, 72
per-connection, 290–291
TCP, 306
of transport layer, 119–120
tier-1 ISPs, 60–61
time-based retransmission, 244–245
time-division multiplexing (TDM), 
56–58, 481–485, 582, 584
time frames, 482
848         Index
timeout events
in GBN protocol, 252
in SR protocol, 256
TCP, 270–271, 273, 274
timeout intervals
doubling, 275–277
TCP, 270–271, 275–277
time slots, 482
in LTE, 587–588
timestamps, 719, 730
time-to-live (TTL), 359
TK. See Temporal Key
TLD. See top-level domain
TLS. See Transport Layer Security
token, 493
token-passing protocol, 493
token ring protocol, 493, 503
Tomlinson, Ray, 88
top-down approach, 78
top-level domain (TLD), DNS servers, 
158, 159
Top of Rack switch (TOR switch), 
523
TOR, 686
torrents, 172–174
TOR switch. See Top of Rack  
switch
TOS. See type of service
total nodal delay, 63
TPID. See Tag Protocol Identifier
Traceroute, 70–71, 448–449
trackers, 172–174
traditional packet filters,  
680–682
traffic classes, 742
isolating, 743–744
traffic conditioning, 748
traffic engineering, 421
MPLS and, 522
traffic intensity, 67
traffic isolation, 515–516, 742
traffic load
buffers and, 353
queuing and, 350
traffic policing, 743
leaky bucket, 744–747
traffic profiles, 749–750
Transmission Control Protocol (TCP), 
33, 219. See also Secure 
Sockets Layer
ACK bit, 681–682
ACK generation recommendation, 
278
acknowledgment number, 265–267
buffers in streaming, 713–714
closing connection, 284–285
congestion avoidance, 301–302
congestion-control algorithm, 
299–304
congestion control in, 297–311
congestion window, 298, 304
connection, 261–264
connection management, 283–287, 
289
connection requests, 225
cumulative acknowledgement, 266
demultiplexing, 224–227
development of, 90
establishing connection, 283–284
fairness and, 307–310
fast recovery, 302–304
fast retransmit, 277–279
flow control, 280–282
full-duplex service, 261
high-bandwidth paths and, 306–
307
Internet checksum in, 476
multimedia applications using, 230
parallel browser connections, 
129–130
parallel connection fairness, 310
pipelining, 271
Index         849
point-to-point connections, 261
receive window, 281, 282
reliable data transfer, 272–379
retransmission timeout interval, 
270–271
RTT estimation, 269–271
securing, 122
segment structure, 264–269
selective acknowledgment, 280
self-clocking, 298
sequence number, 265–267
services provided by, 220
simultaneous connection sockets, 
226
Skype use of, 725
slow start, 300–301
SMTP using, 147
socket client, 194–196
socket programming, 186,  
192–198
socket server, 196–198
steady-state behavior of, 306
three-way handshake, 130, 193, 
262, 284–285, 532
throughput, 306
timeout events, 270–271, 273,  
274
timeout intervals, 270–271,  
275–277
timer management, 272–273
transition to, 90–91
variables, 297–298, 301, 304
Web servers and, 227–228
wireless networks and, 609
transmission delay, 63, 64–67
transmission power, 555
transmission rate, 32
BER and, 555
queuing and, 349–350
transparent, 509
transport layer, 79
application services, 118–121
fragment reassembly and, 362–363
in Internet, 219–221
network layer relationship to, 
216–219
reliable data transfer and, 119
security, 120–121
throughput of, 119–120
timing guarantees, 120
transport-layer multiplexing and 
demultiplexing, 220
transport-layer protocols, 216
Transport Layer Security (TLS), 659
transport-layer segment, 82
transport mode, 669
transport services
application availability of, 118–121
Internet, 121–123
network application requirements, 
121
triangle routing problem, 596
triple-DES, 658
3DES, 630, 669
TTL. See time-to-live
tunnel, 380
tunneling, 380
in 4G networks, 586
tunnel mode, 669
twisted-pair copper wire, 47–48
two-dimensional parity, 475
2G cellular networks, 581–582
Tymnet, 89
type numbers, 505
type of service (TOS), 359, 741, 742
U
ubiquitous WiFi, 551, 580
UCLA, 107, 399
UDP. See User Datagram Protocol
UDP segment, 529
UDP services, 123
850         Index
UDP socket programming, 186, 
187–192
client, 189–191
port numbers, 223–224
server, 191–192
UDP streaming, 709, 711
UMTS (Universal Mobile 
Telecommunications Service), 
583, 584
unchoked peers, 174
undetected bit errors, 473
unguided media, 47
unidirectional data transfer, 236
unlicensed spectrum, 551
unreliable services, 220
unshielded twisted pair (UTP),  
47
URG bit, 265
urgent data pointer field, 265
URLs, SIP, 733–734
user agents, 144
User Datagram Protocol (UDP), 219, 
220, 228–234
advantages of, 229–230
checksum, 232–234
connectionless nature of, 229
DNS using, 229
fairness and, 309–310
Internet checksum in, 476
multimedia applications using, 
230–231
multiplexing and demultiplexing, 
223–224
reliability with, 231–232
RTP and, 728–729
segment structure, 232
Skype use of, 123, 725
in VoIP, 717
user state, cookies, 136–138
utilization, 247
UTP. See unshielded twisted pair
V
VANET. See vehicular ad hoc net-
work
VC networks. See virtual-circuit net-
works
vehicular ad hoc network (VANET), 
553
Verisign Global Registry Services, 
159
video
properties of, 704–705
RTP payloads, 731
Skype quality adaptation for, 725
streaming, 175–176, 180–184, 
707–709
video compression, 705
video conferencing, 728
video streaming, 175–176
CDNs and, 180–181
live, 709
Netflix platform, 182–184
P2P, 709
prefetching, 712–713
processing for, 182
repositioning, 715–716
stored video, 707–708
virtual-circuit networks  
(VC networks), 520
virtual local area networks (VLANs), 
515–519
in data center networks, 525
mobility within, 576
virtual private networks (VPNs), 522, 
665, 666–667
mobility within, 576
viruses, 84
visited network, 590, 603
visitor location register (VLR), 603
call routing and, 605
VLANs. See virtual local area  
networks
Index         851
VLAN tags, 517, 518
VLAN trunking, 517, 518
VLR. See visitor location register
Voice-over-IP (VoIP), 71, 548, 708, 
725–728, 764
best-effort IP service limitations 
and, 716–717
end-to-end delay, 718
jitter removal, 719–722
packet jitter, 718–719
packet loss, 717–718
privacy concerns, 727
RTP, 728–731
SIP, 731–736
VoIP. See Voice-over-IP
VPNs. See virtual private networks
vulnerability attacks, 84
W
web-based e-mail, 154
Web browsers, 91–92, 116–117, 127
conditional GET and, 143–144
cookies, 136–138
email access via, 154
GET requests, 132
header lines from, 135–136
parallel connections, 129–130, 310
SSL support, 659
web caches and, 138–141
Web caching, 138–144
web of trust, 659
Web page, 126
web page requests, 528–533
Web servers, 91, 127
TCP and, 227–228
Wechat, 703, 727
weighted fair queuing (WFQ),  
356–357, 744–747
welcoming socket, 225
well-known application protocols,  
223
well-known port numbers, 222
WEP. See Wired Equivalent Privacy
WFQ. See weighted fair queuing
wide-area wireless Internet access,  
46
WiFi, 32, 33, 471, 548, 560
address fields, 571–573
architecture, 561–565
enterprise usage of, 44–45
frames, 570–573
link layer implementation, 471
MAC addresses in, 571–573
MAC protocol, 565–570
mobility on same IP subnet, 
574–575
packet sniffing, 86
payload and CRC fields, 571
power management, 576
public, 92, 551
rate adaptation, 575–576
sequence number, duration, and 
frame control fields, 573
ubiquitous, 551, 580
wide-area wireless versus, 46
WiFi jungle, 563
wildcards, in flow table entries, 385
WiMAX (World Interoperability for 
Microwave Access), 588, 764
window scaling factor, 264
window size, 250
in SR, 258, 259
Wired Equivalent Privacy (WEP), 
674–676
wireless communication links, 549, 
553–556
differences from wired links, 553
dynamic selection of modulation 
techniques, 555–556
interference, 553
modulation techniques, 554–556
multipath propagation, 553
852         Index
wireless communication links.  
(continued)
signal strength, 553
transmission power, 555
transmission rate, 555
wireless hops, 552–553
wireless hosts, 548
wireless Internet devices, 547–548
wireless LANs, 45, 467
authentication, 564–565
broadcast, 479
CDMA in, 556
4G versus, 580
infrastructure, 562
securing, 674–678
wireless mesh networks, 552
wireless networks, 618
ad hoc, 562
elements of, 548–552
handoff in, 552
higher-layer protocols and,  
608–610
infrastructure and, 550–551, 
552–553
mobile ad hoc, 552–553, 590
packet sniffing, 86
types of, 552–553
vehicular ad hoc, 553
wireless personal area networks 
(WPANs), 577–578
Wireless Philadelphia, 551
Wireshark, 86, 105–106, 515
work-conserving queuing, 355, 356
World Wide Web, 111, 126
worms, 84, 226
WPANs. See wireless personal area 
networks
X
X.25 protocol suite, 91, 545
X.509, 648
Xerox Palo Alto Research Center  
(Xerox PARC), 506
XTP, 476
Y
Yahoo, 92
web-based e-mail, 154
Youku, 175, 703
YouTube, 175, 707
CDNs, 184
data centers, 179
Z
zeroconf, 370
Zigbee, 578–579
Zimmerman, Phil, 658
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank



©Silberschatz, Korth and Sudarshan
3.11
Database System Concepts - 7th Edition
Updates to tables

Insert
•
insert into instructor values ('10211', 'Smith', 'Biology', 66000);

Delete
•
Remove all tuples from the student relation
delete from student

Drop Table
•
drop table r

Alter
•
alter table r add A D

where A is the name of the attribute to be added to relation r and
D is the domain of A.
All exiting tuples in the relation are assigned null as the value for
the new attribute.
•
alter table r drop A
where A is the name of an attribute of relation r
Dropping of attributes not supported by many databases.
©Silberschatz, Korth and Sudarshan
3.12
Database System Concepts - 7th Edition
Basic Query Structure

A typical SQL query has the form:
select A1, A2, ..., An
from r1, r2, ..., rm
where P
•
Ai represents an attribute
•
Ri represents a relation
•
P is a predicate.

The result of an SQL query is a relation.
©Silberschatz, Korth and Sudarshan
3.13
Database System Concepts - 7th Edition
The select Clause

The select clause lists the attributes desired in the result of a query
•
corresponds to the projection operation of the relational algebra

Example: find the names of all instructors:
select name
from instructor

NOTE: SQL names are case insensitive (i.e., you may use upper- or
lower-case letters.)
•
E.g., Name ≡NAME ≡name
•
Some people use upper case wherever we use bold font.
©Silberschatz, Korth and Sudarshan
3.14
Database System Concepts - 7th Edition
The select Clause (Cont.)

SQL allows duplicates in relations as well as in query results.

To force the elimination of duplicates, insert the keyword distinct after
select.

Find the department names of all instructors, and remove duplicates
select distinct dept_name
from instructor

The keyword all specifies that duplicates should not be removed.
select all dept_name
from instructor
©Silberschatz, Korth and Sudarshan
3.15
Database System Concepts - 7th Edition
The select Clause (Cont.)

An asterisk in the select clause denotes “all attributes”
select *
from instructor

An attribute can be a literal with no from clause
select '437'
•
Results is a table with one column and a single row with value “437”
•
Can give the column a name using:
select '437' as FOO

An attribute can be a literal with from clause
select 'A'
from instructor
•
Result is a table with one column and N rows (number of tuples in
the instructors table), each row with value “A”
©Silberschatz, Korth and Sudarshan
3.16
Database System Concepts - 7th Edition
The select Clause (Cont.)

The select clause can contain arithmetic expressions involving the
operation, +, –, ∗, and /, and operating on constants or attributes of
tuples.
•
The query:
select ID, name, salary/12
from instructor
would return a relation that is the same as the instructor relation,
except that the value of the attribute salary is divided by 12.
•
Can rename “salary/12” using the as clause:
select ID, name, salary/12 as monthly_salary
©Silberschatz, Korth and Sudarshan
3.17
Database System Concepts - 7th Edition
The where Clause

The where clause specifies conditions that the result must satisfy
•
Corresponds to the selection predicate of the relational algebra.

To find all instructors in Comp. Sci. dept
select name
from instructor
where dept_name = 'Comp. Sci.'

SQL allows the use of the logical connectives and, or, and not

The operands of the logical connectives can be expressions involving the
comparison operators <, <=, >, >=, =, and <>.

Comparisons can be applied to results of arithmetic expressions

To find all instructors in Comp. Sci. dept with salary > 80000
select name
from instructor
where dept_name = 'Comp. Sci.' and salary > 80000
©Silberschatz, Korth and Sudarshan
3.18
Database System Concepts - 7th Edition
The from Clause

The from clause lists the relations involved in the query
•
Corresponds to the Cartesian product operation of the relational
algebra.

Find the Cartesian product instructor X teaches
select ∗
from instructor, teaches
•
generates every possible instructor – teaches pair, with all attributes
from both relations.
•
For common attributes (e.g., ID), the attributes in the resulting table
are renamed using the relation name (e.g., instructor.ID)

Cartesian product not very useful directly, but useful combined with
where-clause condition (selection operation in relational algebra).
©Silberschatz, Korth and Sudarshan
3.19
Database System Concepts - 7th Edition
Examples

Find the names of all instructors who have taught some course and the
course_id
•
select name, course_id
from instructor , teaches
where instructor.ID = teaches.ID

Find the names of all instructors in the Art department who have taught
some course and the course_id
•
select name, course_id
from instructor , teaches
where instructor.ID = teaches.ID and instructor. dept_name = 'Art'
©Silberschatz, Korth and Sudarshan
3.20
Database System Concepts - 7th Edition
The Rename Operation

The SQL allows renaming relations and attributes using the as clause:
old-name as new-name

Find the names of all instructors who have a higher salary than
some instructor in 'Comp. Sci'.
•
select distinct T.name
from instructor as T, instructor as S
where T.salary > S.salary and S.dept_name = 'Comp. Sci.’

Keyword as is optional and may be omitted
instructor as T ≡instructor T
©Silberschatz, Korth and Sudarshan
3.21
Database System Concepts - 7th Edition
Self Join Example

Relation emp-super

Find the supervisor of “Bob”

Find the supervisor of the supervisor of “Bob”

Can you find ALL the supervisors (direct and indirect) of “Bob”?
©Silberschatz, Korth and Sudarshan
3.22
Database System Concepts - 7th Edition
String Operations

SQL includes a string-matching operator for comparisons on character
strings. The operator like uses patterns that are described using two
special characters:
•
percent ( % ). The % character matches any substring.
•
underscore ( _ ). The _ character matches any character.

Find the names of all instructors whose name includes the substring “dar”.
select name
from instructor
where name like '%dar%'

Match the string “100%”
like '100 \%' escape '\'
in that above we use backslash (\) as the escape character.
©Silberschatz, Korth and Sudarshan
3.23
Database System Concepts - 7th Edition
String Operations (Cont.)

Patterns are case sensitive.

Pattern matching examples:
•
'Intro%' matches any string beginning with “Intro”.
•
'%Comp%' matches any string containing “Comp” as a substring.
•
'_ _ _' matches any string of exactly three characters.
•
'_ _ _ %' matches any string of at least three characters.

SQL supports a variety of string operations such as
•
concatenation (using “||”)
•
converting from upper to lower case (and vice versa)
•
finding string length, extracting substrings, etc.
©Silberschatz, Korth and Sudarshan
3.24
Database System Concepts - 7th Edition
Ordering the Display of Tuples

List in alphabetic order the names of all instructors
select distinct name
from
instructor
order by name

We may specify desc for descending order or asc for ascending order,
for each attribute; ascending order is the default.
•
Example: order by name desc

Can sort on multiple attributes
•
Example: order by dept_name, name
©Silberschatz, Korth and Sudarshan
3.25
Database System Concepts - 7th Edition
Where Clause Predicates

SQL includes a between comparison operator

Example: Find the names of all instructors with salary between $90,000
and $100,000 (that is, ≥$90,000 and ≤$100,000)
•
select name
from instructor
where salary between 90000 and 100000

Tuple comparison
•
select name, course_id
from instructor, teaches
where (instructor.ID, dept_name) = (teaches.ID, 'Biology');
©Silberschatz, Korth and Sudarshan
3.26
Database System Concepts - 7th Edition
Set Operations

Find courses that ran in Fall 2017 or in Spring 2018
(select course_id from section where sem = 'Fall' and year = 2017)
union
(select course_id from section where sem = 'Spring' and year = 2018)

Find courses that ran in Fall 2017 and in Spring 2018
(select course_id from section where sem = 'Fall' and year = 2017)
intersect
(select course_id from section where sem = 'Spring' and year = 2018)

Find courses that ran in Fall 2017 but not in Spring 2018
(select course_id from section where sem = 'Fall' and year = 2017)
except
(select course_id from section where sem = 'Spring' and year = 2018)
©Silberschatz, Korth and Sudarshan
3.27
Database System Concepts - 7th Edition
Set Operations (Cont.)

Set operations union, intersect, and except
•
Each of the above operations automatically eliminates duplicates

To retain all duplicates use the
•
union all,
•
intersect all
•
except all.
©Silberschatz, Korth and Sudarshan
3.28
Database System Concepts - 7th Edition
Null Values

It is possible for tuples to have a null value, denoted by null, for some of
their attributes

null signifies an unknown value or that a value does not exist.

The result of any arithmetic expression involving null is null
•
Example: 5 + null returns null

The predicate is null can be used to check for null values.
•
Example: Find all instructors whose salary is null.
select name
from instructor
where salary is null

The predicate is not null succeeds if the value on which it is applied is
not null.
©Silberschatz, Korth and Sudarshan
3.29
Database System Concepts - 7th Edition
Null Values (Cont.)

SQL treats as unknown the result of any comparison involving a null
value (other than predicates is null and is not null).
•
Example: 5 < null
or
null <> null
or
null = null

The predicate in a where clause can involve Boolean operations (and,
or, not); thus the definitions of the Boolean operations need to be
extended to deal with the value unknown.
•
and : (true and unknown) = unknown,
(false and unknown) = false,
(unknown and unknown) = unknown
•
or:
(unknown or true)
= true,
(unknown or false) = unknown
(unknown or unknown) = unknown

Result of where clause predicate is treated as false if it evaluates to
unknown
©Silberschatz, Korth and Sudarshan
3.30
Database System Concepts - 7th Edition
Aggregate Functions

These functions operate on the multiset of values of a column of a
relation, and return a value
avg: average value
min: minimum value
max: maximum value
sum: sum of values
count: number of values
©Silberschatz, Korth and Sudarshan
3.31
Database System Concepts - 7th Edition
Aggregate Functions Examples

Find the average salary of instructors in the Computer Science department
•
select avg (salary)
from instructor
where dept_name= 'Comp. Sci.';

Find the total number of instructors who teach a course in the Spring 2018
semester
•
select count (distinct ID)
from teaches
where semester = 'Spring' and year = 2018;

Find the number of tuples in the course relation
•
select count (*)
from course;
©Silberschatz, Korth and Sudarshan
3.32
Database System Concepts - 7th Edition
Aggregate Functions – Group By

Find the average salary of instructors in each department
•
select dept_name, avg (salary) as avg_salary
from instructor
group by dept_name;
©Silberschatz, Korth and Sudarshan
3.33
Database System Concepts - 7th Edition
Aggregation (Cont.)

Attributes in select clause outside of aggregate functions must appear in
group by list
•
/* erroneous query */
select dept_name, ID, avg (salary)
from instructor
group by dept_name;
©Silberschatz, Korth and Sudarshan
3.34
Database System Concepts - 7th Edition
Aggregate Functions – Having Clause

Find the names and average salaries of all departments whose average
salary is greater than 42000

Note: predicates in the having clause are applied after the formation of
groups whereas predicates in the where clause are applied before forming
groups
select dept_name, avg (salary) as avg_salary
from instructor
group by dept_name
having avg (salary) > 42000;
©Silberschatz, Korth and Sudarshan
3.35
Database System Concepts - 7th Edition
Nested Subqueries

SQL provides a mechanism for the nesting of subqueries. A subquery is
a select-from-where expression that is nested within another query.

The nesting can be done in the following SQL query
select A1, A2, ..., An
from r1, r2, ..., rm
where P
as follows:
•
From clause: ri can be replaced by any valid subquery
•
Where clause: P can be replaced with an expression of the form:
B <operation> (subquery)
B is an attribute and <operation> to be defined later.
•
Select clause:
Ai can be replaced be a subquery that generates a single value.
©Silberschatz, Korth and Sudarshan
3.36
Database System Concepts - 7th Edition
Set Membership
©Silberschatz, Korth and Sudarshan
3.37
Database System Concepts - 7th Edition
Set Membership

Find courses offered in Fall 2017 and in Spring 2018

Find courses offered in Fall 2017 but not in Spring 2018
select distinct course_id
from section
where semester = 'Fall' and year= 2017 and
course_id in (select course_id
from section
where semester = 'Spring' and year= 2018);
select distinct course_id
from section
where semester = 'Fall' and year= 2017 and
course_id not in (select course_id
from section
where semester = 'Spring' and year= 2018);
©Silberschatz, Korth and Sudarshan
3.38
Database System Concepts - 7th Edition
Set Membership (Cont.)

Name all instructors whose name is neither “Mozart” nor Einstein”
select distinct name
from instructor
where name not in ('Mozart', 'Einstein')

Find the total number of (distinct) students who have taken course
sections taught by the instructor with ID 10101

Note: Above query can be written in a much simpler manner.
The formulation above is simply to illustrate SQL features
select count (distinct ID)
from takes
where (course_id, sec_id, semester, year) in
(select course_id, sec_id, semester, year
from teaches
where teaches.ID= 10101);
©Silberschatz, Korth and Sudarshan
3.39
Database System Concepts - 7th Edition
Set Comparison
©Silberschatz, Korth and Sudarshan
3.40
Database System Concepts - 7th Edition
Set Comparison – “some” Clause

Find names of instructors with salary greater than that of some (at least
one) instructor in the Biology department.

Same query using > some clause
select name
from instructor
where salary > some (select salary
from instructor
where dept name = 'Biology');
select distinct T.name
from instructor as T, instructor as S
where T.salary > S.salary and S.dept name = 'Biology';
©Silberschatz, Korth and Sudarshan
3.41
Database System Concepts - 7th Edition
Definition of “some” Clause

F <comp> some r ⇔∃t ∈r such that (F <comp> t )
Where <comp> can be: <, ≤, >, =, ≠
0
5
6
(5 < some
) = true
0
5
0
) = false
5
0
5
(5 ≠some
) = true (since 0 ≠5)
(read: 5 < some tuple in the relation)
(5 < some
) = true
(5 = some
(= some) ≡in
However, (≠some) ≡not in
©Silberschatz, Korth and Sudarshan
3.42
Database System Concepts - 7th Edition
Set Comparison – “all” Clause

Find the names of all instructors whose salary is greater than the salary of
all instructors in the Biology department.
select name
from instructor
where salary > all (select salary
from instructor
where dept name = 'Biology');
©Silberschatz, Korth and Sudarshan
3.43
Database System Concepts - 7th Edition
Definition of “all” Clause

F <comp> all r ⇔∀t ∈r (F <comp> t)
0
5
6
(5 < all
) = false
6
10
4
) = true
5
4
6
(5 ≠all
) = true (since 5 ≠4 and 5 ≠6)
(5 < all
) = false
(5 = all
(≠all) ≡not in
However, (= all) ≡in
©Silberschatz, Korth and Sudarshan
3.44
Database System Concepts - 7th Edition
Test for Empty Relations

The exists construct returns the value true if the argument subquery is
nonempty.

exists r ⇔r ≠Ø

not exists r ⇔r = Ø
©Silberschatz, Korth and Sudarshan
3.45
Database System Concepts - 7th Edition
Use of “exists” Clause

Yet another way of specifying the query “Find all courses taught in both the
Fall 2017 semester and in the Spring 2018 semester”
select course_id
from section as S
where semester = 'Fall' and year = 2017 and
exists (select *
from section as T
where semester = 'Spring' and year= 2018
and S.course_id = T.course_id);

Correlation name – variable S in the outer query

Correlated subquery – the inner query
©Silberschatz, Korth and Sudarshan
3.46
Database System Concepts - 7th Edition
Use of “not exists” Clause

Find all students who have taken all courses offered in the Biology
department.

Note that X – Y = Ø
⇔
X ⊆Y

Note: Cannot write this query using = all and its variants
select distinct S.ID, S.name
from student as S
where not exists ( (select course_id
from course
where dept_name = 'Biology')
except
(select T.course_id
from takes as T
where S.ID = T.ID));
•
First nested query lists all courses offered in Biology
•
Second nested query lists all courses a particular student took
©Silberschatz, Korth and Sudarshan
3.47
Database System Concepts - 7th Edition
Test for Absence of Duplicate Tuples

The unique construct tests whether a subquery has any duplicate tuples
in its result.

The unique construct evaluates to “true” if a given subquery contains no
duplicates .

Find all courses that were offered at most once in 2017
select T.course_id
from course as T
where unique ( select R.course_id
from section as R
where T.course_id= R.course_id
and R.year = 2017);
©Silberschatz, Korth and Sudarshan
3.48
Database System Concepts - 7th Edition
Subqueries in the From Clause
©Silberschatz, Korth and Sudarshan
3.49
Database System Concepts - 7th Edition
Subqueries in the Form Clause

SQL allows a subquery expression to be used in the from clause

Find the average instructors’ salaries of those departments where the
average salary is greater than $42,000.”
select dept_name, avg_salary
from ( select dept_name, avg (salary) as avg_salary
from instructor
group by dept_name)
where avg_salary > 42000;

Note that we do not need to use the having clause

Another way to write above query
select dept_name, avg_salary
from ( select dept_name, avg (salary)
from instructor
group by dept_name)
as dept_avg (dept_name, avg_salary)
where avg_salary > 42000;
©Silberschatz, Korth and Sudarshan
3.50
Database System Concepts - 7th Edition
With Clause

The with clause provides a way of defining a temporary relation whose
definition is available only to the query in which the with clause occurs.

Find all departments with the maximum budget
with max_budget (value) as
(select max(budget)
from department)
select department.name
from department, max_budget
where department.budget = max_budget.value;
©Silberschatz, Korth and Sudarshan
3.51
Database System Concepts - 7th Edition
Complex Queries using With Clause

Find all departments where the total salary is greater than the average of
the total salary at all departments
with dept _total (dept_name, value) as
(select dept_name, sum(salary)
from instructor
group by dept_name),
dept_total_avg(value) as
(select avg(value)
from dept_total)
select dept_name
from dept_total, dept_total_avg
where dept_total.value > dept_total_avg.value;
©Silberschatz, Korth and Sudarshan
3.52
Database System Concepts - 7th Edition
Scalar Subquery

Scalar subquery is one which is used where a single value is expected

List all departments along with the number of instructors in each
department
select dept_name,
( select count(*)
from instructor
where department.dept_name = instructor.dept_name)
as num_instructors
from department;

Runtime error if subquery returns more than one result tuple

